<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Instance Mining for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Lin</surname></persName>
							<email>linchenhao@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwen</forename><surname>Wang</surname></persName>
							<email>wangsiwendut@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqi</forename><surname>Xu</surname></persName>
							<email>isdongqixu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lu</surname></persName>
							<email>luyu@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object Instance Mining for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection (WSOD) using only image-level annotations has attracted growing attention over the past few years. Existing approaches using multiple instance learning easily fall into local optima, because such mechanism tends to learn from the most discriminative object in an image for each category. Therefore, these methods suffer from missing object instances which degrade the performance of WSOD. To address this problem, this paper introduces an end-to-end object instance mining (OIM) framework for weakly supervised object detection. OIM attempts to detect all possible object instances existing in each image by introducing information propagation on the spatial and appearance graphs, without any additional annotations. During the iterative learning process, the less discriminative object instances from the same class can be gradually detected and utilized for training. In addition, we design an object instance reweighted loss to learn larger portion of each object instance to further improve the performance. The experimental results on two publicly available databases, VOC 2007 and 2012, demonstrate the efficacy of proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Object detection has always been one of the most essential technologies in computer vision field. Deep learning techniques introduced in recent years have significantly boosted state-of-the-art approaches for object detection <ref type="bibr" target="#b8">(Girshick 2015;</ref><ref type="bibr" target="#b11">Liu et al. 2016;</ref><ref type="bibr" target="#b14">Redmon et al. 2016;</ref><ref type="bibr" target="#b15">Ren et al. 2015)</ref>. However, these approaches usually require large-scale manually annotated datasets <ref type="bibr" target="#b16">(Russakovsky et al. 2015)</ref>. The high-cost of time-consuming accurate bounding box annotations, has impeded the wide deployment of CNN-based object detection technologies in real applications.</p><p>To address this limitation, weakly supervised object detection (WSOD) technology, which requires only imagelevel labels for training, has been introduced and explored <ref type="bibr" target="#b1">(Bilen and Vedaldi 2016;</ref><ref type="bibr" target="#b2">Diba et al. 2017;</ref><ref type="bibr" target="#b9">Jie et al. 2017;</ref><ref type="bibr" target="#b12">Oquab et al. 2015;</ref><ref type="bibr" target="#b27">Zhang et al. 2018b;</ref><ref type="bibr" target="#b20">Tang et al. 2017;</ref><ref type="bibr" target="#b26">Zhang et al. 2018a;</ref><ref type="bibr" target="#b17">Shen et al. 2018</ref>; Arun, Jawahar, and * Work performed as an intern in SenseTime Research https://github.com/bigvideoresearch/OIM † Corresponding author <ref type="figure">Figure 1</ref>: The original images and corresponding objectness maps to show the evolution of object instance mining during learning process (from left to right). The first to fourth columns represent random initialization, epoch1, epoch3, and final epoch, respectively. Blue or red bounding boxes indicate the detected instances (top-scoring proposals after NMS) with detection scores &lt; 0.5 or ≥ 0.5.</p><p>Kumar 2019; <ref type="bibr" target="#b13">Pan et al. 2019)</ref>. Although many approaches have been developed for WSOD and achieved promising results, the lack of object instance level annotations in images leads to huge performance gap between WSOD and fully supervised object detection (FSOD) methods. Most previous approaches follow the framework of combining multiple instance learning (MIL) with CNN. This framework usually mines the most confident class-specific object proposals for learning CNN-based classifier, regardless of the number of object instances appearing in an image. For the images with multiple object instances from the same class, the object instances (fully annotated with bounding boxes in FSOD) with lower class-specific scores will be probably regarded as background regions. Many images in the challenging VOC datasets contain more than one object instance from the same class. For example, in VOC2007 trainval set the number of image-level object labels and the annotated object instances are 7,913 and 15,662 respectively, which indicates that at least 7,749 instances are NOT selected during training. In this case, the selected object instances with relatively limited scale and appearance variations, may not be sufficient for training a CNN classifier with strong discriminative power. Moreover, the missing instances may be selected as negative samples during training, which may further degrades the discriminative capability of the CNN classifier.</p><p>In this paper, an end-to-end object instance mining (OIM) framework is proposed to address the problem of multiple object instances in each image for WSOD. OIM is based on two fundamental assumptions: 1) the highest confidence proposal and its surrounding highly overlapped proposals should probably belong to the same class; 2) the objects from the same class should have high appearance similarity. Formally, spatial and appearance graphs are constructed and utilized to mine all possible object instances present in an image and employ them for training. The spatial graph is designed to model the spatial relationship between the highest confidence proposal and its surrounding proposals, while the appearance graph aims at capturing all possible object instances having high appearance similarities with the most confident proposal. By integrating these two graphs into the iterative training process, an OIM approach that attempts to accurately mine all possible object instances in each image with only image-level supervision is proposed. With more object instances for training, a CNN classifier can have stronger discriminative power and generalization capabilities. The proposed OIM can further prevent the learning process from falling into local optima because more objects per-class with high similarity are employed for training. The original images and the corresponding objectness maps shown in <ref type="figure">Figure 1</ref> illustrate that with the increasing number of iterations, multiple object instances belonging to the same class can be detected and are employed for training using the proposed approach.</p><p>Another observation from existing approaches is that the most confident region proposal is easy to concentrate on the locally distinct part of an object, especially for non-rigid objects such as human and animals. This may lead to the problem of detecting only small part of the object. To alleviate this problem, an object instance reweighted loss using the spatial graph is presented to help the network detect more accurate bounding box. This loss tends to make the network pay less attention on the local distinct parts and focus on learning the larger portion of each object.</p><p>Our key contributions can be summarized as follows:</p><p>• An object instance mining approach using spatial and appearance graphs is developed to mine all possible object instances with only image-level annotation, and it can significantly improve the discriminative capability of the trained CNN classifier.</p><p>• An object instance reweighted loss by adjusting the weight of loss function of different instances is proposed to learn more accurate CNN classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>With only image-level annotations, most existing approaches implement weakly supervised object detection <ref type="bibr" target="#b1">(Bilen and Vedaldi 2016;</ref><ref type="bibr" target="#b20">Tang et al. 2017;</ref><ref type="bibr" target="#b9">Jie et al. 2017;</ref><ref type="bibr" target="#b24">Wan et al. 2019</ref>) through multiple instance learning (MIL) framework <ref type="bibr" target="#b3">(Dietterich, Lathrop, and Lozano-Pérez 1997)</ref>. The training images are firstly divided into bag of proposals (instances) containing positive target objects and negative backgrounds and CNN classifier is trained to classify the proposals into different categories. The most discriminative representation of instances is easy to be distinguished by such classifier that may make network trap into local optima.</p><p>Recently, Bilen et al. <ref type="bibr" target="#b1">(Bilen and Vedaldi 2016)</ref> proposed a weakly supervised deep detection network (WSDDN) to perform object localization and classification simultaneously. Following this work, <ref type="bibr" target="#b20">Tang et al. (Tang et al. 2017)</ref> introduced an online instance classifier refinement (OICR) strategy to learn larger portion of the objects. Such approach improves the performance of WSOD. However, it is also easy to trap into local optima since only the most discriminative instance is selected for refinement. Wan et al. <ref type="bibr" target="#b23">(Wan et al. 2018</ref>) developed a min-entropy latent model to classify and locate the objects by minimizing the global and local entropies, which was proved to effectively boost the detection performance. <ref type="bibr" target="#b24">Wan et al. (Wan et al. 2019</ref>) also attempted to address the local minima problem in MIL using continuation optimization method. In references <ref type="bibr" target="#b18">Shen et al. 2019;</ref><ref type="bibr" target="#b10">Li et al. 2019)</ref>, the authors attempted to integrate segmentation task into weakly supervised object detection to obtain more accurate object bounding boxes. However, these methods require complex training framework with high training and test time complexity.</p><p>The authors in <ref type="bibr" target="#b21">(Tang et al. 2018)</ref> proposed to use proposal cluster to divide all proposals into different small bags and then classifier refinement was applied. This approach attempted to classify and refine all possible objects in each image. However, many proposals containing part of the object might be ignored using proposal cluster during the training. <ref type="bibr">Gao et al. (Gao et al. 2018)</ref> introduced a count-guided weakly supervised localization approach to detect per-class objects in each image. A simple count-based region selection algorithm was proposed and integrated into OICR to improve the performance of WSOD. However, the extra count annotations which needs a certain human labor are introduced and their method requires an alternative training process which can be time-consuming. In this paper, the count annotation is replaced by the proposed OIM algorithm without extra labor cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach Overall Framework</head><p>The overall architecture of the proposed framework illustrated in <ref type="figure">Figure 2</ref> mainly consists of two parts. The first part is a multiple instance detector (MID) which is similar to the structure presented in <ref type="bibr" target="#b1">(Bilen and Vedaldi 2016)</ref>. It performs region selection and classification simultaneously using a weighted MIL pooling. The second part is the proposed object instance mining and the proposed instance reweighted  <ref type="figure">Figure 2</ref>: Architecture of the proposed object instance mining framework. MID represents multiple instance detector and OIM indicates proposed object instance mining. L CE is multi-class cross entropy loss and L OIR is proposed instance reweighted loss.</p><p>loss. During the training phase, we firstly adopt MID to classify the region proposals into different predicted classes. Then the detection outputs and proposal features are integrated to search all possible object instances from the same class in each image using spatial and appearance graphs. In addition, the instance reweighted loss is designed to learn larger portion of each object. As can be seen from the <ref type="figure">Figure</ref> 2, the multiple object instances belonging to the same class can be accurately detected using the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Instance Mining</head><p>Previous methods <ref type="bibr" target="#b20">(Tang et al. 2017;</ref><ref type="bibr">Gao et al. 2018;</ref><ref type="bibr" target="#b27">Zhang et al. 2018b;</ref><ref type="bibr" target="#b25">Wei et al. 2018)</ref> often select the most confident proposal from each class as the positive sample to refine the multiple instance detector. The performance improvement can be limited using these methods, since only the top-scoring and surrounding proposals are selected for refinement. While in many conditions, there are multiple object instances belonging to the same class in an image. Those ignored object instances may be regarded as negative samples during the training that may degrade the performance of WSOD. Therefore, we propose an object instance mining (OIM) approach by building spatial graphs and appearance graphs to search all possible object instances in each image and integrate them into the training process.</p><p>Based on the assumption that the top-scoring and surrounding proposals with large overlaps (spatial similarity) should have the same predicted class, the spatial graphs can be built. We also assume that the objects from the same class should have similar appearance. Based on the similarities between the top-scoring proposal and the other proposals, the appearance graphs are built. Then we search all possible object instances in each image and employ them for training through these graphs.</p><p>Given an input image I with class label c, a set of region proposals P = {p 1 , ..., p N } and their corresponding confidence scores X = {x 1 , ..., x N }, the core instance (proposal) p ic with the highest confidence score x ic can be selected.</p><p>Here i c donates the index of this core instance (proposal). The core spatial graph can be defined by G s ic = (V s ic , E s ic ), where each node in V s ic represents a selected proposal which has the overlap, i.e. spatial similarity, with the core instance larger than a threshold T . Each edge in E s ic represents such spatial similarity. All the nodes in spatial graph G s ic will be selected and labelled to the same class as p ic .</p><p>We define feature vectors of each proposal as F = {f 1 , ..., f N } and it can be generated from the fully connected layer. Each vector encodes a feature representation of a region proposal. Then the appearance graph is defined as G a = (V a , E a ), where each node in V a is a selected proposal which has high appearance similarity with the core instance and each edge in E a represents the appearance similarity. This similarity can be calculated from the feature vectors of core instance and one of the other proposals (e.g. p j ) using the Euclidean distance, denoted as follows,</p><formula xml:id="formula_0">D ic,j = f ic − f j 2 .</formula><p>(1)</p><p>Only if the proposal p j meets the condition that D ic,j &lt; αD avg and p j has no overlap with all the proposals previously selected, such proposal will be added into the nodes in G a . D avg represents the average inter-class similarity of the core spatial graph G s ic using average distance of all the nodes in G s ic and it can be defined as follows,</p><formula xml:id="formula_1">D avg = 1 M k D ic,k , s.t. IoU (p ic , p k ) &gt; T.<label>(2)</label></formula><p>where p k represents the node meet the constraints above and M indicates the number of these nodes in G s ic . α is a hyper parameter which is determined by experiments.</p><p>The proposed object instance mining (OIM) approach using spatial and appearance graphs is summarized in Algorithm 1. We also build spatial graph G s for each node in appearance graph G a and then all these nodes will be included for training. If no proposal has high similarity with the core instance, only the core instance and surrounding proposals, i.e. spatial graph G s ic will be employed. In such a way, more instances from the same class with similar appearance and different poses will be employed for training. It results in that not only more object instances can be detected but also more accurate detected boxes can be learned.  <ref type="figure">Figure 3</ref> illustrates the process to detect all possible object instances from to the same class using spatial and appearance graphs. <ref type="figure">Figure 3 (a)</ref> is the core spatial graph and figure 3 (b)-(c) describe the spatial and appearance graphs in different epochs. With the increased number of iterations, more instances can be detected using the proposed OIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Reweighted Loss</head><p>In addition to exploring all possible object instances in each image, we also design an object instance reweighted loss to learn more accurate detected boxes. During the iterative learning process, the CNN-based classifier is easy to learn the most distinct part of each object instance instead of the whole body, especially for the non-rigid one. We propose to assign different proposal weights to individual proposals to balance the weight of the top-scoring proposal and surrounding less discriminative ones. Thus the larger portion of each instance is expected to be detected.</p><p>Given an image with label Y and predicted label Y j = [y 0,j , y 1,j , ..., y C,j ] T ∈ R (C+1)×1 for the j-th proposal in a spatial graph G s , where y c,j = 1 or 0 indicates the proposal belonging to class c or not, and c = 0 is index of background class. The loss in Eq. 3 is similar to the loss in <ref type="bibr" target="#b20">(Tang et al. 2017)</ref>, where w j is the loss weight of j-th proposal. x s c,j with class label c in G s , are the proposals used for training and x s c,ic is center (core) proposal with the highest score.</p><formula xml:id="formula_2">L = − 1 |P| |P| j=1 C+1 c=1 w j y c,j log x s c,j .<label>(3)</label></formula><p>It can be seen from Eq. 3 that proposals in each spatial graph contribute equally. Thus, the non-center proposals with relative low scores in each spatial graph are difficult to be learned during training. To address this problem, an instance reweighted loss function is designed as follows,</p><formula xml:id="formula_3">L = − 1 |P| |P| j=1 C+1 c=1 w j y c,j (1 + z s j ) log x s c,j ,<label>(4)</label></formula><p>where z s j is introduced to balance the proposal weights in spatial graph G s as defined in Eq. 5. β is hyper-parameter.</p><formula xml:id="formula_4">z s j = β, j = i c β − 1, j = i c<label>(5)</label></formula><p>To guide the network to pay more attention on learning the less discriminative regions of the object instance in each graph G s , we balance the weight of the surrounding less discriminative proposals with the center proposal using Eq. 4 and Eq. 5. As a result, gradients of surrounding proposals are scaled up to (1 + β) of its original value, while gradient of the center proposal is scaled to β of its original value during back-propagation. Similar to the implementation in <ref type="bibr">(Gao et al. 2018)</ref>, we also use the standard multi-class cross entropy loss for the multi-label classification and it is combined with the proposed instance reweighted loss for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Evaluation Metrics</head><p>Following the previous state-of-the-art methods on WSOD, we also evaluate our approach two datasets, PASCAL VOC2007 <ref type="bibr" target="#b4">(Everingham et al. 2010</ref>) and VOC2012 <ref type="bibr" target="#b5">(Everingham et al. 2015)</ref>, which both contain 20 object categories. For VOC2007, we train the model on the trainval set (5,011 images) and evaluate the performance on the test set (4,952 images). For VOC2012, the trainval set (11,540 images) and the test set (10,991 images) are used for training and evaluation respectively. Additionally, we train our model on the VOC2012 train set (5,717 images) and proceed evaluation on the val set (5,823 images) to further validate the effectiveness of proposed approach. Following previous work, we use mean average precision (mAP) to evaluate the performance of proposed approach. Correct localization (CorLoc) is applied to evaluate the localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>To make a fair comparison, VGG16 model pre-trained on the ImageNet dataset <ref type="bibr" target="#b16">(Russakovsky et al. 2015</ref>) is adopted as the backbone network to finetune the CNN classifier. The object proposals are generated using Selective Search <ref type="bibr" target="#b22">(Uijlings et al. 2013</ref>). The batch size is set to 2, and the learning rates are set to 0.001 and 0.0001 for the first 40K and the following 50K iterations respectively. During training and test, we take five image scales {480, 576, 688, 864, 1200} along with random horizontal flipping for data augmentation. Following <ref type="bibr" target="#b20">(Tang et al. 2017)</ref>, the threshold T is set to 0.5. With the increased number of iterations, the network has more stable learning ability, we dynamically set the hyper parameters α as α 1 = 5 for the first 70K and α 2 = 2 for the following 20K iterations. β are empirically set to 0.2 in our experiments. We also analyze the influence of these parameters in the ablation experiments section. 100 top-scoring region proposals are kept and Non-Maximun Suppression with IoU of 0.3 per class is performed to calculate mAP and CorLoc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-arts</head><p>State-of-the-art WSOD methods are used for comparison to validate the effectiveness of the proposed approach. Table 1 shows performance comparison in terms of mAP on VOC2007 test set. By only using OIM, better or similar results can be achieved compared with previous SOTA methods such as MELM, SDCN, etc. We attribute this improvement to the OIM, which increases the representation capability of the trained CNN by searching more objects from the same class and employing them into training. As the detected bounding boxes and objectness maps shown in <ref type="figure">Figure  1</ref>, the confidence scores of less discriminative objects are gradually improved and more objects from the same class can be detected during the training. It further proves that integrating the less discriminative objects into training improves the performance for WSOD. Further performance improvement can be achieved using the proposed instance reweighted loss. The proposed approach achieves a mAP of 50.1%, which outperforms the PCL,C-WSL * , SDCN, WS-JDS methods, etc, and the performance is similar to the result of C-MIL. We further used the learned objects as pseudo ground-truth to train a Fast-RCNN-based detector, our approach also achieve better or similar performance as compared with previous state-of-the-art methods. In particular, by only using the proposed OIM strategy, our approach outperforms C-WSL method by 1.4 % without introducing extra per-class count supervision. Our work attempts to include all possible object instances from each class for training since many images contain more than one per-class object instance. <ref type="figure" target="#fig_2">Figure 5</ref> illustrates that most classes in two datasets have more than one object instance in an image. Specifically, almost half of categories contain     <ref type="bibr" target="#b20">(Tang et al. 2017)</ref> 60.6 PCL <ref type="bibr" target="#b21">(Tang et al. 2018)</ref> 62.7 C-WSL* <ref type="bibr">(Gao et al. 2018)</ref> 63.5 MELM <ref type="bibr" target="#b23">(Wan et al. 2018)</ref> 61.4 WS-JDS <ref type="bibr" target="#b18">(Shen et al. 2019)</ref> 64.5 C-MIL <ref type="bibr" target="#b24">(Wan et al. 2019)</ref> 65.0 OICR+W-RPN <ref type="bibr" target="#b19">(Singh and Lee 2019)</ref> 66.5 SDCN <ref type="bibr" target="#b10">(Li et al. 2019)</ref> 66.8 OIM+IR 67.2 C-WSL*+FRCNN <ref type="bibr">(Gao et al. 2018)</ref> 66.1 WS-JDS+FRCNN <ref type="bibr" target="#b18">(Shen et al. 2019)</ref> 68.6 SDCN+FRCNN <ref type="bibr" target="#b10">(Li et al. 2019)</ref> 68.8 Pred Net (FRCNN) <ref type="bibr" target="#b0">(Arun et al. 2019)</ref> 70.9 OIM+IR+FRCNN 68.8 more than two object instances in an image. Especially for class "sheep", which the average number of sheep appearing in an image is larger than 3, our OIM method (57.9 % mAP) performs better than all the other methods. In addition, for most non-rigid objects ("cat", "dog", "horse", "person', etc.), as can be seen from <ref type="table" target="#tab_1">Table 1</ref>, by applying instance reweighted loss more accurate object instance can be detected.</p><formula xml:id="formula_5">OICR+W-RPN - - - - - - - - - - - - - - - - - - - -</formula><p>CorLoc is also used as the evaluation metric to ascertain  <ref type="bibr" target="#b23">(Wan et al. 2018)</ref> train/val 40.2 C-WSL <ref type="bibr">(Gao et al. 2018)</ref> train/val 43.0 OIM+IR train/val 44.4</p><p>OICR <ref type="bibr" target="#b20">(Tang et al. 2017)</ref> trainval/test 37.9 PCL <ref type="bibr" target="#b21">(Tang et al. 2018)</ref> trainval/test 40.6 MELM <ref type="bibr" target="#b23">(Wan et al. 2018)</ref> trainval/test 42.4 WS-JDS <ref type="bibr" target="#b18">(Shen et al. 2019)</ref> trainval   <ref type="bibr" target="#b20">(Tang et al. 2017)</ref> 62.1 PCL <ref type="bibr" target="#b21">(Tang et al. 2018)</ref> 63.2 WS-JDS <ref type="bibr" target="#b18">(Shen et al. 2019)</ref> 63.5 OICR+W-RPN <ref type="bibr" target="#b19">(Singh and Lee 2019)</ref> 67.5 SDCN <ref type="bibr" target="#b10">(Li et al. 2019)</ref> 67.9 OIM+IR 67.1</p><p>Pred Net (FRCNN) <ref type="bibr" target="#b0">(Arun et al. 2019)</ref> 69.5 WS-JDS + FRCNN <ref type="bibr" target="#b18">(Shen et al. 2019)</ref> 69.5 C-MIL + FRCNN <ref type="bibr" target="#b24">(Wan et al. 2019)</ref> 67.4 SDCN + FRCNN <ref type="bibr" target="#b10">(Li et al. 2019)</ref> 69.5 OIM+IR + FRCNN 69.5 the performance of proposed method. <ref type="table" target="#tab_3">Table 2</ref> shows performance comparison in terms of CorLoc on the VOC2007 trainval set. Our result outperforms all existing state-of-theart methods when Fast-RCNN detector is not used. The proposed OIM framework iteratively explores more object in-  stances and larger portion of the instances from the same class with similar appearance and different poses for training, which makes more accurate detected boxes can be learned. Therefore, the proposed approach not only brings the mAP improvements but also makes the detected boxes more accurate which results in better CorLoc. The proposed approach is also evaluated on VOC2012 dataset. Since some approaches <ref type="bibr">(Gao et al. 2018)</ref> only use validation set of VOC2012 for evaluation, we use both test and val set to evaluate the proposed approach. In <ref type="table" target="#tab_4">Table 3</ref>, the detection results in terms of mAP on test and val set are provided respectively. <ref type="table" target="#tab_6">Table 4</ref> lists the CorLoc results on VOC2012 trainval set. The experimental results in <ref type="table" target="#tab_4">Tables 3  and 4</ref> validate the effectiveness of the proposed approach. <ref type="figure" target="#fig_1">Figure 4</ref> visualizes the detection results on the VOC2007 test set. The successful (IoU ≥ 0.5) and failed (IoU &lt; 0.5) detections are marked with red and yellow bounding boxes respectively. The green bounding boxes are the groundtruths. The first two rows indicate our approach can detect tight boxes even multiple objects from the same class cooccur in an image, e.g. "cow", "sheep". The last row shows some failed cases, which are often attribute to localizing the most discriminative parts of non-rigid objects, grouping multiple objects, and background clutter, e.g. "human".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Experiments</head><p>We performed ablation experiments to illustrate the effect of parameters introduced in proposed object instance mining (α) and instance reweighted loss (β). <ref type="table" target="#tab_7">Table 5</ref> indicates when parameter α (α 1 used in the first 70K and α 2 used in the following 20K iterations) becomes smaller or larger, the performance of proposed approach will degrade. If the parameter α is too small, very less instances will be selected in the appearance graph for training. It results in that in many images, only the most discriminative object is selected and used for training. If the parameter α is too large, many false instances (background proposals) will be employed for training and it  <ref type="figure">Figure 6</ref>: Evolution of object detection during learning process w/o using instance reweighted loss (from left to right). The upper part of each subfigure is the result of OICR <ref type="bibr" target="#b20">(Tang et al. 2017</ref>) and the lower part is the result of our method. also leads to performance drop. For the proposed instance reweighted loss, as also can be seen from the <ref type="table" target="#tab_7">Table 5</ref>, with the increasing of β the performance decreases.</p><p>We also studied the WSOD performance by only using appearance graph (AG) or spatial graph (SG) to evaluate their effectiveness separately. The first two columns in Table 6 illustrate the experimental results in terms of mAP on the VOC2007 test set. We can see that the performance can be significantly improved for WSOD by only using appearance or spatial graph.</p><p>The effectiveness of the proposed instance reweighted loss is also evaluated. We apply the network structure in OICR but just replace the loss with instance reweighted loss. The performance achieved using the proposed instance reweighted loss in terms of mAP on the VOC2007 test set is shown in <ref type="table" target="#tab_8">Table 6</ref>. It can be seen that the mAP can be improved from 41.2% <ref type="bibr" target="#b20">(Tang et al. 2017</ref>) to 43.4% by only using instance reweighted loss. The visual comparison shown in <ref type="figure">Figure 6</ref> also illustrates that larger portion of the object can be gradually detected using the proposed loss. By incorporating the OIM with instance reweighted loss, the best performance (mAP 50.1%) can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, an end-to-end object instance mining framework has been presented to address the limitations of existing approaches for WSOD. Object instance mining algorithm is performed using spatial and appearance graphs to make the network learn less discriminative object instances. Thus more possible objects belonging to the same class can be detected accordingly. Without introducing any extra count information, the proposed approach has achieved improved performance comparable to many state-of-the-art results. The object instance reweighted loss is designed to further help the OIM by learning the larger portion of the target object instances in each image. Experimental results on two publicly available datasets illustrate that the proposed approach achieves competitive or superior performance than state-of-the-art methods for WSOD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :5 1 9</head><label>31</label><figDesc>Process to explore all possible object instances from the same class using OIM. (a)-(c) illustrate the spatial and appearance graphs of different epochs and (d) shows all detected instances. Blue bounding boxes represent the detected core instance with the highest confidence score. Red bounding boxes represent the other detected instances which have high appearance similarities with the core instance. Blue and red line represent spatial and appearance graph edge respectively. Red broken line in (b) means the appearance similarities is smaller than the threshold and thus the object instances are not employed in this stage.Algorithm 1: Object Instance Mining Input: Image I, region proposals P = {p1, ..., pN }, image label Y = {y1, y2, ...yc} Output: All the nodes V a in appearance graph 1 Feed Image I and its proposals into the network to produce feature vectors F = {f1, ..., fN } 2 for c in C, C is the list of training data class do 3 if yc == 1 then 4 V s ← ∅, V a ← ∅,D ← 0, Davg ← 0, M ← 0, f lag ← 0 Choose the top-scoring proposal ic 6 V s ic ← pi c , V a ← pi c 7 for j = 1 to N do 8 Compute the appearance similarity Di c ,j using Eq. Compute IoU (pi c ,pj) 10 if IoU (pi c ,pj) ) P based on Di c ,j 15 for j = 1 to N do 16 if Di c ,j &lt; αDavg then 17 if ∃ p k ∈ V a , IoU (p k ,pj) &gt; 0 then 18 f lag ← 1 19 if flag == 0 then 20 V a ← pj, V s j ← pj</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Detections examples on VOC2007 test set. The green bounding boxes represent the ground-truth. The successful detections (IoU ≥ 0.5) are marked with red bounding boxes, and the failed ones are marked with yellow color. We show all detections with scores ≥ 0.5 and NMS is performed to remove duplicate detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Objects number of each class divided by the number of images which the corresponding class occurs on VOC2007 and VOC2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state-of-the-arts in terms of mAP (%) on the VOC2007 test set.</figDesc><table><row><cell>Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>OICR</cell><cell>58.0</cell><cell>62.4</cell><cell>31.1</cell><cell>19.4</cell><cell>13.0</cell><cell>65.1</cell><cell>62.2</cell><cell>28.4</cell><cell>24.8</cell><cell>44.7</cell><cell>30.6</cell><cell>25.3</cell><cell>37.8</cell><cell>65.5</cell><cell>15.7</cell><cell>24.1</cell><cell>41.7</cell><cell>46.9</cell><cell>64.3</cell><cell>62.6</cell><cell>41.2</cell></row><row><cell>PCL</cell><cell>54.4</cell><cell>69.0</cell><cell>39.3</cell><cell>19.2</cell><cell>15.7</cell><cell>62.9</cell><cell>64.4</cell><cell>30.0</cell><cell>25.1</cell><cell>52.5</cell><cell>44.4</cell><cell>19.6</cell><cell>39.3</cell><cell>67.7</cell><cell>17.8</cell><cell>22.9</cell><cell>46.6</cell><cell>57.5</cell><cell>58.6</cell><cell>63.0</cell><cell>43.5</cell></row><row><cell>TS 2 C</cell><cell>59.3</cell><cell>57.5</cell><cell>43.7</cell><cell>27.3</cell><cell>13.5</cell><cell>63.9</cell><cell>61.7</cell><cell>59.9</cell><cell>24.1</cell><cell>46.9</cell><cell>36.7</cell><cell>45.6</cell><cell>39.9</cell><cell>62.6</cell><cell>10.3</cell><cell>23.6</cell><cell>41.7</cell><cell>52.4</cell><cell>58.7</cell><cell>56.6</cell><cell>44.3</cell></row><row><cell>C-WSL*</cell><cell>62.9</cell><cell>64.8</cell><cell>39.8</cell><cell>28.1</cell><cell>16.4</cell><cell>69.5</cell><cell>68.2</cell><cell>47.0</cell><cell>27.9</cell><cell>55.8</cell><cell>43.7</cell><cell>31.2</cell><cell>43.8</cell><cell>65.0</cell><cell>10.9</cell><cell>26.1</cell><cell>52.7</cell><cell>55.3</cell><cell>60.2</cell><cell>66.6</cell><cell>46.8</cell></row><row><cell>MELM</cell><cell>55.6</cell><cell>66.9</cell><cell>34.2</cell><cell>29.1</cell><cell>16.4</cell><cell>68.8</cell><cell>68.1</cell><cell>43.0</cell><cell>25.0</cell><cell>65.6</cell><cell>45.3</cell><cell>53.2</cell><cell>49.6</cell><cell>68.6</cell><cell>2.0</cell><cell>25.4</cell><cell>52.5</cell><cell>56.8</cell><cell>62.1</cell><cell>57.1</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-arts in terms of CorLoc (%) on the VOC2007 trainval set.</figDesc><table><row><cell>Method</cell><cell>Localization (CorLoc)</cell></row><row><cell>OICR</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-arts in terms of mAP (%) on the VOC2012 test set.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>mAP</cell></row><row><cell>MELM</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-arts in terms of CorLoc (%) on the VOC2012 trainval set.</figDesc><table><row><cell>Method</cell><cell>Localization (CorLoc)</cell></row><row><cell>OICR</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Detection performance (mAP%) on the VOC2007 for using different values of parameter α and parameter β.</figDesc><table><row><cell>α1</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell>α2</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>5</cell></row><row><cell>OIM</cell><cell>42.9</cell><cell>48.1</cell><cell>48.2</cell><cell>46.8</cell></row><row><cell>OIM+IR</cell><cell>43.4</cell><cell>49.3</cell><cell>50.1</cell><cell>48.4</cell></row><row><cell>β</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell></cell></row><row><cell>OIM+IR</cell><cell>50.1</cell><cell>48.0</cell><cell>46.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Detection performance comparison of proposed approach on the VOC2007 with various configurations.</figDesc><table><row><cell>SG</cell><cell>AG</cell><cell>OIM(SG+AG)</cell><cell>IR</cell><cell>mAP (%)</cell></row><row><cell>√ √ √</cell><cell>√ √ √</cell><cell>√ √</cell><cell>√ √</cell><cell>34.8 42.2 46.7 48.2 43.4 50.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dissimilarity coefficient based weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9432" to="9441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Count-guided weakly supervised localization</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weakly supervised object detection with segmentation collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00551</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low shot box correction forweakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="890" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial learning towards fast weakly supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You reap what you sow: Using videos to generate high precision object proposals for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9414" to="9422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minentropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4262" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">W2f: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
