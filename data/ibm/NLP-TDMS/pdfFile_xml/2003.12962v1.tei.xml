<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPS-Net: Graph Property Sensing Network for Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinquan</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPS-Net: Graph Property Sensing Network for Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>{ eelinxin,eetakchatsau }@mail.scut.edu.cn chxding@scut.edu.cn dacheng.tao@sydney.edu.au  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Scene graph generation (SGG) aims to detect objects in an image along with their pairwise relationships. There are three key properties of scene graph that have been underexplored in recent works: namely, the edge direction information, the difference in priority between nodes, and the long-tailed distribution of relationships. Accordingly, in this paper, we propose a Graph Property Sensing Network (GPS-Net) that fully explores these three properties for SGG. First, we propose a novel message passing module that augments the node feature with node-specific contextual information and encodes the edge direction information via a tri-linear model. Second, we introduce a node priority sensitive loss to reflect the difference in priority between nodes during training. This is achieved by designing a mapping function that adjusts the focusing parameter in the focal loss. Third, since the frequency of relationships is affected by the long-tailed distribution prob-lem, we mitigate this issue by first softening the distribution and then enabling it to be adjusted for each subjectobject pair according to their visual appearance. Systematic experiments demonstrate the effectiveness of the proposed techniques. Moreover, GPS-Net achieves state-ofthe-art performance on three popular databases: VG, OI, and VRD by significant gains under various settings and metrics. The code and models are available at https: //github.com/taksau/GPS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene Graph Generation (SGG) provides an efficient way for scene understanding and valuable assistance for various computer vision tasks, including image captioning <ref type="bibr" target="#b0">[1]</ref>, visual question answering <ref type="bibr" target="#b1">[2]</ref> and 3D scene synthesis <ref type="bibr" target="#b2">[3]</ref>. This is mainly because the scene graph <ref type="bibr" target="#b3">[4]</ref> not only records the categories and locations of objects in the scene arXiv:2003.12962v1 [cs.CV] 29 Mar 2020 but also represents pairwise visual relationships of objects.</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(a), a scene graph is composed of multiple triplets in the form &lt;subject-relationship-object&gt;. Specifically, an object is denoted as a node with its category label, and a relationship is characterized by a directed edge between two nodes with a specific category of predicate. The direction of the edge specifies the subject and object in a triplet. Due to the complexity in relationship characterization and the imbalanced nature of the training data, SGG has emerged as a challenging task in computer vision.</p><p>Multiple key properties of the scene graph have been under-explored in the existing research, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The first of these is edge direction. Indeed, edge direction not only indicates the subject and object in a triplet, but also affects the class of the relationship. Besides, it influences the context information for the corresponding node, as shown in recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. An example is described in <ref type="figure" target="#fig_1">Figure 1</ref>(b), if the direction flow between man and the other objects is reversed, the focus of the context will change and thus affects the context information for all the related nodes. This is because that the importance of nodes varies according to the number of triplets they are included in the graph. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(c), leg, dog and man are involved in two, three, and four triplets in the graph, respectively. Hence, considering the contribution of each node to this scene graph, the priority in object detection should follow the order: man &gt; dog &gt; leg. However, existing works usually treat all nodes equally in a scene graph.</p><p>Here, we propose a novel direction-aware message passing (DMP) module that makes use of the edge direction information. DMP enhances the feature of each node by providing node-specific contextual information with the following strategies. First, instead of using the popular firstorder linear model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, DMP adopts a tri-linear model based on Tucker decomposition <ref type="bibr" target="#b11">[12]</ref> to produce an attention map that guides message passing. In the tri-linear model, the edge direction affects the attention scores produced. Second, we augment the attention map with its transpose to account for the uncertainty of the edge direction in the message passing step. Third, a transformer layer is employed to refine the obtained contextual information.</p><p>Afterward, we devise a node priority-sensitive loss (NPS-loss) to encode the difference in priority between nodes in a scene graph. Specifically, we maneuver the loss contribution of each node by adjusting the focusing parameter of the focal loss <ref type="bibr" target="#b12">[13]</ref>. This adjustment is based on the frequency of each node included in the triplets of the graph. Consequently, the network can pay more attention to high priority nodes during training. Comparing with <ref type="bibr" target="#b10">[11]</ref> (exploiting a non-differentiable local-sensitive loss function to represent the node priority), the proposed NPS-loss is differentiable and convex, and so it can be easily optimized by gradient descent based methods and deployed to other SGG models.</p><p>Finally, the frequency distribution of relationships has proven to be useful as prior knowledge in relationship prediction <ref type="bibr" target="#b6">[7]</ref>. However, since this distribution is long-tailed, its effectiveness as the prior is largely degraded. For example, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(d), one SGG model tends to misclassify sitting on as has since the occurrence rate of the latter is relatively high. Accordingly, we propose two strategies to handle this problem. First, we utilize a logsoftmax function to soften the frequency distribution of relationships. Second, we propose an attention model to adaptively modify the frequency distribution for each subjectobject pair according to their visual appearance.</p><p>In summary, the innovation of the proposed GPS-Net is three-fold: (1) DMP for message passing, which enhances the node feature with node-specific contextual information;</p><p>(2) NPS-loss to encode the difference in priority between different nodes; and (3) a novel method for handling the long-tailed distribution of relationships. The efficacy of the proposed GPS-Net is systematically evaluated on three popular SGG databases: Visual Genome (VG) <ref type="bibr" target="#b13">[14]</ref>, OpenImages (OI) <ref type="bibr" target="#b14">[15]</ref> and Visual Relationship Detection (VRD) <ref type="bibr" target="#b15">[16]</ref>. Experimental results demonstrate that the proposed GPS-Net consistently achieves top-level performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Context Modeling: Recent approaches for visual context modeling can be divided into two categories, which model the global and object-specific context, respectively. To model the global context, SENet <ref type="bibr" target="#b16">[17]</ref> and PSANet <ref type="bibr" target="#b17">[18]</ref> adopt rescaling to different channels in feature maps for feature fusion. In addition, Neural Motif <ref type="bibr" target="#b6">[7]</ref> represents the global context via Long Short-term Memory Networks.</p><p>To model the object-specific context, NLNet <ref type="bibr" target="#b18">[19]</ref> adopts self-attention mechanism to model the pixel-level pairwise relationships. CCNet <ref type="bibr" target="#b19">[20]</ref> accelerates NLNet via stacking two criss-cross blocks. However, as pointed out in <ref type="bibr" target="#b20">[21]</ref>, these methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> may fail to learn object-specific context due to the utilization of the first-order linear model. To address this issue, we design a direction-aware message passing module to generate node-specific context via a trilinear model.</p><p>Scene Graph Generation. Existing SGG approaches can be roughly divided into two categories: namely, onestage methods and two-stage methods. Generally speaking, most one-stage methods focus on object detection and relationship representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>, but almost ignore the intrinsic properties of scene graphs, e.g., the edge direction and node priority. To further capture the attributes of scene graph, two-stage methods utilize an extra training stage to refine the results produced by the first stage training. For example, <ref type="bibr" target="#b23">[24]</ref> utilizes the permutation- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Properties of Scene Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Direction</head><p>Node Priority Long-tailed Distribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPS-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Object Detection Result</head><p>Transformer Layer invariant representations of scene graphs to refine the results of <ref type="bibr" target="#b6">[7]</ref>. Besides, <ref type="bibr" target="#b1">[2]</ref> utilizes dynamic tree structure to characterize the acyclic property of scene graph. Meanwhile, <ref type="bibr" target="#b10">[11]</ref> adopts a graph-level metric to learn the node priority of scene graph. However, the adopted loss functions in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> are non-differentiable and therefore hard to optimize. The proposed approach is a one-stage method but has the following advantages comparing with existing works. First, it explores the properties of the scene graph more appropriately. Second, it is easy to optimize and deploy to existing models. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates the proposed GPS-Net. We employ Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> to obtain object proposals for each image. We adopt exactly the same way as <ref type="bibr" target="#b6">[7]</ref> to obtain the feature for each proposal. There are O object categories (including background) and R relationship categories (including non-relationship). The visual feature for the i-th proposal is formed by concatenating the appearance features v i ∈ R 2048 , object classification confidence scores s i ∈ R O , and the spatial feature b i ∈ R 4 . Then, the concatenated feature is projected into a 512-dimensional subspace and denoted as x i . Besides, we further extract features from the union box of one pair of proposal i and j, denoted as u ij ∈ R 2048 . To better capture properties of scene graph, we make contributions from three perspectives. First, a direction-aware message passing (DMP) module is introduced in Section 3.1. Second, a node priority sensitive loss (NPS-loss) is introduced in Section 3.2. Third, an adaptive reasoning module (ARM) is designed in Section 3.3.</p><formula xml:id="formula_0">(a) DMP (c) ARM ① man ② cat ③ paw ④ shirt ⑤ ear ① man ② cat ③ paw ④ shirt ⑤ ear (b) NPS-Loss</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Direction-aware Message Passing</head><p>The message passing (MP) module takes a node features x i as input. Its output for the i-th node is denoted as z i , and the neighborhood of this node is represented as N i . For all MP modules in this section, N i includes all nodes but the i-th node itself. Following the definition in graph attention network <ref type="bibr" target="#b7">[8]</ref>, given two nodes i and j, we represent the direction of i → j as forward and i ← j as backward for the i-th node. In the following, we first review the design of the one representative MP module, which is denoted as Global Context MP (GCMP) in this paper. GCMP adopts the softmax function for normalization. Its structure is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>(a) and can be formally expressed as as the pairwise contextual coefficient between nodes i and j in the forward direction. However, it has been revealed that utilizing the concatenation operation in Equation <ref type="formula" target="#formula_1">(1)</ref> may not obtain node-specific contextual information <ref type="bibr" target="#b20">[21]</ref>. In fact, it is more likely that x i in Equation <ref type="formula" target="#formula_1">(1)</ref> is ignored by w. Therefore, GCMP actually generates the same contextual information for all nodes. Inspired by this observation, Equation <ref type="formula" target="#formula_1">(1)</ref> can be simplified as follows <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_1">z i = x i +W z σ j∈Ni exp(w T [x i , x j ]) m∈Ni exp(w T [x i , x m ]) W v x j ,<label>(1)</label></formula><formula xml:id="formula_2">z i = x i + W z σ j∈Ni exp(w T e x j ) m∈Ni exp(w T e x m ) W v x j ,<label>(2)</label></formula><p>where w e ∈ R 512 is a projection vector. As depicted in <ref type="figure" target="#fig_4">Figure 3(b)</ref>, we denote this model as Simplified Global Context MP (S-GCMP) module. The above two MP modules may not be optimal for SGG because they ignore the edge direction information and cannot provide node-specific contextual information. Accordingly, we propose the DMP module to solve the above problems. As illustrated in <ref type="figure" target="#fig_4">Figure  3</ref>(c), DMP consists of two main components: directionaware context modeling and one transformer layer.</p><p>Direction-aware Context Modeling: This component aims to learn node-specific context and guide message passing via the edge direction information. Inspired by the multi-modal low rank bilinear pooling method <ref type="bibr" target="#b33">[34]</ref>, we formulate the contextual coefficient e ij between two nodes i and j as follows:</p><formula xml:id="formula_3">e ij = w T e (W s x i W o x j W u u ij ),<label>(3)</label></formula><p>where represents Hadamard product. W s , W o , and W u ∈ R 512×512 are projection matrices for fusion. Equation <ref type="formula" target="#formula_3">(3)</ref> can be considered as a tri-linear model based on Tucker decomposition <ref type="bibr" target="#b11">[12]</ref>. Compared with the first two MP modules, Equation (3) has four advantages. First, it employs union box features to expand the receptive field in context modeling. Second, the tri-linear model is a more powerful way to model highorder interactions between three types of features. Third, since features for the two nodes and the union box are coupled together by Hadamard product in Equation <ref type="formula" target="#formula_3">(3)</ref>, they jointly affect context modeling. In this way, we obtain nodespecific contextual information. Fourth, Equation <ref type="formula" target="#formula_3">(3)</ref> specifies the position of subject and object; therefore, it considers the edge direction information of the edge.</p><p>However, the direction of the edge is unclear in the MP step of SGG, since the relationship between two nodes is still unknown. Therefore, we consider the contextual coefficient for both the forward and backward directions by stacking them as a two-element-vector [α ij α ji ] T , where α ij denotes the normalized contextual coefficient. Finally, the output of the first component of DMP for the i-th node can be denoted as</p><formula xml:id="formula_4">j∈Ni α ij α ji ⊗W t3 x j ,<label>(4)</label></formula><p>where ⊗ denotes Kronecker product. W t3 ∈ R 256×512 is a learnable projection matrix.</p><p>Transformer Layer: The contextual information obtained above may contain redundant information. Inspired by <ref type="bibr" target="#b20">[21]</ref>, we employ a transformer layer to refine the obtained contextual information. Specifically, it is consisted of two fully-connected layers with ReLU activation and layer normalization (LN) <ref type="bibr" target="#b32">[33]</ref>. Finally, residual connection is applied to fuse the original feature and the contextual information. Our whole DMP module can be expressed as  </p><formula xml:id="formula_5">z i = x i + W t1 σ LN W t2 j∈Ni α ij α ji ⊗W t3 x j ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Node Priority Sensitive Loss</head><p>Existing works for SGG tend to utilize cross-entropy loss as objective function for object classification, which implicitly regards the priority of all nodes is equal for the scene graph. However, their priority varies according to the number of triplets they are involved. Recently, a local-sensitive loss has been proposed to address this problem in <ref type="bibr" target="#b10">[11]</ref>. As the loss is non-differentiable, the authors in <ref type="bibr" target="#b10">[11]</ref> adopt a two-stage training strategy, where the second stage is realized by a complicated policy gradient method <ref type="bibr" target="#b45">[46]</ref>.</p><p>To handle this problem, we propose a novel NPS-loss that not only captures the node priority in scene graph but also has the benefit of differentiable and convex formulation. NPS-loss is inspired by focal loss that reduces weights of well-classified objects using a focusing parameter, which is denoted as γ in this paper. Compared with focal loss, NPloss has the following key differences: (1) it is mainly used to solve the node-priority problem in SGG. In comparison, focal loss is designed to solve the class imbalance problem in object detection; <ref type="bibr" target="#b1">(2)</ref> γ is fixed in <ref type="bibr" target="#b12">[13]</ref>. In NPS-loss, it depends on the node priority. Specifically, we first calculate the priority θ i for the i-th node according to its contribution to the scene graph:</p><formula xml:id="formula_6">θ i = t i T ,<label>(6)</label></formula><p>where t i denotes the number of triplets that include the i-th node and T is the total number of triplets in one graph. Given θ i , one intuitive way to obtain the focusing parameter γ is a linear transformation, e.g., γ(θ i ) = −2θ i + 2. However, this transformation exaggerates the difference between nodes of high-priority and middle-level priority, and narrows the difference between nodes of middle-level priority and low-priority. To solve this problem, we design a nonlinear mapping function that transforms θ i to γ:</p><formula xml:id="formula_7">γ(θ i ) = min (2, −(1 − θ i ) µ log(θ i )) ,<label>(7)</label></formula><p>where µ denotes a controlling factor, which controls the influence of θ i to the value of γ. As depicted in <ref type="figure" target="#fig_5">Figure 4</ref>, curve for the mapping function changes quickly for nodes with low priority, and slowly for nodes of high priority.</p><p>Moreover, a larger µ leads to more nodes to be highlighted during training. Finally, we obtain the NPS-loss that guides the training process according to node priority:</p><formula xml:id="formula_8">L nps (p i ) = −(1 − p i ) γ(θi) log(p i ),<label>(8)</label></formula><p>where p i denotes the object classification score on the ground-truth object class for the i-th node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Reasoning Module</head><p>After obtaining the refined node features by DMP and the object classification scores by NPS-loss, we further propose an adaptive reasoning module (ARM) for relationship classification. Specifically, ARM provides prior for classification by two steps: frequency softening and bias adaptation for each triplet. In what follows, we introduce the two steps in detail.</p><p>Frequency Softening: Inspired by the frequency baseline introduced in <ref type="bibr" target="#b6">[7]</ref>, we employ the frequency of relationships as prior to promote the performance of relationship classification. However, the original method in <ref type="bibr" target="#b6">[7]</ref> suffers from the long-tailed distribution problem of relationships. Therefore, it may fail to recognize relationships of low frequency. To handle this problem, we first adopt a log-softmax function to soften the original frequency distribution of relationships as follows:</p><formula xml:id="formula_9">p i→j = log softmax p i→j ,<label>(9)</label></formula><p>where p i→j ∈ R R denotes the original frequency distribution vector between the i-th and the j-th nodes. The same as <ref type="bibr" target="#b6">[7]</ref>, this vector is determined by the object class of the two nodes.p i→j is the normalized vector of p i→j . Bias Adaptation: To enable the frequency prior adjustable for each node pair, we further propose an adpative attention mechanism to modify the prior according to the visual appearance of the node pair. Specifically, a sigmoid function is applied to obtain attention on the frequency prior: d = sigmoid (W p u ij ), where W p ∈ R R×2048 is transformation matrix. Then, the classification score vector of relationships can be obtained as follows:</p><formula xml:id="formula_10">p ij = softmax W r (z i * z j * u ij ) + d p i→j ,<label>(10)</label></formula><p>where W r ∈ R R×1024 denotes the classifier, and d p i→j is the bias. * represents a fusion function defined in <ref type="bibr" target="#b46">[47]</ref></p><formula xml:id="formula_11">: x * y = ReLU (W x x + W y y)−(W x x − W y y) (W x x − W y y),</formula><p>where W x and W y project x, y to 1024-dimensional space, respectively.</p><p>Relationship Prediction: During testing, the category of relationship between i-th and j-th nodes is predicted by:</p><formula xml:id="formula_12">r ij = arg max r∈R (p ij (r)),<label>(11)</label></formula><p>where R represents the set of relationship categories.  <ref type="table">Table 1</ref>: Comparisons with state-of-the-arts on VG. Since some works do not evaluate on R@20, we compute the mean on all tasks over R@50 and R@100. and ‡ denote the methods using the same Faster-RCNN detector and evaluation metric as <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b5">[6]</ref>, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGDET SGCLS PREDCLS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experimental results on three datasets: Visual Genome (VG) <ref type="bibr" target="#b13">[14]</ref>, OpenImages (OI) <ref type="bibr" target="#b14">[15]</ref>, and Visual Relationship Detection (VRD) <ref type="bibr" target="#b15">[16]</ref>. We first report evaluation settings, followed by comparisons with state-of-the-art methods and the ablation studies. Besides, qualitative comparisons between GPS-Net and other approaches are provided in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Settings</head><p>Visual Genome: We use the same data and evaluation metrics that have been widely adopted in recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11]</ref>. Specifically, the most frequent 150 object categories and 50 relationship categories are utilized for evaluation. After preprocessing, the scene graph for each image consists of 11.6 objects and 6.2 relationships on average. The data is divided into one training set and one testing set. The training set includes 70% images, with 5K images as a validation subset. The testing set is composed of the remaining 30% images. In the interests of fair comparisons, we also adopt Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> with VGG-16 backbone to obtain the location and features of ob-  ject proposals. Moreover, since SGG performance highly depends on the pre-trained object detector, we utilize the same set of hyper-parameters as <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b5">[6]</ref> respectively. We follow three conventional protocols for evaluation: (1) Scene Graph Detection (SGDET): given an image, detect object bounding boxes and their categories, and predict their pair-wise relationships; (2) Scene Graph Classification (SG-CLS): given ground-truth object bounding boxes, predict the object categories and their pair-wise relationships; (3) Predicate Classification (PREDCLS): given the object categories and their bounding boxes, predict their pair-wise relationships only. All algorithms are evaluated by Recall@K metrics, where K=20, 50, and 100, respectively. Considering that the distribution of relationships is highly imbalanced in VG, we further utilize mean recall@K (mR@K) to evaluate the performance of each relationship <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>OpenImages: The training and testing sets contain 53,953 images and 3,234 images respectively. We utilize Faster R-CNN associated with the pre-trained ResNeXt-101-FPN <ref type="bibr" target="#b5">[6]</ref> as the backbone. We also follow the same data   <ref type="table" target="#tab_8">Table 4</ref>: Comparisons with state-of-the-arts on VRD (− denotes unavailable). Pre., Phr., and Rel. represent predication detection, phrase detection, and relation detection, respectively. † and * denote using the same object detector.</p><p>processing and evaluation metrics as in <ref type="bibr" target="#b5">[6]</ref>. More specifically, the results are evaluated by calculating Recall@50 (R@50), weighted mean AP of relationships (wmAP rel ), and weighted mean AP of phrase (wmAP phr ). The final score is given by score wtd = 0.2 × R@50 + 0.4 × wmAP rel + 0.4 × wmAP phr . Note that the wmAP rel evaluates the AP of the predicted triplet where both the subject and object boxes have an IoU of at least 0.5 with ground truth. The wmAP phr is similar, but utilized for the union area of the subject and object boxes. Visual Relationship Detection: We apply the same object detectors as in <ref type="bibr" target="#b5">[6]</ref>. More specifically, two VGG16based backbones are provided, which were trained on Im-gaeNet and COCO, respectively. The evaluation metric is the same as in <ref type="bibr" target="#b15">[16]</ref>, which reports R@50 and R@100 for relationship, predicate, and phrase detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>To ensure compatibility with the architectures of previous state-of-the-art methods, we utilize ResNeXt-101-FPN as our OpenImages backbone on OI and VGG-16 on VG and VRD. During training, we freeze the layers before the ROIAlign layer and optimize the model jointly considering the object and relationship classification losses. Our model is optimized by SGD with momentum, with the initial learning rate and batch size set to 10 −3 and 6 respectively. For the SGDET task, we follow <ref type="bibr" target="#b6">[7]</ref> that we only predict the relationship between proposal pairs with overlapped bounding boxes. Besides, the top-64 object proposals in each image are selected after per-class non-maximal suppression (NMS) with an IoU of 0.3. Moreover, the ratio between pairs without any relationship (background pairs) and those with relationship during training is sampled to 3:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-Art Methods</head><p>Visual Genome: <ref type="table">Table 1</ref> shows that GPS-Net outperforms all state-of-the-arts methods on various metrics. Specifically, GPS-Net outperforms one very recent onestage model, named KERN <ref type="bibr" target="#b22">[23]</ref>, by 1.8% on average at R@50 and R@100 over the three protocols. In more detail, it outperforms KERN by 1.9%, 2.7% and 1.2% at R@100 on SGDET, SGCLS, and PRECLS, respectively. Even when compared with the best two-stage model CMAT <ref type="bibr" target="#b10">[11]</ref>, GPS-Net still demonstrates a performance improvement of 0.5% on average over the three protocols. Meanwhile, compared with the one-stage version of VCTREE <ref type="bibr" target="#b1">[2]</ref> and CMAT <ref type="bibr" target="#b10">[11]</ref>, GPS-Net respectively achieves 1.5% and 2.5% performance gains on SGCLS at Recall@100. Another advantage of GPS-Net over VCTREE and CMAT is that GPS-Net is much more efficient, as the two methods adopt policy gradient for optimization, which is timeconsuming <ref type="bibr" target="#b45">[46]</ref>. Moreover, when compare with RelDN using the same backbone, the performance gain by GPS-Net is even more dramatic, namely, 5.5% promotion on SGCLS at Recall@100 and 2.5% on average over three protocols.</p><p>Due to the class imbalance problem in VG, previous works usually achieve low performance for less frequent categories. Hence, we conduct an experiment utilizing the Mean Recall as evaluation metric <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>. As shown in Table 2 and <ref type="figure" target="#fig_6">Figure 5</ref>, GPS-Net shows a large absolute gain for both the Mean Recall and Recall metrics, which indicates that GPS-Net has advantages in handling the class imbalance problem of SGG.</p><p>OpenImages: We present results compared with RelDN <ref type="bibr" target="#b5">[6]</ref> in <ref type="table" target="#tab_7">Table 3</ref>. RelDN is an improved version of the model that won the Google OpenImages Visual Relationship Detection Challenge, with the same object detector, GPS-Net outperforms RelDN by 2.4% on the overall metric score wtd . Moreover, despite of the severe class imbalance problem, GPS-Net still achieves outstanding performance in AP rel for each category of relationships. The largest gap between GPS-Net and RelDN in AP rel is 24.5% for wears and 20.6% for hits.</p><p>Visual Relationship Detection:  <ref type="table">Table 5</ref>: Ablation studies on the proposed methods. We consistently use the same backbone as <ref type="bibr" target="#b6">[7]</ref>. itate fair comparison, we adopt the two backbone models provided in RelDN <ref type="bibr" target="#b5">[6]</ref> to train GPS-Net, respectively. It is shown that GPS-Net consistently achieves superior performance with both backbone models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To prove the effectiveness of our proposed methods, we conduct four ablation studies. Results of the ablation studies are summarized in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_9">Table 6</ref>, respectively.</p><p>Effectiveness of the Proposed Modules. We first perform an ablation study to validate the effectiveness of DMP, NPS-loss, and ARM. Results are summarized in <ref type="table">Table 5</ref>. We add the above modules one by one to the baseline model. In <ref type="table">Table 5</ref>, Exp 1 demotes our baseline that is based on the MOTIFNET-NOCONTEXT method <ref type="bibr" target="#b6">[7]</ref> with our feature construction strategy for relationship prediction. From Exp 2-5, we can clearly see that the performance improves consistently when all the modules are used together. This shows that each module plays a critical role in inferring object labels and their pair-wise relationships.</p><p>Effectiveness of the Stacking Operation in DMP. We conduct additional analysis on the stacking operation in DMP. The stacking operation accounts for the uncertainty in the edge direction information. As shown in the left sub <ref type="table" target="#tab_9">-table of Table 6</ref>, the stacking operation consistently improves the performance of DMP over various metrics. Therefore, its effectiveness is justified.</p><p>Comparisons between Three MP Modules. We compare the performance of three MP modules in Section 3.1: GCMP, S-GCMP, and DMP. To facilitate fair comparison, we implement the same transformer layer as DMP to the other two modules. As shown in the middle sub-table in <ref type="table" target="#tab_9">Table 6</ref>, the performance of DMP is much better than the other two modules. This is because DMP encodes the edge direction information and provides node-specific contextual information for each node involved in message passing.</p><p>Design Choices in NPS-loss. The value of the controlling factor µ determines the impact of node priority on object classification. As shown in the right sub <ref type="table" target="#tab_9">-table of Table  6</ref>, we show the performance of NPS-loss with three different values of µ. We also compare NPS-loss with the focal loss <ref type="bibr" target="#b12">[13]</ref>. NPS-loss achieves the best performance when µ equals to 4. Moreover, NPS-loss outperforms the focal loss, justifying its effectiveness to solve the node priority problem for SGG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we devise GPS-Net to address the main challenges in SGG by capturing three key properties of scene graph. Specifically, (1) edge direction is encoded when calculating the node-specific contextual information via the DMP module; (2) the difference in node priority is characterized by a novel NPS-loss; and (3) the long-tailed distribution of relationships is alleviated by improving the usage of relationship frequency through ARM. Through extensive comparative experiments and ablation studies, we validate the effectiveness of GPS-Net on three datasets.    <ref type="figure">Figure 6</ref>: Qualitative comparisons between GPS-Net and MOTIF with R@20 in the SGDET setting. Green boxes are detected objects with IOU larger than 0.5 with the ground-truth. Green edges are predictions of relationships that are consistent with the ground-truth. Yellow boxes (edges) denote reasonable detections of objects (relationships), but are not annotated in the database. Red boxes (edges) represent ground-truth objects (relationships) that have no match with the detection results by the algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Influence of the direction information of the edge (c) Graph nodes with different priorities (d) Misclassified relationship with low frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) The ground-truth scene graph for one image. (b) The direction of the edge specifies the subject and object, and also affects the relationship type and node-specific context. (c) The priority of nodes varies, according to the number of triplets included in the graph. (d) The long-tailed distribution of relationships causes error for low-frequency relationships, e.g., the failure in recognizing sitting on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The framework of GPS-Net. GPS-Net adopts Faster R-CNN to obtain the location and visual feature of object proposals. It includes three new modules for SGG: (1) a novel message passing module named DMP that enhances the node feature with node-specific contextual information; (2) a new loss function named NPS-loss that reflects the difference in priority between different nodes; (3) an adaptive reasoning module (ARM) to handle the long-tailed distribution of relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the three MP modules in Section 3.1. , ⊕, ⊗, represent Hadamard product, element-wise addition, and Kronecker product, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>i ) = min (2, −(1 − θ i ) µ log(θ i ))focusing parameter γ(θ i ) Mapping function γ(θ i ) with different controlling factors µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The R@100 improvement in PREDCLS of GPS-Net compared with the VCTREE [2]. The Top-35 categories of relationship are selected according to their occurrence frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where σ represents the ReLU function. W v and W z ∈ R 512×512 are linear transformation matrices. w ∈ R 1024 is a projection vector, and [, ] represents the concatenation operation. For simplicity, we define c ij = exp(w T [xi,xj ])</figDesc><table><row><cell>m∈N i</cell><cell>exp(w T [xi,xm])</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the mR@100 metric between various methods across all the 50 relationship categories.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>AP rel per class Model R@50 wmAP rel wmAP phr score wtd at on holds plays interacts with wears hits inside of under RelDN, L 0 [6] 74.67 34.63 37.89 43.94 32.40 36.51 41.84 36.04 40.43 5.70 55.40 44.17 25.00 RelDN[6] 74.94 35.54 38.52 44.61 32.90 37.00 43.09 41.04 44.16 7.83 51.04 44.72 50.00 GPS-Net 77.27 38.78 40.15 47.03 35.10 38.90 51.47 45.66 44.58 32.35 71.71 47.21 57.28</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with state-of-the-arts on OI. We adopt the same evaluation metrics as<ref type="bibr" target="#b5">[6]</ref> </figDesc><table><row><cell></cell><cell>Pre.</cell><cell>Rel.</cell><cell>Phr.</cell></row><row><cell>Model</cell><cell cols="3">R@50 R@50 R@100 R@50 R@100</cell></row><row><cell>VTransE [37]</cell><cell cols="3">44.8 19.4 22.4 14.1 15.2</cell></row><row><cell>ViP-CNN [39]</cell><cell>-</cell><cell cols="2">17.3 20.0 22.8 27.9</cell></row><row><cell>VRL [40]</cell><cell>-</cell><cell cols="2">18.2 20.8 21.4 22.6</cell></row><row><cell>KL distilation [43]</cell><cell cols="3">55.2 19.2 21.3 23.1 24.0</cell></row><row><cell>MF-URLN [44]</cell><cell cols="3">58.2 23.9 26.8 31.5 36.1</cell></row><row><cell>Zoom-Net  *  [42]</cell><cell cols="3">50.7 18.9 21.4 24.8 28.1</cell></row><row><cell>CAI + SCA-M  *  [42]</cell><cell cols="3">56.0 19.5 22.4 25.2 28.9</cell></row><row><cell cols="4">GPS-Net  *  (ImageNet) 58.7 21.5 24.3 28.9 34.0</cell></row><row><cell>RelDN  † [6]</cell><cell>-</cell><cell cols="2">25.3 28.6 31.3 36.4</cell></row><row><cell>GPS-Net  † (COCO)</cell><cell cols="3">63.4 27.8 31.7 33.8 39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>presents com-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The left sub-table shows the effectiveness of the stacking operation in DMP. The middle sub-table compares the performance of the three MP modules in Section 3.1 with the same transformer layer. The right sub-table compares NPS-loss and the focal loss, and shows the influence of the controlling factor µ.</figDesc><table><row><cell cols="2">w. stack w.o. stack</cell><cell cols="2">GCMP S-GCMP DMP</cell><cell>Focal µ = 3 µ = 4 µ = 5</cell></row><row><cell>R@20 35.7</cell><cell>36.1</cell><cell>R@20 34.3</cell><cell>34.8 36.1</cell><cell>R@20 35.8 36.0 36.1 35.8</cell></row><row><cell>SGCLS R@50 38.8</cell><cell>39.2</cell><cell>SGCLS R@50 37.2</cell><cell>37.7 39.2</cell><cell>SGCLS R@50 39.0 38.9 39.2 39.1</cell></row><row><cell>R@100 39.6</cell><cell>40.1</cell><cell>R@100 37.9</cell><cell>38.4 40.1</cell><cell>R@100 39.8 39.9 40.1 39.9</cell></row><row><cell>R@20 22.4</cell><cell>22.6</cell><cell>R@20 21.7</cell><cell>22.1 22.6</cell><cell>R@20 22.4 22.5 22.6 22.5</cell></row><row><cell>SGDET R@50 28.3</cell><cell>28.4</cell><cell>SGDET R@50 27.5</cell><cell>28.0 28.4</cell><cell>SGDET R@50 28.2 28.2 28.4 28.3</cell></row><row><cell>R@100 31.5</cell><cell>31.7</cell><cell>R@100 30.8</cell><cell>31.2 31.7</cell><cell>R@100 31.5 31.6 31.7 31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Mean recall (%) of various methods across all the 50 relationship categories. All methods in this table adopt the same Faster-RCNN detector.</figDesc><table><row><cell></cell><cell></cell><cell>Pre.</cell><cell></cell><cell></cell><cell>Rel.</cell><cell></cell><cell></cell><cell></cell><cell>Phr.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>k=1</cell><cell></cell><cell>k=1</cell><cell></cell><cell></cell><cell>k=70</cell><cell>k=1</cell><cell></cell><cell></cell><cell>k=70</cell></row><row><cell cols="2">Pretrained Model</cell><cell cols="10">R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100</cell></row><row><cell></cell><cell>VRD-Full [16]</cell><cell>47.9</cell><cell>47.9</cell><cell>16.2</cell><cell>17.0</cell><cell>-</cell><cell>-</cell><cell>13.9</cell><cell>14.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PPRFCN [35]</cell><cell>47.4</cell><cell>47.4</cell><cell>19.6</cell><cell>23.2</cell><cell>-</cell><cell>-</cell><cell>14.4</cell><cell>15.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>VTranse [37]</cell><cell>44.8</cell><cell>44.8</cell><cell>19.4</cell><cell>22.4</cell><cell>-</cell><cell>-</cell><cell>14.1</cell><cell>15.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Vip-CNN [39]</cell><cell>-</cell><cell>-</cell><cell>17.3</cell><cell>20.0</cell><cell>-</cell><cell>-</cell><cell>22.8</cell><cell>27.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Unknown VRL [40]</cell><cell>-</cell><cell>-</cell><cell>18.2</cell><cell>20.8</cell><cell>-</cell><cell>-</cell><cell>21.4</cell><cell>22.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">KL distilation[43] 55.2</cell><cell>55.2</cell><cell>19.2</cell><cell>21.3</cell><cell>22.7</cell><cell>31.9</cell><cell>23.1</cell><cell>24.0</cell><cell>26.3</cell><cell>29.4</cell></row><row><cell></cell><cell>MF-URLN [44]</cell><cell>58.2</cell><cell>58.2</cell><cell>23.9</cell><cell>26.8</cell><cell>-</cell><cell>-</cell><cell>31.5</cell><cell>36.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Zoom-Net [42]</cell><cell>50.7</cell><cell>50.7</cell><cell>18.9</cell><cell>21.4</cell><cell>21.4</cell><cell>27.3</cell><cell>24.8</cell><cell>28.1</cell><cell>29.1</cell><cell>37.3</cell></row><row><cell>ImageNet</cell><cell cols="2">CAI+SCA-M [42] 56.0 RelDN[6] -</cell><cell>56.0 -</cell><cell>19.5 19.8</cell><cell>22.4 23.0</cell><cell>22.3 21.5</cell><cell>28.5 26.4</cell><cell>25.2 26.4</cell><cell>28.9 31.4</cell><cell>29.6 28.2</cell><cell>18.4 25.4</cell></row><row><cell></cell><cell>GPS-Net</cell><cell>58.7</cell><cell>58.7</cell><cell>21.5</cell><cell>24.3</cell><cell>23.6</cell><cell>28.9</cell><cell>28.9</cell><cell>34.0</cell><cell>30.4</cell><cell>38.2</cell></row><row><cell>COCO</cell><cell>RelDN [6] GPS-Net</cell><cell>-63.4</cell><cell>-63.4</cell><cell>25.3 27.8</cell><cell>28.6 31.7</cell><cell>28.2 30.6</cell><cell>33.9 37.0</cell><cell>31.3 33.8</cell><cell>36.4 39.2</cell><cell>34.5 36.8</cell><cell>42.1 44.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison with state-of-the-art methods on the VRD dataset. Pre., Phr., and Rel. represent predicate detection, phrase detection, and relation detection, respectively. − denotes that the result is unavailable.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This supplementary document is organized as follows:</p><p>• Section A shows more detailed comparison results under the Mean Recall metric between GPS-Net and state-of-the-art methods on the VG database. Results are summarized in <ref type="table">Table 7</ref>. Meanwhile, we also treat the number of relationship predictions per object pair (k) as a hyper-parameter on VRD, and report Recall with respect to different k in <ref type="table">Table 8</ref>.</p><p>• Section B first provides the qualitative comparisons between GPS-Net and a strong baseline named MO-TIFS <ref type="bibr" target="#b6">[7]</ref> under the SGDET protocol in <ref type="figure">Figure 6</ref>. Then, attention maps of different MP modules are visualized in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Mean Recall for Scene Graph on VG</head><p>As shown in <ref type="table">Table 7</ref>, GPS-Net shows the best performance under all protocols. In particular, GPS-Net outperforms one very recent work named VCTREE-HL [2] by 2.3% on average over the three protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Performance Comparison on VRD with various k</head><p>As revealed in previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>, each object pair may be described by several plausible predicates. In other words, it should have been formulated as a multi-label classification problem. Therefore, evaluation metrics based on the top-1 prediction (k=1) per object pair only may be unreasonable. Following <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>, we further report recall with respect to different k (k=1, 70) and compare with state-of-the-art methods. As shown in <ref type="table">Table 8</ref>, GPS-Net consistently achieves the best performance among state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Analysis</head><p>B.1. Generated Scene Graph <ref type="figure">Figure 6</ref> illustrates qualitative comparisons between GPS-Net and MOTIFS <ref type="bibr" target="#b6">[7]</ref>. In <ref type="figure">Figure 6</ref>(a), it is shown that for nodes with low priority and relationships with high frequency, GPS-Net still makes better predictions than MO-TIFS. Therefore, we owe this performance gain to the DMP module that encodes edge direction information and provide node-specific context. In <ref type="figure">Figure 6(b)</ref>, it is shown that GPS-Net makes fewer mistakes than MOTIFS for nodes of high priority. We give this credit to the NPS-loss. Finally, in <ref type="figure">Figure 6(c)</ref>, it can be observed that GPS-Net makes outstanding improvement in predicting low-frequency relationships, e.g., walking on and wearing, via the help of the ARM module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Attention Maps of Different MP Modules</head><p>We make qualitative comparisons between GCMP, S-GCMP, and DMP in <ref type="figure">Figure 7. Figures 7(a)</ref> and (b) show ground-truth object regions and the ground-truth relationship matrix. More specifically, in the relationship matrix, yellow cube denotes one relationship is presented, and purple cube represents the opposite. Figures 7(c)(d)(e) show the attention maps produced by GCMP, S-GCMP, and DMP respectively. It is clear that GCMP and S-GCMP produce very similar context for each node (elements in each column are similar). Only DMP obtains node-specific context (elements in each column are diverse). Furthermore, the attention map produced by DMP is highly consistent with the ground-truth relationship matrix in <ref type="figure">Figures 7(b)</ref>. Therefore, our proposed DMP module plays a key role in SGG, helping GPS-Net to achieve state-of-the-art performance.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Humancentric Indoor Scene Synthesis Using Stochastic Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graphical contrastive losses for scene graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting Edge Features in Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentive Relational Networks for Mapping Images to Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counterfactual Critic Multi-Agent Training for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The open imagesdataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledgeembedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tensorize, factorize and regularize: Robust visual relationship learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale visual relationship un-derstanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting Edge Features for Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weaklysupervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vip-cnn: A visual phrase reasoning convolutional neural network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep variationstructured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards context-aware interaction recognition for visual relationship detec-tion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowl-edge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On Exploring Undetermined Relationships for Visual Relationship Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Evolved policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<editor>NeurlPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prugel-Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
