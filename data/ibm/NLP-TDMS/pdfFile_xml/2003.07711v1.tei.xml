<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">F , B, Alpha Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Forte</surname></persName>
							<email>fortem@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pitié</surname></persName>
							<email>fpitie@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">F , B, Alpha Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Matting</term>
					<term>compositing</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cutting out an object and estimating its opacity mask, known as image matting, is a key task in many image editing applications. Deep learning approaches have made significant progress by adapting the encoder-decoder architecture of segmentation networks. However, most of the existing networks only predict the alpha matte and post-processing methods must then be used to recover the original foreground and background colours in the transparent regions. Recently, two methods have shown improved results by also estimating the foreground colours, but at a significant computational and memory cost. In this paper, we propose a low-cost modification to alpha matting networks to also predict the foreground and background colours. We study variations of the training regime and explore a wide range of existing and novel loss functions for the joint prediction. Our method achieves the state of the art performance on the Adobe Composition-1k dataset for alpha matte and composite colour quality. It is also the current best performing method on the alphamatting.com online evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alpha matting refers to the problem of extracting the opacity mask, alpha matte, of an object in an image. It is an essential task in visual media production as it is required to selectively apply effects on image layers or recompose objects onto new backgrounds. The alpha matting model for images is known as the Compositing Equation <ref type="bibr" target="#b1">[2]</ref> and is defined as follows.</p><formula xml:id="formula_0">C i = α i F i + (1 − α i )B i<label>(1)</label></formula><p>In this equation, C i is the observed colour value at pixel site i, F i , B i are the pixel colours in the foreground and background layers at that same site and α i is the level of mixing between the two layers. An α value of 1 thus indicates a pure foreground pixel, a value of 0 indicates a pure background pixel. The user would typically define a trimap indicating regions of definite background (α = 0), definite foreground (α = 1) and unknown α.</p><p>Estimating the α-matte is clearly an ill-posed problem as we have 3 nonlinear equations for 7 unknowns, without any well defined spatial dependency to help us. The task is especially difficult when the foreground and background Input Image GT:α gt , F = F gt Ours:α, F =F Input Image Our alpha and composite Our alphaα Naive:α, F =αI <ref type="bibr" target="#b17">[20]</ref>:α, F = F <ref type="bibr" target="#b17">[20]</ref> Trimap CA Matting <ref type="bibr" target="#b12">[13]</ref> alpha and composite <ref type="figure">Fig. 1</ref>. Alpha matting and compositing. Left: We show the need for foreground prediction and compare our composite with a post-processing method <ref type="bibr" target="#b17">[20]</ref>. Right: We show our method better excludes background colours compared to the state of the art method for simultaneous prediction <ref type="bibr" target="#b12">[13]</ref>. .</p><p>colours are similar or when the background is highly textured but recently, deep convolutional neural networks (CNNs) have progressed the state-of-the-art in α-matte prediction ( <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b12">13]</ref>) by better modelling natural image priors and α-mattes priors. These α-matting networks are based on the core architectures of semantic segmentation neural networks. This makes sense as α-matting can be seen as an extension of binary segmentation, where instead of estimating a binary {0, 1} label field we estimate a floating point α ∈ [0, 1] field. For instance, the original Deep Image Matting paper of <ref type="bibr" target="#b35">[37]</ref> adapted the FCN architecture of <ref type="bibr" target="#b30">[32]</ref> to take as an input the original image and a trimap and as an output the α-matte instead of a binary mask.</p><p>As the level of details required in the output matte is much higher than in segmentation, most of the recent literature has focused on making architectural changes that can increase the resolution ability of the core encoder-decoder architecture (see <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b12">13]</ref>). Curiously, most of these methods only estimate α, and not F or B, which are however also required by most applications. Unmixing the foreground and background colours is usually left as a post-processing step, (e.g. Levin et al. <ref type="bibr" target="#b17">[20]</ref>). Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">34]</ref> have started to recognise the importance of jointly estimating α, F, B, but the resulting architectures are computationally expansive. In this paper, we propose a novel architecture for jointly estimating α, F, B and study the benefit of estimating F and B through the examination of different possible loss functions.</p><p>Whereas in binary segmentation the choice of Loss functions is relatively straightforward (i.e. cross-entropy or IoU), the nature of the α-matte gives us more options to consider. A few of the α losses proposed in previous works include: Huber, L 1 , L 1 on the gradient, pyramid Laplacian, discriminative loss, etc. Also predicting F and B further extends the choice of possible loss functions and we propose to systematically study the merits of these different losses.</p><p>Parallel to these considerations on loss and architecture, interesting issues around training have also emerged. Indeed, as matting is an intrinsically harder task than segmentation, training can be long and trickier to setup correctly. Seemingly insignificant implementation details can turn out to be of critical importance. A few research groups have reported difficulties in reproducing the results of Deep Image Matting [10, <ref type="bibr" target="#b22">25,</ref><ref type="bibr">16]</ref>. We discovered that simply setting the batch size to 1 in <ref type="bibr" target="#b35">[37]</ref> had a critical impact on the performance and could, by itself, explain the reported training failures.</p><p>Contributions. We propose therefore that, of equal importance to the core architecture design, are the considerations around training and loss functions, and the main contribution of this paper is to propose a novel architecture for jointly estimating α, F, B and to systematically explore the impact of key choices in losses and training regimes. Our study include 17 experiments that contribute to three study areas:</p><p>1. a comparison of min-batch and stochastic gradient descent and the use of batchnorm vs. groupnorm. 2. a study of the different α-matte losses (L 1 , gradient, laplacian pyramid, compositing loss). 3. a study of the potential benefit of also predicting F and B alongside α and the possible losses associated with this (L 1 loss and exclusion loss).</p><p>Related works are presented in section 2, our proposed method is proposed in section 3. The experimental studies and their results are discussed in section 4. Our resulting network achieves the state of the art performance on the Adobe Composition-1k dataset for alpha matte and composite colour quality. It is also the current best performing method on the alphamatting.com online evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Alpha Matting. Cutting out objects has been a key task in image editing and visual media post-production for some time now and the way this is currently done in the industry is through the use of greenscreen and manual rotoscopy, which are the only reliable methods available to date. Early attempts at automating this task without a greenscreen include the development of sampling strategies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b14">17]</ref> to find suitable candidate pairs for the foreground and background colours F i and B i at each pixel i. These samplings techniques typically lack spatial consistency and it is only in the 2007 landmark paper by <ref type="bibr">Levin et al . [20]</ref>, that a comprehensive spatial prior model for Matting was first proposed. This smoothness model has since then been widely adopted in the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b15">18]</ref> and it is only with the advent of deep convolutional neural networks that more expressive spatial priors have been designed <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b21">24]</ref>.</p><p>Deep Alpha Matting Network Architectures. The α-matting networks have mainly been designed as adaptions of segmentation architectures. In 2017 by Xu et al . <ref type="bibr" target="#b35">[37]</ref> first proposed in Deep Image Matting to adapt the FCN architecture of <ref type="bibr" target="#b30">[32]</ref> and adjoin, to the RGB image, the user trimap as an extra channel. From there on, the focus has shifted towards improving the core encoder-decoder architecture, so as to increase the resolution of the output predictions. For instance, in VDRN Matting (2019) <ref type="bibr" target="#b31">[33]</ref> deeper encoder and decoder residual networks have been proposed. In IndexNet (2019) <ref type="bibr" target="#b21">[24]</ref>, the encoder-decoder uses a learnable index pooling. In Context Aware Matting (2019) <ref type="bibr" target="#b12">[13]</ref>, two encoders with atrous convolutions are fused together. Some other works have also tried to better exploit the user generated trimap. In GCA Matting (2020) <ref type="bibr" target="#b18">[21]</ref>, an attention mechanism is designed to use the trimap as a guide. In <ref type="bibr" target="#b2">[3]</ref>, a two stage strategy is proposed to first refine the original trimap then proceed with alpha matting.</p><p>Losses. The exact nature of the output α-matte is a key aspect of matting. On one hand, α = {0, 1} represents a binary label field similar to the one found in binary segmentation. On other hand, the α-matte is also a continuous field with values between 0 and 1, which shares some resemblance to natural images. This opens up a wide range of possible losses. In Deep Image Matting, Xu et al . <ref type="bibr" target="#b35">[37]</ref> originally proposed a simple Huber loss on α. In later works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">21]</ref> the L 1 loss was preferred instead. The absolute values of α may however not be as important as the gradient of α. Indeed, errors in the reproduction of the hair strands shapes are more noticeable than slight errors in the overall opacity level. Gradient fidelity is in fact one of the commonly used quality metric in image matting benchmarks. Gradient related losses include the use of L 1 gradient loss <ref type="bibr" target="#b32">[34]</ref> and the use of the pyramid Laplacian loss (see Context Aware Matting <ref type="bibr" target="#b12">[13]</ref>). Beyond these set losses, <ref type="bibr" target="#b22">Lutz et al . (2018)</ref>  <ref type="bibr" target="#b22">[25]</ref> have also proposed a discriminative loss on α as part of their GAN architecture.</p><p>The matting problem is however not limited to the α field alone as (α, F, B) are interdependent. In Deep Image Matting, Xu et al . <ref type="bibr" target="#b35">[37]</ref> thus proposed to combine their Huber loss on α with a compositing loss L c , which is defined as the reconstruction error L c (α) = i C i − (α i F i + (1 −α i )B i ) 1 when using the ground truth foreground and background colours F i and B i at pixel i. This loss has then been used in later works <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b31">33]</ref>.</p><p>Foreground &amp; Background Predictions. A few prior non-deep learning methods have proposed to predict F and B along side α [26,11,1] but it is only very recently that Hou and Liu (2019) <ref type="bibr" target="#b12">[13]</ref> and   <ref type="bibr" target="#b32">[34]</ref> have started to look into jointly estimating fg/bg colours with alpha in CNNs. In <ref type="bibr" target="#b12">[13]</ref>, F and α are decoded in sequence from the same shared decoder. In <ref type="bibr" target="#b32">[34]</ref>, the estimation is also done in the sequence B, F and α, using this time three full separate encoder-decoder networks. The main drawback here is that sequential estimation depends on the success of each of the individual predictions. Also, stacking full encoder-decoders as in <ref type="bibr" target="#b32">[34]</ref> results in a very deep and large networks.</p><p>These works have chosen the L 1 loss on the predicted F and B. In <ref type="bibr" target="#b12">[13]</ref>, they also use a VGG16 Features Loss on the predicted pre-multiplied ForegroundαF.</p><p>Also predictingB allows <ref type="bibr" target="#b32">[34]</ref> to introduce a new composition loss for the predictedF andB, based on the ground-truth values of α:</p><formula xml:id="formula_1">L c (F,B) = i C i − α iFi + (1 − α i )B i 1 .</formula><p>Other losses are however possible. One particular loss we want to investigate is the exclusion loss (L excl (F,B) = i ∇F i 1 ∇B i 1 ), which has been introduced in a similar form in the reflection removal literature <ref type="bibr" target="#b37">[39]</ref> to enforce a clean separation between F and B and avoid that structures of the original image to leak into both F and B.</p><p>3 Proposed Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Similarly to most previous works, we use an encoder-decoder with Unet <ref type="bibr" target="#b25">[28]</ref> style architecture. The main difference is that our network also predicts F and B directly from this single encoder-decoder. Jointly estimating for α, F and B is motivated by the applications in compositing requiring an estimate for the foreground, and also by results in Multi-Task learning <ref type="bibr" target="#b26">[29]</ref>. We choose to do this in the simplest way by extending the number of output channels from one to seven (1 for α, 3 for F and 3 for B). In contrast to previous works <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b32">[34]</ref>, which adopt a sequential prediction, our approach requires minimal extra parameters and avoids the delicate chaining of estimations.</p><p>The encoder architecture is ResNet-50, and the weights are initialised by pretraining on ImageNet <ref type="bibr" target="#b6">[7]</ref> for classification <ref type="bibr" target="#b24">[27]</ref>. Two modifications are made to the encoder network.</p><p>First, we increase the number of input channels from 3 to 9 to allow for the extra trimap. We encode the trimap using Gaussian blurs of the definite foreground and background masks at three different scales (in a similar way to the method of <ref type="bibr" target="#b16">[19]</ref> in interactive segmentation). This encoding differs from existing approaches in deep image matting, as they usually encode the trimap as a single channel with value 1 if foreground, 0.5 for unknown and 0 for background.</p><p>Second, we remove the striding from 'layer 3' and 'layer 4' of ResNet-50 and increase the dilation to 2 and 4 respectively, in a similar way to what was proposed in AlphaGAN. In this way, the information can be processed at the highest scales without lowering the tensor resolution.</p><p>The Decoder then passes encoder features to a Pyramid Pooling layer <ref type="bibr" target="#b38">[40]</ref>. The pooled features are fed into a decoder which contains seven convolutional layers interleaved with three bilinear upsampling layers and skip connections. We provide full network details in the supplementary materials.</p><p>The Output Layer contains 7 channels for α, F, B. Experiments in section 4 show that clamping the values of α between 0 and 1 with a hardtanh activation as in Deep Image Matting <ref type="bibr" target="#b35">[37]</ref> gives a small improvement over using a sigmoid function as in other previous works <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. The F, B channels also go through a sigmoid activation functions so a to also stay in the [0, 1] range. </p><formula xml:id="formula_2">L α 1 = i αi − αi 1 L α c = i Ci −αiFi − (1 −αi)Bi 1 L α lap = 5 s=1 2 s−1 L s pyr (α) − L s pyr (α) 1 L α g = i ∇αi − ∇αi 1 L FB 1 = i F i − Fi 1 + B i − Bi 1 L FB excl = i ∇Fi 1 ∇Bi 1 L FB c = i Ci − αiF − (1 − αi)B) 1 L FB lap = L F lap + L B lap</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Batch Normalisation vs. Group Normalisation</head><p>Training Matting networks can take a long time and small implementation details can sometimes matter. One such detail that we discovered to have critical importance is the mini-batch size. All prior matting works have adopted a relatively small mini-batch size of around 6-16 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">21]</ref>. But through observing opensource re-implementations[14] of the Deep Image Matting method we found that a mini-batch size of one can greatly increase the network accuracy in the original Deep Image Matting <ref type="bibr" target="#b35">[37]</ref> paper. A mini-batch size of one is however incompatible with our ResNet-50 encoder as ResNet-50 uses Batch-Normalisation, which assumes i.i.d batches larger than size 1. We propose thus to use instead Group Normalisation (32 channels per group) with Weight Standardisation <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b24">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">F, B, α Losses</head><p>As discussed in Section 2, numerous losses have been proposed to improve the training of alpha matting networks. To train our model we use a sum of all of these losses; the L 1 loss on alpha L α 1 , the composition loss L α c , the gradient loss L α g , and the Laplacian pyramid loss that is computed over multiple scales s of the Laplacian pyramid L s pyr . For training the foreground and background we also use a sum of loss functions; L FB 1 , laplacian loss L FB lap , compositional loss L FB c , and a gradient exclusion loss L FB excl . See <ref type="table" target="#tab_0">Table 1</ref> for the definition of each loss function. Our final loss function is</p><formula xml:id="formula_3">L F Bα = L α 1 + L α c + L α g + L α lap + 0.25 L FB 1 + L FB lap + L FB excl + L FB c<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4F,B,α Fusion</head><p>One limitation of our joint prediction approach is that our predictions forα, F andB are decoupled and, even if they are based on the same decoder, the relationship given by the compositing <ref type="figure">Equation 1</ref> is not explicitly enforced. We propose here a fusion mechanism based on the maximum likelihood estimate of p(α, F, B|α,F,B). By assuming independence of the prediction errors and ignoring any spatial dependence between pixels, we can build a simplified likelihood model, derived from the individual and reconstruction losses:</p><formula xml:id="formula_4">p(α, F, B|α,F,B) ∝ p(α|α)p(F|F)p(B|B)p(α, F, B)<label>(3)</label></formula><p>Assuming Gaussian distributions for the predictions and reconstruction errors:</p><formula xml:id="formula_5">p(F|F) ∝ exp − F −F 2 2 2σ 2 F B p(B|B) ∝ exp − B −B 2 2 2σ 2 F B p(α|α) ∝ exp − (α −α) 2 2σ 2 α p(α, F, B) ∝ exp − C − αF − (1 − α)B 2 2 2σ 2 C</formula><p>Essentially, we have simplified the model by ignoring the spatial losses (gradient and Laplacian pyramid) and replaced the L 1 losses by L 2 losses. This simplified model still yields a non-linear optimisation because of the reconstruction term is non-linear but we can adopt an iterative block solver approach. Starting at F (0) =F,B (0) =B,α (0) =α, the update equations are as follows:</p><formula xml:id="formula_6">F (n+1) =F + σ 2 F σ 2 Cα (n) C −α (n)F(n) − (1 −α (n) )B (n)<label>(4)</label></formula><formula xml:id="formula_7">B (n+1) =B + σ 2 B σ 2 C (1 −α (n) ) C −α (n)F(n) − (1 −α (n) )B (n) (5) α (n+1) =α (n) + σ 2 α σ 2 C (C −B (n+1) ) (F (n+1) −B (n+1) ) 1 + σ 2 α σ 2 C (F (n+1) −B (n+1) ) (F (n+1) −B (n+1) )<label>(6)</label></formula><p>These equations are related to the Bayesian Matting estimation scheme <ref type="bibr" target="#b5">[6]</ref>, except that the covariance matrices for F, B, C are not available. In practice, we found that 1 iteration through these block updates was enough. These equations give us a simple mechanism to fuse the 3 predictions by taking into account the matting model of Eq. 1. Our experiments in section 4, with σ 2 C , σ 2 F , σ 2 B = 1, σ 2 α = 10, show that these updated estimates reliably produce better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Dataset and Modifications</head><p>The large Deep Image Matting dataset constructed by Xu et al. <ref type="bibr" target="#b35">[37]</ref> has been instrumental for training state of the art matting algorithms in recent years. The dataset is a collection of 431 foreground and alpha channel pairs. Training samples are created by using the alpha channel and the compositing equation 1 to composite the foreground onto a randomly chosen background from the MSCOCO dataset <ref type="bibr" target="#b19">[22]</ref>.</p><p>The issue is that the foreground images provided are only valid for non-zero alpha values and data augmentation during training can 'spill' invalid colours into these areas. For example, operations like resize and rotation re-sample pixels indiscriminately from both valid and invalid regions (see <ref type="figure">Figure 2</ref>). To remedy this, we re-estimated the foreground colours for all images, using Levin's F, B estimation technique <ref type="bibr" target="#b17">[20]</ref>. This allowed us to extend the foreground estimation to the entire picture, and not only areas where α &gt; 0. That way, augmentation techniques become possible and makes this modified Deep Image Matting training set suitable for foreground prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training Details</head><p>Similarly to what can be found in the literature, our training patches of dimensions 640 × 640, 480 × 480, 320 × 320 are randomly cropped from unknown regions of the trimap. The training trimaps are generated from the ground truth α-mattes by random erosion and dilation of 3 to 25 pixels. For data augmentation, we adopt random flip, mirroring, gamma, and brightness augmentations. To further increase the dataset diversity, we randomly composite a new foreground object with 50% probability, as in <ref type="bibr" target="#b32">[34]</ref>. The training data is shuffled after each epoch. Additionally, every second mini-batch is sampled from the 2× image of the previous image, so as to increase the invariance to scale.</p><p>We use step-decay learning rate policy with the RAdam optimiser <ref type="bibr" target="#b20">[23]</ref>, with momentum and weight decay set to 0.9 and 0.0001 respectively. The initial learning rate is set at 10 −5 and then dropped to 10 −6 at 40 epochs and fine-tuned for 5 more epochs. We apply weight decay of 0.005, 10 −5 to convolutional weights, and the GN parameters respectively. The training process takes 16 days with a single 1080ti GPU. During inference, the full-resolution input images and trimaps are concatenated as 4-channel input and fed into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Test Time Augmentation (TTA)</head><p>Convolutional neural networks are not invariant to flipping, rotation, zooming and cropping of the input. Randomly augmenting the training samples in these non-destructive ways has the effect of enlarging the training set and improve the final network accuracy <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b18">21]</ref>. However, training with augmented examples does not ensure total invariance to the augmentations. Predictions from augmented inputs are normally distributed, and the average of the transformations tends to the true value <ref type="bibr" target="#b33">[35]</ref>. Test-time eight-way rotation of the input image and trimap was used by Tang et al. <ref type="bibr" target="#b31">[33]</ref> but its influence on alpha matte accuracy was not published. We use a comprehensive test-time augmentation, combining rotation, flipping and scaling, and results are shown in <ref type="table" target="#tab_3">Tables 4 and 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section we report quantitative and qualitative results of our model. We perform an ablation study of our model and we also show state of the art results <ref type="figure">Fig. 2</ref>. Colour-spill from Data Augmentation. From left to right: the groundtruth α-matte and ground-truth Fg colours as provided in <ref type="bibr" target="#b35">[37]</ref>, our re-estimated extended Fg colours, a composite image of the provided F after a resize augmentation on an orange background, our composite after the same resize operation. Resizing can cause the undefined Fg colours (in dark blue) to spill into the composite image.</p><p>in comparisons to existing matting methods. We use the Composition-1k <ref type="bibr" target="#b35">[37]</ref> dataset for testing, as it contains 1000 testing images and other methods have reported their results on it for comparison. The dataset provides 50 ground truth foreground images and alpha mattes, and they are composed onto 20 different backgrounds each from Pascal <ref type="bibr" target="#b8">[9]</ref>.</p><p>We evaluate the results of both the alpha matte and also the combinedαF foreground composite; supplemental results of the background prediction can be found in the supplemental material.</p><p>The alpha matte results are computed using four standard metrics <ref type="bibr" target="#b7">[8]</ref>, Sum of Absolute Differences (SAD), Mean Squared Error (MSE), Gradient Error (GRAD) and Connectivity Error (CONN). The gradient and connectivity metrics were shown at the time to be more aligned to human perception of matte quality. To measure foreground composite results we measure the MSE and SAD of the predictedαF and the ground truth αF.</p><p>Most of the ablation study is done over a 20 epochs training. The training was pushed to 45 epochs, including 5 epochs of fine-tuning at a 10 −6 learning rate, on the most complete models.</p><p>Because of the scale of the experiments -each training taking two to four weeks to complete-the experiments have only been done on a single training run. Precise confidence intervals are thus not known but our earlier experiments seem to suggest that most of the observations made below are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluating the α Loss Functions</head><p>As discussed previously, four existing alpha matting losses emerge as reasonable options to be summed for training our network. These are the L 1 loss, the composition loss, the laplacian loss and the gradient loss. Here we evaluate our network trained with combinations of these losses, models <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref> in <ref type="table" target="#tab_1">Table 2</ref>. We see that the compositing loss decreases errors for all metrics except for the gradient error. The laplacian loss proposed by Hou et al . <ref type="bibr" target="#b12">[13]</ref> gives a significant reduction in errors across all metrics. We note this network, training and loss configuration is enough to achieve state of the art results, see <ref type="table" target="#tab_3">Table 4</ref>. The gradient loss proposed by Tang et al. <ref type="bibr" target="#b32">[34]</ref> seems to increase the errors on all metrics. This was also reported in <ref type="bibr" target="#b18">[21]</ref>. As this discovery was made late in our research, the gradient loss is included in all of our subsequent models. We leave the further examination of this loss to future work. We also perform an experiment on the choice of alpha channel activation function ( <ref type="table" target="#tab_1">Table 2)</ref>. A clipping activation was used in the original Deep Image Matting work <ref type="bibr" target="#b35">[37]</ref>, yet all subsequent works used a sigmoid activation, without reference to this change. However, we find that the sigmoid activation underperforms compared to the clipping activation (models (6) vs. <ref type="formula">(7)</ref>), see also <ref type="table">Table 3</ref> model <ref type="formula">(9)</ref> vs. (10). This also moves against the trend to use sigmoid for other image-to-image translation tasks <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b36">38]</ref>, however in these cases the rgb values are not usually 0 or 1 unlike alpha mattes, which are mostly 0 or 1. The sigmoid activation only achieves 0 and 1 for infinite valued inputs. Thus we use clipping activation for subsequent models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Batch-Size and BatchNorm vs. GroupNorm</head><p>As discussed in the Proposed Approach section, we discovered that a mini-batch size of 1 greatly increases network accuracy for α-matting. We report the results of an experiment on this in <ref type="table" target="#tab_1">Table 2</ref>. We use a model trained with a batchsize 6 and BatchNorm, model (4), as a baseline. BatchNorm is however, by definition, incompatible with training with mini-batch sizes of 1, so we use Group Normalisation with Weight Standardisation (WS) <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b24">27]</ref> for single image minibatches, model <ref type="bibr" target="#b5">(6)</ref>.</p><p>We also train an intermediate model <ref type="bibr" target="#b4">(5)</ref> with GroupNorm and WS to isolate the effects of batch-size. As expected, we see a significant reduction in error from models <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5)</ref> to model <ref type="bibr" target="#b5">(6)</ref> showing that a batch-size of 1 is best suited to α-matting. When comparing BN to GN with batch-size 6 it is clear that GroupNormalisation gives no hidden advantage to our hypothesis. <ref type="table">Table 3</ref>. Ablation study of foreground results on the Composition-1k dataset. Here</p><formula xml:id="formula_8">L F B = L F B 1 + L F B lap + L F B c .</formula><p>In column two the * indicates that the L F B 1 , L F B lap are computed over the entire image as opposed to just the unknown region of the trimap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>+LF B +L excl output αF α SAD MSE SAD MSE Closed-form Matting <ref type="bibr" target="#b17">[20]</ref> 251.67 22.96 161.3 85.3 Context-Aware Matting <ref type="bibr" target="#b12">[13]</ref> 70.00 11.49 38.1 8.9</p><p>Training at 20 epochs: In <ref type="table">Table 3</ref> we examine the potential benefits of jointly estimating F, B, α over α alone. We measure the MSE and the SAD of the αF composite and the SAD of the alpha alone, across the Composition-1k testing set. We also record an ablation study of our method to show the benefit of each component. We see that using our foreground and background loss L FB maintains alpha matte accuracy and allows for foreground prediction, model (6) vs. <ref type="bibr" target="#b8">(9)</ref>. Our foreground, background exclusion loss gives a minor benefit across all metrics, (8) vs. <ref type="bibr" target="#b8">(9)</ref>. We observe an interesting trend in foreground prediction results from Context-Aware Matting <ref type="bibr" target="#b12">[13]</ref>, see <ref type="figure">Figure 4</ref>. The predicted foreground is quite poor in areas near the boundary or in highly transparent regions, and radically incorrect in nearby regions where α = 0. This causes issues of colour bleeding artifacts when compositing onto novel backgrounds. We believe this is due to them only computing the foreground loss on the regions where α &gt; 0, and this leads to the network not prioritising foreground prediction in areas of very high transparency. We address this with our novel foreground dataset that has valid foreground ground truth for all pixels so we can compute the foreground prediction loss over the entire image. Our dataset also allows us to use the laplacian loss function for the foreground loss. In <ref type="table">Table 3</ref> we see that training with the L FB throughout the entire image we improve the composite αF results, (10) vs. <ref type="bibr" target="#b10">(11)</ref>.</p><p>Our F, B, α fusion method, used at test-time, improves results for composite quality and for alpha matte quality, (11) vs. Ours FBα . This small improvement has been consistently observed in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Other Works</head><p>We compare our models Ours α and Ours FBα with existing state of the art approaches to alpha matting.</p><p>The Composition-1k dataset On the Composition-1k dataset, <ref type="table" target="#tab_3">Table 4</ref>, each of our models significantly outperform previous approaches on all four metrics. We see that both our models Ours α and Ours FBα achieve state of the art results. The model Ours FBα which includes the foreground and background loss, with fusion, has improved connectivity and gradient measures but worse on the SAD measure. Our best model Ours FBα TTA with test time augmentation achieves an average reduction in error of 34% over the best performing method for each metric, and the gradient error has the largest relative improvement. We give a qualitative comparison of alpha matte quality in <ref type="figure">Figure 3</ref>. Our method can separate very fine structures from the background even where the foreground and background colours are very similar. In <ref type="figure">Figure 4</ref> we display the foreground, background and alpha outputs of our method and compare our predicted composite to that of Context-Aware matting <ref type="bibr" target="#b12">[13]</ref>. We see that for CA Matting the foreground prediction can be quite poor in areas where alpha is close to zero, and colour spill is still visible in theαF composite. Our method, on the other hand, matches the ground truth much more closely. In the first image our background prediction successfully removes the glass from the image, which shows our method has the additional use for removing semi-transparent objects from images, e.g. watermarks.</p><p>Input Image Trimap Closed Form <ref type="bibr" target="#b17">[20]</ref> KNN <ref type="bibr" target="#b3">[4]</ref> DCNN <ref type="bibr" target="#b4">[5]</ref> IFM <ref type="bibr" target="#b0">[1]</ref> Deep Matting <ref type="bibr" target="#b35">[37]</ref> IndexNet <ref type="bibr" target="#b21">[24]</ref> CA <ref type="bibr" target="#b12">[13]</ref> GCA <ref type="bibr" target="#b18">[21]</ref> Ours FBα TTA Ground Truth</p><p>Input Image Trimap Closed Form <ref type="bibr" target="#b17">[20]</ref> KNN <ref type="bibr" target="#b3">[4]</ref> DCNN <ref type="bibr" target="#b4">[5]</ref> IFM <ref type="bibr" target="#b0">[1]</ref> Deep Matting <ref type="bibr" target="#b35">[37]</ref> IndexNet <ref type="bibr" target="#b21">[24]</ref> GCA <ref type="bibr" target="#b18">[21]</ref> CA <ref type="bibr" target="#b12">[13]</ref> Ours FBα TTA Ground Truth <ref type="figure">Fig. 3</ref>. Qualitative comparison of the alpha matte results on the Adobe Composition-1k test set <ref type="bibr" target="#b35">[37]</ref>.  <ref type="figure">Fig. 4</ref>. Qualitative foreground, background and alpha matte results on the Adobe Composition-1k test set <ref type="bibr" target="#b35">[37]</ref>. <ref type="table">Table 5</ref>. Our scores in the alphamatting.com benchmark <ref type="bibr" target="#b7">[8]</ref> together with the topperforming published methods. S, L, U denote the three trimap sizes, small, large and user, included in the benchmark. The scores denote the average rank of each method in these metrics across the 8 evaluation images. The alphamatting.com Dataset The alphamatting.com benchmark is an established online evaluation for natural image matting methods. It includes 27 training images and 8 testing images with 3 different kinds of trimaps, namely, small, large and user. Although there are only 8 testing images, it serves as an important tool for comparing alpha matting methods as the ground truth mattes are not released and the benchmark contains entries from most popular matting methods. In <ref type="table">Table 5</ref> we see that our method, Ours FBα TTA, has the lowest average rank among the top four previous approaches. In particular we see that our method is a clear leader in the connectivity metric where Deep-Learning based matting methods have usually performed poorly. For comparison, Deep Image Matting <ref type="bibr" target="#b35">[37]</ref> ranks best among these methods for connectivity yet is ranked 9th overall, behind Closed-Form Matting <ref type="bibr" target="#b17">[20]</ref> which ranks 7th, our method ranks 1st. We also submitted our model Ours α TTA for to evaluate the effect of our foreground and background loss and fusion, and we found that the Ours FBα TTA performed best in all metrics except gradient. The full tables together with alpha matte predictions are available on www.alphamatting.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a deep-learning method for simultaneously estimating the foreground, background and the alpha map from a single natural image and trimap. Our primary contribution to the alpha matting training regime is to use a batch-size of one. This, combined with longer training at 45 epochs and TTA, has a bigger impact than the choices of loss functions. Finally, our proposed solution for simultaneous α, F, B prediction, can achieve state of the art performance for all predictions, without increased computational or memory cost. Our reworked foreground dataset, new exclusion loss and fusion mechanism improve the final composite quality and can easily be incorporated in future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Training Loss Functions.</figDesc><table><row><cell>α Losses</cell><cell>F, B Losses</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of α-mattes results on the Composition-1k dataset.</figDesc><table><row><cell cols="4">Model Norm. Batch-Size Loss</cell><cell cols="2">MSE SAD GRAD CONN</cell></row><row><cell cols="3">Training at 20 epochs:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(1)</cell><cell>BN</cell><cell>6</cell><cell>L α 1</cell><cell>11.2 36.3 14.9</cell><cell>32.5</cell></row><row><cell>(2)</cell><cell>BN</cell><cell>6</cell><cell>L α 1 + L α c</cell><cell>9.1 34.5 15.0</cell><cell>31.3</cell></row><row><cell>(3)</cell><cell>BN</cell><cell>6</cell><cell>L α 1 + L α c + L α lap</cell><cell>7.4 33.5 12.9</cell><cell>28.5</cell></row><row><cell>(4)</cell><cell>BN</cell><cell>6</cell><cell>L α 1 + L α c + L α lap + L α g</cell><cell>8.1 36.3 13.8</cell><cell>32.0</cell></row><row><cell>(5)</cell><cell>GN</cell><cell>6</cell><cell>L α 1 + L α c + L α lap + L α g</cell><cell>10.3 36.2 15.1</cell><cell>32.0</cell></row><row><cell>(6)</cell><cell>GN</cell><cell>1</cell><cell>L α 1 + L α c + L α lap + L α g</cell><cell>7.2 32.8 13.3</cell><cell>28.6</cell></row><row><cell>(7)</cell><cell>GN</cell><cell>1</cell><cell cols="2">L α 1 + L α c + L α lap + L α g + clip α 6.9 31.2 12.9</cell><cell>27.1</cell></row><row><cell cols="3">Training at 45 epochs:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Oursα GN</cell><cell>1</cell><cell cols="2">L α 1 + L α c + L α lap + L α g + clip α 5.3 26.5 10.6</cell><cell>21.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Alpha map results on the Composition-1k test set<ref type="bibr" target="#b35">[37]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">SAD MSE x10 3 Gradient Connectivity</cell></row><row><cell>Closed-Form Matting [20]</cell><cell>168.1</cell><cell>91.0</cell><cell>126.9</cell><cell>167.9</cell></row><row><cell>KNN-Matting [4]</cell><cell>175.4</cell><cell>103.0</cell><cell>124.1</cell><cell>176.4</cell></row><row><cell>DCNN Matting [5]</cell><cell>161.4</cell><cell>87.0</cell><cell>115.1</cell><cell>161.9</cell></row><row><cell>Information-flow Matting [1]</cell><cell>75.4</cell><cell>66.0</cell><cell>63.0</cell><cell>-</cell></row><row><cell>Deep Image Matting [37]</cell><cell>50.4</cell><cell>14.0</cell><cell>31.0</cell><cell>50.8</cell></row><row><cell>AlphaGan-Best [25]</cell><cell>52.4</cell><cell>30.0</cell><cell>38.0</cell><cell>-</cell></row><row><cell>IndexNet Matting [24]</cell><cell>45.8</cell><cell>13.0</cell><cell>25.9</cell><cell>43.7</cell></row><row><cell>VDRN Matting [33]</cell><cell>45.3</cell><cell>11.0</cell><cell>30.0</cell><cell>45.6</cell></row><row><cell>AdaMatting [3]</cell><cell>41.7</cell><cell>10.2</cell><cell>16.9</cell><cell>-</cell></row><row><cell cols="2">Learning Based Sampling [34] 40.4</cell><cell>9.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Context Aware Matting [13]</cell><cell>35.8</cell><cell>8.2</cell><cell>17.3</cell><cell>33.2</cell></row><row><cell>GCA Matting [21]</cell><cell>35.3</cell><cell>9.1</cell><cell>16.9</cell><cell>32.5</cell></row><row><cell>Oursα</cell><cell>26.5</cell><cell>5.3</cell><cell>10.6</cell><cell>21.8</cell></row><row><cell>OursFBα</cell><cell>26.4</cell><cell>5.4</cell><cell>10.6</cell><cell>21.5</cell></row><row><cell>OursFBα TTA</cell><cell>25.8</cell><cell>5.2</cell><cell>10.6</cell><cell>20.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Art and Science of Digital Compositing: Techniques for Visual Effects, Animation and Motion Graphics (The Morgan Kaufmann Series in Computer Graphics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brinkman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">KNN matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptually motivated benchmark for video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="99" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Foamliu: Deep-image-matting-pytorch</title>
		<ptr target="https://github.com/foamliu/Deep-Image-Matting-PyTorch" />
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
			<date type="published" when="2010" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995495</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2011.5995495" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/huochaitiantang/pytorch-deep-image-matting" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>) 14. huochaitiantang: Pytorch implementation of deep image matting</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://github.com/Joker316701882/Deep-Image-Matting" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Deep-image-matting</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alpha matting with kl-divergence-based sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4523" to="4536" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alpha matting with kl-divergence-based sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4523" to="4536" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive boundary prediction for object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04069</idno>
		<title level="m">Natural image matting via guided contextual attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alphagan: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous foreground, background, and alpha estimation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<title level="m">Weight standardization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2013.88</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.88" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving image matting using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep residual network for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEUROCOMPUTING</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4786" to="4794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
