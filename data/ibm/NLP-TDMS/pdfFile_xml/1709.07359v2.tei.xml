<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-Splitting Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><forename type="middle">L</forename><surname>Grinblat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">French Argentine International Center for Information and Systems Sciences</orgName>
								<orgName type="institution" key="instit1">CIFASIS</orgName>
								<orgName type="institution" key="instit2">UNR-CONICET</orgName>
								<address>
									<postCode>S2000EZP</postCode>
									<settlement>Rosario</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">C</forename><surname>Uzal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">French Argentine International Center for Information and Systems Sciences</orgName>
								<orgName type="institution" key="instit1">CIFASIS</orgName>
								<orgName type="institution" key="instit2">UNR-CONICET</orgName>
								<address>
									<postCode>S2000EZP</postCode>
									<settlement>Rosario</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">M</forename><surname>Granitto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">French Argentine International Center for Information and Systems Sciences</orgName>
								<orgName type="institution" key="instit1">CIFASIS</orgName>
								<orgName type="institution" key="instit2">UNR-CONICET</orgName>
								<address>
									<postCode>S2000EZP</postCode>
									<settlement>Rosario</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Class-Splitting Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(Under consideration at Pattern Recognition Letters)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The irruption of Generative Adversarial Nets (GAN) <ref type="bibr" target="#b0">[1]</ref> produced a great leap for image data generation. Samples were generated simply by applying a neural network transform to an input random vector sampled from a uniform distribution. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples <ref type="bibr" target="#b0">[1]</ref>. GAN based generative models did not take long to reach impressive image quality <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> at least for some specific datasets.</p><p>However, current GAN models cannot produce convincing samples when trained on datasets of images with high variability, even for relatively low resolution images. On the other hand, it is observed that sample quality improves when class information is taken into account in a conditional GAN setup <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. These findings suggest that it is hard to learn a multimodal distribution from a smooth transform of a uniform (or Gaussian) distribution and that providing categorical class information to the generator alleviates this problem.</p><p>Our proposal is inspired in two observations. First, as mentioned above, conditioning generation with categorical class labels with high level of abstraction improves image quality. Second, as early observed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, the adversarial network pair learns an useful hierarchy of representations in an unsupervised setup. We propose to exploit the same representation space learned by the GAN model in order to generate new class labels with a high level of abstraction. This is done by applying a simple clustering method in this representation space. By conditioning generation with this new class labels the model is able to generate better samples. This can be done either when prior class information is available or not.</p><p>The main contributions of the present paper are 1 :</p><p>• We propose a method for increasing the number of class labels during conditional GAN training based on clustering in the representation space learned by the same GAN model. We base our implementation on the more stable Wasserstein GAN formulation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>• We show an effectively way of adapting networks architecture to handle this increasing number of classes.</p><p>• We show that this splitting GAN strategy improves samples quality both in the supervised and unsupervised tasks, reaching state-of-theart Inception scores for CIFAR-10 and STL-10 datasets in both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In the original GAN formulation <ref type="bibr" target="#b0">[1]</ref>, the generator is a neural network that transform noise input z ∼ p(z) into fake samples and the discriminator D is a neural network with a single scalar output with a sigmoid activation. This output is interpreted as the model probability for its input being a real image from the dataset distribution against being a fake image sampled by generator G. The discriminator D is trained using the standard binary classification formulation by minimizing the binary cross-entropy between fake and real distributions. On the other hand, the generator G is simultaneously trained to mislead the discriminator. This is accomplished in practice by updating G parameters minimising the same loss but with fake samples tagged with a 'true' label <ref type="bibr" target="#b0">[1]</ref>. In other words, the discriminator is updated by gradient descent over a negative log likelihood loss</p><formula xml:id="formula_0">L D = − E x∼Pr [log(D(x))] − E z∼p(z) [log(1 − D(G(z)))],<label>(1)</label></formula><p>while the generator minimizes</p><formula xml:id="formula_1">L G = − E z∼p(z) [log(D(G(z)))].<label>(2)</label></formula><p>The main issue in the original GAN formulation was the instability of the training process that made very hard to improve architectures and to scale up to bigger images. In <ref type="bibr" target="#b1">[2]</ref> a deep convolutional architecture was proposed for both generator and discrimi-nator which presents some degree of stability for adversarial training. This work was the first one producing convincing image samples for datasets with low variability (Bedrooms and Faces) and relatively low resolution (64x64). However, standard GAN formulation fails to generate globally consistent samples when trained on datasets with high variability like ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AC-GAN</head><p>In order to tackle datasets with high variability, Odena et al. <ref type="bibr" target="#b6">[7]</ref> proposed to improve the quality of the generated samples by adding more structure to the GAN latent space and an auxiliary classifier. This approach requires the dataset to include class labels, i.e. to work in a supervised setting. The generator receives the noise vector z and also the selected label c so that the generated sample is a function of both. Furthermore, the discriminator has, in addition to the usual objective, the task of correctly classifying the real and generated samples (through an auxiliary classifier). The generator is optimized not only to deceive the discriminator, but also to generate fake samples that minimize the auxiliary classifier error, i.e. to produce well class-defined samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">WGAN</head><p>In order to address the problem of instability in GAN training, Arjovsky et al. in a series of works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref> proposed a reformulation of the function to be optimized. They argue that the original loss function presents discontinuities and vanishing gradients with respect to generator parameters. Instead, they proposed a distance for distributions known as Earth-Mover distance or Wasserstein-1, which captures the cost of transporting mass in order to transform one distribution into the other. From this distance they derive the WGAN loss function for the minimax objective</p><formula xml:id="formula_2">min G max D∈D E x∼Pr [D(x)] − E z∼p(z) [D(G(z))]<label>(3)</label></formula><p>where D (called critic in WGAN formulation) is not anymore a binary classifier and is restricted to be in the set D of 1-Lipschitz functions. Again, z is a noise vector sampled from a simple distribution (uniform or Gaussian distribution). The Lipschitzness of D was imposed by weight clipping in this first version of WGAN. The importance of Arjovsky's contribution lies on a gain in the robustness of the adversarial training process and a reduction in the frequency of the mode collapse phenomenon. Furthermore, the proposed loss function correlates well with the observed sample quality as opposed to the original GAN loss which gives little information about training progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">WGAN-GP</head><p>An improved version of WGAN was recently proposed by Gulrajani et al. <ref type="bibr" target="#b8">[9]</ref>. They found that the weight clipping can cause convergence problems in some settings and propose to enforce the Lipschitz constraint on the critic D by penalizing its gradient's norm.</p><p>The penalty term is computed over a set of random pointsx uniformly sampled from straight lines between real and fake sample points. Naming as Px the latter distribution, the new loss can be written as</p><formula xml:id="formula_3">L = Ẽ x∼Pg [D(x)] − E x∼Pr [D(x)] +λ Ê x∼Px ( ∇xD(x) 2 − 1) 2<label>(4)</label></formula><p>where the penalty coefficient is set to λ = 10. This improved WGAN formulation exhibits high robustness against changing model architecture. The authors tested six different network designs for both G and D, which typically break standard GAN training but show stable WGAN training for all cases. Furthermore, WGAN formulation helps achieving better quality samples. Quantitative results are reported by the authors in terms of the Inception score <ref type="bibr" target="#b2">[3]</ref> over CIFAR-10 dataset, which is the most recurrent benchmark for image generation in GAN literature. In the unsupervised setting (without using any class information) they reach the state-of-the-art, while in the supervised case (following the strategy of AC-GAN and without tuning any hyperparameter nor architecture) they reach the second place behind SGAN <ref type="bibr" target="#b10">[11]</ref> 2 . All this advantageous features made -to our knowledge-WGAN the current standard formulation for adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method: Splitting GAN</head><p>The main idea is to generate new artificial classes based on the representation learned by the last hidden layer of the critic after enough training iterations. This is done by applying k-means to each class set in this representation space (see <ref type="figure" target="#fig_1">Fig. 1</ref>). We divide each set in two clusters only when the class has more samples than a certain threshold. After that, training is resumed replacing the old labels with the new ones for the entire dataset. Algorithm 1 resumes the proposed method.</p><p>With this procedure we need to make two minor modifications to the model architecture before resuming learning:</p><p>1. The auxiliary classifier needs to predict a different number of classes, so we extend the last layer of this classifier adding a copy of the weights of the parent class for each child class.</p><p>2. In the conditional WGAN-GP implementation <ref type="bibr" target="#b11">[12]</ref>, the class labels are injected in each batch normalization layer of the generative network by setting a specific gain and bias parameters (γ and b) for each class. We follow this strategy in our proposal and, for the class splitting, we set the new pair (γ, b) for each child class as γ child = γ f ather + ∆γ and b child = b f ather + ∆b, with initialization ∆γ = 0 and ∆b = 0 when the new classes are created. This formulation implies that child classes start both with the father class params and they eventually become different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>To demonstrate the effectiveness of our proposal, we conducted several experiments with CIFAR-10 [13], a   Improved GAN (-L+HA) <ref type="bibr" target="#b2">[3]</ref> 6.86 ± 0.06 EGAN-Ent-VI <ref type="bibr" target="#b14">[15]</ref> 7.07 ± 0.10 DFM <ref type="bibr" target="#b15">[16]</ref> 7.72 ± 0.13 Splitting GAN ResNet-B (ours) 7.80 ± 0.08 WGAN-GP ResNet-B 7.81 ± 0.10 WGAN-GP ResNet-A <ref type="bibr" target="#b8">[9]</ref> 7.86 ± 0.07 Splitting GAN ResNet-A (ours)</p><p>7.90 ± 0.09 dataset containing 50000 32x32 images corresponding to 10 different classes, and the unlabeled set of STL-10, containing 100000 larger and more diverse images <ref type="bibr" target="#b13">[14]</ref>. We based our model on the improved WGAN algorithm proposed by Gulrajani et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. In all cases, during training, we sample 50000 images from the current model to select the best one so far based on the Inception score. Finally, we sample another 50000 with the best model in order to calculate the reported score, following <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR-10</head><p>With CIFAR-10, an unsupervised test was performed starting from all the examples considered as a single class and dividing them into two clusters every ap- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inception Score Improved GAN <ref type="bibr" target="#b2">[3]</ref> 8.09 ± 0.07 AC-GAN <ref type="bibr" target="#b6">[7]</ref> 8.25 ± 0.07 WGAN-GP ResNet-A <ref type="bibr" target="#b8">[9]</ref> 8.42 ± 0.10 SGAN <ref type="bibr" target="#b10">[11]</ref> 8.59 ± 0.12 WGAN-GP ResNet-B</p><p>8.67 ± 0.14 Splitting GAN ResNet-A (ours)</p><p>8.73 ± 0.08 Splitting GAN ResNet-B (ours)</p><p>8.87 ± 0.09   <ref type="table">Table ?</ref>?</p><p>and <ref type="table">Table ?</ref>?. Also for comparison, samples obtained with the proposed method ( <ref type="figure" target="#fig_2">Figure 2)</ref> and obtained with the WGAN-GP supervised model <ref type="figure" target="#fig_3">(Figure 3</ref>) are shown. <ref type="figure" target="#fig_4">Figure 4</ref> has real samples of CIFAR-10 corresponding to each of the 20 clusters found with our method. <ref type="figure" target="#fig_5">Figures 5 and 6</ref> show generated images and clusters found in the unsupervised test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">STL-10</head><p>We treat STL-10 in the same way as <ref type="bibr" target="#b15">[16]</ref>. That is, we downsample each dimension by 2, resulting in 48x48 RGB images. We tested our algorithm with the ResNet-A architecture, with the minimum changes necessary for the model to generate 48x48 images.    <ref type="table">Table ?</ref>? shows the resulting Inception score. <ref type="figure" target="#fig_7">Figures  7 and 8</ref> show the generated images and the clusters found by the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Several things can be observed from the results presented in the previous section. First, regarding the obtained clusterization of the real samples ( <ref type="figure" target="#fig_4">Figure 4</ref> for the supervised case and <ref type="figure">Figure 6</ref> for the unsupervised one), we can visually find rules that define the vast majority of samples, for at least several clusters. As an example, in the supervised case ( <ref type="figure" target="#fig_4">Figure 4)</ref> we can see in the left side of the fourth row cats looking forward and in the left side of the eighth row horse side views. Compare with cats and horses in several positions corresponding to the clusters in the right side. In the unsupervised case ( <ref type="figure">Figure 6</ref>) we can see a tendency to generate clusters for cars, planes or ships, but in general they are much more mixed.</p><p>Regarding the generated samples in the supervised case <ref type="figure" target="#fig_2">(Figure 2</ref> for our method and <ref type="figure" target="#fig_3">Figure 3</ref> for WGAN-GP), we can see that the class splits allows the model to generate better samples. Not only for the more uniform clusters such as the horse side views or the cats looking forward, but for the whole original where to make a clustering step. Require: kmeans threshold, do not divide classes with less samples than this. while parameters have not converged do Make a WGAN-GP with auxiliary classifier step, as in <ref type="bibr" target="#b8">[9]</ref>. if current iteration is in clustering iterations then for all class in current classes with more than kmeans threshold samples do Propagate through the critic all samples of class up to the last hidden layer. Normalize these representations.</p><p>Apply K-Means to the representations in order to obtain two new child classes. Replace the label class in the dataset with the new child classes. end for for each new child class, child, with parent class parent do</p><p>In the last layer of the auxiliary classifier, copy its parent parameters. for each Batch Normalization layer, i, of the Generator do γ i,child = γ i,parent +∆γ i,child , with initialization ∆γ i,child = 0. b i,child = b i,parent + ∆b i,child , with initialization ∆b i,child = 0. end for end for end if end while <ref type="figure">Figure 6</ref>: The 25 clusters found in the unsupervised case (real CIFAR-10 samples). Each line has two different clusters. class. Compare for example the fourth row (cats) or the eighth row (horses) in <ref type="figure" target="#fig_2">Figure 2</ref> with those rows in <ref type="figure" target="#fig_3">Figure 3</ref>, corresponding to the same model trained with WGAN-GP. Note that these samples do not differ too much from those shown in <ref type="bibr" target="#b8">[9]</ref>. Even in classes where the clustering step does not seem to have found an obvious separation rule, such as cars (second row), a better sample quality can be observed than in the original WGAN-GP.</p><p>In the unsupervised case with CIFAR-10 ( <ref type="figure" target="#fig_5">Figure  5</ref>), although the Inception score is similar than the one obtained by the state-of-the-art so far, the samples generated seem to be of a higher quality. Nevertheless, they do not reach the quality of the generated images in a supervised setting. It is always advisable to start the division into clusters from the predefined classes, if this information is available.</p><p>In the case of STL-10 <ref type="figure" target="#fig_7">(Figure 7)</ref>, there is a noticeable difference in the Inception score. The reason of this may be that STL-10 is a much more diverse dataset, so a division into a large number of classes can be beneficial. It should be noted that in this case the state-of-the-art is much further from the actual dataset score than in the case of CIFAR-10.</p><p>The success of our Splitting GAN method suggests that reinjecting high level information from critic to the generative model improves sampling quality. This breaks the strictly adversarial training and allows some degree of information sharing between both networks. We believe that this simple (but successful) strategy could inspire a new and better adversarial training formulation where a small amount of high level information directly flows from critic's last layers to generator input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work we showed that our Splitting GAN method allows generating better images. This can be seen in the results on CIFAR-10 and STL-10 for which clearly better images were obtained. This is supported by an Inception score well above the previous state-of-the-art for both datasets.</p><p>A future direction of research to improve the current Splitting GAN version is oriented to understand how a given model architecture or dataset determines the optimal number of clusters (or classes). Also, clusterization could be enhanced during adversarial training with the addition of an extra loss term like in <ref type="bibr" target="#b16">[17]</ref>.</p><p>We are also currently working in a generalization of the Splitting GAN ideas following two paths: First, making the high level information from the critic's representation flows continually to the generator, avoiding special purpose steps at predefined times in the training process. Second, avoiding the hard threshold clustering step and replacing it with some sort of continuous representation capturing the same amount of information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our Class Splitting GAN proposal is based on generating new classes by clustering in the representation space learned by the critic. This new classes are used in the standard supervised setup of WGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Samples generated with our Splitting GAN method with supervised training on CIFAR-10 dataset. Each line has samples of one of the original classes. Each side has samples corresponding to one of the two clusters generated for each class. We use ResNet-B architecture (see text for details).proximately 10 epochs. This was done with the Gulrajani's ResNet architecture without changes (named ResNet-A) and a modified version (ResNet-B) doubling the number of maps in each convolutional layer. A supervised test was also conducted with these two architectures, starting from the original 10 classes of CIFAR and dividing them into two at approximately 20 training epochs. For comparison we also trained the ResNet-B architecture with the original WGAN-GP algorithm. The results are detailed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Class-conditioned samples generated with WGAN-GP method<ref type="bibr" target="#b8">[9]</ref> over the ResNet-B architecture for CIFAR-10 dataset. Each row has samples of a given class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Real CIFAR-10 samples corresponding to the 20 clusters found by our method. Each line is divided in the same way as inFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Samples generated with the ResNet-B architecture trained with Splitting GAN over CIFAR-10 without class labels (unsupervised).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Splitting GAN Require: A dataset with initial labels (the same label for all the examples in the unsupervised case). Require: clustering iterations, list of iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Images generated by the model trained on STL-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The 15 clusters found by the model (real STL-10 samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Unsupervised Inception scores on CIFAR-10 Method Inception Score</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Supervised Inception scores on CIFAR-10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Unsupervised Inception scores for STL-10</figDesc><table><row><cell>Method</cell><cell>Inception Score</cell></row><row><cell>Original Dataset [16]</cell><cell>26.08 ± 0.26</cell></row><row><cell>DFM [16]</cell><cell>8.51 ± 0.13</cell></row><row><cell>WGAN-GP ResNet-A</cell><cell>9.05 ± 0.12</cell></row><row><cell>Splitting GAN ResNet-A (ours)</cell><cell>9.50 ± 0.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://github.com/ CIFASIS/splitting_gan</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">By simply duplicating the number of feature maps of Gulrajani's networks we found WGAN outperforms SGAN score. See Sec. 4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>The authors acknowledge grant support from AN-PCyT PICT-2016-4374.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04357</idno>
		<title level="m">Stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<ptr target="https://github.com/igul222/improved_wgan_training" />
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Calibrating energy-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01691</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
