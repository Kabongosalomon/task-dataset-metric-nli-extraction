<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Message Passing for Hyper-Relational Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
							<email>mikhail.galkin@iais.fraunhofer.de</email>
							<affiliation key="aff0">
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
							<email>priyansh.trivedi@iais.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
							<email>gaurav.maheshwari@iais.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
							<email>ricardo.usbeck@iais.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<email>jens.lehmann@iais.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="department">Fraunhofer IAIS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Message Passing for Hyper-Relational Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder -STARE capable of modeling such hyper-relational KGs. Unlike existing approaches, STARE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset -WD50K. Our experiments demonstrate that STARE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of link prediction over knowledge graphs (KGs) has seen a wide variety of advances over the years <ref type="bibr">(Ji et al., 2020)</ref>. The objective of this task is to predict new links between entities in the graph based on the existing ones. A majority of these approaches are designed to work over triplebased KGs, where facts are represented as binary relations between entities. This data model, however, doesn't allow for an intuitive representation of facts with additional information. For instance, in <ref type="figure" target="#fig_0">Fig. 1</ref>.A, it is non-trivial to add information which can help disambiguate whether the two universities attended by Albert Einstein awarded him with the same degree.</p><p>This additional information can be provided in the form of key-value restrictions over instances of binary relations between entities in recent knowledge graph models <ref type="bibr" target="#b34">(Vrandecic and Krötzsch, 2014;</ref><ref type="bibr" target="#b25">Pellissier-Tanon et al., 2020;</ref><ref type="bibr" target="#b15">Ismayilov et al., 2018)</ref>. Such restrictions are known as qualifiers in the Wikidata statement model <ref type="bibr" target="#b34">(Vrandecic and Krötzsch, 2014)</ref> or triple metadata in RDF* <ref type="bibr" target="#b13">(Hartig, 2017)</ref> and RDF reification approaches <ref type="bibr" target="#b10">(Frey et al., 2019)</ref>. These complex facts with qualifiers can be represented as hyper-relational facts (See Sec. 3). In our example ( <ref type="figure" target="#fig_0">Fig. 1.B)</ref>, hyper-relational facts allow to observe that Albert Einstein obtained different degrees at those universities.</p><p>Existing representation learning approaches for such graphs largely treat a hyper-relational fact as an n-ary (n&gt;2) composed relation (e.g., educatedAt academicDegree) <ref type="bibr" target="#b41">(Zhang et al., 2018;</ref><ref type="bibr" target="#b20">Liu et al., 2020)</ref> losing entity-relation attribution; ignoring the semantic difference between a triple relation (educatedAt) and qualifier relation (academicDegree) <ref type="bibr" target="#b12">(Guan et al., 2019)</ref>, or decomposing a hyper-relational instance into multiple quintuples comprised of a triple and one qualifier key-value pair <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref>. In this work, we propose an alternate graph representation learning mechanism capable of encoding hyper-relational KGs with arbitrary number of qualifiers, while keeping the semantic roles of qualifiers and triples intact.</p><p>To accomplish this, we leverage the advances in Graph Neural Networks (GNNs), many of which are instances of the message passing <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref> framework, to learn latent representations of nodes and edges of a given graph. Recently, GNNs have been demonstrated <ref type="bibr" target="#b31">(Vashishth et al., 2020)</ref> to be capable of encoding mutli-relational (tripled based) knowledge graphs. Inspired by them, we further extend this framework to incorporate hyperrelational KGs, and propose STARE 1 , which to the best of our knowledge is the first GNN-based approach capable of doing so (see <ref type="bibr">Sec. 4</ref>).</p><p>Furthermore, we show that WikiPeople <ref type="bibr" target="#b12">(Guan et al., 2019)</ref>, and JF17K <ref type="bibr" target="#b37">(Wen et al., 2016)</ref> -two commonly used benchmarking datasets for LP over hyper-relational KGs exhibit some design flaws, which render them as ineffective benchmarks for the hyper-relational link prediction task (see Sec. 5). JF17K suffers from significant test leakage, while most of the qualifier values in WikiPeople are literals which are conventionally ignored in KG embedding approaches, rendering the dataset largely devoid of qualifiers. Instead, we propose a new hyper-relational link prediction dataset -WD50K extracted from Wikidata <ref type="bibr" target="#b34">(Vrandecic and Krötzsch, 2014</ref>) that contains statements with varying amounts of qualifiers, and use it to benchmark our approach.</p><p>Through our experiments (Sec. 6), we find that STARE based model generally outperforms other approaches on the task of link prediction (LP) over hyper-relational knowledge graphs. We provide further evidence of the fact, independent of STARE, that triples enriched with qualifier pairs provide additional signal beneficial for the LP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early approaches for modelling hyper-relational graphs stem from conventional triple-based KG embedding algorithms, which often simplify complex property attributes (qualifiers). For instance, m-TransH <ref type="bibr" target="#b37">(Wen et al., 2016)</ref> requires star-to-clique conversion which results in a permanent loss of entity-relation attribution. Later models, e.g., RAE <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>, HypE and HSimple introduced in <ref type="bibr" target="#b8">(Fatemi et al., 2020)</ref>, converted hyperrelational facts into n-ary facts with one abstract relation which is supposed to loosely represent a combination of all relations of the original fact.</p><p>Recently, GETD <ref type="bibr" target="#b20">(Liu et al., 2020)</ref> extended TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref> tensor factorization approach for n-ary relational facts. However, the model still expects only one relation in a fact and is not able to process facts of different arity in one dataset, e.g., 3-ary and 4-ary facts have to be split and trained separately.</p><p>NaLP <ref type="bibr" target="#b12">(Guan et al., 2019</ref>) is a convolutional model that supports multiple entities and relations in one fact. However, every complex fact with k qualifiers has to be broken down into k + 2 keyvalue pairs with an artificial split of the main (s,p,o) triple into (p s : s) and (p o : o) pairs. Consequently, all key-value pairs are treated equally thus the model does not distinguish between the main triple and relation-specific qualifiers.</p><p>HINGE <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref> also adopts a convolutional framework for modeling hyper-relational facts. A main triple is iteratively convolved with every qualifier pair as a quintuple followed by min pooling over quintuple representations. Although it retains the hyper-relational nature of facts, HINGE operates on a triple-quintuple level that lacks granularity of representing a certain relation instance with its qualifiers. Additionally, HINGE has to be trained sequentially in a curriculum learning <ref type="bibr" target="#b2">(Bengio et al., 2009</ref>) fashion requiring sorting all facts in a KG in an ascending order of the amount of qualifiers per fact which might be prohibitively expensive for large-scale graphs.</p><p>Instead, our approach directly augments a relation representation with any number of attached qualifiers properly separating auxiliary entities and relations from those in the main triple. Additionally, we do not force any restrictions on input order of facts nor on the amount of qualifiers per fact.</p><p>Parallel to our approach are the methods that work over hypergraphs, e.g., DHNE , Hyper-SAGNN , and knowledge hypergraphs like HypE <ref type="bibr" target="#b8">(Fatemi et al., 2020)</ref>. We deem hyper-relational graphs and hypergraphs are conceptually different. As hyperedges contain multiple nodes, such hyperedges are closer to n-ary relations r(e 1 , . . . , e n ) with one abstract relation. The attribution of entities to the main triple or qualifiers is lost, and qualifying relations are not defined. Combining a certain set of main and qualifying relations into one abstract r k () would lead to a combinatorial explosion of typed hyperedges since, in principle, any relation could be used in a qualifier, and there the amount of qualifiers per fact is not limited. Therefore, modeling qualifiers in hypergraphs becomes non-trivial, and we leave such a study for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>GNNs on Undirected Graphs: Consider an undirected graph G = (V, E), where V represents the set of nodes and E denotes the set of edges. Each node v ∈ V has an associated vector h v and neighbourhood N (v). In the message passing framework <ref type="bibr" target="#b11">(Gilmer et al., 2017)</ref>, the node representations are learned iteratively via aggregating representations (messages) from their neighbors:</p><formula xml:id="formula_0">h k+1 v = UPD h k v , AGGR u∈N (v) φ(h k v , h k u , e vu ) (1)</formula><p>where AGGR(·) and UPD(·) are differentiable functions for neighbourhood aggregation and node update, respectively; h (k) v is the representation of a node v at layer k; e vu is the representation of an edge between nodes v and u.</p><p>Different GNN architectures implement their own aggregation and update strategy. For example, in case of Graph Convolutional Networks (GCNs) (Kipf and Welling, 2017) the representations of neighbours are first transformed via a weight matrix W and then combined and passed through a non-linearity f (·) such as ReLU. A GCN layer k can be represented as:</p><formula xml:id="formula_1">h (k) v = f   u∈N (v) W (k) h (k−1) u  <label>(2)</label></formula><p>GCN and other seminal architectures such as GAT <ref type="bibr" target="#b33">(Velickovic et al., 2018)</ref> and GIN <ref type="bibr" target="#b39">(Xu et al., 2019)</ref> do not model relation embeddings explicitly and require further modifications to support multirelational KGs.</p><p>GNN on Directed Multi-Relational Graphs: In case of a multi-relational graph G = (V, R, E) where R represents the set of relations r, and E denotes set of directed edges (s, r, o) where nodes s ∈ V and o ∈ V are connected via relation r. The GCN formulation by <ref type="bibr" target="#b21">(Marcheggiani and Titov, 2017)</ref> assumes that the information in a directed edge flows in both directions. Thus for each edge (s, r, o), an inverse edge (o, r −1 , s) is added to E. Further, self-looping relations (v, r self , v), for each node v ∈ V are added to E, enabling an update of a node state based on its previous one, and further improving normalization.</p><p>For directed multi-relational graphs, Equation 2 can be extended by introducing relation specific weights W r <ref type="bibr" target="#b21">(Marcheggiani and Titov, 2017;</ref><ref type="bibr" target="#b27">Schlichtkrull et al., 2018)</ref> </p><formula xml:id="formula_2">h (k) v = f   (u,r)∈N (v) W (k) r h (k−1) u  <label>(3)</label></formula><p>However, such networks are known to be overparameterized. Instead, CompGCN <ref type="bibr" target="#b31">(Vashishth et al., 2020)</ref> proposes to learn specific edge type vectors:</p><formula xml:id="formula_3">h (k) v = f   (u,r)∈N (v) W (k) λ(r) φ(h (k−1) u , h (k−1) r )   (4) where φ(·)</formula><p>is a composition function of a node u with its respective relation r, and W λ(r) is a direction-specific shared parameter for incoming, outgoing, and self-looping relations. The composition φ : R d × R d → R d can be any entity-relation function akin to TransE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref> or DistMult <ref type="bibr" target="#b40">(Yang et al., 2015)</ref>.</p><p>Hyper-Relational Graphs:</p><p>In case of a hyper-relational graph G = (V, R, E), E is a list (e 1 , . . . , e n ) of edges with</p><formula xml:id="formula_4">e j ∈ V × R × V × P(R × V) for 1 ≤ j ≤ n, where P denotes the power set. A hyper- relational fact e j ∈ E is usually written as a tuple (s, r, o, Q), where Q is the set of quali- fier pairs {(qr i , qv i )} with qualifier relations qr i ∈ R and qualifier values qv i ∈ V. (s, r, o)</formula><p>is referred to as the main triple of the fact. We use the notation Q j to denote the qualifier pairs of e j . For example, under this representation scheme, one of the edges in <ref type="figure" target="#fig_0">Fig. 1</ref>.B would be (Albert Einstein, educated at, University of Zurich, (academic degree, Doctorate), (academic major, Physics)) r q r1 q v1 q r2 q v2 o ϕ q ϕ q ∑ P 6 9 P 5 1 2 Q 8 4 9 6 9 7 P 8 1 2 Q 8 5 3 0 7 7 Q 2 0 6 7 0 2 <ref type="figure">Figure 2</ref>: The mechanism in which STARE encodes a hyper-relational fact from <ref type="figure" target="#fig_0">Fig. 1</ref>.B. Qualifier pairs are passed through a composition function φ q , summed and transformed by W q . The resulting vector is then merged via γ, and φ r with the relation and object vector, respectively. Finally, node Q937 aggregates messages from this and other hyper-relational edges.</p><formula xml:id="formula_5">W q γ ϕ r ∑ s W λ(r) Q937</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STARE</head><p>In this section, we introduce our main contribution -STARE, and show how we use it for link prediction (LP). STARE (cf. <ref type="figure">Fig. 2</ref> for the intuition) incorporates statement qualifiers {(qr i , qv i )}, along with the main triple (s, r, o) into a message passing process. To do this, we extend Equation 4 by combining the edge-type embedding h r with a fixed-length vector h q representing qualifiers associated with a particular relation r between nodes u and v. The resultant equation is thus:</p><formula xml:id="formula_6">h v = f   (u,r)∈N (v) W λ(r) φ r (h u , γ(h r , h q ) vu )   (5) where γ(·)</formula><p>is a function that combines the main relation representation with the representation of its qualifiers, e.g., concatenation [h r , h q ], elementwise multiplication h r h rq , or weighted sum:</p><formula xml:id="formula_7">γ(h r , h q ) = α h r + (1 − α) h q<label>(6)</label></formula><p>where α is a hyperparameter that controls the flow of information from qualifier vector h q to h r .</p><p>Finally, the qualifier vector h q is obtained through a composition φ q of a qualifier relation h qr and qualifier entity h qv . The composition function φ q may be any entity-relation function akin to φ (Equation 4). The representations of different  </p><formula xml:id="formula_8">h q = W q (qr,qv)∈Q jr vu φ q (h qr , h qv )<label>(7)</label></formula><p>This formalisation allows to (i) incorporate an arbitrary number of qualifier pairs and (ii) can take into account whether entities/relations occur in the main triple or the qualifier pairs. STARE is the first GNN model for representation learning of hyperrelational KGs that has these characteristics.</p><p>STARE for Link Prediction. STARE is a general representation learning framework for capturing the structure of hyper-relational graphs, and thus can be applied to multiple downstream tasks. In this work, we focus on LP and leave other tasks such as node classification for future work. In LP, given a query (s, r, Q), the task is to predict an entity corresponding to the object position o.</p><p>Our link prediction model (see <ref type="figure" target="#fig_2">Fig. 3</ref>) is composed of two parts namely (i) a STARE based  <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> based decoder similar to CoKE <ref type="bibr" target="#b35">(Wang et al., 2019a)</ref>, which are jointly trained. We initialize two embedding matrices R, V corresponding to relations (R), and entities (V) present in the dataset 2 .</p><p>In every iteration, STARE updates the embeddings (R,V) by message passing across every edge in the training set. In the decoding step, we first linearize the given query, and use the updated embeddings (R,V) to encode the entities and relations within it. Then, this linearized sequence is passed through the Transformer block, whose output is averaged to get a fixed-dimensional vector representation of the query. The vector is then passed through a fully-connected layer, multiplied withV and then passed through a sigmoid, to obtain a probability distribution over all entities. Thereafter, it is trivial to retrieve the top n candidate entities for the o position in the query. Note that we can use different decoders in this architecture. An explanation and evaluation of few decoders is provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">WD50K Dataset</head><p>Recent approaches <ref type="bibr" target="#b12">(Guan et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2020;</ref><ref type="bibr" target="#b26">Rosso et al., 2020)</ref> for embedding hyperrelational KGs often use WikiPeople and JF17K as benchmarking datasets. We advocate that those datasets can not fully capture the task complexity.</p><p>In WikiPeople, about 13% of statements contain at least one literal. Literals (e.g. numeric values, date-time instances or other strings, etc) in KGs are conventionally ignored <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref> by embedding approaches, or are incorporated through specific means <ref type="bibr" target="#b19">(Kristiadi et al., 2019)</ref>. However, after removing statements with literals, less than 3% of the remaining statements contain any qualifier pairs. Out of those, about 80% possess only one qualifier. This fact renders WikiPeople less sensitive to hyper-relational models as performance on triple-only facts dominates the overall score.</p><p>The authors of JF17K reported 3 the dataset to contain redundant entries. In our own analysis, we detected that about 44.5% of the test statements share the same main (s, r, o) triple as the train statements. We consider this fact as a major data leakage which allows triple-based models to memorize subjects and objects appearing in the test set.</p><p>To alleviate the above problems, we propose a new dataset, WD50K, extracted from Wikidata statements. The following steps are used to sample our dataset from the Wikidata RDF dump of August 2019 4 . We begin with a set of seed nodes corresponding to entities from FB15K-237 having a direct mapping in Wikidata (P646 "Freebase ID"). Then, for each seed node, all statements whose main object and qualifier values correspond to wikibase:Item are extracted. This step results in the removal of all literals in object position. Similarly, all literals are filtered out from the qualifiers of the obtained statements. To increase the connectivity in the statements graph, all the entities mentioned less than twice are dropped.</p><p>All the statements of WD50K are randomly split into the train, test, and validation sets. To eliminate test set leakages we remove all statements from train and validation sets that share the same main triple (s,p,o) with test statements. Finally, we remove statements from the test set that contain entities and relations not present in the train or validation sets. WD50K contains 236,507 statements describing 47,156 entities with 532 relations where about 14% of statements have at least one qualifier pair. See <ref type="table" target="#tab_3">Table 3</ref>, and Appendix A for further details. The dataset is publicly available 5 .  <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref>. Best results among hyper-relational models are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exp Method</head><p>WikiPeople JF17K </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we discuss the setup and results of multiple experiments conducted towards (i) assessing the performance of our proposed approach on the link prediction task, and (ii) analyzing the effects of including hyper-relational information during link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluating STARE on the LP Task</head><p>In this experiment, we evaluate our proposed approach on the task of LP over hyper-relational graphs. We designed it to both compare STARE with the state of the art algorithms, and to better understand the contribution of the STARE encoder. Datasets: We use WikiPeople 6 and JF17K 7 , despite their design flaws (see Sec. 5) to illustrate the performance differences with existing approaches. We also provide a benchmark of our approach on the WD50K dataset introduced in this article. Note that as described by <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref>, we drop all statements containing literals in WikiPeople. Further datasets statistics are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Baselines: In this experiment, we compare against previous hyper-relational approaches namely: (i) m-TransH <ref type="bibr" target="#b37">(Wen et al., 2016)</ref>, ii) RAE <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>, (iii) NaLP-Fix (an improved version of NaLP <ref type="bibr" target="#b12">(Guan et al., 2019)</ref> as proposed in <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref>), and (iv) HINGE <ref type="bibr" target="#b26">(Rosso et al., 2020)</ref>.</p><p>To assess the significance of the STARE encoder, we also train a simpler model where the Trans-6 Downloaded from:</p><p>https://github.com/ gsp2014/NaLP/tree/master/data/WikiPeople 7 Downloaded from: https://www. dropbox.com/sh/ryxohj363ujqhvq/ AAAoGzAElmNnhXrWEj16UiUga?dl=0 former based decoder directly uses the randomly initialized embedding matrices without the STARE encoder. We call this model Transformer (H), and the one with the STARE encoder STARE (H) + Transformer (H). Here (H) represents that the input to the model is a hyper-relational fact. Later, we also experiment with triples as input and represent them with (T) (see Sec. 6.4).</p><p>Evaluation: For all the systems discussed above, we report various performance metrics when predicting the subject and object of hyperrelational facts. We adopt the filtered setting introduced in <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref> for computing mean reciprocal rank (MRR) and hits at 1, 5, and 10 (H@1, H@5, H@10). The metrics are computed for subject and object prediction separately and are then averaged.</p><p>Training: We train the model in 1-N setting using binary cross entropy loss with label smoothing as in <ref type="bibr" target="#b6">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b31">Vashishth et al., 2020)</ref> with Adam (Kingma and Ba, 2015) optimizer for 500 epochs on WikiPeople and for 400 epochs on JF17K and WD50K datasets. Hyperparameters were selected by manual fine tuning with further details in Appendix C. STARE is implementated with PyTorch Geometric <ref type="bibr" target="#b9">(Fey and Lenssen, 2019)</ref> and is publicly available here 8 .</p><p>Results and Discussion: The results of this experiment can be found in <ref type="table" target="#tab_1">Table 2</ref>. We observe that the STARE encoder based model outperforms the other hyper-relational models across WikiPeople and JF17K. On JF17K, STARE (H) + Transformer (H) reports a gain of 11.3 (25%) MRR points, 13 (33%) H@1, and 7.8 (12%) H@10 points when  compared to the next-best approach. Recall that JF17K suffers from a major test set leakage (Sec. 5), which we investigate in greater detail in Exp. 4 (Sec. 6.4) below. On WikiPeople, HINGE has a higher H@1 score than STARE (H) + Transformer (H). However, its H@10 is lower than H@5 of our approach, i.e., top five predictions of the STARE model are more likely to contain a correct answer than top 10 predictions of HINGE. We can thus claim our STARE based model to be competitive with, if not outperforming the state of the art on the task of link prediction over hyper-relational KGs, albeit on less-than-ideal baselines.</p><p>We further present the performance of our approach as a baseline on the WD50K dataset in <ref type="table" target="#tab_3">Table 3</ref>. With an MRR score of 0.349, H@1 of 0.271, and H@10 of 0.496, we find that the task is far from solved, however, the STARE-based approaches provide effective, non-trivial baselines.</p><p>Note that Transformer (H) (without STARE) also performs competitively to HINGE. This suggests that the aforementioned gains in metrics of our approach cannot all be attributed to STARE's innate ability to effectively encode the hyper-relational information. That said, upon comparing the performance of STARE (H) + Transformer (H) and Transformer (H), we find that using STARE is consistently advantageous across all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Ratio of Statements with and Without Qualifier Pairs</head><p>Based on the relatively high performance of Transformer (H) (without the encoder) in the previous experiment, we study the relationship between the amount of hyper-relational information (qualifiers), and the ability of STARE to incorporate it for the LP task. Here, we sample datasets from WD50K, with varying ratio of facts with qualifier pairs to the total number of facts in the KG. Specifically, we sample three datasets namely, WD50K (33), WD50K (66), and WD50k <ref type="formula">(100)</ref> containing approximately 33%, 66%, and 100% of such hyper-relational facts, respectively. We use the same experimental setup as the one discussed in the previous section. <ref type="table" target="#tab_3">Table 3</ref> presents the result of this experiment.</p><p>We observe that across all metrics, STARE (H) + Transformer (H) performs increasingly better than Transformer (H), as the ratio of qualifier pairs increases in the dataset. Concretely, the difference in their H@1 scores is 4.1, 6.8, and 8.9 points on WD50K (33), WD50K (66), and WD50K (100) respectively. These and the Sec. 6.1 results confirm that STARE is better suited to utilize the qualifier information available in the KG, (ii) which when leveraged by a transformer decoder, outperforms other hyper-relational LP approaches, and (iii) that STARE's positive effects increases as the amount of qualifiers in the task increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact of Number of Qualifiers per Statement</head><p>In WD50K, as in Wikidata, the number of qualifiers corresponding to a statement varies significantly (see Appendix A). In this experiment, we intend to quantify its effect on the model performance.</p><p>To do so, we create multiple variants of WD50K, each containing statements with up to n qualifiers(n ∈ [1, 6]). In other words, for a given number n, we collect all the statements which have less than n qualifiers. If a statement contains more than n qualifiers, we arbitrarily choose n qualifiers amongst them. Thus, the total number of facts remains the same across these variants. <ref type="figure" target="#fig_4">Figure 4</ref> presents the result of this experiment.</p><p>For all the datasets, we find that two qualifier pairs are enough for our model performance to saturate. This might be an attribute of the underlying characteristic of the dataset or the model's inability to aggregate information from longer statements. We leave the further analysis of this for the future work. However, we observe that in case of WD50K and other datasets, STARE (H) + Transformer (H) slightly improves or remains stable with increase of statement length, while Transformer (H) shows degradation in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison to Triple Baselines</head><p>To further understand the role of qualifier information in the LP task, we design an experiment to gauge the performance difference between models on hyper-relational KG and triplebased KG. Concretely, we create a new tripleonly dataset by pruning all qualifier information from the statements in WikiPeople, JF17K, and WD50K. That is, two statements that describe the same main fact (s, r, o, {(qr 1 , qv 1 ), (qr 2 , qv 2 )} and (s, r, o, {(qr 3 , qv 3 ), (qr 4 , qv 4 )}) are reduced to one triple (s, r, o). Thus, the overall amount of distinct entities and relations is reduced, but the amount of subjects and objects in main triples for the LP task is the same. We introduce STARE (T) + Transformer (T), a model for this experiment. STARE (T) is similar to <ref type="bibr">CompGCN (Vashishth et al., 2020)</ref>, and can only model triple-based (s, r, o) facts. Since inputs to the Transformer decoder are linearized queries, we can trivially implement Transformer (T) by ignoring qualifier pairs during this linearization. The results are available in <ref type="table" target="#tab_1">Table 2, and Table 3</ref>.</p><p>We observe that triple-only baselines yield competitive results on JF17K and WikiPeople compared to hyper-relational models (See <ref type="table" target="#tab_1">Table 2</ref>). As WikiPeople contains less than 3% of hyperrelational facts, the biggest contribution to the overall performance is dominated by the triple-only performance. We attribute the strong performance of the triple-only baseline on JF17K to the identified data leakage pertaining to this dataset. In other words, JF17K in its hyper-relational form exhibits similar issues identified by <ref type="bibr" target="#b0">(Akrami et al., 2020)</ref> as in FB15k and WN18 datasets proposed in <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref> for triple-based LP task. We thus perform another experiment after cleaning JF17K from the assumed data leakage and report the results in <ref type="table" target="#tab_4">Table 4</ref> below. We observe a drastic performance drop of about 20 MRR points in both models which provide experimental evidence of the flaws discussed in Sec. 5. We encourage future works in this domain to refrain from using these datasets in experiments.</p><p>In the case of WD50K (where about 13% of facts have qualifiers) the STARE (H) + Transformer (H) yields about 16%, 23%, and 11% of relative improvement over the best performing triple-only baseline across MRR, H@1 and H@10, respectively (see <ref type="table" target="#tab_3">Table 3</ref>). Akin to the previous experiment, we observe that increasing the ratio of hyperrelational facts in the dataset leads to even higher performance boosts. In particular, on WD50K (100), the H@1 of our hyper-relational model is higher than the H@10 of the triple baseline. This difference corresponds to 30 MRR and 32 H@1 points which is about 85% and 123% relative improvement, respectively.</p><p>Based on the above observations we therefore conclude, that information in hyper-relational facts indeed helps to better predict subjects and objects in the main triples of those facts.</p><p>We presented STARE, an instance of the message passing framework for representation learning over hyper-relational KGs. Experimental results suggest that STARE performs competitively on link prediction tasks over existing hyper-relational approaches and greatly outperforms triple-only baselines. In the future, we aim at applying STARE for node and graph classification tasks as well as extend our approach to large-scale KGs.</p><p>We also identified significant flaws in existing link prediction datasets and proposed WD50K, a novel, Wikidata-based hyper-relational dataset that is closer to real-world graphs and better captures the complexity of the link prediction task. In the future, we plan to enrich WD50K entities with class labels and probe it against node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further details on WD50K</head><p>In contrast with Freebase which is no longer supported nor updated, we choose Wikidata as the source KG for our dataset since it has an active community and has seen contributions from various companies that merge their knowledge with it. Additionally, many new NLP tasks <ref type="bibr" target="#b38">(Xiong et al., 2020;</ref><ref type="bibr" target="#b14">Hayashi et al., 2019;</ref><ref type="bibr" target="#b4">Chakraborty et al., 2019)</ref>, as well as datasets <ref type="bibr" target="#b36">(Wang et al., 2019b;</ref><ref type="bibr" target="#b22">Mesquita et al., 2019;</ref><ref type="bibr" target="#b7">Dubey et al., 2019)</ref>, are using Wikidata as a reference KG.</p><p>The combined statistics of our dataset are presented in <ref type="table" target="#tab_0">Table 1</ref>. WD50k consists of 47,156 entities, and 532 relations, amongst which 5,460 entities and 45 relations are found only within qualifier (q p , q e ) pairs. <ref type="figure" target="#fig_5">Fig. 5</ref> illustrates how qualifiers are distributed among statements, i.e., 236,393 statements (99.9%) contain up to five qualifiers whereas remaining 114 statements in a long tail contain up to 20 qualifiers. <ref type="figure">Fig. 6</ref> illustrates the in-degree dis-0 1 2 3 4 5 6 7 8 9 101112131516171920 Recall that we augmented our dataset to reduce test set leakage by removing all instances from the train, and validation sets whose main triple (s, p, o) can be found in the test instances (Sec. 5). Another form of test leakage, as discovered in <ref type="bibr" target="#b29">(Toutanova and Chen, 2015)</ref>, may still persist in our dataset. To estimate this, we count the instances in the test set whose main triple's "direct" inverse (o, p, s), or "semantic" inverse (based on the relation P1696 in Wikidata, i.e., inverse of) is present in the train set. This amounts to less than 4% (1.6k out of 46k) instances in the test set.   Each fact has a unique integer index k which is shared between two COO matrices, i.e., the first one is for main triples, the second one is for qualifiers. Qualifiers that belong to the same fact share the index k.</p><p>Storing full adjacency matrices of large KGs is impractical due to O(|V| 2 ) memory consumption. GNNs encourage using sparse matrix representations and adopting sparse matrices is shown <ref type="bibr" target="#b5">(Cohen et al., 2020)</ref> to be scalable to graphs with millions of edges. As illustrated in <ref type="figure" target="#fig_7">Figure 7</ref>, we employ two sparse COO matrices to model hyper-relational KGs. The first COO matrix is of a standard format with rows containing indices of subjects, objects, and relations associated with the main triple of a hyper-relational fact.</p><p>In addition, we store index k that uniquely identifies each fact. The second COO matrix contains rows of qualifier relations qr and entities qe that are connected to their main triple (and the overall hyper-relational fact) through the index k, i.e., if a fact has several qualifiers those columns corresponding to the qualifiers of the fact will share the same index k. The overall memory consumption is therefore O(|E| + |Q|) and scales linearly to the total number of qualifiers |Q|. Given that most open-domain KGs rarely qualify each fact, e.g., as of August 2019, out of 734M Wikidata statements approximately 128M (17.4%) have at least one qualifier, this sparse qualifier representation saves limited GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head><p>We tuned the model (STARE encoder with Transformer decoder) on the validation set using the hyperparameters reported in <ref type="table" target="#tab_6">Table 5</ref>. Implementations of mult, ccorr, and rotate functions in φ q and φ r correspond to DistMult <ref type="bibr" target="#b40">(Yang et al., 2015)</ref>, circular correlation <ref type="bibr" target="#b24">(Nickel et al., 2016)</ref>, and Ro-tatE <ref type="bibr" target="#b28">(Sun et al., 2019)</ref>, respectively. The selected hyperparameters include two STARE layers, embedding dimension of 200, batch size of 128, Adam optimizer with 0.0001 learning rate and 0.1 label smoothing. φ r and φ q are rotate functions, γ(·) is a weighted sum function with α of 0.8, qualifiers are aggregated using a simple summation, and 0.3 dropout rate. We use 2-layer Transformer block with the hidden dimension of 512, and 4 attention heads with 0.1 dropout rate as our decoder. For WD50K and JF17K datasets we set the maximum length of a hyper-relational fact to 15 (i.e., a statement can contain at most 6 qualifier pairs), and 7 for WikiPeople.</p><p>Infrastructure and Parameters. We train all models on one Tesla V100 GPU. Due to a large number of parameters, owing to large trainable embedding matrices, it is advisable to a GPU with at least 12GB of VRAM. Running STARE (H) + Transformer (H) models with the selected hyperparams on WD50K requires approximately 2 days to train and has 10.8M parameters 9 ; on JF17k the model has 7.1M parameters and takes about 10 hours to train; on WikiPeople the model has 8.2M parameters which we run for 500 epochs and takes about 4 days.</p><p>StarE (H) + Transformer (H) models on reduced datasets: the model corresponding to WD50K (33) has 9M parameters and takes 20 hours to train while WD50K model has 6.8M parameters and takes about 9 hours to train. In case of WD50K (100), the model has 5M parameters and takes 5 hours to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Decoders</head><p>As an additional experiment, we pair STARE with different decoders and evaluate them over WD50K datasets. Along with the main reported model denoted as StarE + Trf, we implemented two CNNbased decoders and another Transformer-based decoder. All models are trained with the same encoder hyperparameters as chosen in the main reported model.</p><p>StarE + ConvE relies on the ConvE (Dettmers et al., 2018)-like decoder but expanded for statements with qualifiers. Given a query (s, r, {(qr i , qv i ), ... }), we stack entities and relations embeddings row-wise and reshape the tensor into an image of size H × W . For instance, for a statement with 6 qualifier pairs, i.e., query length of 14, and an embedding size of 200, we obtain images of size 40 × 70. We then apply a 2D convolutional layer with a 7 × 7 kernel for each image, apply ReLU, flatten the resulting tensor, and pass it through a fully-connected layer. We used 200 filters and the learning rate was set to 0.001.</p><p>StarE + ConvKB is based on the Con-vKB (Nguyen et al., 2018)-like decoder adjusted for statements with qualifiers. Given a query (s, r, {(qr i , qv i ), ... }), we stack entities and relations embeddings row-wise and apply a 2D convolutional layer with a L Q × 7 kernel, e.g., for queries of length 14 the kernel size is 14 × 7. We then apply ReLU, flatten the resulting tensor, and <ref type="table">Table 6</ref>: Effect of different decoders on the link prediction task over WD50K, and its variations. pass it through a fully-connected layer. We used 200 filtersand the learning rate was set to 0.001. StarE + MskTrf denotes a Transformer decoder with an explicit [MASK] token at the object position of each query. Given a query (s, r, {(qr i , qv i ), ... }), we extract relevant entities and relation embeddings and insert the [MASK] token, transforming it into (s, r, [MASK], {(qr i , qv i ), ... }). We then pass it through the Transformer layers and retrieve the representation of the [MASK] token. Finally, the token representation is passed through a fully-connected layer. We trained the model with 0.0001 as the learning rate. <ref type="table">Table 6</ref> reports link prediction results on a variety of WD50K datasets with with different decoders. The default StarE + Trf decoder generally attains superior results with biggest gains along H@1 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Relation-Qualifiers Aggregation</head><p>In this experiment, we measure the impact of the choice of γ(·) function which is used for aggregating representations of a relation and its qualifiers (see Eq. 5). To evaluate its impact we use STARE (H) + Transformer (H) models, on four WD50K datasets using three functions, i.e., concatenation [h r , h q ], element-wise multiplication h r h q , and weighted sum α h r + (1 − α) h q where α is fixed to 0.8.</p><p>The results are presented in <ref type="figure" target="#fig_8">Fig.8</ref>. We find that all the three settings have similar performance indi-cating model's stability with respect to the choice of γ(·) function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A comparison of triple-based and hyperrelational facts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of a STARE based link prediction model. STARE updates theV,R matrices, which are then used to encode the relations in a given query before passing them through the Transformer, Pooling and fully connected layers. The fixed-dimensional output is then compared toV, the result of which is passed through a sigmoid function to yield a probability distribution over entities.qualifier pairs are then aggregated via a positioninvariant summation function and passed through a parameterized projection W q :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Statement length experiment. STARE (H) + Transformer (H) saturates after two qualifiers with slightly increase, whereas Transformer (H) is unstable in handling qualifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Number of qualifiers per statement tribution (with 50 bins, values higher than 1000 are omitted) of the WD50K graph structure where most of the nodes have in-degrees up to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 6: In-degree distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Sparse representation for hyper-relational facts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Gamma experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets -E in quals (R in quals) denote the amount of entities (relations) appearing only in qualifiers.</figDesc><table><row><cell>Dataset</cell><cell>Statements</cell><cell>w/ Quals (%)</cell><cell cols="4">Entities Relations E in quals R in quals</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>WD50K</cell><cell>236,507</cell><cell cols="2">32,167 (13.6%) 47,156</cell><cell>532</cell><cell>5460</cell><cell>45</cell><cell cols="3">166,435 23,913 46,159</cell></row><row><cell>WD50K (33)</cell><cell>102,107</cell><cell cols="2">31,866 (31.2%) 38,124</cell><cell>475</cell><cell>6463</cell><cell>47</cell><cell cols="3">73,406 10,568 18,133</cell></row><row><cell>WD50K (66)</cell><cell>49,167</cell><cell cols="2">31,696 (64.5%) 27,347</cell><cell>494</cell><cell>7167</cell><cell>53</cell><cell>35,968</cell><cell>5,154</cell><cell>8,045</cell></row><row><cell>WD50K (100)</cell><cell>31,314</cell><cell cols="2">31,314 (100%) 18,792</cell><cell>279</cell><cell>7862</cell><cell>75</cell><cell>22,738</cell><cell>3,279</cell><cell>5,297</cell></row><row><cell>WikiPeople</cell><cell>369,866</cell><cell>9,482 (2.6%)</cell><cell>34,839</cell><cell>375</cell><cell>416</cell><cell>35</cell><cell cols="3">294,439 37,715 37,712</cell></row><row><cell>JF17K</cell><cell>100,947</cell><cell cols="2">46,320 (45.9%) 28,645</cell><cell>322</cell><cell>3652</cell><cell>180</cell><cell>76,379</cell><cell>-</cell><cell>24,568</cell></row><row><cell cols="3">encoder, and (b) a Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Link prediction on WikiPeople and JF17K. Results of m-TransH, RAE, NaLP-Fix and HINGE are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Link prediction on WD50K graphs with different ratio of qualifiers. Best results are in bold.</figDesc><table><row><cell>Exp</cell><cell>Dataset →</cell><cell>WD50K</cell><cell>WD50K (33)</cell><cell>WD50K (66)</cell><cell>WD50K (100)</cell></row><row><cell>#</cell><cell>Method ↓</cell><cell cols="4">MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: StarE (H) + Transformer (H) denoted as (H)</cell></row><row><cell cols="5">and Transformer (T) as (T) on the original JF17K and</cell></row><row><cell cols="2">cleaned JF17K</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">JF17K (original) JF17K (cleaned)</cell></row><row><cell></cell><cell>H</cell><cell>T</cell><cell>H</cell><cell>T</cell></row><row><cell>MRR</cell><cell>0.574</cell><cell>0.534</cell><cell>0.376</cell><cell>0.334</cell></row><row><cell>H@1</cell><cell>0.496</cell><cell>0.471</cell><cell>0.278</cell><cell>0.242</cell></row><row><cell>H@5</cell><cell>0.658</cell><cell>0.602</cell><cell>0.485</cell><cell>0.428</cell></row><row><cell cols="2">H@10 0.725</cell><cell>0.661</cell><cell>0.582</cell><cell>0.514</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>This table reports the major hyperparameters of our approach, and their corresponding bounds. Note that "Trf" refers to Transformers. Selected values are in bold.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>STARE layers</cell><cell>{1, 2}</cell></row><row><cell>Embedding dim</cell><cell>{100, 200}</cell></row><row><cell>Batch size</cell><cell>{128, 256, 512}</cell></row><row><cell>Learning rate</cell><cell>{0.0001, 0.0005, 0.001}</cell></row><row><cell>φ q</cell><cell>mult, ccorr, rotate</cell></row><row><cell>φ r</cell><cell>mult, ccorr, rotate</cell></row><row><cell>γ</cell><cell>weighted sum concat, mul</cell></row><row><cell>Weighted sum α</cell><cell>[0.0, 1.0] step 0.1</cell></row><row><cell>Quals aggregation</cell><cell>sum, mean</cell></row><row><cell>Trf layers</cell><cell>{1, 2}</cell></row><row><cell>Trf hidden dim</cell><cell>{256, 512, 768}</cell></row><row><cell>Trf heads</cell><cell>{2, 4}</cell></row><row><cell>StarE dropout</cell><cell>{0.1, 0.2, 0.3}</cell></row><row><cell>Trf dropout</cell><cell>{0.1, 0.2, 0.3}</cell></row><row><cell>Label smoothing</cell><cell>{0.0, 0.1}</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The title is inspired by the RDF* (Hartig, 2017) "RDF star" proposal for standardizing hyper-relational KGs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As mentioned in Section 3, while pre-processing, we add inverse and self-loop relations to the graph. Note, we retain the same set of qualifiers as in the original fact while generating inverse hyper-relational facts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.site.uottawa.ca/˜yymao/ JF17K/ 4 https://dumps.wikimedia.org/ wikidatawiki/20190801/ 5 https://zenodo.org/record/4036498</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/migalkin/StarE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">According to a built-in PyTorch counter.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the Center for Information Services and High Performance Computing (ZIH) at TU Dresden for generous allocations of computer time. We acknowledge the support of the following projects: SPEAKER (FKZ 01MK20011A), JOSEPH (Fraunhofer Zukunftsstiftung), H2020 Cleopatra (GA 812997), ML2R (FKZ 01 15 18038 A/B/C), MLwin (01IS18050 D/F), ScaDS (01IS18026A).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Realistic re-evaluation of knowledge graph completion methods: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">Samiul</forename><surname>Saeef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2003.08001</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to neural network based approaches for question answering over knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<idno>abs/1907.09361</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable neural methods for reasoning with a symbolic knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Alex</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Siegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lc-quad 2.0: a large dataset for complex question answering over wikidata and dbpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debayan</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Abdelrahman Abdelkawi, and Jens Lehmann</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge hypergraphs: Prediction beyond binary relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2191" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<idno>abs/1903.02428</idno>
		<imprint>
			<date type="published" when="2019-01" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of metadata representations in RDF stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Esther</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link prediction on n-ary relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Saiping Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Foundations of rdf and sparql (an alternative approach to statement-level metadata in RDF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Hartig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Alberto Mendelzon International Workshop on Foundations of Data Management and the Web</title>
		<meeting>the 11th Alberto Mendelzon International Workshop on Foundations of Data Management and the Web</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Latent relation language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>abs/1908.07690</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wikidata through the eyes of dbpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ismayilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="503" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">2020. A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2002.00388</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incorporating literals into knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agustinus</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Asif</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2019</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11778</biblScope>
			<biblScope unit="page" from="347" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing tensor decomposition for n-ary relational knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1104" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledgenet: A benchmark dataset for knowledge base population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
	<note>Paramita Mirza, and Denilson Barbosa</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yago 4: A reason-able knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pellissier-Tanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond triplets: Hyper-relational knowledge graph embedding for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1885" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structural deep embedding for hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 23rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems NIPS 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Coke: Contextualized knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1911.02168</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">KE-PLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1911.06136</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the representation and embedding of knowledge bases beyond binary relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shini</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1300" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><forename type="middle">Jegelka</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable instance reconstruction in knowledge bases via relatedness affiliated embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1185" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hyper-sagnn: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
