<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PeTra: A Sparsely Supervised Memory Model for People Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
							<email>aettinger@uchicago.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
							<email>klivescu@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PeTra: A Sparsely Supervised Memory Model for People Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding text narratives requires maintaining and resolving entity references over arbitrary-length spans. Current approaches for coreference resolution <ref type="bibr" target="#b4">(Clark and Manning, 2016b;</ref><ref type="bibr" target="#b17">Lee et al., 2017</ref><ref type="bibr" target="#b18">Lee et al., , 2018</ref><ref type="bibr" target="#b33">Wu et al., 2019)</ref> scale quadratically (without heuristics) with length of text, and hence are impractical for long narratives. These models are also cognitively implausible, lacking the incrementality of human language processing <ref type="bibr" target="#b28">(Tanenhaus et al., 1995;</ref><ref type="bibr" target="#b14">Keller, 2010)</ref>. Memory models with finite memory and online/quasi-online entity resolution have linear runtime complexity, offering more scalability, cognitive plausibility, and interpretability.</p><p>Memory models can be viewed as general problem solvers with external memory mimicking a Turing tape <ref type="bibr" target="#b7">(Graves et al., 2014</ref><ref type="bibr" target="#b8">(Graves et al., , 2016</ref>. Some of the earliest applications of memory networks in language understanding were for question answering, where the external memory simply stored all of the word/sentence embeddings for a document <ref type="bibr" target="#b27">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b16">Kumar et al., 2016)</ref>. To endow more structure and interpretability to memory, key-value memory networks were introduced by <ref type="bibr" target="#b24">Miller et al. (2016)</ref>. The key-value architecture has since been used for narrative understanding and other tasks where the memory is intended to learn to track entities while being guided by varying degrees of supervision <ref type="bibr" target="#b9">(Henaff et al., 2017;</ref><ref type="bibr" target="#b19">Liu et al., 2018a</ref><ref type="bibr">Liu et al., ,b, 2019a</ref>.</p><p>We propose a new memory model, PeTra, for entity tracking and coreference resolution, inspired by the recent Referential Reader model <ref type="bibr" target="#b21">(Liu et al., 2019a)</ref> but substantially simpler. Experiments on the GAP <ref type="bibr" target="#b29">(Webster et al., 2018</ref>) pronoun resolution task show that PeTra outperforms the Referential Reader with fewer parameters and simpler architecture. Importantly, while Referential Reader performance degrades with larger memory, PeTra improves with increase in memory capacity (before saturation), which should enable tracking of a larger number of entities. We conduct experiments to assess various memory architecture decisions, such as learning of memory initialization and separation of memory slots into key/value pairs.</p><p>To test interpretability of memory models' entity tracking, we propose a new diagnostic evaluation based on entity counting-a task that the models are not explicitly trained for-using a small amount of annotated data. Additionally, we conduct a small scale human evaluation to assess quality of people tracking based on model memory logs. PeTra substantially outperforms Referential Reader on both measures, indicating better and more interpretable tracking of people. 1  <ref type="figure">Figure 1</ref>: Illustration of memory cell updates in an example sentence where IG = ignore, OW = overwrite, CR = coref. Different patterns indicate the different entities, and an empty pattern indicates that the cell has not been used. The updated memory cells at each time step are highlighted. <ref type="figure">Figure 2</ref> depicts PeTra, which consists of three components: an input encoder that given the tokens generates the token embeddings, a memory module that tracks information about the entities present in the text, and a controller network that acts as an interface between the encoder and the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Encoder</head><formula xml:id="formula_0">w t h t M t−1 M t . . . . . . Controller e t , o t , c t . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU Hidden States</head><p>Input Tokens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controller Outputs</head><p>Figure 2: Proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Encoder</head><p>Given a document consisting of a sequence of tokens {w 1 , · · · , w T }, we first pass the document through a fixed pretrained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to extract contextual token embeddings. Next, the BERT-based token embeddings are fed into a single-layer unidirectional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">(Cho et al., 2014)</ref> running left-to-right to get task-specific token embeddings {h 1 , · · · , h T }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory</head><p>The memory M t consists of N memory cells. The i th memory cell state at time step t consists of a tuple (m i t , u i t ) where the vector m i t represents the content of the memory cell, and the scalar u i t ∈ [0, 1] represents its recency of usage. A high value of u i t is intended to mean that the cell is tracking an entity that has been recently mentioned.</p><p>Initialization Memory cells are initialized to the null tuple, i.e. (0, 0); thus, our memory is parameterfree. This is in contrast with previous entity tracking models such as EntNet <ref type="bibr" target="#b9">(Henaff et al., 2017)</ref> and the Referential Reader <ref type="bibr" target="#b21">(Liu et al., 2019a)</ref> where memory initialization is learned and the cells are represented with separate key and value vectors. We will later discuss variants of our memory with some of these changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Controller</head><p>At each time step t the controller network determines whether token t is part of an entity span and, if so, whether the token is coreferent with any of the entities already being tracked by the memory. Depending on these two variables, there are three possible actions:</p><p>(i) IGNORE: The token is not part of any entity span, in which case we simply ignore it.</p><p>(ii) OVERWRITE: The token is part of an entity span but is not already being tracked in the memory.</p><p>(iii) COREF: The token is part of an entity span and the entity is being tracked in the memory.</p><p>Therefore, the two ways of updating the memory are OVERWRITE and COREF. There is a strict ordering constraint to the two operations: OVERWRITE precedes COREF, because it is not possible to corefer with a memory cell that is not yet tracking anything. That is, the COREF operation cannot be applied to a previously unwritten memory cell, i.e. one with u i t = 0. <ref type="figure">Figure 1</ref> illustrates an idealized version of this process.</p><p>Next we describe in detail the computation of the probabilities of the two operations for each memory cell at each time step t.</p><p>First, the entity mention probability e t , which reflects the probability that the current token w t is part of an entity mention, is computed by:</p><formula xml:id="formula_1">e t = σ(MLP 1 (h t )) (1)</formula><p>where MLP 1 is a multi-layer perceptron and σ is the logistic function.</p><p>Overwrite and Coref If the current token w t is part of an entity mention, we need to determine whether it corresponds to an entity being currently tracked by the memory or not. For this we compute the similarity between the token embedding h t and the contents of the memory cells currently tracking entities. For the i th memory cell with memory vector m i t−1 the similarity with h t is given by:</p><formula xml:id="formula_2">sim i t = MLP 2 ([h t ; m i t−1 ; h t m i t−1 ; u i t−1 ])</formula><p>(2) where MLP 2 is a second MLP and is the Hadamard (elementwise) product. The usage scalar u i t−1 in the above expression provides a notion of distance between the last mention of the entity in cell i and the potential current mention. The higher the value of u i t−1 , the more likely there was a recent mention of the entity being tracked by the cell. Thus u i t−1 provides an alternative to distance-based features commonly used in pairwise scores for spans <ref type="bibr" target="#b17">(Lee et al., 2017)</ref>.</p><p>Given the entity mention probability e t and similarity score sim i t , we define the coref score cs i t as:</p><formula xml:id="formula_3">cs i t = sim i t − ∞ · 1[u i t−1 = 0]<label>(3)</label></formula><p>where the second term ensures that the model does not predict coreference with a memory cell that has not been previously used, something not enforced by <ref type="bibr" target="#b21">Liu et al. (2019a)</ref>. 2 Assuming the coref score for a new entity to be 0, 3 we compute the coref probability c i t and new entity probability n t as follows:</p><formula xml:id="formula_4">     c 1 t . . . c N t n t      = e t · softmax      cs 1 t . . . cs N t 0     <label>(4)</label></formula><p>Based on the memory usage scalars u i t and the new entity probability n t , the overwrite probability for 2 A threshold higher than 0 can also be used to limit coreference to only more recent mentions. <ref type="bibr">3</ref> The new entity coref score is a free variable that can be assigned any value, since only the relative value matters. each memory cell is determined as follows:</p><formula xml:id="formula_5">o i t = n t · 1 i=arg min j u j t−1 (5)</formula><p>Thus we pick the cell with the lowest usage scalar u j t−1 to OVERWRITE. In case of a tie, a cell is picked randomly among the ones with the lowest usage scalar. The above operation is non-differentiable, so during training we instead use</p><formula xml:id="formula_6">o i t = n t · GS 1 − u i t−1 τ i (6)</formula><p>where GS(.) refers to Gumbel-Softmax <ref type="bibr" target="#b11">(Jang et al., 2017)</ref>, which makes overwrites differentiable. For each memory cell, the memory vector is updated based on the three possibilities of ignoring the current token, being coreferent with the token, or considering the token to represent a new entity (causing an overwrite):</p><formula xml:id="formula_7">m i t = IGNORE (1 − (o i t + c i t ))m i t−1 + OVERWRITE o i t · h t + c i t · MLP 3 ([h t ; m i t−1 ]) COREF<label>(7)</label></formula><p>In this expression, the coreference term takes into account both the previous cell vector m i t−1 and the current token representation h t , while the overwrite term is based only on h t . In contrast to a similar memory update equation in the Referential Reader which employs a pair of GRUs and MLPs for each memory cell, our update parameter uses just MLP 3 which is memory cell-agnostic.</p><p>Finally, the memory usage scalar is updated as</p><formula xml:id="formula_8">u i t = min(1, o i t + c i t + γ · u i t−1 )<label>(8)</label></formula><p>where γ ∈ (0, 1) is the decay rate for the usage scalar. Thus the usage scalar u i t keeps decaying with time unless the memory is updated via OVERWRITE or COREF in which case the value is increased to reflect the memory cell's recent use.</p><p>Memory Variants In vanilla PeTra, each memory cell is represented as a single vector and the memory is parameter-free, so the total number of model parameters is independent of memory size. This is a property that is shared with, for example, differentiable neural computers <ref type="bibr" target="#b8">(Graves et al., 2016)</ref>. On the other hand, recent models for entity tracking, such as the EntNet <ref type="bibr" target="#b9">(Henaff et al., 2017)</ref> and the Referential Reader <ref type="bibr" target="#b21">(Liu et al., 2019a)</ref>, learn memory initialization parameters and separate the memory cell into key-value pairs. To compare these memory cell architectures, we investigate the following two variants of PeTra:</p><p>1. PeTra + Learned Initialization: memory cells are initialized at t = 0 to learned parameter vectors.</p><p>2. PeTra + Fixed Key: a fixed dimensions of each memory cell are initialized with learned parameters and kept fixed throughout the document read, as in EntNet <ref type="bibr" target="#b9">(Henaff et al., 2017)</ref>.</p><p>Apart from initialization, the initial cell vectors are also used to break ties for overwrites in Eqs. <ref type="formula">(5)</ref> and <ref type="formula">(6)</ref> when deciding among unused cells (with u i t = 0). The criterion for breaking the tie is the similarity score computed using Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Coreference Link Probability</head><p>The probability that the tokens w t 1 and w t 2 are coreferential according to, say, cell i of the memory depends on three things: (a) w t 1 is identified as part of an entity mention and is either overwritten to cell i or is part of an earlier coreference chain for an entity tracked by cell i, (b) Cell i is not overwritten by any other entity mention from t = t 1 + 1 to t = t 2 , and (c) w t 2 is also predicted to be part of an entity mention and is coreferential with cell i. Combining these factors and marginalizing over the cell index results in the following expression for the coreference link probability:</p><formula xml:id="formula_9">P CL (w t 1 , w t 2 ) = N i=1 (o i t 1 + c i t 1 ) · t 2 j=t 1 +1 (1 − o i j ) · c i t 2 (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Losses</head><p>The GAP <ref type="bibr" target="#b29">(Webster et al., 2018)</ref> training dataset is small and provides sparse supervision with labels for only two coreference links per instance. In order to compensate for this lack of supervision, we use a heuristic loss L ent over entity mention probabilities in combination with the end task loss L coref for coreference. The two losses are combined with a tunable hyperparameter λ resulting in the following total loss: L = L coref + λL ent .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Coreference Loss</head><p>The coreference loss is the binary cross entropy between the ground truth labels for mention pairs and the coreference link probability P CL in Eq. (9). Eq. (9) expects a pair of tokens while the annotations are on pairs of spans, so we compute the loss for all ground truth token pairs:</p><formula xml:id="formula_10">L coref = (sa,s b ,y ab )∈G wa∈sa w b ∈s b H(y ab , P CL (w a , w b ))</formula><p>where G is the set of annotated span pairs and H(p, q) represents the cross entropy of the distribution q relative to distribution p. Apart from the ground truth labels, we use "implied labels" in the coreference loss calculation. For handling multi-token spans, we assume that all tokens following the head token are coreferential with the head token (self-links). We infer more supervision based on knowledge of the setup of the GAP task. Each GAP instance has two candidate names and a pronoun mention with supervision provided for the {name, pronoun} pairs. By design the two names are different, and therefore we use them as a negative coreference pair.</p><p>Even after the addition of this implied supervision, our coreference loss calculation is restricted to the three mention spans in each training instance; therefore, the running time is O(T ) for finite-sized mention spans. In contrast, <ref type="bibr" target="#b21">Liu et al. (2019a)</ref> compute the above coreference loss for all token pairs (assuming a negative label for all pairs outside of the mentions), which results in a runtime of O(T 3 ) due to the O(T 2 ) pairs and O(T ) computation per pair, and thus will scale poorly to long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Entity Mention Loss</head><p>We use the inductive bias that most tokens do not correspond to entities by imposing a loss on the average of the entity mention probabilities predicted across time steps, after masking out the labeled entity spans. For a training instance where spans s A and s B correspond to the person mentions and span s P is a pronoun, the entity mention loss is</p><formula xml:id="formula_11">L ent = T t=1 e t · m t T t=1 m t where m t = 0 if w t ∈ s A ∪ s B ∪ s P and m t = 1 otherwise.</formula><p>Each GAP instance has only 3 labeled entity mention spans, but the text typically has other entity mentions that are not labeled. Unlabeled entity mentions will be inhibited by this loss. However, on average there are far more tokens outside entity spans than inside the spans. In experiments without this loss, we observed that the model is susceptible to predicting a high entity probability for all tokens while still performing well on the end task of pronoun resolution. We are interested in tracking people beyond just the entities that are labeled in the GAP task, for which this loss is very helpful.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>GAP is a gender-balanced pronoun resolution dataset introduced by <ref type="bibr" target="#b29">Webster et al. (2018)</ref>. Each instance consists of a small snippet of text from Wikipedia, two spans corresponding to candidate names along with a pronoun span, and two binary labels indicating the coreference relationship between the pronoun and the two candidate names. Relative to other popular coreference datasets <ref type="bibr" target="#b26">(Pradhan et al., 2012;</ref><ref type="bibr" target="#b1">Chen et al., 2018)</ref>, GAP is comparatively small and sparsely annotated. We choose GAP because its small size allows us to do extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Details</head><p>For the input BERT embeddings, we concatenate either the last four layers of BERT BASE , or layers 19-22 of BERT LARGE since those layers have been found to carry the most information related to coreference <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref>. The BERT embeddings are fed to a 300-dimensional GRU model, which matches the dimensionality of the memory vectors.</p><p>We vary the number of memory cells N from 2 to 20. The decay rate for the memory usage scalar γ is 0.98. The MLPs used for predicting the entity probability and similarity score consist of two 300-dimensional ReLU hidden layers. For the Fixed Key variant of PeTra we use 20 dimensions for the learned key vector and the remaining 280 dimensions as the value vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>All models are trained for a maximum of 100 epochs with the Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>. The learning rate is initialized to 10 −3 and is reduced by half, until a minimum of 10 −4 , whenever there is no improvement on the validation performance for the last 5 epochs. Training stops when there is no improvement in validation performance for the last 15 epochs. The temperature τ of the Gumbel-Softmax distribution used in the OVERWRITE operation is initialized to 1 and halved every 10 epochs. The coreference loss terms in Section 2.5.1 are weighted differently for different coreference links: (a) self-link losses for multi-token spans are given a weight of 1, (b) positive coreference link losses are weighted by 5, and (c) negative coreference link losses are multiplied by 50. To prevent overfitting: (a) we use early stopping based on validation performance, and (b) apply dropout at a rate of 0.5 on the output of the GRU model. Finally, we choose λ = 0.1 to weight the entity prediction loss described in Section 2.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">People Tracking Evaluation</head><p>One of the goals of this work is to develop memory models that not only do well on the coreference resolution task, but also are interpretable in the sense that the memory cells actually track entities. Hence in addition to reporting the standard metrics on GAP, we consider two other ways to evaluate memory models.</p><p>As our first task, we propose an auxiliary entitycounting task. We take 100 examples from the GAP validation set and annotate them with the number of unique people mentioned in them. <ref type="bibr">4</ref> We test the models by predicting the number of people from their memory logs as explained in Section 3.5. The motivation behind this exercise is that if a memory model is truly tracking entities, then its memory usage logs should allow us to recover this information.</p><p>To assess the people tracking performance more holistically, we conduct a human evaluation in which we ask annotators to assess the memory models on people tracking performance, defined as:(a) detecting references to people including pronouns, and (b) maintaining a 1-to-1 correspondence between people and memory cells. For this study, we pick the best run (among 5 runs) of PeTra and the Referential Reader for the 8-cell configuration using BERT BASE (PeTra: 81 F1; Referential Reader: 79 F1). Next we randomly pick 50 documents (without replacement) from the GAP dev set and split those into groups of 10 to get 5 evaluation sets. We shuffle the original 50 documents and follow the same steps to get another 5 evaluation sets. In the end, we have a total of 10 evaluation sets with 10 documents each, where each unique document belongs to exactly 2 evaluation sets.</p><p>We recruit 10 annotators for the 10 evaluation sets. The annotators are shown memory log visualizations as in <ref type="figure">Figure 5</ref>, and instructed to compare the models on their people tracking performance (detailed instructions in Appendix A.3). For each document the annotators are presented memory logs of the two models (ordered randomly) and asked whether they prefer the first model, prefer the second model, or have no preference (neutral).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>GAP Given a pronoun span s P and two candidate name spans s A &amp; s B , we have to predict binary labels for potential coreference links between (s A , s P ) and (s B , s P ). Thus, for a pair of entity spans, say s A and s P , we predict the coreference link probability as:</p><formula xml:id="formula_12">P CL (s A , s P ) = max w A ∈s A ,w P ∈s P P CL (w A , w P )</formula><p>where P CL (w A , w P ) is calculated using the procedure described in Section 2.4 5 . The final binary prediction is made by comparing the probability against a threshold.</p><p>Counting unique people For the test of unique people counting, we discretize the overwrite operation, which corresponds to new entities, against a threshold α and sum over all tokens and all memory cells to predict the count as follows:</p><formula xml:id="formula_13"># unique people = T t=1 N i=1 1[o i t ≥ α]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation Metrics</head><p>For GAP we evaluate models using F-score. <ref type="bibr">6</ref> First, we pick a threshold from the set {0.01, 0.02, · · · , 1.00} which maximizes the validation F-score. This threshold is then used to evaluate performance on the GAP test set.</p><p>For the interpretability task of counting unique people, we choose a threshold that minimizes the absolute difference between ground truth count and predicted count summed over the 100 annotated examples. We select the best threshold from the set {0.01, 0.02, · · · , 1.00}. The metric is then the number of errors corresponding to the best threshold. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Baselines</head><p>The Referential Reader <ref type="bibr" target="#b21">(Liu et al., 2019a)</ref> is the most relevant baseline in the literature, and the most similar to PeTra. The numbers reported by <ref type="bibr" target="#b21">Liu et al. (2019a)</ref> are obtained by a version of the model using BERT BASE , with only two memory cells. To compare against PeTra for other configurations, we retrain the Referential Reader using the code made available by the authors. <ref type="bibr">8</ref> We also report the results of Joshi et al. <ref type="formula">(2019)</ref> and <ref type="bibr" target="#b33">Wu et al. (2019)</ref>, although these numbers are not comparable since both of them train on the much larger OntoNotes corpus and just test on GAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GAP results</head><p>We train all the memory models, including the Referential Reader, with memory size varying from {2, 4, · · · , 20} memory cells for both BERT BASE and BERT LARGE , with each configuration being trained 7 Note that the error we report is therefore a best-case result. We are not proposing a way of counting unique people in new test data, but rather using this task for analysis. 8 https://github.com/liufly/refreader   5 times. <ref type="figure" target="#fig_1">Figure 3</ref> shows the performance of the models on the GAP validation set as a function of memory size. The Referential Reader outperforms PeTra (and its memory variants) when using a small number of memory cells, but its performance starts degrading after 4 and 6 memory cells for BERT BASE and BERT LARGE respectively. PeTra and its memory variants, in contrast, keep improving with increased memory size (before saturation at a higher number of cells) and outperform the best Referential Reader performance for all memory sizes ≥ 6 cells. With larger numbers of memory cells, we see a higher variance, but the curves for PeTra and its memory variants are still consistently higher than those of the Referential Reader. Among different memory variants of PeTra, when using BERT BASE the performances are comparable with no clear advantage for any particular choice. For BERT LARGE , however, vanilla PeTra has a clear edge for almost all memory sizes, suggesting the limited utility of initialization. The results show that PeTra works well without learning vectors for initializing the key or memory cell contents. Rather, we can remove the key/value distinction and simply initialize all memory cells with the zero vector.</p><p>To evaluate on the GAP test set, we pick the memory size corresponding to the best validation performance for all memory models. <ref type="table" target="#tab_1">Table 1</ref> shows that the trends from validation hold true for test as well, with PeTra outperforming the Referential Reader and the other memory variants of PeTra. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results for the proposed interpretability task of counting unique people. For both BERT BASE and BERT LARGE , PeTra achieves the lowest error count. Interestingly, from <ref type="figure" target="#fig_2">Figure 4b</ref> we can see that for ≥ 14 memory cells, the other memory variants of PeTra perform worse than the Referential Reader while being better at the GAP validation task (see <ref type="figure" target="#fig_1">Figure 3b</ref>). This shows that a better performing model is not necessarily better at tracking people.  To test the relationship between the GAP task and the proposed interpretability task, we compute the correlation between the GAP F-score and the negative count of unique people for each model separately. 9 <ref type="table" target="#tab_3">Table 2</ref> shows the Spearman's correlation between these measures. For all models we see a positive correlation, indicating that a dip in coreference performance corresponds to an increase in Amelia Shepherd1 , M.D. is a fictional character on the ABC American television medical drama Private Practice, and the spinoff series' progenitor show, Grey's Anatomy, portrayed by Caterina Scorsone2 . In her1 debut appearance in season three, Amelia1 visited her former sister-in-law, Addison Montgomery3 , and became a partner at the Oceanside Wellness Group. (b) Memory log of PeTra with 8 memory cells. The model correctly links "she" and "Julie" but fails at linking the three "Bethenny" mentions, and also fails at detecting "Jason".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Counting unique people</head><p>Figure 5: Visualization of memory logs for different configurations of PeTra. The documents have their GAP annotations highlighted in red (italics) and blue (bold), with blue (bold) corresponding to the right answer. For illustration purposes only, we highlight all the spans corresponding to mentions of people and mark cluster indices as subscript. In the plot, X-axis corresponds to document tokens, and Y-axis corresponds to memory cells. Each memory cell has the OW=OVERWRITE and CR=COREF labels. Darker color implies higher value. We skip text, indicated via ellipsis, when the model doesn't detect people for extended lengths of text.</p><p>error on counting unique people. The correlations for PeTra are especially high, again suggesting it's greater interpretability.   <ref type="table" target="#tab_5">Table 3</ref> summarizes the results of the human evaluation for people tracking. The annotators prefer PeTra in 74% cases while the Referential Reader for only 8% instances (see Appendix A.4 for visualizations comparing the two). Thus, PeTra easily outperforms the Referential Reader on this task even though they are quite close on the GAP evaluation. The annotators agree on 68% of the documents, disagree between PeTra and Neutral for 24% of the documents, and disagree between PeTra and the Referential Reader for the remaining 8% documents. For more details, see Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Evaluation for People Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Runs</head><p>We visualize two runs of PeTra with different configurations in <ref type="figure">Figure 5</ref>. For both instances the model gets the right pronoun resolution, but clearly in <ref type="figure">Figure 5b</ref> the model fails at correctly tracking repeated mentions of "Bethenny". We believe these errors happen because (a) GAP supervision is limited to pronoun-proper name pairs, so the model is never explicitly supervised to link proper names, and (b) there is a lack of span-level features, which hurts the model when a name is split across multiple tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There are several strands of related work, including prior work in developing neural models with external memory as well as variants that focus on modeling entities and entity relations, and neural models for coreference resolution.</p><p>Memory-augmented models. Neural network architectures with external memory include memory networks <ref type="bibr" target="#b27">Sukhbaatar et al., 2015)</ref>, neural Turing machines <ref type="bibr" target="#b7">(Graves et al., 2014)</ref>, and differentiable neural computers <ref type="bibr" target="#b8">(Graves et al., 2016)</ref>. This paper focuses on models with inductive biases that produce particular structures in the memory, specifically those related to entities.</p><p>Models for tracking and relating entities. A number of existing models have targeted entity tracking and coreference links for a variety of tasks. EntNet <ref type="bibr" target="#b9">(Henaff et al., 2017)</ref> aims to track entities via a memory model. EntityNLM <ref type="bibr" target="#b12">(Ji et al., 2017)</ref> represents entities dynamically within a neural language model. <ref type="bibr" target="#b10">Hoang et al. (2018)</ref> augment a reading comprehension model to track entities, incorporating a set of auxiliary losses to encourage capturing of reference relations in the text. <ref type="bibr" target="#b6">Dhingra et al. (2018)</ref> introduce a modified GRU layer designed to aggregate information across coreferent mentions.</p><p>Memory models for NLP tasks. Memory models have been applied to several other NLP tasks in addition to coreference resolution, including targeted aspect-based sentiment analysis <ref type="bibr" target="#b20">(Liu et al., 2018b)</ref>, machine translation <ref type="bibr" target="#b23">(Maruf and Haffari, 2018)</ref>, narrative modeling <ref type="bibr" target="#b19">(Liu et al., 2018a)</ref>, and dialog state tracking <ref type="bibr" target="#b25">(Perez and Liu, 2017)</ref>. Our study of architectural choices for memory may also be relevant to models for these tasks.</p><p>Neural models for coreference resolution. Several neural models have been developed for coreference resolution, most of them focused on modeling pairwise interactions among mentions or spans in a document <ref type="bibr" target="#b31">(Wiseman et al., 2015;</ref><ref type="bibr" target="#b3">Clark and Manning, 2016a;</ref><ref type="bibr" target="#b17">Lee et al., 2017</ref><ref type="bibr" target="#b18">Lee et al., , 2018</ref>. These models use heuristics to avoid computing scores for all possible span pairs in a document, an operation which is quadratic in the document length T assuming a maximum span length. Memory models for coreference resolution, including our model, differ by seeking to store information about entities in memory cells and then modeling the relationship between a token and a memory cell. This reduces computation from O(T 2 ) to O(T N ), where N is the number of memory cells, allowing memory models to be applied to longer texts by using the global entity information. Past work <ref type="bibr" target="#b32">(Wiseman et al., 2016)</ref> have used global features, but in conjunction with other features to score span pairs. Referential Reader. Most closely related to the present work is the Referential Reader <ref type="bibr" target="#b21">(Liu et al., 2019a)</ref>, which uses a memory model to perform coreference resolution incrementally. We significantly simplify this model to accomplish the same goal with far fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We propose a new memory model for entity tracking, which is trained using sparse coreference resolution supervision. The proposed model outperforms a previous approach with far fewer parameters and a simpler architecture. We propose a new diagnostic evaluation and conduct a human evaluation to test the interpretability of the model, and find that our model again does better on this evaluation. In future work, we plan to extend this work to longer documents such as the recently released dataset of <ref type="bibr" target="#b0">Bamman et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Best Runs vs. Worst Runs</head><p>As <ref type="table" target="#tab_1">Table 1</ref> shows, there is significant variance in the performance of these memory models. To analyze how the best runs diverge from the worst runs, we analyze how the controller network is using the different memory cells in terms of overwrites. For this analysis, we choose the best and worst among the 5 runs for each configuration, as determined by GAP validation performance. For the selected runs, we calculate the KL-divergence of the average overwrite probability distribution from the uniform distribution and average it for each model type. <ref type="table" target="#tab_7">Table 4</ref> shows that for the memory variants Learned Init and Fixed Key, the worst runs overwrite more to some memory cells than others (high average KLdivergence). Note that both PeTra and Referential Reader are by design intended to have no preference for any particular memory cell (which the numbers support), hence the low KL-divergence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Human Evaluation Results</head><p>The agreement matrix for the human evaluation study described in Section 4.3 is shown in <ref type="figure" target="#fig_4">Figure 6</ref>. This agreement matrix is a result of the two annotations per document that we get as per the setup described in Section 3.4. Note that the annotations are coming from two sets of annotators rather than two individual annotators. This is also the reason why we don't report standard inter-annotator agreement coefficients. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Instructions for Human Evaluation</head><p>The detailed instructions for the human evaluation study described in Section 4.3 are shown in <ref type="figure" target="#fig_5">Figure 7</ref>.</p><p>We simplified certain memory model specific terms such as "overwrite" to "new person" since the study was really about people tracking.</p><p>A.4 Comparative visualization of memory logs of PeTra and the Referential Reader <ref type="figure" target="#fig_7">Figure 8</ref> and 9 compare the memory logs of PeTra and the Referential Reader.</p><p>• In this user study we will be comparing memory models at tracking people.</p><p>• What are memory models? Memory models are neural networks coupled with an external memory which can be used for reading/writing.</p><p>• (IMPORTANT) What does it mean to track people for memory models?</p><p>-Detect all references to people which includes pronouns.</p><p>-A 1-to-1 correspondence between people and memory cells i.e. all references corresponding to a person should be associated with the same memory cell AND each memory cell should be associated with at most 1 person.</p><p>• The memory models use the following scores (which are visualized) to indicate the tracking decisions:</p><p>-New Person Probability (Cell i): Probability that the token refers to a new person (not introduced in the text till now) and we start tracking it in cell i. -Coreference Probability (Cell i): Probability that the token refers to a person already being tracked in cell i.</p><p>• The objective of this study is to compare the models on the interpretability of their memory logs i.e. are the models actually tracking entities or not. You can choose how you weigh the different requirements for tracking people (from 3).</p><p>• For this study, you will compare two memory models with 8 memory cells (represented via 8 rows). The models are ordered randomly for each instance.</p><p>• For each document, you can choose model A or model B, or stay neutral in case both the models perform similarly. Neef 1 took an individual silver medal at the 1994 European Cup behind Russia's Svetlana Goncharenko 2 and returned the following year to win gold. She 1 was a finalist individually at the 1994 European Championships and came sixth for Scotland at the 1994 Commonwealth Games.</p><p>(a) GAP validation instance 293. The ground truth GAP annotation is indicated via colors.  (c) Memory log of the Referential Reader with 8-memory cells. The Referential Reader completely misses out on all the mentions in the first half of the document (which is not penalized in GAP evaluations where the relevant annotations are typically towards the end of the document). Apart from this, the model ends up tracking Robert Fripp in as many as 6 memory cells, and Steve Ball in 3 memory cells. <ref type="figure">Figure 9</ref>: PeTra clearly performs better than the Referential Reader at people tracking for this instance. PeTra's output is more sparse, detects more relevant mentions, and is better at maintaining a 1-to-1 correspondence between memory cells and people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Mean F1 score on the GAP validation set as a function of the number of memory cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Error in counting unique people as a function of number of memory cells; lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A successful run of PeTra with 4 memory cells. The model accurately links all the mentions of "Amelia" to the same memory cell while also detecting other people in the discourse.Bethenny1 calls a meeting to get everyone on the same page, but Jason2 is hostile with the group, making things worse and forcing Bethenny1 to play referee. Emotions are running high with Bethenny1 's assistant, Julie3 , who breaks down at a lunch meeting when asked if she3 is committed to the company for the long haul.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Agreement matrix for human evaluation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Instructions for the human evaluation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Memory log of PeTra with 8 memory cells. PeTra uses only 2 memory cells for the 2 unique people, namely Neef and Svetlana Goncharenko, and correctly resolves the pronoun. log of the Referential Reader with 8-memory cells. The Referential Reader does successfully resolve the pronoun in the topmost memory cell but it ends up tracking Neef in as many as 4 memory cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Both the models only weakly detect "Svetlana Goncharenko" which could be due to lack of span modeling.Fripp 1 has performed Soundscapes in several situations: * Fripp 1 has featured Soundscapes on various King Crimson albums. He 1 has also released pure Soundscape recordings as well: * On May 4, 2006, Steve Ball 2 invited Robert Fripp 1 back to the Microsoft campus for a second full day of work on Windows Vista following up on his 1 first visit in the Fall of 2005.(a) GAP validation instance 17. The ground truth GAP annotation is indicated via colors. Memory log of PeTra with 8-memory cells. PeTra is pretty accurate at tracking Robert Fripp but it misses out on connecting "Fripp" from the earlier part of the document to "Robert Fripp".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Results (%F1) on the GAP test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Spearman's correlation between GAP validation F1 and negative error count for unique people.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Human Evaluation results for people tracking.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>A comparison of best runs vs. worst runs.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In the GAP dataset, the only relevant entities are people.(a) BERTBASE (b) BERTLARGE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The computation of this probability includes the mention detection steps required byWebster et al.(2018).6  GAP also includes evaluation related to gender bias, but this is not a focus of this paper so we do not report it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Each correlation is computed over 50 runs (5 runs each for 10 memory sizes).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under Award Nos. 1941178 and 1941160.</p><p>We thank the ACL reviewers, Sam Wiseman, and Mrinmaya Sachan for their valuable feedback. We thank Fei Liu and Jacob Eisenstein for answering questions regarding the Referential Reader. Finally, we want to thank all the annotators at TTIC who participated in the human evaluation study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Lewke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anya</forename><surname>Mansoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01140</idno>
		<title level="m">An Annotated Dataset of Coreference in English Literature</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Rong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving Coreference Resolution by Learning Entity-Level Distributed Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Models for Reasoning over Multiple Mentions Using Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural Turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriàpuigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Koray Kavukcuoglu, and Demis Hassabis</title>
		<meeting><address><addrLine>Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="page" from="471" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entity Tracking Improves Cloze-style Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic Entity Representations in Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT for Coreference Resolution: Baselines and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cognitively Plausible Models of Human Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Higher-Order Coreference Resolution with Coarseto-Fine Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Narrative Modeling with Memory Chains and Semantic Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Referential Reader: A recurrent entity network for anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic Knowledge and Transferability of Contextual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document Context Neural Machine Translation with Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameen</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dialog state tracking, a machine reading approach using Memory Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoNLL-2012 Shared Task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mk Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5217</biblScope>
			<biblScope unit="page">268</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Memory Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Global Features for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01746</idno>
		<title level="m">Coreference Resolution as Query-based Span Prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
