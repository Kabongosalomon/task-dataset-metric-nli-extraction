<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ImageNet-21K Pretraining for the Masses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-04">4 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<email>tal.ridnik@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<email>asaf.noy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ImageNet-21K Pretraining for the Masses</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-04">4 May 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which contains more pictures and classes, is used less frequently for pretraining, mainly due to its complexity, and underestimation of its added value compared to standard ImageNet-1K pretraining. This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone. Via a dedicated preprocessing stage, utilizing Word-Net hierarchies, and a novel training scheme called semantic softmax, we show that various models, including small mobile-oriented models, significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks. We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT. Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at: https://github.com/Alibaba-MIIL/ImageNet21K</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageNet-1K dataset, introduced for the ILSVRC2012 visual recognition challenge <ref type="bibr" target="#b38">[39]</ref>, has been at the center of modern advances in deep learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref>. ImageNet-1K serves as the main dataset for pretraining of models for computer-vision transfer learning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>, and improving performances on ImageNet-1K is often seen as a litmus test for general applicability on downstream tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>ImageNet-1K is a subset of the full ImageNet dataset <ref type="bibr" target="#b8">[9]</ref>, which consists of <ref type="bibr" target="#b13">14,</ref><ref type="bibr">197,</ref><ref type="bibr">122</ref> images, divided into 21,841 labels. We shall refer to the full dataset as ImageNet-21K, similar to <ref type="bibr" target="#b23">[24]</ref> (although other papers sometimes described it as ImageNet-22K <ref type="bibr" target="#b5">[6]</ref>). ImageNet-1K was created by selecting a subset of 1.2M images from ImageNet-21K, that belong to 1000 mutually exclusive classes. ImageNet-21K Pretraining ImageNet-1K Pretraining <ref type="figure">Figure 1</ref>. Transfer learning results: comparing transfer learning results, for different models and pretraining schemes, on iNaturalist dataset. Markers sizes are proportional to the models' memory footprint. Similar results on datasets from other computer-vision domains and tasks are shown later in the paper.</p><p>Even though some previous works showed that pretraining on ImageNet-21K could provide better downstream results for large models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref>, pretraining on ImageNet-1K remained far more popular. A main reason for this discrepancy is that ImageNet-21K labels are not mutually exclusive -the labels are taken from WordNet <ref type="bibr" target="#b32">[33]</ref>, where each image is labeled with one label only, not necessarily at the highest possible hierarchy of WordNet semantic tree. For example, ImageNet-21K dataset contains the labels "chair" and "furniture". A picture with an actual chair can be labeled as "chair", but sometimes be labeled as the semantic parent of "chair", "furniture". This kind of tagging methodology complicates the training process, and makes evaluating models on ImageNet-21K far less accurate. Other challenges of ImageNet-21K dataset are the lack of official train-validation split, the fact that training is longer than ImageNet-1K and requires highly efficient training schemes, and that the raw dataset is large -1.2TB.</p><p>Several works in the past did use ImageNet-21K for pretraining, mostly in comparison to larger datasets, which are not publicly available, such as JFT-300M <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b36">[37]</ref> used ImageNet-21K and JFT-300M to train expert models according to the datasets hierarchies, and com- <ref type="bibr">Figure 2</ref>. Our end-to-end pretraining pipeline on ImageNet-21K. We start with a dataset preparation and preprocessing stage. Via WordNet's synsets, we convert all the single-label inputs to semantic multi-labels, resulting in a semantic structure for ImageNet-21K, with 11 possible hierarchies. For each hierarchy, we apply a dedicated softmax activation, and aggregate the losses with hierarchy balancing. bined them to ensembles on downstream tasks; <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b11">[12]</ref> compared pretraining JFT-300M to ImageNet-21K on large models such as ViT and ResNet-50x4. Many papers used these pretrained models for downstream tasks ( <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1]</ref> for example). There are also works on ImageNet-21K that did not focus on pretraining: <ref type="bibr" target="#b52">[53]</ref> used extra (unlabled) data from ImageNet-21K to improve knowledge-distillation training on ImageNet-1K; <ref type="bibr" target="#b10">[11]</ref> used ImageNet-21k for testing few-shot learning; <ref type="bibr" target="#b47">[48]</ref> tested efficient softmax schemes on ImageNet-21k; <ref type="bibr" target="#b14">[15]</ref> tested pooling operations schemes on animal-oriented subset of ImageNet-21k.</p><p>However, previous works have not methodologically studied and optimized the pretraining process specifically on ImageNet-21K. Since this is a large-scale, high-quality, publicly available dataset, this kind of study can be highly beneficial to the community. We wish to close this gap in this work, and make efficient top-quality pretraining on ImageNet-21K accessible to all deep learning practitioners. An example of the benefit gained from high-quality pretraining is given in <ref type="figure">Figure 1</ref>, showing significant accuracy improvement, for a variety of architectures, when switching from ImageNet-1K to our ImageNet-21K pretraining scheme.</p><p>Our pretraining pipeline starts by preprocessing ImageNet-21K to ensure all classes have enough images for meaningful learning, splitting the dataset to a standardized train-validation split, and resizing all images to reduce memory footprint. Using WordNet semantic tree <ref type="bibr" target="#b32">[33]</ref>, we show that ImageNet-21K can be transformed into a semantic multi-label dataset. We thoroughly analyze the advantages and disadvantages of single-label and multi-label training. Extensive tests on downstream tasks show that multi-label pretraining does not improve results on downstream tasks despite having more information per image. To effectively utilize the semantic data, we develop a novel training method, called semantic softmax, which exploits the hierarchical structure of ImageNet-21K tagging to train the network over several semantic softmax layers, instead of the single layer. Using semantic softmax pretraining, we consistently outperform both single-label and multi-label pretraining on downstream tasks. By integrating semantic softmax into a dedicated semantic knowledge distillation loss, we further improved results. The complete end-to-end pretraining pipeline appears in <ref type="figure">Figure 2</ref>.</p><p>Using semantic softmax pretraining on ImageNet-21K we achieve significant improvement on downstream tasks, compared to standard ImageNet-1K pretraining. Unlike previous works, which focused on pretraining of large models only <ref type="bibr" target="#b23">[24]</ref>, we show that ImageNet-21K pretraining benefits a wide variety of models, from larger models like TResNet-L <ref type="bibr" target="#b37">[38]</ref>, through medium-sized models like ResNet50 <ref type="bibr" target="#b16">[17]</ref>, and even small mobile-dedicated models like OFA-595 <ref type="bibr" target="#b3">[4]</ref> and MobileNetV3 <ref type="bibr" target="#b17">[18]</ref>. Our proposed pretraining scheme also outperforms other ImageNet-21K pretraining schemes, such as the scheme used to trained ViT <ref type="bibr" target="#b11">[12]</ref>. Finally, we obtain SoTA results on transfer learning to ImageNet-1K. For example, ResNet50 model achieves top-1 accuracy of 82.0%. The paper contribution can be summarized as follows:</p><p>• We develop a methodical preprocess procedure to transform raw ImageNet into a viable dataset for efficient, high-quality pretraining. • Using WordNet semantic tree, we can convert each single-label tagging to semantic multi-labels, and compare the pretrain quality of two baseline methods: singlelabel and multi-label pretraining. We show that while a multi-label approach provides more information per image, it has significant optimization drawbacks, resulting in inferior results on downstream tasks. • We develop a novel training scheme called semantic softmax, which exploits the hierarchical structure of ImageNet-21K. With semantic softmax pretraining, we outperform both single-label and multi-label pretraining on downstream tasks. We further improve results by integrating semantic softmax into a dedicated semantic knowledge distillation scheme. • We show that compared to ImageNet-1K pretraining, pretraining on ImageNet-21K significantly improves results not only for large architectures, but also for mobileoriented architectures. Out pretraining scheme also outperforms previous pretraining schemes for prominent new models like Vision-Transformer (ViT).</p><p>In the past, pretraining on ImageNet-21K was out of scope for the common deep learning practitioner. With our proposed pipeline, high-quality efficient pretraining on ImageNet-21K will be more accessible to the deep learning community. Our preprocessing scheme, training code and pretrained models will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset Preparation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing ImageNet-21K</head><p>Our preprocessing stage consists of three steps, as described in <ref type="figure">Figure 2</ref>: (1) invalid classes cleaning, (2) creating a validation set, (3) image resizing. Details are as follows</p><p>Step 1 -cleaning invalid classes: the full ImageNet-21K dataset <ref type="bibr" target="#b8">[9]</ref> consists of 14,197,122 images, each tagged in a single-label fashion by one of 21,841 possible classes. The dataset has no official train-validation split, and the classes are not well-balanced -some classes contain only 1-10 samples, while others contain thousands of samples. Classes with few samples cannot be learned efficiently, and may hinder the entire training process and hurt the pretrain quality <ref type="bibr" target="#b19">[20]</ref>. Hence we start our preprocessing stage by removing invalid classes, with less than 500 labels. After this stage, the dataset contains 12,358,688 images from 11,221 classes. Notice that the cleaning process of invalid classes reduces the number of total classes by half, but removes only 13% of the original pictures.</p><p>Step 2 -validation split: For the valid classes, we allocate 50 images per class for a standardized validation split, that can be used for future benchmarks and comparisons.</p><p>Step 3 -image resizing: ImageNet-1K training usually uses crop-resizing <ref type="bibr" target="#b18">[19]</ref> which favours loading the original images at full resolution and resizing them on-the-fly. To make ImageNet-21K dataset more accessible and accelerate training, we resized during the pre-process stage all the images to 224 resolution (equivalent to squish-resizing <ref type="bibr" target="#b18">[19]</ref>). While somewhat limiting scale augmentations, this stage significantly reduces the dataset's memory footprint, from 1.2TB to 250GB, and makes loading the data during training faster.</p><p>After finishing the preprocessing stage, we kept only valid classes, produced a standardized train-validation split, and significantly reduced the dataset size. We shall name this processed dataset ImageNet-21K-P (P for Processed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Utilizing Semantic Data</head><p>We now wish to analyze the semantic structure of ImageNet-21K-P dataset. This structure will enable us to better understand ImageNet-21K-P tagging methodology, and employ and compare different pretraining schemes.</p><p>From a single label to semantic multi labels: Each image in the original ImageNet-21K dataset was labeled with a single label, that belongs to WordNet synsets <ref type="bibr" target="#b32">[33]</ref>. Using the WordNet synset hyponym (subtype) and hypernym (supertype) relations, we can obtain for each class its parent class, if exists, and a list of child classes, if exists. When applying the parenthood relation recursively, we can build a semantic tree, that enables us to transform ImageNet-21K-P dataset into a multi-label dataset, where each image is associated with several labels -the original label, and also its parent class, parent-of-parent class, and so on. Example is given in <ref type="figure">Figure 2</ref> (middle picture). The original image was labeled as 'swan'. By utilizing the semantic tree, we can produce a list of semantic labels for the image -swan, aquatic bird, bird, vertebrate, animal. Notice that the labels are sorted by hierarchy: 'animal' label belongs to hierarchy 0, while 'swan' label belongs to hierarchy 4. A label from hierarchy k has k ancestors.</p><p>Understanding the inconsistent tagging methodology: The semantic structure of ImageNet-21K enables us to understand its tagging methodology better. According to the stated tagging methodology of ImageNet-21K <ref type="bibr" target="#b8">[9]</ref>, we are not guaranteed that each image was labeled at the highest possible hierarchy. An example is given in <ref type="figure" target="#fig_1">Figure 3</ref>. Two pictures, that contain the animal cow, were labeled differently -one with the label 'animal', the other with the label 'cow'. Notice that 'animal' is a semantic ancestor of 'cow' (cow → placental → mammal → vertebrate → animal). This kind of incomplete tagging methodology, which is common in large datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>, hinders and complicates the training process. A dedicated scheme that tackles this tagging methodology will be presented in section 3.3.</p><p>Semantic statistics: By using WordNet synsets, we can calculate for each class the number of ancestors it has -its hierarchy. In total, our processed dataset, ImageNet-21K-P, has 11 possible hierarchies. Examples of classes from different hierarchies appear in <ref type="table" target="#tab_0">Table 1</ref>. In <ref type="figure" target="#fig_4">Figure 5</ref> in ap-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy</head><p>Example Classes 0 person, animal, plant, food, artifact 1 domestic animal, basketball court, clothing ... <ref type="bibr" target="#b5">6</ref> whitetip shark, ortolan, grey kingbird pendix A we present the number of classes per hierarchy. We see that while there are 11 possible hierarchies, the vast majority of classes belong to the lower hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pretraining Schemes</head><p>In this section, we will review and analyze two baseline schemes for pretraining on ImageNet-21K-P -single-label and multi-label training. We will also present a novel new scheme for pretraining on ImageNet-21K-P -semantic softmax, and analyze its advantages over the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-label Training Scheme</head><p>The straightforward way to pretrain on ImageNet-21K-P is to use the original tagging, as-is, and train our models in a single-label fashion, meaning applying softmax on the outputted logits, and use cross-entropy loss. Our single-label training scheme is similar to the common efficient training schemes on ImageNet-1K <ref type="bibr" target="#b37">[38]</ref>, with three main adaptations:</p><p>1. Due to the incomplete tagging of ImageNet-21K-P, we increase label-smooth factor from 0.1 to 0.2, to better handle ground-truth inconsistencies. 2. As explained in section 2.1, we use squish-resizing instead of crop-resizing. 3. To shorten the training times, we initialize our model from ImageNet-1K training, and train on ImageNet-21K for 80 epochs. On an 8xV100 NVIDIA GPU machine, training with mixed-precision takes 40 minutes per epoch on ResNet50 and TResNet-M architectures (∼ 5000 img sec ), leading to a total training time of 54 hours. Since we aim for an efficient scheme with maximal throughput, our training scheme does not incorporate any tricks that might significantly increase training times. Full training details appear in appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pros of using single-label training:</head><p>• Well-balanced dataset -With single-label training on ImageNet-21K-P, the dataset is well-balanced, meaning each class appears, roughly, the same number of times. • Single-task training -training with a softmax (a single loss) makes convergence easy and efficient, and avoids many optimization problems associated with multi-loss learning, such as different gradient magnitudes and gradient interference <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Cons of using single-label training:</p><p>• Inconsistent ground-truth -due to the tagging methodology of ImageNet-21K-P, where we are not guaranteed that an image was labeled at the highest possible hierarchy, ground-truth labels are inherently inaccurate. There cannot be a single-label correct tagging for an image. • No semantic data -we are hiding data from the network, and not revealing the full semantic tree via the singlelabel ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-label Training Scheme</head><p>Using the semantic tree, we can convert any (single) label to semantic multi labels, and train our models on ImageNet-21K-P in a multi-label fashion, expecting that the additional data per image will improve the pretrain quality.</p><p>As commonly done in multi-label classification <ref type="bibr" target="#b2">[3]</ref>, we reduce the problem to a series of binary classification tasks. Given N labels, the base network outputs one logit per label, z n . Each logit is independently activated by a sigmoid function σ(z n ). Let's denote y n as the ground-truth for class n. The total classification loss, L tot , is obtained by aggregating a binary loss from N labels:</p><formula xml:id="formula_0">L tot = N n=1 L (σ(z n ), y n ) .<label>(1)</label></formula><p>From Eq. 1 we see that in practice, multi-label classification can be viewed as a multi-task problem. Since we have a very large number of classes <ref type="bibr" target="#b10">(11,</ref><ref type="bibr">221)</ref>, this is an extreme multi-task case. For training, we adopted the high-quality training scheme described in <ref type="bibr" target="#b2">[3]</ref>, which was tested on other large-scale multi-label datasets, such as Open Images <ref type="bibr" target="#b27">[28]</ref>. Full training details appear in appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pros of using multi-label training</head><p>• More information per image -we are not hiding relevant data from the network, and providing for each image all the available semantic labels. • Metrics are more accurate -since we have more informative ground-truth, training metrics should be more accurate and reflective than single-label training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cons of using multi-label training</head><p>• Extreme multi-tasking -with multi-label training, each class is learned separately (sigmoids instead of softmax). This extreme multi-task learning makes the optimization process harder, and may cause convergences to a weak local minimum <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref>]. • Extreme imbalancing -As a multi-label dataset with large number of classes, ImageNet-21K-P suffers from large positive-negative imbalance <ref type="bibr" target="#b2">[3]</ref>. In addition, due to the semantic structure, multi-label training is hindered by a large class imbalance <ref type="bibr" target="#b20">[21]</ref> -on average, classes from a lower hierarchy will appear far more frequent than classes from a higher hierarchy.</p><p>In appendices C and E we show that ASL loss <ref type="bibr" target="#b2">[3]</ref>, that was designed to cope with large positive-negative imbalancing, significantly outperforms cross-entropy loss, both on upstream and downstream tasks. This supports our analysis of extreme imbalancing as a major optimization challenge of multi-label training. Notice that we also list extreme multi-tasking as another optimization pitfall of multilabel training, and a dedicated scheme for dealing with it might further improve results. However, most methods that tackle multi-task learning, such as GradNorm <ref type="bibr" target="#b4">[5]</ref> and PC-Grad <ref type="bibr" target="#b51">[52]</ref>, require computation of gradients for each class separately. This is computationally infeasible for a dataset with a large number of classes, such as ImageNet-21K-P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Softmax Training Scheme</head><p>We want to develop a dedicated training method that utilizes the advantages of both the single label and the multilabel pretraining. Specifically, our training scheme should achieve the following goals:</p><p>• No hiding of data from the network -present for each input image all the available semantic labels. • Use softmax activations instead of independent sigmoids, to avoid extreme multi-tasking and extreme positive-negative imbalancing • Provide the network direct data on the semantic hierarchies. Notice that this is not achieved even in multi-label training, the hierarchical structure is implicit there. • Have fully accurate ground-truth and training metrics.</p><p>• Remain an efficient scheme in terms of training times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Semantic Softmax Formulation</head><p>To meet these goals, we develop a new training scheme we call semantic softmax training. As we saw in section 2.2, each label in ImageNet-21K-P can belong to one of 11 possible hierarchies. By definition, for each hierarchy there can be only one ground-truth label per input image. Hence, instead of single-label training with a single softmax, we shall have 11 softmax layers for the 11 hierarchies. Each softmax will sample the relevant logits from the corresponding hierarchy, as shown in <ref type="figure">Figure 2</ref> (rightmost image).</p><p>To deal with the partial tagging of ImageNet-21K-P, not all softmax layers will propagate gradients from each sample. Instead, we will activate only softmax layers from the relevant hierarchies. An example is given in <ref type="figure" target="#fig_2">Figure 4</ref>. The original image had a label from hierarchy 5. We transform it to 6 semantic ground-truth labels, for hierarchies 0-5, and activate only the 6 first semantic softmax layers. Only activated layers will propagate gradients.</p><p>Compared to single-label and multi-label schemes, semantic softmax training has the following advantages:</p><p>1. We avoid extreme multi-tasking <ref type="bibr">(11, 221 uncoupled</ref> losses in multi-label training). Instead, we have only 11 losses, as the number of softmax layers. 2. We present for each input image all the possible semantic labels. The loss scheme even provides direct data on the hierarchical structure. 3. Unlike single-label and multi-label training, semantic softmax ground-truth and training metrics are fully accurate. If a sample has no labels at hierarchy k, we don't propagate gradients from the k softmax, instead of assuming that all the labels on missing hierarchies are negative. 4. Calculating several softmax instead of a single one has negligible overhead, and in practice training times are similar to single-label training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Weighing the Different Softmax Layers</head><p>For each input image we have K losses <ref type="bibr" target="#b10">(11)</ref>. As commonly done in multi-task training <ref type="bibr" target="#b4">[5]</ref>, we need to aggregate them to a single loss. A naive solution will be to sum them:</p><formula xml:id="formula_1">L tot = K−1 k=0 L k .<label>(2)</label></formula><p>Where L k , the loss per softmax layer, is zero when the layer is not activated. However, Eq. 2 ignores the fact that softmax layers in lower hierarchies will be activated much more frequently than softmax layers in higher hierarchies. This will lead to over-emphasizing classes from lower hierarchies, on account of under-emphasizing classes from higher hierarchies. To deal with this imbalancing, we will employ a balancing logic: let N j be the total number of classes in hierarchy j (as presented in <ref type="figure" target="#fig_4">Figure 5</ref>). Due to the semantic structure, the relative number of occurrences of hierarchy k loss will be:</p><formula xml:id="formula_2">O k = k−1 j=0 N j .<label>(3)</label></formula><p>Hence, to balance the contribution of different hierarchies we can use a normalization factor W k = 1 O k , and obtain a balanced aggregation loss:</p><formula xml:id="formula_3">L tot = K−1 k=0 W k L k .<label>(4)</label></formula><p>This is the loss we will use in our semantic softmax training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Semantic Knowledge Distillation</head><p>Knowledge distillation (KD) is a known method to improve not only upstream, but also downstream results <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. We want to combine our semantic softmax scheme with KD trainingsemantic KD. In addition to the general benefit of KD training <ref type="bibr" target="#b15">[16]</ref>, semantic KD has another significant advantage -it can predict the missing tags that arise from the inconsistent tagging of ImageNet-21K-P. For example, for the left picture in <ref type="figure" target="#fig_1">Figure 3</ref>, the teacher model can predict the missing labels (cow, placental, mammal, vertebrate). A vanilla implementation of semantic KD loss will be as following: for each hierarchy, we calculate for both the teacher and the student the corresponding probability dis-</p><formula xml:id="formula_4">tributions {T i } K−1 i=0 , {S i } K−1 i=0</formula><p>. The KD loss of hierarchy i will be:</p><formula xml:id="formula_5">L KD i = KDLoss(T i , S i )<label>(5)</label></formula><p>Where KDLoss is a standard measurement for distance between distributions, that can be chosen as Kullback-Leibler divergence <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref>, or as MSE loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46]</ref>. We have found that the latter converges faster, and used it. The total loss will be the sum of the the losses from different hierarchies:</p><formula xml:id="formula_6">L KD = K−1 i=0 L KDi .<label>(6)</label></formula><p>However, Eq. 6 assumes that all the hierarchies are relevant for each image. This is inaccurate -usually higher hierarchies represent subspecies of animals or plants, and are not applicable for a picture of a chair, for example. So we need to determine from the teacher predictions which hierarchies are relevant, and weight the different hierarchy losses appropriately. Let's assume that for each hierarchy we can calculate the teacher confidence level, P i . A rectified version the the KD loss will be:</p><formula xml:id="formula_7">L KD = K−1 i=0 P i L KDi .<label>(7)</label></formula><p>Eq. 7 is our proposed semantic KD loss. In appendix F we present a method to calculate the teacher confidence level, P i , from the teacher predictions, similar to <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Study</head><p>In this section, we will present upstream and downstream results for the different training schemes, and show that semantic softmax pretraining outperforms single-label and multi-label pretraining. We will also demonstrate how semantic KD further improves results on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Upstream Results</head><p>In appendix C we provide upstream results for the three training schemes. Since each scheme has its own training metrics, we cannot use these results to compare (pre)training quality directly. However, since we are using a standardized dataset, ImageNet-21K-P, with a fixed trainvalidation split, our upstream results for each pretrainig method can be used for future comparison and benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Results Comparison</head><p>To compare the pretrain quality of different training schemes, we will test our models via transfer learning. To ensure that we are not overfitting a specific dataset or task, we chose a wide variety of downstream datasets, from different computer-vision tasks. We also ensured that our downstream datasets represent a variety of domains, and have diverse sizes -from small datasets of thousands of images, to larger datasets with more than a million images. For single-label classification, we transferred our models to ImageNet-1K <ref type="bibr" target="#b26">[27]</ref>, iNaturalist 2019 <ref type="bibr" target="#b46">[47]</ref>, CIFAR-100 <ref type="bibr" target="#b25">[26]</ref> and Food 251 <ref type="bibr" target="#b21">[22]</ref>. For multi-label classification, we transferred our models to MS-COCO <ref type="bibr" target="#b29">[30]</ref> and Pascal-VOC <ref type="bibr" target="#b13">[14]</ref> datasets. For video action recognition, we transferred our models to Kinetics 200 dataset <ref type="bibr" target="#b22">[23]</ref>. In appendix D we provide full training details on all downstream datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparing Different Pretraining Schemes</head><p>In <ref type="table" target="#tab_1">Table 2</ref> we compare downstream results for three pretraining schemes: single-label, multi-label and semantic softmax. We see that on 6 out of 7 datasets tested, semantic softmax pretraining outperforms both single-label and multi-label pretraining. In addition, we see from <ref type="table" target="#tab_1">Table 2</ref> that single-label pretraining performs better than multi-label pretraining (scores are higher on 5 out of 7 datasets tested).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Label Pretrain</head><p>Mutli Label Pretrain Sematic Softmax Pretrain ImageNet1K <ref type="bibr" target="#b0">(1)</ref> 81.1 81.0 81.4 iNaturalist <ref type="bibr" target="#b0">(1)</ref> 71.5 71.0 72.0 Food 251 <ref type="bibr" target="#b0">(1)</ref> 75.4 75.2 75.8 CIFAR 100 <ref type="bibr" target="#b0">(1)</ref> 89.5 90.6 90.4 MS-COCO <ref type="bibr" target="#b1">(2)</ref> 80.8 80.6 81.3 Pascal-VOC <ref type="bibr" target="#b1">(2)</ref> 88.1 87.9 89.7 Kinetics 200 <ref type="bibr" target="#b2">(3)</ref> 81.9 81.9 83.0 These results support our analysis of the pros and cons of the different pretraining schemes from Section 3: with multi-label training, we have more information per input image, but the optimization process is far less efficient due to extreme multi-tasking and extreme imbalancing. All-inall, multi-label training does not improve downstream results. Single-label training, despite its shortcomings from the partial tagging methodology and the minimal information per image, provides a better pretraining baseline. Our semantic softmax scheme, which utilizes semantic multilabel data without falling to the optimization pitfalls of extreme multi-label training, outperforms both single-label and multi-label training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Semantic KD</head><p>In <ref type="table" target="#tab_2">Table 3</ref> we compare downstream results of semantic softmax pretraining, with and without semantic KD. We see</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sematic Softmax Pretrain</head><p>Sematic Softmax + Semanic KD Pretrain ImageNet1K <ref type="bibr" target="#b0">(1)</ref> 81.4 82.2 iNaturalist <ref type="bibr" target="#b0">(1)</ref> 72.0 72.7 Food 251 <ref type="bibr" target="#b0">(1)</ref> 75.8 76.1 CIFAR 100 <ref type="bibr" target="#b0">(1)</ref> 90.4 91.7 MS-COCO <ref type="bibr" target="#b1">(2)</ref> 81.3 82.2 Pascal-VOC <ref type="bibr" target="#b1">(2)</ref> 89.7 89.8 Kinetics 200 <ref type="bibr" target="#b2">(3)</ref> 83.0 84.4 that on all tasks and datasets tested, adding semantic KD to our pretraining process improves downstream results. In appendix G we compare single-label with KD, to semantic softmax with semantic KD, and show that the latter achieves better results on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the previous chapters we developed a dedicated pretraining scheme for ImageNet-21K-P dataset, semantic softmax, and showed that it outperforms two baseline pretraining schemes, single-label and multi-label, in terms of downstream results. Now we wish to compare our semantic softmax pretraining on ImageNet-21K-P to other known pretraining schemes and datasets from the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison to Other ImageNet-21K Pretraining Schemes</head><p>We would like to compare our proposed training scheme to other ImageNet-21K training schemes from the literature. However, to the best of our knowledge, no previous works have published their upstream scores on ImageNet-21K, or shared thorough details about their training scheme or preprocessing stage. Recently, a prominent model called ViT <ref type="bibr" target="#b11">[12]</ref> was published, and official pretrained weights were released. In <ref type="table" target="#tab_3">Table 4</ref> we compare downstream results when using ViT official ImageNet-21K weights, and when using weights from our semantic softmax pretraining scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViT-B-16 (Official) ImageNet-21K Pretrain</head><p>ViT-B-16 (Ours) ImageNet-21K Pretrain ImageNet1K <ref type="bibr" target="#b0">(1)</ref> 83.3 83.9 iNaturalist <ref type="bibr" target="#b0">(1)</ref> 71.7 73.1 Food 251 <ref type="bibr" target="#b0">(1)</ref> 74.6 76.0 CIFAR 100 <ref type="bibr" target="#b0">(1)</ref> 91.5 92.9 MS-COCO <ref type="bibr" target="#b1">(2)</ref> 81.1 82.6 Pascal-VOC <ref type="bibr" target="#b1">(2)</ref> 78.7 93.1 Kinetics 200 <ref type="bibr" target="#b2">(3)</ref> 82.7 84.1 We see from <ref type="table" target="#tab_3">Table 4</ref> that our pretraining scheme consistently and significantly outperforms the original pretrain, on all downstream tasks. As transformers-for-vision models are gaining popularity, the ability to train them to top results, using a publicly available and efficient scheme like ours can be highly beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream</head><p>Task <ref type="table" target="#tab_0">Dataset  MobileNetV3  OFA-595  ResNet50  TResNet-M  TResNet-L  1K  21K  1K  21K  1K  21K  1K  21K  1K 21K</ref> Single-label Classification iNaturalist <ref type="bibr" target="#b0">(1)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to ImageNet-1K Pretraining</head><p>In <ref type="table" target="#tab_5">Table 5</ref> we compare, for different models, downstream results when using standard ImageNet-1K pretraining and our proposed semantic ImageNet-21K-P pertraining. We can see that our pretraining scheme significantly outperforms standard pretraining on all datasets, for all models tested. For example, on iNaturalist dataset we improve the average top-1 accuracy by 2.9%.</p><p>Notice that some previous works stated that pretraining on a large dataset benefits only large models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref>. Mo-bileNetV3 backbone, for example, has only 4.2M parameters, while ViT-B model has 85.6M parameters. Previous works assumed that a large number of parameters, like ViT has, is needed to properly utilize pretraining on large datasets. However, we show consistently and significantly that even small mobile-oriented models, like MobileNetV3 and OFA-595, can benefit from pretraining on a large (publicly available) dataset like ImageNet-21K-P. Due to their low inference times and reduced heating, mobile-oriented models are used frequently for deployment. Hence, improving their performance by using better pretrain weights can enhance "real-world" products, without any increase in training complexity or inference times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ImageNet-1K SoTA Results</head><p>In <ref type="table">Table 6</ref> we bring downstream results on ImageNet-1K for different models, when using semantic softmax pretraining. To achieve top results, similar to previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50]</ref>, we added standard knowledge distillation loss into our ImageNet-1K training. To the best of our knowledge, for all the models in <ref type="table">Table 6</ref> we achieve a new SoTA record (for input resolution 224). Unlike previous top works, which used private datasets <ref type="bibr" target="#b42">[43]</ref>, we are using a publicly available dataset for pretraining. Also, note that the gap from the original reported accuracies is significant.  <ref type="table">Table 6</ref>: Transfer learning results On ImageNet-1K, when using ImageNet-21K-P pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented an end-to-end scheme for high-quality efficient pretraining on ImageNet-21K dataset. We start by standardizing the dataset preprocessing stage. Then we show how we can transform ImageNet-21K dataset into a multi-label one, using WordNet semantic. Via extensive tests on downstream tasks, we demonstrate how single-label training outperforms multi-label training, despite having less information per image. We then develop a new training scheme, called semantic softmax, which utilizes ImageNet-21K hierarchical structure to outperform both single-label and multi-label training. We also integrate the semantic softmax scheme into a dedicated knowledge distillation loss to further improve results.</p><p>On a variety of computer vision datasets and tasks, different models significantly benefit from our ImageNet-21K pretraining scheme, including small mobile-oriented models. We also show that our pretraining scheme outperforms previous pretraining schemes for prominent new models like ViT.</p><p>To this day, pretraining on ImageNet-21K was out of scope for most deep learning practitioners. With our proposed pipeline, high-quality efficient pretraining on ImageNet-21K will be more accessible to the deep learning community.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Single-label ImageNet-21K-P Training Details</head><p>To shorten ImageNet-21K-P training times, we initialized our models from standard ImageNet-1K pretraining (pretraining weights taken from <ref type="bibr" target="#b48">[49]</ref>). We trained the models with input resolution 224, using an Adam optimizer with learning rate of 3e-4 and one-cycle policy <ref type="bibr" target="#b41">[42]</ref>. We found that 80 epochs are enough for achieving strong pretrain results. For regularization, we used RandAugment <ref type="bibr" target="#b7">[8]</ref>, Cutout <ref type="bibr" target="#b9">[10]</ref>, Label-smoothing <ref type="bibr" target="#b43">[44]</ref> and True-weight-decay <ref type="bibr" target="#b31">[32]</ref>. We found that the common ImageNet statistics normalization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref> does not improve the training accuracy, and instead normalized all the RGB channels to be between 0 and 1. Unless stated otherwise, all runs and tests were done on TResNet-M architecture.</p><p>On an 8xV100 NVIDIA GPU machine, training with mixed-precision takes 40 minutes per epoch on ResNet50 and TResNet-M architectures (∼ 5000 img sec ), leading to a total training time of 54 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Multi-label ImageNet-21K-P Training Details</head><p>For multi-label training, we convert each image single label input to semantic multi labels, as described in section 2.2. Multi-label training details are similar to singlelabel training (number of epochs, optimizer, augmentations, learning rate, models initialization and so on), and training times are also similar. The main difference between single-label and multi-label training relies in the loss function: for multi-label training we tested 3 loss functions, following <ref type="bibr" target="#b2">[3]</ref>: cross-entropy (γ − = γ + = 0), focal loss (γ − = γ + = 2) and ASL (γ − = 4, γ + = 0). For ASL, we tried different values of γ − to obtain the best mAP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Upstream Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Singe-label Upstream Results</head><p>For single-label training, regular top-1 accuracy metric becomes somewhat irrelevant -if pictures with similar content have different ground-truth labels, the network has no clear "correct" answer. Top-5 accuracy metric is more representative, but still limited. Upstream results of singlelabel training are given in <ref type="table" target="#tab_8">Table 7</ref>. We can see that the top-1 accuracies obtained on ImageNet-21K-P, 37% − 46%, are significantly lower than the ones obtained on ImageNet-1K, 75% − 85%. This accuracy drop is mainly due to the semantic structure and inconsistent tagging methodology of ImageNet-21K-P. However, as we take bigger and better architectures, we see from <ref type="table" target="#tab_8">Table 7</ref> that the accuracies continue to improve, so we are not completely hindered by the inconsistent tagging.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Multi-label Upstream Results</head><p>For multi-label training, we will use the common micro and macro mAP accuracy <ref type="bibr" target="#b2">[3]</ref> as training metrics. However, due to the missing labels in the validation (and train) set, this metric also is not fully accurate. In <ref type="table" target="#tab_11">Table 9</ref> we compare the results for three possible loss functions for multi-label classification -cross-entropy, focal loss and ASL. We see that ASL loss <ref type="bibr" target="#b2">[3]</ref>, that was designed to cope  with large positive-negative imbalancing, outperform crossentropy and focal loss. This is in agreement with our analysis in section 3.2, where we identify extreme imbalancing as one of the optimization challenges that stems from multilabel training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Semantic Softmax Upstream Results</head><p>With semantic softmax training, we can calculate for each hierarchy its top-1 accuracy metric. We can also cal-culate the total accuracy by weighting the different accuracies by the number of classes in each hierarchy (see <ref type="figure" target="#fig_4">Figure  5</ref>). Notice that we are not using classes above the maximal hierarchy for our metrics calculation. Hence, and unlike single-label and multi-label training, with semantic softmax training our metrics are fully accurate.</p><p>In <ref type="figure" target="#fig_5">Figure 6</ref> we present the top-1 accuracies achieved by different models on different hierarchy levels, when trained with semantic softmax (with KD). As we have a standard- ized dataset with train-validation split, and since the metrics of semantic softmax are fully accurate, the results in <ref type="figure" target="#fig_5">Figure  6</ref> can be used for future benchmark and comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Downstream Datasets Training Details</head><p>For single-label classification, our downstream datasets were ImageNet-1K <ref type="bibr" target="#b26">[27]</ref>, iNaturalist 2019 <ref type="bibr" target="#b46">[47]</ref>, CIFAR-100 <ref type="bibr" target="#b25">[26]</ref> and Food-251 <ref type="bibr" target="#b21">[22]</ref>. For multi-label classification, our downstream datasets were MS-COCO <ref type="bibr" target="#b29">[30]</ref> and Pascal-VOC <ref type="bibr" target="#b13">[14]</ref>. For video action recognition, our downstream dataset was Kinetics-200 <ref type="bibr" target="#b22">[23]</ref>. General details:</p><p>• To minimize statistical uncertainty, for datasets with less than 150, 000 images (CIFAR-100, Food-251, MS-COCO, Pascal-VOC), we report result of averaging 3 runs with different seeds. • All results are reported for input resolution 224.</p><p>• For all downstream datasets we used cutout of 0.5, rand-Augment and true-weight-decay of 1e-4. • All single-label datasets are trained with label-smooth of 0.1 • Unless stated otherwise, dataset was trained for 40 epochs with Adam optimizer, learning rate of 3e-4, onecycle policy and and squish-resizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific dataset details:</head><p>• ImageNet-1K -We finetuned our networks for 100 epochs using SGD optimizer, and learning rate of 4e-4. We used crop-resizing with the common minimal crop factor of 0.08.</p><p>• MS-COCO -We used ASL loss with γ − = 4.</p><p>• Pascal-VOC -We used ASL loss with γ − = 4, and learning rate of 5e-5. • Kinetics-200 -we trained for 30 epochs with learning rate of 8e-5. We used the training method described in <ref type="bibr" target="#b40">[41]</ref>, with simple averaging of the embedding from each sample along the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Downstream Results for Different Multilabel Losses</head><p>In <ref type="table" target="#tab_11">Table 9</ref> we compare downstream results when using multi-label pretraining with vanilla cross-entropy (CE) loss and ASL loss. We see that on all downstream datasets, pretraining with ASL leads to significantly better results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Multi Label Pretrain (CE)</p><p>Multi Label Pretrain (ASL) ImageNet1K <ref type="bibr" target="#b0">(1)</ref> 79.6 81.0 iNaturalist <ref type="bibr" target="#b0">(1)</ref> 69.4 71.0 Food 251 <ref type="bibr" target="#b0">(1)</ref> 74.3 75.2 CIFAR 100 <ref type="bibr" target="#b0">(1)</ref> 89.9 90.6 MS-COCO <ref type="bibr" target="#b1">(2)</ref> 79.1 80.6 Pascal-VOC <ref type="bibr" target="#b1">(2)</ref> 87.6 87.9 Kinetics 200 <ref type="bibr" target="#b2">(3)</ref> 81.1 81.9 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Calculating Teacher Confidence</head><p>Using the teacher prediction for hierarchy i and the semantic ground-truth, we want to evaluate the teacher confidence level, P i , so we can weight properly the contribution of different hierarchies in the KD loss. Our proposed logic for calculating the teacher's (semantic) confidence is simple: if the ground-truth highest hierarchy is higher than i, set P i to 1. Else, calculate the sum probabilities of the top 5% classes in the teacher prediction (we deliberately don't take the probability of only the highest class, to account for class similarities).</p><p>In <ref type="figure" target="#fig_6">Figure 7</ref> we present the teacher confidence level for different hierarchies, averaged over an epoch. We can see that lower hierarchies have, in average, higher confidence levels. This stems from the fact that not all hierarchies are relevant for each image. For the picture in <ref type="figure" target="#fig_2">Figure 4</ref>, for example, only hierarchies 0-5 are relevant, so we expect the teacher will have low confidence for hierarchies higher than 5.  <ref type="bibr" target="#b0">(1)</ref> 81.5 82.2 iNaturalist <ref type="bibr" target="#b0">(1)</ref> 72.4 72.7 Food 251 <ref type="bibr" target="#b0">(1)</ref> 76.0 76.1 CIFAR 100 <ref type="bibr" target="#b0">(1)</ref> 91.0 91.7 MS-COCO <ref type="bibr" target="#b1">(2)</ref> 81.6 82.2 Pascal-VOC <ref type="bibr" target="#b1">(2)</ref> 89.0 89.8 Kinetics 200 <ref type="bibr" target="#b2">(3)</ref> 83.6 84.4 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example of inconsistent tagging in ImageNet-21K dataset. Two pictures containing the same animal were labeled differently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Semantic softmax gradient propagation logic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Number of classes in different hierarchies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Top-1 accuracies on different hierarchies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Teacher average confidence levels for different hierarchies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of classes from different ImageNet-21K-P hierarchies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparing downstream results for different pretraining schemes.</figDesc><table><row><cell>Dataset types and metrics: (1) -</cell></row><row><cell>single-label, top-1 Acc.[%] ; (2) -multi-label, mAP [%];</cell></row><row><cell>(3) -action recognition, top-1 Acc. [%].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Testing the effect of semantic KD on down- stream results. Dataset types and metrics: (1) -single- label, top-1 Acc.[%] ; (2) -multi-label, mAP [%]; (3) -ac- tion recognition, top-1 Acc. [%].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparing downstream results for different ImageNet-21K pretraining schemes.</figDesc><table><row><cell>Dataset types and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparing</figDesc><table><row><cell>downstream results for ImageNet-1K standard pretraining, and our proposed ImageNet-21K-P</cell></row><row><cell>pretraining scheme. (1) -single-label dataset, top-1 Acc [%] metric; (2) -multi-label dataset, mAP [%] metric; (3) -action</cell></row><row><cell>recognition dataset, top-1 Acc [%] metric.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>For example, MobileNetV3 reported accuracy was 75.2% [18] -we achieved 78.0%. ResNet50 reported accuracy was 76.0% [17] -we achieved 82.0%.</figDesc><table><row><cell>Model Name</cell><cell>ImageNet-1K + KD Top-1 Acc. [%]</cell></row><row><cell>MobileNetV3</cell><cell>78.0</cell></row><row><cell>OFA-595</cell><cell>81.0</cell></row><row><cell>ResNet50</cell><cell>82.0</cell></row><row><cell>TResNet-M</cell><cell>83.1</cell></row><row><cell>TResNet-L</cell><cell>83.9</cell></row><row><cell>ViT-B-16</cell><cell>84.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Accuracy of different models in single-label training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparing different loss functions for multilabel classification on ImageNet-21K-P.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparing downstream results for different losses of multi-label pretraining. Dataset types and metrics: (1) -single-label, top-1 Acc.[%] ; (2) -multi-label, mAP [%]; (3) -action recognition, top-1 Acc. [%].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparing KD with different schemes.</figDesc><table><row><cell>Dataset types and metrics: (1) -single-label, top-1 Acc.[%]</cell></row><row><cell>; (2) -multi-label, mAP [%]; (3) -action recognition, top-1</cell></row><row><cell>Acc. [%].</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivek Natarajan, and Mohammad Norouzi. Big self-supervised models advance medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caruana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6184</idno>
		<title level="m">Do deep nets really need to be deep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale out for large minibatch sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeriu</forename><surname>Codreanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Podareanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Saletore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04291</idno>
	</analytic>
	<monogr>
		<title level="m">Residual network training on imagenet-1k with improved accuracy and reduced time to train</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Crawshaw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09796,2020.4</idno>
		<title level="m">Multi-task learning with deep neural networks: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<title level="m">Adapting auxiliary losses using gradient similarity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taxonomy-regularized semantic deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjoon</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fastai: A layered api for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<idno>108, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Foodx-251: a dataset for fine-grained food classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06167</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyemin</forename><surname>Tae Kwan Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonmo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cptr: Full transformer network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10804</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep ensembles for low-data transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06866</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words, what is a video worth?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno>2021. 12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep networks with large output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7479</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6509" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
