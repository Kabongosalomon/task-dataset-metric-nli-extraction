<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Git Loss for Deep Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Calefati</surname></persName>
							<email>a.calefati@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Science</orgName>
								<orgName type="institution">University of Insubria</orgName>
								<address>
									<settlement>Varese</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kamran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National University of Sciences and Technology</orgName>
								<address>
									<settlement>Islamabad</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shah</forename><surname>Nawaz</surname></persName>
							<email>snawaz@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Science</orgName>
								<orgName type="institution">University of Insubria</orgName>
								<address>
									<settlement>Varese</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignazio</forename><surname>Gallo</surname></persName>
							<email>ignazio.gallo@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Science</orgName>
								<orgName type="institution">University of Insubria</orgName>
								<address>
									<settlement>Varese</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Git Loss for Deep Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CALEFATI ET AL.: GIT LOSS FOR DEEP FACE RECOGNITION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have been widely used in computer vision tasks, such as face recognition and verification, and have achieved state-of-the-art results due to their ability to capture discriminative deep features. Conventionally, CNNs have been trained with softmax as supervision signal to penalize the classification loss. In order to further enhance discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances. Such minimization and maximization of deep features is considered ideal for face recognition task. We perform experiments on two popular face recognition benchmarks datasets and show that our proposed loss function achieves maximum separability between deep face features of different identities and achieves state-of-the-art accuracy on two major face recognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). However, it should be noted that the major objective of Git loss is to achieve maximum separability between deep features of divergent identities. The code has also been made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The current decade is characterized by the widespread use of deep neural networks for different tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. Similarly, deep convolutional networks have brought about a revolution in face verification, clustering and recognition tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Majority of face recognition methods based on deep convolutional networks (CNNs) differ along three primary attributes as explained in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. The first is the availability of large scale datasets for training deep neural networks. Datasets such as VGGFace2 <ref type="bibr" target="#b3">[4]</ref>, CASIA-WebFace <ref type="bibr" target="#b30">[31]</ref>, UMDFaces <ref type="bibr" target="#b1">[2]</ref>, MegaFace <ref type="bibr" target="#b12">[13]</ref> and MS-Celeb-1M <ref type="bibr" target="#b7">[8]</ref> contain images ranging from thousands to millions. The second is the emergence of powerful and scalable network architectures such as Inception-ResNet <ref type="bibr" target="#b9">[10]</ref> to train on large scale datasets. The last attribute is the <ref type="figure">Figure 1</ref>: A toy example depicting the aim of our work: a CNN trained for face recognition supervised by our Git loss function that maximizes the distance d2 between features and centroids of different classes and minimizes the distance d1 between features and the centroid of the same class. development of loss functions to effectively modify inter and intra-class variations such as Contrastive loss <ref type="bibr" target="#b19">[20]</ref>, Triplet loss <ref type="bibr" target="#b18">[19]</ref> and Center loss <ref type="bibr" target="#b26">[27]</ref>, given that softmax penalizes only the overall classification loss.</p><p>In this paper, we employ all three attributes associated with face recognition. We use a large scale publicly available dataset, VGGFace2, to train the powerful Inception ResNet-V1 network. We propose a new loss function named Git loss to enhance the discriminative power of deeply learned face features. Specifically, the Git loss simultaneously minimizes intra-class variations and maximizes inter-class distances. A toy example that explains our approach is shown in <ref type="figure">Figure 1</ref>. The name of the loss function is inspired from two common Git version control software commands, "push" and "pull", which are semantically similar to the aim of this work: push away features of different identities while pulling together features belonging to the same identity.</p><p>In summary, main contributions of our paper include: -A novel loss function which leverages on softmax and center loss to provide segregative abilities to deep architectures and enhance the discrimination of deep features to further improve the face recognition task -Easy implementation of the proposed loss function with standard CNN architectures. Our network is end-to-end trainable and can be directly optimized by fairly standard optimizers such as Stochastic Gradient Descent (SGD).</p><p>-We validate our ideas and compare Git loss against different supervision signals. We evaluate the proposed loss function on available datasets, and demonstrate state-ofthe-art results.</p><p>The organization of the paper is as follows: we review the literature on face recognition in Section 2 and introduce our supervision signal with details in Section 3. We discuss experimental results in Section 4 followed by conclusion and future work in Section 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent face recognition works are roughly divided into four major categories: (i) Deep metric learning methods, (ii) Angle-based loss functions, (iii) Imbalanced classes-aware loss functions and (iv) Joint supervision with Softmax. These methods have the aim of enhancing the discriminative power of the deeply learned face features. Deep learning methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> successfully employed triplet and contrastive loss functions for face recognition tasks. However, space and time complexities are higher due to the exponential growth of the datasets cardinality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Metric Learning Approaches</head><p>Deep metric learning methods focus on optimizing the similarity (contrastive loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>) or relative similarity (triplet loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>) of image pairs, while contrastive and triplet loss effectively enhance the discriminative power of deeply learned face features, we argue that both these methods can not constrain on each individual sample and require carefully designed pair and/or triplets. Thus, they suffer from dramatic data expansion while creating sample pairs and triplets from the training set with space complexity being O(n 3 ) for triplet networks..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Angle-based Loss Functions</head><p>Angular loss constrains the angle at the negative point of triplet triangles, leading to an angle and scale invariant method. In addition, this method is robust against the large variation of feature map in the dataset. ArcFace <ref type="bibr" target="#b6">[7]</ref> maximizes decision boundary in angular space based on the L2 normalized weights and features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Class Imbalance-Aware Loss Functions</head><p>Majority of loss functions do not penalize long tail distributions or imbalanced datasets. Range loss <ref type="bibr" target="#b32">[33]</ref> employs the data points occurring in the long tail during the training process to get the k greatest ranges harmonic mean values in a class and the shortest inter-class distance in the batch. Although range loss effectively reduces kurtosis of the distribution, it requires intensive computation, hampering the convergence of the model. Furthermore, inter-class maximization is limited because a mini-batch contains only four identities. Similarly, center-invariant loss <ref type="bibr" target="#b28">[29]</ref> handles imbalanced classes by selecting the center for each class to be representative, enforcing the model to treat each class equally regardless to the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Joint Supervision with Softmax</head><p>In joint supervision with softmax based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, the discriminative power of the deeply learned face features is enhanced. The work in <ref type="bibr" target="#b26">[27]</ref> penalizes the distance between deep features and their corresponding centers to enhance the discriminative ability of the deeply learned face features. With joint supervision of softmax loss and center loss function, inter-class dispersion and intra-class compactness is obtained. However, this comes with the cost of drastic memory consumption with the increase of CNN layers. Similarly, marginal loss <ref type="bibr" target="#b5">[6]</ref> improves the discriminative ability of deep features by simultaneously minimizing the intra-class variances as well as maximizing the inter-class distances by focusing on marginal samples.  Inspired from two available works in the literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>, we propose a new loss function with joint supervision of softmax to simultaneously minimize the intra-class variations and maximize inter-class distances.</p><formula xml:id="formula_0">-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 x i − c L C = λ C x i − c y i 2 2 L G = λ G /(1 + x i − c y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Git Loss</head><p>In this paper, we propose a new loss function called Git loss inspired from the center loss function proposed in <ref type="bibr" target="#b26">[27]</ref>. The center loss combines the minimization of the distance between the features of a class and their centroid with the softmax loss to improve the discriminating power of CNNs in face recognition.</p><p>In this work, to further improve the center loss function, we add a novel function that maximizes the distance between deeply learned features belonging to different classes (push) while keeping features of the same class compact (pull). The new Git loss function is described in Equation 1:</p><formula xml:id="formula_1">L = L S + λ C L C + λ G L G = − m ∑ i=1 log e W T y i x i +b y i ∑ n j=1 e W T j x i +b j + λ C 2 n ∑ i=1 x i − c y i 2 2 + λ G m ∑ i, j=1,i = j 1 1 + x i − c y j 2 2 (1) where L G is equal to 1 1+ x i −c y j 2 2</formula><p>which is responsible for maximizing the distance between divergent identities. The deep features of the i-th samples belonging to the y i -th identity are denoted by x i ∈ R d . The feature dimension d is set as 128, as reported in <ref type="bibr" target="#b18">[19]</ref>. W j ∈ R d denotes the j-th column of the weights W ∈ R d×n in the last fully connected layer and b ∈ R n is the bias term. c y i is the center of all deep features x i belonging to the y i -th identity. When the parameter λ G = 0 the center loss function can be obtained.</p><p>The gradient ( ∂ L G ∂ x i ) of the L G with respect to x i can be computed as:</p><formula xml:id="formula_2">∂ L G ∂ x i = ∂ ∂ x i ( 1 1 + x i − c y j 2 2 )</formula><p>(2) <ref type="figure">Figure 3</ref>: Some sample images taken from the VGGFace2 dataset aligned and cropped to 160 × 160 pixels.</p><formula xml:id="formula_3">Let u = 1 + ||x i − c y j || 2 2 , thus f = u −1 .</formula><p>The equation 2 can be solved to compute the final gradient. We substitute the values of u and get ∂ u −1</p><formula xml:id="formula_4">∂ u ∂ ∂ x i (1 + x i − c y j 2 2 ). Solving ∂ u −1 ∂ u and ∂ ∂ x i (1 + x i − c y j<label>2</label></formula><p>2 ), we get −1 u 2 and 2(x i − c y j ) respectively. Substituting these values, the final equation becomes −1 u 2 (2(x i − c y j )). We can simplify this equation to obtain the final gradient equation 3.</p><formula xml:id="formula_5">= −2(x i − c y j ) (1 + (x i − c y j ) 2 ) 2<label>(3)</label></formula><p>Git loss simultaneously minimizes the intra-class variances using the L C function and maximizes the inter-class distances using the L G function. Parameters λ C and λ G are used to balance the two functions L C and L G respectively. From the plot of L C and L G functions, shown in <ref type="figure">Figure 2</ref>, it can be observed how these two functions have opposite behaviors: to minimize L C we have to reduce the distance between features and the centers, while to maximize L G we must maximize the distance between features and all centroids of other classes. L G is a continuous and differentiable function, thus it can be used to train CNNs optimized by the standard Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report experimental results on currently popular face recognition benchmark datasets, Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b11">[12]</ref> and YouTube Faces (YTF) <ref type="bibr" target="#b27">[28]</ref>. We also report a set of experiments to investigate the hyper-parameters associated with the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training Data.</head><p>We use data from VGGFace2 <ref type="bibr" target="#b3">[4]</ref> dataset to train our model. It contains 3.31 million images of 9, 131 identities, with an average of 362.6 images for each identity. Moreover, the dataset is characterized by a large range of poses, face alignments, ages and ethnicities. The dataset is split into train and test sets with 8, 631 and 500 identities respectively, but we only used the train set. Some representative images taken from the VGGFace2 dataset are shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Data Preprocessing.</head><p>The label noise is minimized through automated and manual filtering in the VGGFace2 dataset. We applied horizontal flipping and random cropping data augmentation techniques to images, then face images are aligned using the Multi-Task CNN <ref type="bibr" target="#b31">[32]</ref> and finally cropped to a size of 160 × 160 pixels before feeding to the network. We noticed that VGGFace2 contains 496 overlapping identities with LFW and 205 with YTF datasets, therefore, we removed overlapping identities from both datasets to report fair results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Network Settings.</head><p>We implemented the proposed Git loss in Tensorflow <ref type="bibr" target="#b0">[1]</ref> and the network was trained using Nvidia's GeForce GTX 1080 GPU. The implementation is inspired from the facenet work, available on Github 2 . We employ the Inception ResNet-V1 network architecture and process 90 images in a batch. We use adaptive learning rate for the training process with a starting value of −1 and decreased it by a factor of 10 with Adam Optimizer <ref type="bibr" target="#b13">[14]</ref>, thus adding robustness to noisy gradient information and various data modalities across the dataset, improving the performance of the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Test Settings.</head><p>Deep face features (128 − dimensional) are taken from the output of the fully connected layer. Since the features are projected to Euclidean space, the score is computed using the Euclidean distance between two deep features. Threshold comparison is obtained with 10 − f old cross validation for verification task. We employ two different trained models for LFW and YTF datasets due to number of overlapping identities with VGGFace2 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments with λ C and λ G parameters</head><p>Parameters λ C and λ G are used to balance two loss functions L C and L G with softmax. In our model, λ C controls intra-class variance while λ G controls inter-class distances. We conducted various experiments to investigate the sensitiveness of these two parameters. These tests are systematic random search heuristics based. The major reason for employing heuristic methodologies over techniques like GridSearch is that when the dimensionality is high, the number of combinations to search becomes enormous and thus techniques like Grid-Search become an overhead. The work in <ref type="bibr" target="#b2">[3]</ref> argues why performance of GridSearch is not satisfactory as compared to other techniques. <ref type="table" target="#tab_0">Table 1</ref> shows average result values over 10 runs on MNIST dataset, we have following outcomes: (i) Smaller values of λ C increase inter-class distance, but they also increase intra-class distances which is undesirable in face recognition. (ii) Our loss function produces higher inter-class distances and lower intra-class distances. An example displaying the qualitative and quantitative results of our Git loss and Center loss is shown in <ref type="figure">Figure 4</ref>. Note that these results are obtained with a single run on MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on LFW and YTF datasets</head><p>We evaluate the proposed loss function on two famous face recognition benchmark datasets: LFW <ref type="bibr" target="#b11">[12]</ref> and YTF <ref type="bibr" target="#b27">[28]</ref> in unconstrained environments i.e. under open-set protocol. LFW dataset contains 13, 233 web-collected images from 5, 749 different identities, with large variations in pose, expression and illumination. We follow the standard protocol of unrestricted with labeled outside data and tested on 6, 000 face pairs. Results are shown in <ref type="table" target="#tab_2">Table 2</ref>. YTF dataset consists of 3, 425 videos of 1, 595 different people, with an average of 2.15 videos per person. The duration of each video varies from 48 to 6, 070 frames, with an average length of 181.3 frames. We follow the same protocol and reported results on 5, 000 video pairs in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We compare the Git loss against many existing state-of-the-art face recognition methods in <ref type="table" target="#tab_2">Table 2</ref>. From results, we can see that the proposed Git loss outperforms the softmax loss by a significant margin, from 98.40% to 99.30% in LFW and from 93.60% to 95.30% in YTF. In addition, we compare our results with center loss method using the same network architecture (Inception-ResNet V1) and dataset (VGGFace2). The Git loss outperforms the center loss, obtaining an accuracy of 99.30% as compared to 99.20% on LFW and 95.30% compared to 95.10% on YTF. These results indicate that the proposed Git loss further enhances the discriminative power of deeply learned face features. Moreover, we trained our model with ≈ 3M images which are far less than other state-of-the-art methods such as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, reported in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>This paper proposes a new loss function, named Git loss, which makes deep feature more discriminable. We exploit the softmax as supervision signal and the well-known property of the center loss which compacts patterns of a class, lowering intra-class distances. The result is a new loss function which minimizes intra-class variations and maximizes inter-class distances simultaneously. We evaluate capabilities of the new loss function on two common  face benchmark datasets such as LFW and YTF. Results obtained clearly demonstrate the effectiveness and generalization abilities of the proposed loss function. However, the major objective of Git loss is achieving maximum separability between dissimilar classes and compactness between similar classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>In future, we would like to extend Git loss to deal with class imbalance problem where penalization of data points occurring in long tail is not effective. We would also like to explore different baseline architectures and greater number of baseline images to further the experimental analysis. The objective of Git loss is not just empirical results alone, but the achieving of discriminatory ability in feature space. Furthermore, we believe that Git loss can be extended to multiple modalities where data from different input streams is combined and projected on a common feature space and thus would like to explore this perspective as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>j 2 2 )Figure 2 :</head><label>22</label><figDesc>Graphical representation of L C and L G varying the distance (x i − c) in the range [−2, 2]. The L G function takes a maximum value of λ G at x i − c = 0 and has an horizontal asymptote L G = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>24 Figure 4 :</head><label>244</label><figDesc>(a) λ C = 0.01 λ G = 0 (b) λ C = 0.01 λ G = 0.1 inter-class= 21.21, inter-class= 23.32 intra-class= 1.57 intra-class= 1.Two plots showing the behavior of Center loss (a) and Git loss (b) on MNIST training set. Using the Git loss function features are more compact (smaller intra-class distances) and more spaced (larger inter-class distances), further enhancing the discriminative power of deep features. Points with different colors denote features from different classes. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between Center loss (λ C ) and Git loss (λ G ) on MNIST dataset. Values are obtained by averaging 10 runs. We highlighted best results compared to Center loss (λ G = 0) for each configuration. We achieved reduced intra-class distance along with higher inter-class distance compared to Center loss.</figDesc><table><row><cell>λ C</cell><cell>λ G</cell><cell>Loss</cell><cell cols="4">Train Acc.(%) Val. Acc.(%) Inter Dist. Intra Dist.</cell></row><row><cell cols="3">0.0001 0.0001 0.0001 0.0132 0 0.020 0.0001 0.001 0.016 0.0001 0.01 0.020 0.0001 0.1 0.032 0.0001 1 0.466 0.0001 1.5 0.641 0.0001 2 1.001</cell><cell>99.77 100.00 99.77 99.77 99.61 89.77 80.63 69.84</cell><cell>98.52 98.65 98.66 98.62 98.46 88.95 79.56 68.51</cell><cell>85.95 87.87 89.82 88.48 96.76 137.37 160.28 125.84</cell><cell>8.39 8.52 8.20 8.54 9.56 15.45 16.91 17.78</cell></row><row><cell cols="2">0.001 0.001 0.0001 0 0.001 0.001 0.001 0.01 0.001 0.1 0.001 1 0.001 1.5 0.001 2</cell><cell>0.021 0.117 0.024 0.025 0.053 0.779 0.460 0.757</cell><cell>99.61 96.75 99.84 99.77 99.22 76.10 89.22 80.94</cell><cell>98.67 95.66 98.53 98.62 98.69 76.33 89.06 81.89</cell><cell>44.36 51.77 47.81 46.13 51.22 68.77 89.67 96.02</cell><cell>3.10 4.75 3.23 3.16 3.46 6.55 7.45 9.14</cell></row><row><cell cols="2">0.01 0.01 0.0001 0 0.01 0.001 0.01 0.01 0.01 0.1 0.01 1 0.01 1.5 0.01 2</cell><cell>0.025 0.031 0.024 0.051 0.037 0.937 0.368 0.824</cell><cell>99.65 99.69 100.00 99.53 99.84 71.17 97.58 83.44</cell><cell>98.89 98.77 98.75 98.61 98.70 71.38 96.50 84.10</cell><cell>21.36 22.07 20.63 21.96 22.93 30.25 50.56 46.35</cell><cell>1.09 1.16 1.18 1.09 1.22 2.02 3.10 3.03</cell></row><row><cell cols="2">0.1 0.1 0.0001 0 0.1 0.001 0.1 0.01 0.1 0.1 0.1 1 0.1 1.5 0.1 2</cell><cell>0.040 0.049 0.024 0.026 0.040 1.508 1.741 1.536</cell><cell>99.74 99.53 100.00 99.92 100.00 57.11 53.59 67.98</cell><cell>98.89 98.85 98.96 99.00 98.96 57.90 54.03 66.65</cell><cell>9.76 9.65 10.33 9.76 10.99 10.52 10.81 15.43</cell><cell>0.38 0.42 0.38 0.37 0.37 0.55 1.03 1.03</cell></row><row><cell></cell><cell>1 1 0.0001 0 1 0.001 1 0.01 1 0.1 1 1 1 1.5 1 2</cell><cell>0.031 0.178 0.023 0.027 0.064 0.264 0.330 0.847</cell><cell>100.00 96.72 100.00 100.00 99.92 99.92 100.00 91.10</cell><cell>99.00 95.72 99.03 99.04 99.04 99.02 98.96 89.79</cell><cell>5.12 5.86 4.94 5.03 5.25 8.30 9.73 9.22</cell><cell>0.14 0.22 0.12 0.12 0.14 0.21 0.23 0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance verification of different state-of-the-art methods on LFW and YTF datasets. The last three rows show results using the same architecture (Inception-ResNet V1) trained on the VGGFace2 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/davidsandberg/facenet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
	<note>Biometrics (IJCB</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08092</idno>
		<title level="m">Vggface2: A dataset for recognising faces across pose and age</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPRW), Faces &quot;in-the-wild</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPRW), Faces &quot;in-the-wild</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuážěn Trážgn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatuporn</forename><forename type="middle">Toy</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="579" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparsifying neural network connections for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4856" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning a metric embedding for face recognition using the multibatch method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tadmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Rosenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep face recognition with center invariant loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.578</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
