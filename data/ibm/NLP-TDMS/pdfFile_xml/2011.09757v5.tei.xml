<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Zhe</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional unsupervised multi-source domain adaptation (UMDA) methods assume all source domains can be accessed directly. However, this assumption neglects the privacy-preserving policy, where all the data and computations must be kept decentralized. There exist three challenges in this scenario: (1) Minimizing the domain distance requires the pairwise calculation of the data from source and target domains, while the data on the source domain is not available. <ref type="formula">(2)</ref> The communication cost and privacy security limit the application of existing UMDA methods, such as the domain adversarial training. (3) Since users cannot govern the data quality, the irrelevant or malicious source domains are more likely to appear, which causes negative transfer. To address the above problems, we propose a privacy-preserving UMDA paradigm named Knowledge Distillation based Decentralized Domain Adaptation (KD3A), which performs domain adaptation through the knowledge distillation on models from different source domains. The extensive experiments show that KD3A significantly outperforms state-of-theart UMDA approaches. Moreover, the KD3A is robust to the negative transfer and brings a 100× reduction of communication cost compared with other decentralized UMDA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most deep learning models are trained with large-scale datasets via supervised learning. Since it is often costly to get sufficient data, we usually use other similar datasets to train the model. However, due to the domain shift, naively combining different datasets often results in unsatisfying performance. Unsupervised Multi-source Domain Adaptation (UMDA) <ref type="bibr" target="#b21">(Zhang et al., 2015)</ref> addresses such problems by establishing transferable features from multiple source domains to an unlabeled target domain.</p><p>Based on the consensus quality of different source domains, we devise a dynamic weighting strategy named Consensus Focus to identify the malicious and irrelevant source domains. Finally, we derive a decentralized optimization strategy of H-divergence named BatchNorm MMD. Moreover, we analyze the decentralized generalization bound for KD3A from a theoretical perspective. The extensive experiments show our KD3A has the following advantages:</p><p>• The KD3A brings a 100× reduction of communication cost compared with other decentralized UMDA methods and is robust to the privacy leakage attack.</p><p>• The KD3A assigns low weights to those malicious or irrelevant domains. Therefore, it is robust to negative transfer.</p><p>• The KD3A significantly outperforms the state-of-theart UMDA approaches with 51.1% accuracy on the large-scale DomainNet dataset.</p><p>In addition, our KD3A is easy to implement and we create an open-source framework to conduct KD3A on different benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Unsupervised multi-source domain adaptation</head><p>Unsupervised Multi-source Domain Adaptation (UMDA) establish the transferable features by reducing the Hdivergence between source domain D S and target domain D T . There are two prevailing paradigms that provide the optimization strategy of H-divergence, i.e. maximum mean discrepancy (MMD) and the adversarial training. In addition, knowledge distillation is also used to perform modellevel knowledge transfer.</p><p>MMD based methods <ref type="bibr" target="#b15">(Tzeng et al., 2014)</ref> construct a reproducing kernel Hilbert space (RKHS) H κ with the kernel κ, and optimize the H-divergence by minimizing the MMD distance d κ MMD (D S , D T ) on H κ . Using the kernel trick, MMD can be computed as</p><formula xml:id="formula_0">d κ MMD (D S , D T ) = −2E X S ,X T ∼D S ,D T κ(X S , X T )+ E X S ,X S ∼D S κ(X S , X S ) + E X T ,X T ∼D T κ(X T , X T )<label>(1)</label></formula><p>Recent works propose the variations of MMD, e.g., multikernel MMD <ref type="bibr" target="#b8">(Long et al., 2015)</ref>, class-weighted MMD <ref type="bibr" target="#b17">(Yan et al., 2017)</ref> and domain-crossing MMD <ref type="bibr" target="#b12">(Peng et al., 2019)</ref>. However, all these methods require the pairwise calculation of the data from source and target domains, which is not allowed under the decentralization constraints.</p><p>The adversarial training strategy <ref type="bibr" target="#b14">(Saito et al., 2018;</ref><ref type="bibr" target="#b22">Zhao et al., 2018)</ref> apply adversarial training in feature space to optimize H-divergence. It is proved that with the adversarial training strategy, the UMDA model can work under the privacy-preserving policy <ref type="bibr" target="#b13">(Peng et al., 2020)</ref>. However, the adversarial training requires each source domain to exchange and update model parameters with the target domain after every single batch, which consumes huge communication resources.</p><p>Knowledge distillation in domain adaptation. Knowledge distillation (KD) <ref type="bibr" target="#b4">(Hinton et al., 2015)</ref> is an efficient way of transferring knowledge between different models. Recent works <ref type="bibr" target="#b10">(Meng et al., 2018;</ref><ref type="bibr" target="#b24">Zhou et al., 2020)</ref> extend the knowledge distillation into domain adaptation with a teacher-student training strategy: training multiple teacher models on source domains and ensembling them on target domain to train a student model. This strategy outperforms other UMDA method in practice. However, due to the irrelevant and malicious source domains, the conventional KD strategies may fail to obtain proper knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Federated learning</head><p>Federated learning <ref type="bibr" target="#b6">(Konecný et al., 2016</ref>) is a distributed machine learning approach, it can train a global model by aggregating the updates of local models from multiple decentralized datasets. Recent works <ref type="bibr" target="#b9">(McMahan et al., 2017)</ref> find a trade-off between model performance and communication efficiency, that is, to make the global model achieve better performance, we need to conduct more communication rounds, which raises the communication costs. Besides, the frequent communication will also cause privacy leakage <ref type="bibr" target="#b16">(Wang et al., 2019)</ref>, making the training process insecure.</p><p>Federated domain adaptation. There are few works discussing the decentralized UMDA methods. FADA <ref type="bibr" target="#b13">(Peng et al., 2020)</ref> first raises the concept of federated domain adaptation. It applies the adversarial training to optimize the H-divergence without accessing data. However, FADA consumes high communication costs and is vulnerable to the privacy leakage attack. SHOT <ref type="bibr" target="#b7">(Liang et al., 2020)</ref> provides a self-supervised method to solve the single source decentralized domain adaptation. However, it is vulnerable to the negative transfer in multi-source situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">KD3A: decentralized domain adaptation via knowledge distillation</head><p>Let D S and D T denote the source domain and target domain. In UMDA, we have</p><formula xml:id="formula_1">K source domains {D k S } K k=1 where each domain contains N k labeled examples as D k S := {(X k i , y k i )} N k i=1 and a target domain D T with N T unlabeled examples as D T := {X T i } N T i=1 .</formula><p>The goal of UMDA is to learn a model h which can minimize the task risk D T in D T , i.e. D T (h) = Pr (X,y)∼D T [h(X) = y]. Without loss of generality, we consider C-way classification task and assume the target domain shares the same tasks with the source domains. In a common UMDA, we combine K source domains with different domain weights as α, and perform domain adaptation by minimizing the following generalization bound <ref type="bibr" target="#b1">(Ben-David et al., 2010;</ref><ref type="bibr" target="#b22">Zhao et al., 2018)</ref> with the multiple source domains as:</p><formula xml:id="formula_2">Theorem 1 Let H be the model space, { D k S (h)} K k=1</formula><p>and D T (h) be the task risks of source domains {D k S } K k=1 and the target domain D T , and α ∈ R K + , K k=1 α k = 1 be the domain weights. Then for all h ∈ H we have:</p><formula xml:id="formula_3">D T (h) ≤ K k=1 α k D k S (h) + 1 2 d H∆H (D k S , D T ) + λ 0</formula><p>(2) where λ 0 is a constant according to the task risk of the optimal model on the source domains and target domain.</p><p>In decentralized UMDA, we apply knowledge distillation to perform domain adaptation without accessing the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extending source domains with consensus knowledge</head><p>Knowledge distillation can perform knowledge transfer through different models. Suppose we have K fully-trained models from K source domains denoted by {h k S } K k=1 . we use q k S (X) to denote the confidence for each class and use the class with the maximum confidence as label, i.e. h k S (X) = arg c max[q k S (X)] c . As shown in <ref type="figure" target="#fig_1">Figure 1</ref>(a), the knowledge distillation in UMDA consists of two steps. First, for each target domain data X T i , we obtain the inferences of the source domain models. Then, we use the ensemble method to get the consensus knowledge of the source models, e.g., p i = 1 K K k=1 q k S (X T i ). In order to utilize the consensus knowledge for domain adaptation, we define an extended source domain D K+1 S with the consensus knowledge p i for each target domain data X T i as</p><formula xml:id="formula_4">D K+1 S = {(X T i , p i )} N T i=1</formula><p>We also define the related task risk for D K+1</p><formula xml:id="formula_5">S as D K+1 S (h) = Pr (X,p)∼D K+1 S [h(X) = arg c max p c ].</formula><p>With this new source domain, we can train the source model h K+1 S through the knowledge distillation loss as</p><formula xml:id="formula_6">L kd (X T i , q K+1 S ) = D KL (p i q K+1 S (X T i )).<label>(3)</label></formula><p>In decentralized UMDA, we get the target model as the aggregation of the source models, i.e. h T :=  (proof in Appendix A). With this insight, we can derive the generalization bound for knowledge distillation as follows (proof in Appendix B):</p><p>Proposition 1 (The generalization bound for knowledge distillation). Let H be the model space and D K+1 S (h) be the task risk of the new source domain D K+1 S based on knowledge distillation. Then for all h T ∈ H, we have:</p><formula xml:id="formula_7">D T (h T ) ≤ D K+1 S (h T ) + 1 2 d H∆H (D K+1 S , D T ) + min{λ 1 , sup h∈H | D K+1 S (h) − D T (h)|}<label>(4)</label></formula><p>where λ 1 is a constant for the task risk of the optimal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge vote: producing good consensus</head><p>Proposition 1 shows the new source domain D K+1 S will improve the generalization bound if the consensus knowl-edge is good enough to represent the ground-truth label, i.e. sup h∈H | D T (h) − D K+1 S (h)| ≤ λ 1 . However, due to the irrelevant and malicious source domains, the conventional ensemble strategies (e.g., maximum and mean ensemble) may fail to obtain proper consensus. Therefore, we propose the Knowledge Vote to provide high-quality consensus.</p><p>The main idea of knowledge vote is that if a certain consensus knowledge is supported by more source domains with high confidence (e.g., &gt; 0.9), then it will be more likely to be the true label. As shown in <ref type="figure" target="#fig_1">Figure 1(b)</ref>, it takes three steps to perform Knowledge Vote:</p><p>1. Confidence gate. For each X T i ∈ D T , we firstly use a high-level confidence gate to filter the predictions {q k S (X T i )} K k=1 of teacher models and eliminate the unconfident models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Consensus class vote.</head><p>For the models remained, the predictions are added up to find the consensus class which has the maximum value. Then we drop the models that are inconsistent with the consensus class.</p><p>3. Mean ensemble. After the class vote, we obtain a set of models that all support the consensus class. Finally, we get the consensus knowledge p i by conducting the mean ensemble on these supporting models. We also record the number of domains that support p i , denoted by n pi . For those X T with all teacher models eliminated by the confidence gate, we simply use the mean ensemble to get p and assign a relatively low weight to them as n p = 0.001.</p><p>After Knowledge Vote, we obtain the new source domain</p><formula xml:id="formula_8">D K+1 S = {(X T i , p i , n pi )} N T i=1 .</formula><p>We use the n pi to re-weight the knowledge distillation loss as</p><formula xml:id="formula_9">L kv (X T i , q) = n pi · D KL (p i q(X T i ))<label>(5)</label></formula><p>Compared with other ensemble strategies, our Knowledge Vote makes model learn high-quality consensus knowledge since we assign high weights to those items with high confidence and many support domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Consensus focus: against negative transfer</head><p>Domain weights α determine the contribution of each source domain. <ref type="bibr" target="#b1">Ben-David et al. (2010)</ref> proves the optimal α should be proportional to the amount of data when all source domains are equally important. However, this condition is hard to satisfy in KD3A since some source domains are usually very different from the target domain, or even malicious domains with corrupted labels. These bad domains lead to negative transfer. One common solution Algorithm 1 KD3A training process with epoch t.</p><p>Input:</p><formula xml:id="formula_10">Source domains S = {D k S } K k=1 . Target domain D T ; Target model h (t−1) T with parameters Θ (t−1) ; Confidence gate g (t) ; Output: Target model h (t)</formula><p>T with parameters Θ (t) . 1: // Locally training on source domains:</p><formula xml:id="formula_11">2: for D k S in S do 3: Model initialize: (h k S , Θ k S ) ← (h (t−1) , Θ (t−1)</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Train h k S with classification loss on D k S . 5: end for</p><formula xml:id="formula_12">6: Upload {(h k S , Θ k S )} K k=1 to the target domain. 7: // Knowledge Vote: 8: D K+1 S ← KnowledgeVote(D T , g (t) , {h k S } K k=1 ). 9: Train h K+1 S with L kv loss (5) on D K+1 S . 10: // Consensus Focus: 11: α CF ← ConsensusFocus(D T , {h k S } K k=1 , {N k } K k=1 ). 12: // Model Aggregation: 13: Θ (t) ← K+1 k=1 α CF k · Θ k S . 14: // BatchNorm MMD: 15: Obtain {E[π k l ] i , i = 1, 2} L,K+1 l,k=1 from {(h k S , Θ k S )} K+1 k=1 16: Train h (t) T with BatchNorm MMD on D T . 17: Return (h (t) T , Θ (t) )</formula><p>. <ref type="bibr" target="#b23">(Zhao et al., 2020)</ref> is to re-weight each source domain with the H-divergence as</p><formula xml:id="formula_13">α k = N k e −d H (D k S ,D T ) / k N k e −d H (D k S ,D T ) .<label>(6)</label></formula><p>However, calculating H-divergence requires to access the source domain data. Besides, H-divergence only measures the domain similarity on the input space, which does not utilize the label information and fails to identify the malicious domain. Reasonably, we propose Consensus Focus to identify those irrelevant and malicious domains. As mentioned in Knowledge Vote, the UMDA performance is related to the quality of consensus knowledge. With this motivation, the main idea of Consensus Focus is to assign high weights to those domains which provide high-quality consensus and penalize those domains which provide bad consensus. To perform Consensus Focus, we first derive the definition of consensus quality and then calculate the contribution to the consensus quality for each source domain.</p><p>The definition of consensus quality. Suppose we have a set of source domains denoted by S = {D k S } K k=1 . For each coalition of source domains S , S ⊆ S, we want to estimate the quality of the knowledge consensus obtained from S . Generally speaking, if one consensus class is supported by more source domains with higher confidence, then it will be more likely to represent the true label, which means the consensus quality gets better. Therefore, for each X T i ∈ D T with the consensus knowledge (p i (S ), n pi (S )) obtained from S , We define the related consensus quality as n pi (S ) · max p i (S ) and the total consensus quality Q is</p><formula xml:id="formula_14">Q(S ) = X T i ∈D T n pi (S ) · max p i (S )<label>(7)</label></formula><p>With the consensus quality defined in <ref type="formula" target="#formula_14">(7)</ref>, we derive the consensus focus (CF) value to quantify the contribution of each source domain as</p><formula xml:id="formula_15">CF(D k S ) = Q(S) − Q(S \ {D k S })<label>(8)</label></formula><p>CF(D k S ) describes the marginal contribution of the single source domain D k S to the consensus quality of all source domains S. If one source domain is a bad domain, then removing it will not decrease the total quality Q, which leads to a low consensus focus value. With the CF value, we can assign proper weights to different source domains. Since we introduce a new source domain D K+1 S in Knowledge Vote, we compute the domain weights with two steps. First, we obtain</p><formula xml:id="formula_16">α K+1 = N T /( K k=1 N k + N T ) for D K+1 S</formula><p>based on the amount of data. Then we use the CF value to re-weight each original source domain as</p><formula xml:id="formula_17">α CF k = (1 − α K+1 ) · N k · CF(D k S ) K k=1 N k · CF(D k S )<label>(9)</label></formula><p>Compared with the re-weighting strategy in (6), our Consensus Focus has two advantages. First, the calculation of α CF does not need to access the original data. Second, α CF obtained through Consensus Focus is based on the quality of consensus, which utilize both data and label information and can identify malicious domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">BatchNorm MMD: decentralized optimization strategy of H−divergence</head><p>To get a better UMDA performance, we need to minimize the H-divergence between source domains and target domain, where the kernel-based MMD distance is widely used. Existing works <ref type="bibr" target="#b8">(Long et al., 2015;</ref><ref type="bibr" target="#b12">Peng et al., 2019)</ref> use the feature π extracted by the fully-connected (fc) layers to build kernel as κ(X S , X T ) = π S , π T and the related optimization target is</p><formula xml:id="formula_18">min h∈H K+1 k=1 α k d κ MMD (D k S , D T )<label>(10)</label></formula><p>However, these methods is not applicable in decentralized UMDA since the source domain data is unavailable. Besides, only using the high-level features from fc-layers may lose the detailed 2-D information. Therefore, we propose the BatchNorm MMD, which utilizes the mean and variance parameters in each BatchNorm layer to optimize the H−divergence without accessing data.</p><p>BatchNorm (BN) <ref type="bibr" target="#b5">(Ioffe &amp; Szegedy, 2015)</ref> is a widelyused normalization technique. For the feature π, Batch-Norm is expressed as BN(π) = γ · π−E(π) √ Var(π) + β, where (E(π), Var(π)) are estimated in training process 1 . Supposing the model contains L BatchNorm layers, we consider the quadratic kernel for the feature π l of the l-th BN-layer, i.e. κ(X S , X T ) = π S l , π T l + 1 2 2 . The MMD distance based on this kernel is</p><formula xml:id="formula_19">d κ MMD (D k S , D T ) = E(π k l ) − E(π T l ) 2 2 + E[π k l ] 2 − E[π T l ] 2 2 2<label>(11)</label></formula><p>Compared with other works using the quadratic kernel <ref type="bibr" target="#b12">(Peng et al., 2019)</ref>, we can obtain all required parameters in (11) through the parameters (E(π l ), Var(π l )) of BN-layers in source domain models without accessing data 2 . Based on this advantage, BatchNorm MMD can perform the decentralized optimization strategy of H−divergence with two steps. First, we obtain {(E(π k l ), Var(π k l ))} l l=1 from the models on different source domains. Then, for every minibatch X T ∈ D T , we train the model h T to optimize the domain adaptation target (10) with the following loss</p><formula xml:id="formula_20">L l=1 K+1 k=1 α k µ(π T l ) − E(π k l ) 2 2 + µ[π T l ] 2 − E[π k l ] 2 2 2</formula><p>where (π T 1 , . . . , π T L ) are the features of target model h T from BatchNorm layers corresponding to the input X T . In training process, We use the mean value µ of every minibatch to estimate the expectation E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The algorithm of KD3A</head><p>In the above sections, we have proposed three essential components that work well in KD3A, and the complete algorithm of KD3A can be obtained by using these components in tandem: First, we obtain an extra source domain D K+1 S and train the source model h K+1 S through Knowledge Vote. Then, we get the target model by aggregating K + 1 source models through Consensus Focus, i.e. h T := K+1 k=1 α k h k S . Finally, we minimize the H−divergence of the target model through Batchnorm MMD. The decentralized training process of KD3A is shown in Algorithm 1. Confidence gate is the only hyper-parameter in KD3A, and should be treated carefully. If the confidence gate is too large, almost all data in target domain would be eliminated and the knowledge vote loss would not work. If too small, then the consensus quality would be reduced. Therefore, we gradually increase it from low (e.g., 0.8) to high (e.g., 0.95) in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Generalization bound for KD3A</head><p>We further derive the generalization bound for KD3A by combining the original bound (2) and the knowledge distillation bound (4). The related generalization bound is: </p><formula xml:id="formula_21">D T (h T ) ≤ K+1 k=1 α CF k D k S (h T ) + 1 2 d H∆H (D k S , D T ) +λ 2<label>(12)</label></formula><p>The generalization performance of KD3A bound (12) depends on the quality of the consensus knowledge, as the following proposition shows (see Appendix C for proof):</p><p>Proposition 2 The KD3A bound <ref type="formula" target="#formula_0">(12)</ref> is a tighter bound than the original bound (2), if the task risk gap between the knowledge distillation domain D K+1 S and the target domain D T is smaller than the following upper-bound for all source domain k ∈ {1, · · · , K}, that is, D K+1 S (h) should satisfy:</p><formula xml:id="formula_22">sup h∈H | D K+1 S (h) − D T (h)| ≤ inf h∈H | D K+1 S (h) − D k S (h)| + 1 2 d H∆H (D k S , D T ) + λ k S</formula><p>Proposition 2 points out two tighter bound conditions: (1) For those good source domains with small H−divergence and low optimal task risk λ k S , the model should take their advantages to provide better consensus knowledge, i.e. the task risk D K+1 S gets close enough to D T . (2) For those irrelevant and malicious source domains with high H−divergence and λ, the model should filter out their knowledge, i.e. the task risk D K+1 S stays away from that for bad domains.</p><p>The KD3A has heuristically achieved the above two conditions through the Knowledge Vote and Consensus Focus. We also conduct sufficient experiments to show our KD3A achieves tighter bound with better performance than other UMDA approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Domain adaptation performance</head><p>We perform experiments on 3 benchmark datasets: (1) Digit-5, which is a digit recognition dataset including 5 domains.</p><p>(2) Office-Caltech10 <ref type="bibr" target="#b3">(Gong et al., 2012)</ref>, which contains 10 object categories from four domains. (3) DomainNet <ref type="bibr" target="#b12">(Peng et al., 2019)</ref>, which is a recently introduced benchmark for large-scale multi-source domain adaptation with 345 classes and six domains, i.e. Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel) and Sketch (skt), as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We follow the protocol used in prevailing works, selecting each domain in turn as the target domain and using the rest domains as source domains. Due to space limitations, we present results on DomainNet; more results on Digit-5 and Office-Caltech10 are provided in Appendix.</p><p>Baselines. We conduct extensive comparison experiments with the current best UMDA approaches from 4 categories:</p><p>(1) H-divergence based methods, i.e. the multi-domain adversarial network (MDAN) <ref type="bibr" target="#b22">(Zhao et al., 2018)</ref> and moment matching (M 3 SDA) <ref type="bibr" target="#b12">(Peng et al., 2019)</ref>. (2) Knowledge ensemble based methods, i.e. the domain adaptive ensemble learning (DAEL) <ref type="bibr" target="#b24">(Zhou et al., 2020)</ref>. (3) Source selection based methods, i.e. the curriculum manager (CMSS) <ref type="bibr" target="#b24">(Yang et al., 2020)</ref>. (4) Decentralized UMDA, i.e. SHOT <ref type="bibr" target="#b7">(Liang et al., 2020)</ref> and FADA <ref type="bibr" target="#b13">(Peng et al., 2020)</ref>. The DSBN proposes a domain-specific BatchNorm, which is similar to Batchnorm MMD, so we also take it into comparison. In addition, We report two baselines without domain adaptation, i.e. oracle and source-only. Oracle directly performs supervised learning on target domains and source-only naively combines source domains to train a single model. Implementation details. Following the settings in previous UMDA works <ref type="bibr" target="#b12">(Peng et al., 2019;</ref><ref type="bibr" target="#b24">Yang et al., 2020)</ref>, we use a 3-layer CNN as backbone for Digit-5, and use the ResNet101 pre-trained on ImageNet for Office-Caltech10 and DomainNet. The settings of communication rounds r is important in decentralized training. Since the models on different source domains have different convergence rates, we need to aggregate models r times per epoch. To perform the r-round aggregation, we uniformly divide one epoch into r stages and aggregate model after each stage. The KD3A Algorithm 1 is a decentralized training strategy with r = 1 and we use this setting in all experiments. For model optimization, We use the SGD with 0.9 momentum as the optimizer Ablation study. To evaluate the contributions of each component, We perform ablation study for KD3A , as shown in <ref type="figure">Figure 3</ref>. Knowledge Vote, Consensus Focus and Batchnorm MMD are all able to improve the accuracy, while most contributions are from Knowledge Vote, which indicates our KD3A can also perform well on those tasks that cannot use Batchnorm MMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Robustness to negative transfer</head><p>We construct irrelevant and malicious source domains on DomainNet and conduct synthesized experiments to show that with Consensus Focus, our KD3A is robust to negative transfer.</p><p>Since Quickdraw is very different from other domains, and all models perform bad on it, we take Quickdraw as the irrelevant domain, denoted by IR-qdr. To construct malicious domains, we perform poisoning attack <ref type="bibr" target="#b0">(Bagdasaryan et al., 2020)</ref>    qdr, we select the remaining five domains in turn as target domains and train KD3A with the rest source domains. In training process, we plot the curve of the mean weight α assigned to IR-qdr by Consensus Focus. We also report the average UMDA accuracy across all target domains. For the malicious domain MA-m, we conduct the same process on the remained four domains except for Quickdraw. We report the same experiment results as IR-qdr.</p><p>We consider two advanced weighting strategies for comparison: the H-divergence re-weighting in equation <ref type="formula" target="#formula_13">(6)</ref> and the Info Gain in FADA <ref type="bibr" target="#b13">(Peng et al., 2020)</ref>. In addition, we also report the average UMDA accuracy of KD3A model with the bad domain dropped. According to the results provided in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, we can get the following insights:</p><p>(1) For IR-qdr and MA-(30,50), the negative transfer occurs since the domain-drop outperforms the others.</p><p>(2) The three weighting strategies are robust to the irrelevant domain since they all assign low weights to IR-qdr.</p><p>(3) Consensus Focus outperforms other strategies in malicious domains since it assigns extremely low weights to the bad domain (i.e. 5% for MA-30), while other strategies can not identify the malicious domain. Moreover, our KD3A can use the correct information of less malicious domains (i.e. MA-(15,30)) and achieves better performance than the domain-drop. KD3A 50.5 50.9 51.1 51.3 51.3 52.0 <ref type="table">Table 3</ref>. Average UMDA accuracy (%) with different communication rounds r for our KD3A and FADA. KD3A achieves good performance with low communication cost (e.g., r ≤ 1). <ref type="figure">Figure 5</ref>. The gradient leakage attack <ref type="bibr" target="#b25">(Zhu et al., 2019)</ref> on decentralized training strategy. KD3A is robust to this attack while FADA causes the privacy leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Communication efficiency and privacy security</head><p>To evaluate the communication efficiency, We train the KD3A with different communication rounds r and report the average UMDA accuracy on DomainNet. We take the FADA method as a comparison. The results in <ref type="table">Table 3</ref> show the following properties: (1) Due to the adversarial training strategy, FADA works under large communication rounds (i.e. r = 100).</p><p>(2) Our KD3A works under the low communication cost with r = 1, leading to a 100 × communication reduction.</p><p>(3) KD3A is robust to communication rounds. For example, the accuracy only drops 0.9% when r decreases from 100 to 1. Moreover, we consider two extreme cases where we synchronize models every 2 and 5 epochs, i.e. r = 0.5 and 0.2. In these cases, FADA performs worse than the source-only baseline while our KD3A can still achieve state-of-the-art results.</p><p>In decentralized training process, the frequent communication will cause privacy leakage <ref type="bibr" target="#b16">(Wang et al., 2019)</ref>, making the training process insecure. To verify the privacy protection capabilities, we perform the advanced gradient leakage attack <ref type="bibr" target="#b25">(Zhu et al., 2019)</ref> on KD3A and FADA. As shown in <ref type="figure">Figure 5</ref>, the source images used in FADA are recovered under the attack, which causes privacy leakage. However, due to the low communication cost, our KD3A is robust to this attack, which demonstrates high privacy security.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose an effective approach KD3A to address the problems in decentralized UMDA. </p><formula xml:id="formula_23">= {(X T i , p i )} N T i=1 , training the related source model h K+1 S with the knowledge distillation loss L kd (X T i , q K+1 S ) = D KL (p i q K+1 S (X T i )) equals to optimizing the task risk D K+1 S (h) = Pr (X,p)∼D K+1 S [h(X) = arg c max p c ].</formula><p>Proof:</p><p>First, we prove that ∀c = 1, . . . , C,</p><formula xml:id="formula_24">|q K+1 S (X T i )) c − p i,c | ≤ 1 2 D KL (p i q K+1 S (X T i )) (1)</formula><p>The widely used Pinsker's inequality states that, if P and Q are two probability distributions on a measurable space (X, Σ), then</p><formula xml:id="formula_25">δ(P, Q) ≤ 1 2 D KL (P Q) where δ(P, Q) = sup{|P (A) − Q(A)||A ∈ Σ,</formula><p>Σ is a measurable event.}</p><p>In our situation, we choose the event A as the probability of classifying the input X T i into class c, and the related probability under P, Q is p i,c and q K+1 S (X T i )) c . With Pinsker's inequality, it is easy to prove (1). Since the inequality (1) holds for all class c, minimizing the knowledge distillation loss will make q K+1 </p><formula xml:id="formula_26">S (X T i )) → p i , that is, D K+1 S (h) → 0.</formula><formula xml:id="formula_27">D T (h T ) ≤ D K+1 S (h T ) + 1 2 d H∆H (D K+1 S , D T ) + min{λ 1 , sup h∈H | D K+1 S (h) − D T (h)|}<label>(2)</label></formula><p>where λ 1 is a constant for the task risk of the optimal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>Following </p><formula xml:id="formula_28">D T (h T ) ≤ D K+1 S (h T ) + 1 2 d H∆H (D K+1 S , D T ) + λ 1 (3)</formula><p>where λ 1 is constant of the optimal model on the source domain and the target domain as λ 1 = min h∈H D K+1</p><formula xml:id="formula_29">S (h) + D T (h).</formula><p>In addition, the following inequality also holds for all h T ∈ H:</p><formula xml:id="formula_30">D T (h T ) − D K+1 S (h T ) ≤ sup h∈H | D T (h) − D K+1 S (h)| (4) where sup h∈H | D T (h) − D K+1</formula><p>S (h)| is the upper bound of the task risk gap between the target domain D T and the extended domain D K+1 S . Notice D K+1 S shares the same input space with D T since they all use {X T i } N T i=1 as inputs. Therefore, we have</p><formula xml:id="formula_31">d H∆H (D K+1 S , D T ) = 0<label>(5)</label></formula><p>Substituting <ref type="formula" target="#formula_9">(5)</ref> into <ref type="formula" target="#formula_7">(4)</ref>, we have</p><formula xml:id="formula_32">D T (h T ) ≤ D K+1 S (h T )+ 1 2 d H∆H (D K+1 S , D T )+ sup h∈H | D T (h)− D K+1 S (h)|<label>(6)</label></formula><p>Combining <ref type="formula" target="#formula_6">(3)</ref> and <ref type="formula" target="#formula_13">(6)</ref>, we get the Proposition 1. </p><formula xml:id="formula_33">D K+1 S (h) ≤ˆ D K+1 S (h) + 4 N T (d log 2eN T d + log 4 δ ) d H∆H (D K+1 S , D T ) ≤d H∆H (D K+1 S , D T ) + 4 d log(2N T + log( 2 δ ) N T<label>(7)</label></formula><p>where d is the VC-dimension of model space H.</p><p>Combining <ref type="formula" target="#formula_27">(2)</ref> and <ref type="formula" target="#formula_14">(7)</ref>, we get the generalization bound for knowledge distillation with the empirical learning error as follows:</p><formula xml:id="formula_34">D T (h T ) ≤ˆ D K+1 S (h) + 1 2d H∆H (D K+1 S , D T ) + C 1 (8)</formula><p>where C 1 is a constant as</p><formula xml:id="formula_35">C 1 = min{ λ 1 + 4 N T (d log 2eN T d + log 4 δ ) + 4 d log(2N T + log( 2 δ ) N T , sup h∈H | D T (h) −ˆ D K+1 S (h)| + 4 N T (d log 2eN T d + log 4 δ ). }<label>(9)</label></formula><p>Layer Configuration 1 2D Convolution with kernel size 5*5 and output feature channels 64 2 BatchNorm, ReLU, MaxPool 3 2D Convolution with kernel size 5*5 and output feature channels 64 4</p><p>BatchNorm, ReLU, MaxPool 5 2D Convolution with kernel size 5*5 and output feature channels 128 6</p><p>BatchNorm, ReLU 7</p><p>Fully connection layer with output channels 10 8 Softmax  comparison, we report the results on both conditions, i.e. with/without data-augmentations. The results are shown in <ref type="table">Table 3</ref>-5. The ablation study in data augmentations indicates that mixup strategy can unify different augmentation strategies on different doman adaptation datasets with only one hyper-parameter. Moreover, KD3A can achieve good results even without data-augmentation.</p><p>7.6. Results on Digit-5 and Office-caltech10.</p><p>In this section, we report the experiment results on Digit-5 and Office-Caltech10. Digit-5 is a digit classification dataset including MNIST (mt), MNISTM(mm), SVHN (sv), Synthetic (syn), and USPS (up). Office-Caltech10 contains 10 object categories from four domains, i.e. Amazon (A), Caltech (C), DSLR (D). and Webcam (W). Note that results are directly cited from published papers if we follow the same setting. The results on </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>K+1 k=1 α k h k S . A common question is, how does the new model h K+1 S improve the UMDA performance? It is easy to find that minimizing KD loss (3) leads to the optimization of D K+1 S (h) (a) Knowledge distillation process in UMDA. (b) Knowledge vote ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>(a) Knowledge distillation in UMDA consists of two steps: obtaining the inferences from source domain models and performing knowledge ensemble to get the consensus knowledge. (b) Our knowledge vote extracts strong consensus knowledge with 3 steps: confidence gate, consensus class vote and mean ensemble. '×' means the eliminated model in each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The large-scale dataset DomainNet. Real is a domain of high quality containing real-world images, while Quickdraw is an irrelevant source domain and may cause the negative transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Weights assigned to the irrelevant and malicious domains in the training process. Our consensus focus can identify these bad domains with the low weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Theorem 2 (The decentralized generalization bound for KD3A). Let h T be the target model of KD3A, {D k S } K+1 k=1 be the extended source domains through Knowledge Vote and α CF ∈ R K+1</figDesc><table><row><cell>+</cell><cell>,</cell><cell>K+1 k=1 α CF k = 1 be the domain weights</cell></row><row><cell cols="3">through Consensus Focus. Then we have:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Source-only 52.1 ±0.51 23.1 ±0.28 47.7 ±0.96 13.3 ±0.72 60.7 ±0.32 46.5 ±0.56 ±0.62 23.4 ±0.43 60.9 ±0.71 16.4 ±0.28 72.7 ±0.55 60.6 ±0.32 51.1 UMDA accuracy (%) on the DomainNet dataset. Our model KD3A achieves 51.1% accuracy, significantly outperforming all other baselines. Moreover, KD3A achieves the oracle performance on two domains: clipart and sketch. *: The best results recorded in our re-implementation. The ablation study of KD3A. Results show that Knowledge Vote, Consensus Focus and BatchNorm MMD all contribute to the UMDA performance in all target domains.and take the cosine schedule to decay learning rate from high (i.e. 0.05 for Digit5 and 0.005 for Office-Caltech10 and DomainNet) to zero. We conduct each experiment five times and report the results with the form mean ±std . Since SHOT and DSBN do not report the results on DomainNet, we re-implement them with the official code and report the best testing results. Compared with DAEL, the KD3A provides better consensus knowledge on the high-quality domains such as Clipart and Real, while it also identifies the bad domains such as Quickdraw. CMSS select domains by checking the quality of each data with an independent network. Compared with CMSS, the KD3A does not introduce additional modules and can perform source selection in privacy-preserving scenarios. Moreover, our KD3A outperforms other decentralized models (e.g., SHOT and FADA) through the advantages in knowledge ensemble and source selection.</figDesc><table><row><cell>Standards</cell><cell>Methods</cell><cell>Clipart</cell><cell>Infograph</cell><cell>Painting</cell><cell>Quickdraw</cell><cell>Real</cell><cell>Sketch</cell><cell>Avg</cell></row><row><cell>W/o DA</cell><cell>Oracle</cell><cell>69.3 ±0.37</cell><cell>34.5 ±0.42</cell><cell>66.3 ±0.67</cell><cell>66.8 ±0.51</cell><cell>80.1 ±0.59</cell><cell>60.7 ±0.48</cell><cell>63.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.6</cell></row><row><cell>H−divergence</cell><cell>MDAN M 3 SDA</cell><cell cols="3">60.3 ±0.41 58.6 ±0.53 26.0 ±0.89 52.3 ±0.55 25.0 ±0.43 50.3 ±0.36</cell><cell>8.2 ±1.92 6.3 ±0.58</cell><cell>61.5 ±0.46 62.7 ±0.51</cell><cell>51.3 ±0.58 49.5 ±0.76</cell><cell>42.8 42.6</cell></row><row><cell>Knowledge Ensemble</cell><cell>DAEL</cell><cell>70.8 ±0.14</cell><cell>26.5 ±0.13</cell><cell>57.4 ±0.28</cell><cell>12.2 ±0.7</cell><cell>65.0 ±0.23</cell><cell>60.6 ±0.25</cell><cell>48.7</cell></row><row><cell>Source Selection</cell><cell>CMSS</cell><cell>64.2 ±0.18</cell><cell>28.0 ±0.2</cell><cell>53.6 ±0.39</cell><cell>16.0 ±0.12</cell><cell>63.4 ±0.21</cell><cell>53.8 ±0.35</cell><cell>46.5</cell></row><row><cell>Others</cell><cell>DSBN  *</cell><cell>60.3</cell><cell>22.6</cell><cell>52.3</cell><cell>9.1</cell><cell>62.7</cell><cell>47.6</cell><cell>42.4</cell></row><row><cell></cell><cell>SHOT  *</cell><cell>61.7</cell><cell>22.2</cell><cell>52.6</cell><cell>12.2</cell><cell>67.7</cell><cell>48.6</cell><cell>44.2</cell></row><row><cell>Decentralized</cell><cell>FADA  *</cell><cell>59.1</cell><cell>21.7</cell><cell>47.9</cell><cell>8.8</cell><cell>60.8</cell><cell>50.4</cell><cell>41.5</cell></row><row><cell>UMDA</cell><cell>FADA</cell><cell>45.3 ±0.7</cell><cell>16.3 ±0.8</cell><cell>38.9 ±0.7</cell><cell>7.9 ±0.4</cell><cell>46.7 ±0.4</cell><cell>26.8 ±0.4</cell><cell>30.3</cell></row><row><cell></cell><cell>KD3A</cell><cell>72.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>DomainNet. The results on DomainNet are presented in Table 1. In general, our KD3A outperforms all the baselines by a large margin. Moreover, KD3A achieves the oracle performance on clipart and sketch. Table 1 also shows the UMDA performance can benefit from the knowledge ensemble (DAEL) and source domain selection (CMSS).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The main idea of KD3A is to perform domain adaptation through the knowledge distillation without accessing the source domain data. Extensive experiments on the large-scale DomainNet demonstrate that our KD3A outperforms other state-of-the-art UMDA approaches and is robust to negative transfer. Moreover, KD3A has a great advantage in communication efficiency and is robust to the privacy leakage attack.</figDesc><table><row><cell>7. Appendix</cell></row><row><cell>7.1. Appendix A</cell></row><row><cell>Claim For the extended source domain D K+1 S</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>T ) at the training time, We now proceed to give a learning bound for empirical risk minimization using N T sampled training data.Following the learning bound Lemma 1,5 in<ref type="bibr" target="#b1">Ben-David et al. (2010)</ref>, for all 0 &lt; δ &lt; 1, with probability at least 1 − δ, we have:</figDesc><table><row><cell cols="3">The learning bound with empirical risk error. Proposi-</cell></row><row><cell cols="3">tion 1 shows how to relate the extended source domain</cell></row><row><cell>D K+1 S</cell><cell cols="2">and the target domain D T . Since we use the fi-</cell></row><row><cell cols="3">S nite samples to empirically estimate theˆ D K+1</cell><cell>(h) and</cell></row><row><cell cols="2">d H (D K+1 S</cell><cell>, D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>The 3-layers CNN backbone for Digit-5.</figDesc><table><row><cell>Parameters</cell><cell cols="2">Benchmark Datasets</cell><cell></cell></row><row><cell></cell><cell>Digit-5</cell><cell>Office-Caltech10</cell><cell>DomainNet</cell></row><row><cell>Data Augmentation</cell><cell cols="2">Mixup (α = 0.2)</cell><cell></cell></row><row><cell>Backbone</cell><cell>3-layers CNN (pretrained = False)</cell><cell cols="2">Resnet101 (pretrained = True)</cell></row><row><cell>Optimizer</cell><cell cols="2">SGD with momentum = 0.9</cell><cell></cell></row><row><cell cols="4">Learning rate schedule From 0.05 to 0.001 with cosine decay From 0.005 to 0.0001 with cosine decay</cell></row><row><cell>Batchsize</cell><cell>100</cell><cell>32</cell><cell>50</cell></row><row><cell>Total epochs</cell><cell></cell><cell>40</cell><cell></cell></row><row><cell>Communication rounds</cell><cell></cell><cell>r=1</cell><cell></cell></row><row><cell>Confidence gate</cell><cell>From 0.9 to 0.95</cell><cell></cell><cell>From 0.8 to 0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Implementation details of our KD3A on three benchmark datasets: Digit-5, Office-Caltech10 and DomainNet. ±0.62 23.4 ±0.43 60.9 ±0.71 51.1 ±0.28 72.7 ±0.55 60.6 ±0.32 51.1 The ablation study for data-augmentation strategies on DomainNet. †: Methods trained without data-augmentation.</figDesc><table><row><cell></cell><cell>Clipart</cell><cell>Infograph</cell><cell>Painting</cell><cell>Avg</cell></row><row><cell>KD3A  †</cell><cell>70.2 ±0.67</cell><cell>23.2 ±0.35</cell><cell>58.8 ±0.66</cell><cell>49.7</cell></row><row><cell>KD3A</cell><cell>72.5 Quickdraw</cell><cell>Real</cell><cell>Sketch</cell><cell></cell></row><row><cell>KD3A  †</cell><cell>16.1 ±0.21</cell><cell>70.4 ±0.54</cell><cell>59.5 ±0.41</cell><cell>49.7</cell></row><row><cell>KD3A</cell><cell>16.4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>,8 show that our KD3A outperforms other UMDA methods and advanced decentralized UMDA methods. Moreover, our KD3A provides better consensus knowledge on the hard domains such as the MNISTM domain on the Digit-5, which outperforms other methods by a large margin. Source-only 92.3 ±0.91 63.7 ±0.83 71.5 ±0.75 83.4 ±0.79 90.71 ±0.54 80.3 ±0.12 87.3 ±0.23 85.6 ±0.17 89.4 ±0.28 98.5 ±0.25 92.0 Table 7. UMDA accuracy (%) on the Digit-5. *: The best results recorded in our re-implementation. †: Methods trained without data-augmentation. Our model KD3A achieves 92.0% accuracy and outperforms all other baselines. ±0.07 95.2 ±0.08 97.9 ±0.11 99.6 ±0.03 97.2 KD3A 97.4 ±0.08 96.4 ±0.11 98.4 ±0.08 99.7 ±0.02 97.9 Table 8. UMDA accuracy (%) on the Office-Caltech10. *: The best results recorded in our re-implementation. †: Methods trained without data-augmentation.</figDesc><table><row><cell></cell><cell cols="7">KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation</cell></row><row><cell>Methods</cell><cell>mt</cell><cell></cell><cell>mm</cell><cell></cell><cell>sv</cell><cell></cell><cell>syn</cell><cell>usps</cell><cell>Avg</cell></row><row><cell>Oracle</cell><cell cols="2">99.5 ±0.08</cell><cell cols="2">95.4 ±0.15</cell><cell cols="2">92.3 ±0.14</cell><cell>98.7 ±0.04</cell><cell>99.2 ±0.09</cell><cell>97.0</cell></row><row><cell>MDAN</cell><cell cols="2">97.2 ±0.98</cell><cell cols="2">75.7 ±0.83</cell><cell cols="2">82.2 ±0.82</cell><cell>85.2 ±0.58</cell><cell>93.3 ±0.48</cell><cell>86.7</cell></row><row><cell>M 3 SDA</cell><cell cols="2">98.4 ±0.68</cell><cell cols="2">72.8 ±1.13</cell><cell cols="2">81.3 ±0.86</cell><cell>89.6 ±0.56</cell><cell>96.2 ±0.81</cell><cell>87.7</cell></row><row><cell>CMSS</cell><cell cols="2">99.0 ±0.08</cell><cell cols="2">75.3 ±0.57</cell><cell cols="2">88.4 ±0.54</cell><cell>93.7 ±0.21</cell><cell>97.7 ±0.13</cell><cell>90.8</cell></row><row><cell>DSBN  *</cell><cell>97.2</cell><cell></cell><cell>71.6</cell><cell></cell><cell>77.9</cell><cell></cell><cell>88.7</cell><cell>96.1</cell><cell>86.3</cell></row><row><cell>FADA</cell><cell cols="2">91.4 ±0.7</cell><cell cols="2">62.5 ±0.7</cell><cell cols="2">50.5 ±0.3</cell><cell>71.8 ±0.5</cell><cell>91.7 ±1</cell><cell>73.6</cell></row><row><cell>FADA  *</cell><cell>92.5</cell><cell></cell><cell>64.5</cell><cell></cell><cell>72.1</cell><cell></cell><cell>82.8</cell><cell>91.7</cell><cell>80.8</cell></row><row><cell>SHOT</cell><cell cols="2">98.2 ±0.37</cell><cell cols="2">80.2 ±0.41</cell><cell cols="3">84.5 ±0.32 91.1 ±0.23</cell><cell>97.1 ±0.28</cell><cell>90.2</cell></row><row><cell>KD3A  †</cell><cell cols="2">99.1 ±0.15</cell><cell cols="2">86.9 ±0.11</cell><cell cols="2">82.2 ±0.26</cell><cell>89.2 ±0.19</cell><cell>98.4 ±0.11</cell><cell>91.2</cell></row><row><cell cols="2">KD3A 99.2 Methods</cell><cell>A</cell><cell></cell><cell>C</cell><cell></cell><cell>D</cell><cell>W</cell><cell>Avg</cell></row><row><cell cols="2">Oracle</cell><cell cols="2">99.7</cell><cell cols="2">98.4</cell><cell cols="2">99.8</cell><cell>99.7</cell><cell>99.4</cell></row><row><cell cols="2">Source-only</cell><cell cols="2">86.1</cell><cell cols="2">87.8</cell><cell cols="2">98.3</cell><cell>99.0</cell><cell>92.8</cell></row><row><cell cols="2">MDAN</cell><cell cols="2">98.9</cell><cell cols="2">98.6</cell><cell cols="2">91.8</cell><cell>95.4</cell><cell>96.1</cell></row><row><cell cols="2">M 3 SDA</cell><cell cols="2">94.5</cell><cell cols="2">92.2</cell><cell cols="2">99.2</cell><cell>99.5</cell><cell>96.4</cell></row><row><cell cols="2">CMSS</cell><cell cols="2">96.0</cell><cell cols="2">93.7</cell><cell cols="2">99.3</cell><cell>99.6</cell><cell>97.2</cell></row><row><cell cols="2">DSBN  *</cell><cell cols="2">93.2</cell><cell cols="2">91.6</cell><cell cols="2">98.9</cell><cell>99.3</cell><cell>95.8</cell></row><row><cell cols="2">FADA</cell><cell cols="2">84.2 ±0.5</cell><cell cols="2">88.7 ±0.5</cell><cell cols="2">87.1 ±0.6</cell><cell>88.1 ±0.4</cell><cell>87.1</cell></row><row><cell cols="2">SHOT</cell><cell cols="2">96.4</cell><cell cols="2">96.2</cell><cell cols="2">98.5</cell><cell>99.7</cell><cell>97.7</cell></row><row><cell cols="2">KD3A  †</cell><cell>96.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Zhejiang University. Correspondence to: Wei Chen &lt;chen-vis@zju.edu.cn&gt;. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Implemented with running-mean and running-var in Pytorch. 2 Notice E[π] 2 = Var(π) + [E(π)] 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/ICML2021-13/KD3A KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We perform UMDA on those datasets with multiple domains. During experiments, we choose one domain as the target domain, and use the remained domains as source domains. Finally, we report the average UMDA results among all domains. The code, with which the most important results can be reproduced, is available at Github 3 . We also provide the code as well as the detailed documentation in "SourceCode.zip". In this section, we discuss the implementation details. Following previous settings <ref type="bibr" target="#b12">(Peng et al., 2019)</ref>, we use a 3-layer CNN as backbone for Digit-5, as shown in <ref type="table">Table 4</ref>, and use the pretrained ResNet101 for Office-Caltech10 and DomainNet. The details of hyper-parameters are provided in <ref type="table">Table 5</ref> and the backbones and training epochs are set to same in all method comparison experiments. In training process, We use the SGD as optimizer and take the cosine schedule to decay learning rate from high (i.e. 0.05 for Digit5 and 0.005 for Office-Caltech10 and DomainNet) to zero. Data augmentations. Data augmentations are important in deep network training process. Since different datasets require different augmentation strategies (e.g. rotate, scale, and crop), which introduces extra hyper-parameters, we use mixup  as a unified augmentation strategy and simply set the mix-parameter α = 0.2 in all experiments. For fair</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Appendix C Proposition 2</head><p>The KD3A bound is a tighter bound than the original bound, if the task risk gap between the knowledge distillation domain D K+1 S and the target domain D T is smaller than the following upper-bound for all source domain k ∈ {1, · · · , K}, that is, D K+1 S (h) should satisfy:</p><p>Proof:</p><p>Following the Theorem 2 in <ref type="bibr" target="#b1">Ben-David et al. (2010)</ref>, for each source domain D k S and for all h T ∈ H, we have</p><p>and we have the following relations between λ 0 and λ k S :</p><p>With (11 − 13), the original bound (12) can be considered as the weighted combination of the source domains. In addition, the KD3A bound is also the combination of the original bound (12) and the knowledge distillation bound (2). Then we get that the KD3A bound is a tighter bound than the original bound if the knowledge distillation bound (2) is tighter than the single source bound (11) for each source domain D k S , that is, for all source domain k ∈ {1, · · · , K} and all h T ∈ H, the knowledge distillation bound should satisfy:</p><p>Since d H∆H (D K+1 S , D T ) = 0 and λ 1 is a constant, the task risk gap sup h∈H | D K+1 S (h) − D T (h)| should satisfy the following condition for all h T ∈ H, that is:</p><p>Since condition (15) holds for all h T ∈ H, we have the tighter bound condition as</p><p>7.4. Appendix D 7.5. Implementation details.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to backdoor federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Chiappa, S. and Calandra, R.</editor>
		<meeting><address><addrLine>Online [Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-08-28" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2938" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domainspecific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R. and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno>abs/1610.05492</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6028" to="6039" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R. and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<editor>Singh, A. and Zhu, X. J.</editor>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-04-22" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
	<note>54 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial teacher-student learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461682</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="5949" to="5953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Federated adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno>abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond inferring class representatives: User-level KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation privacy leakage from federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Communications, INFOCOM 2019</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-02" />
			<biblScope unit="page" from="2512" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Curriculum manager for source selection in multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th</title>
		<editor>Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<idno type="DOI">10.1007/978-3-030-58568-6\36</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part XIV</title>
		<meeting>Part XIV<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<editor>Bonet, B. and Koenig, S.</editor>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple source domain adaptation with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-source distilling domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12975" to="12983" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Domain adaptive ensemble learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep leakage from gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="14747" to="14756" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
