<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zhang</surname></persName>
							<email>zhaohui.zhang@rokid.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Rokid Corportation Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Xie</surname></persName>
							<email>shipeng.xie@rokid.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Rokid Corportation Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rokid Corportation Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhu</surname></persName>
							<email>haichao.zhu@rokid.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Rokid Corportation Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand pose estimation from 3D depth images, has been explored widely using various kinds of techniques in the field of computer vision. Though, deep learning based method improve the performance greatly recently, however, this problem still remains unsolved due to lack of large datasets, like ImageNet or effective data synthesis methods. In this paper, we propose HandAugment, a method to synthesize image data to augment the training process of the neural networks. Our method has two main parts: First, We propose a scheme of twostage neural networks. This scheme can make the neural networks focus on the hand regions and thus to improve the performance. Second, we introduce a simple and effective method to synthesize data by combining real and synthetic image together in the image space. Finally, we show that our method achieves the first place in the task of depth-based 3D hand pose estimation in HANDS 2019 challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hand pose estimation from a single depth image lays the foundation of human-computer interaction technique on a head-mounted Augmented Reality (AR) device, e.g., Microsoft Hololens, Magical Leap One. It has the advantage that users can provide input to devices efficiently. Despite recent remarkable progress, this problem still remains unsolved because of the large pose variation, large view point variation, self-similarities and self-occlusion of finger joints .</p><p>Recently, Deep Learning has become popular in the community of computer vision and also achieves state-of-the-art on the 3D hand pose estimation tasks. These methods can be roughly classified into two categories. The first category treats the input depth image as a single channel image and apply 2D convolutional neural network directly on the depth image. Representative methods are A2J <ref type="bibr" target="#b10">[Xiong et al., 2019]</ref>, <ref type="bibr">DeepPrior++ [Oberweger and Lepetit, 2017]</ref>. The second category of methods use 3D information. These methods either convert depth images into 3D voxels <ref type="bibr" target="#b5">[Moon et al., 2018]</ref>, <ref type="bibr" target="#b2">[Ge et al., 2017]</ref> or point clouds <ref type="bibr" target="#b3">[Ge et al., 2018]</ref> and then followed by 3D CNN or point net respectively. These neural networks are trained with hand regions extracted from depth images. Intuitively, the quality of extracted hand region is important for hand pose estimation. However, the region extract method used in previous methods are naive. For example, in <ref type="bibr" target="#b7">[Sinha et al., 2016]</ref>, the users wears a colorful wristband which is used to determine the hand regions as show in <ref type="figure" target="#fig_0">Figure 1 (A)</ref>. This method is impractical in real cases. In <ref type="bibr" target="#b0">[Chen et al., 2019]</ref>, hand regions are initialized using a shallow CNN. However, it can introduce arms or other foreground regions <ref type="figure" target="#fig_0">(Figure 1 (B)</ref>). In <ref type="bibr" target="#b9">[Wan et al., 2018]</ref>, hand regions are obtained using groundtruth annotation which is not available in real application <ref type="figure" target="#fig_0">(Figure 1  (C)</ref>).</p><p>In addition, these deep learning based methods are effective only if a large amount of training data is available. The data is usually collected and labelled manually, which is tedious and time consuming. This labeling problem is even worse for 3D computer vision problems which require to label with 3D data and this task is more difficult for humans. Recently, many works therefore focus on using computer graphics methods to synthesize image data <ref type="bibr" target="#b6">[Rad et al., 2018]</ref> and corresponding annotation data automatically. However, the resulting performances are usually suboptimal because synthetic images do not correspond exactly to real images.</p><p>In this paper, we propose HandAugment, a method to synthesize image data to augment the training process of hand pose estimation neural networks. First, We propose a scheme of two-stage neural networks to tackle the hand region extraction. This scheme can gradually find the hand regions in the input depth maps. Second, we propose a data synthesis method based on <ref type="bibr">MANO [Romero et al., 2017]</ref>. Because synthetic images do not correspond exactly to real data, therefore we combine real data with synthetic images together. Finally, we apply HandAugment to different datasets and the experiment shows our method can greatly improve the performance and achieves state-of-the-art results. Our method achieves the first place in the task of depth-based 3D hand pose estimation in HANDS 2019 challenge. Our codes are available upon request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relate Work</head><p>In this section we review related works of our proposed method, including depth-based 3D hand pose estimation and data augmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Depth-Based 3D Hand Pose Estimation</head><p>Hand pose estimation, has been explored widely using various kinds of techniques in the field of computer vision. Related neural network-based hand pose estimation approaches using depth images are reviewed as follows. The goal of hand pose estimation is to estimate the 3D location of hand joints from one or more frames recorded from a depth camera. The neural network based methods can be roughly classified into two categories: 2D and 3D deep learning respectively. 2D deep learning based approach. The 2D deep learning based approaches estimate hand pose directly from depth images. Representative methods include a cascaded multistage method <ref type="bibr" target="#b0">[Chen et al., 2019]</ref>, a structure-aware regression approach <ref type="bibr" target="#b8">[Taylor et al., 2016]</ref>, and hierarchical tree-like structured CNNs <ref type="bibr" target="#b5">[Madadi et al., 2017]</ref>. Due to end-to-end working manner, deep learning technology holds strong fitting ability for visual pattern characterization. 2D CNN has already achieved great success for 2D pose estimation. But these methods are unable to fully capture the 3D information from 3D hand poses, because these methods take depth maps as 2D single channel images for the input. 3D deep learning based approach. To better reveal the 3D information within depth map for performance improvement. Some recent research tried 3D deep learning. The 3D deep learning based approach convert 2D depth images into 3D data structure, such as 3D voxel grids for <ref type="bibr" target="#b5">[Moon et al., 2018]</ref> or D-TSDF volumes <ref type="bibr" target="#b2">[Ge et al., 2017]</ref>. These 3D method is very accurate in 3D hand pose estimation problem and they produce state-of-the-art results. However, the 3D CNN is relatively hard to train due to a large number of parameters. Meanwhile, using 3D CNN also leads to high computational burden both on memory storage and running time. Therefore, it is less computational efficient than 2D methods. Accordingly, HandAugment belongs to 2D deep learning based methods. We use a 2D CNN as the backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation Methods</head><p>Data augmentation is a strategy that can significantly increase the diversity of data for training deep network, without collecting additional training data. In recent years, Data augmentation techniques such as cropping, padding and flipping are commonly used in deep learning. This strategy can improve the performance of these data-driven tasks, suck like <ref type="figure">Figure 2</ref>: System Overview. The input depth image is feed into the first neural network N et1 to obtain a augmented hand region. Then, this augmented hand region is feed into the second neural network N et2 to estimate the hand pose. object recognition and hand pose estimation. It has already been widely used in recent work <ref type="bibr" target="#b10">[Xiong et al., 2019]</ref>, <ref type="bibr" target="#b10">[Yang et al., 2019]</ref> and <ref type="bibr" target="#b6">[Oberweger and Lepetit, 2017]</ref>. Most of these data augmentation methods use image transformation methods, including in-plain translation, rotating, scaling and mirroring. Specifically, for color-based methods, training images can be augmented by adjusting the hue channel of the color images <ref type="bibr" target="#b10">[Yang et al., 2019]</ref>. For depth-based methods, images can be augmented by applying 3D transformation, such as <ref type="bibr" target="#b2">[Ge et al., 2017]</ref> which randomly rotates and stretches the 3D point cloud to synthesize training data.</p><p>Another way to synthesize training data is to use training samples rendered from 3D models <ref type="bibr" target="#b5">[Hinterstoisser et al., 2018]</ref>. Such annotated samples are very easy to acquire due to the presence of large scale 3D model datasets. However, using synthetic data requires carefully designed train process to prevent the network from overfitting on the synthetic appearance of the data. This is due to the fact that the distribution of synthesize data is quite different from the distribution of real data.</p><p>Accordingly, our method use the rendered method to synthesize training data and perform data augmentation. We will show that the accuracy of hand pose estimation can be significantly improved by combining some real images and many synthetic images together through our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first give an overview of HandAugment in Section 3.1. After that we present details about two stage network scheme in Section 3.2. In Section 3.3 we illustrate how to synthesize data for data augmentation. Finally, the implementation details are given in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a depth image I, the task of hand pose estimation is to estimate the 3D locations (x, y, z) of N hand joints. We use a scheme of two-stage neural networks to estimate the hand poses, as illustrated in <ref type="figure">Figure 2</ref>. We first feed the input depth image into the first neural network (denoted as N et 1 ) which estimates an initial hand pose (denoted as P ose 1 ). Then, this initial hand pose P ose 1 is used to extract a augmented hand region from input depth image. Finally, this augmented hand region is feed into the second neural network (denoted as N et 2 ) to estimate the final hand pose P ose 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-Stage Network Scheme</head><p>We use a scheme of two-stage neural networks to estimate the hand poses. The input of the first stage neural network is a coarse patch extracted from input depth image. This coarse patch, denoted as patch 1 , usually contains noisy regions which appear around hand regions. These noisy regions can be arm regions or background objects, as shown in <ref type="figure">Figure 3</ref>. Obviously, these noisy regions can degrade the performance. Thus, our solution is to remove these noisy regions from coarse patches to get augmented patches for hands. To get the augmented hand patch, we first train the network N et 1 on coarse patch data patch 1 to predict an initial hand pose P ose 1 . After that, we find the maximum and minimum values from P ose 1 in 3D coordinate: x min , x max , y min , y max , z min , z max , and use them to determine a 3D bounding box. Then, this 3D bounding box are used to loose crop from coarse patch and get an augment hand patch. Specifically, for any point (x, y, z) in patch 1 that is out of the range   are parameters to control extended range of 3D bounding box, z thickness denotes the thickness of finger. The reason to extend the ranges of 3D bounding boxes is that the sizes of effective hand regions are usually larger than the sizes of hand skeleton bounding boxes. If using 3D bounding box directly without extending the range, the hand region extraction might not obtain full area of hand region in some cases, as shown in <ref type="figure">Figure 4</ref>. Expanding range of bounding box can cover the gap between hand skeleton and hand skin (silhouette). In addition, due to the fact that the hand skin is always in front of hand skeleton in depth image, thus, we add parameter z thickness to handle this problem. The above procedure is denoted as first stage, and the augment hand patch obtained from first stage is denoted as patch 2 . In the second stage, the patch 2 is fed into N et 2 to get the final hand poses. This process is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><formula xml:id="formula_0">[x min − x of f set , x max + x of f set ], [y min − y of f set , y max + y of f set ], and [z min − z of f set − z thickness , z max + z of f set ] is removed from patch 1 . While, x of f set ,</formula><p>The architectures of our two networks are based on EfficientNet-B0 <ref type="bibr" target="#b8">[Tan and Le, 2019]</ref>. We give the architecture of our modified EfficientNet-B0 in <ref type="table" target="#tab_1">Table 1</ref>. The input of these two networks are image patches cropped from input depth images. The cropped patches are resized to 224 × 224 before feeding into the networks. The output of these two networks is a 3 × 21-dimensional vector indicates the 3D locations of the 21 hand joints (14 hand joints for NYU experiment). Beside the input and output, the rest of the architectures are the same as that of the original EfficientNet-B0.</p><p>We train N et 1 and N et 2 with a Wing Loss <ref type="bibr" target="#b1">[Feng et al., 2018]</ref>, because the Wing Loss is robust for both small and large pose deviations. Given an estimated pose p i of the i-th joint and its corresponding ground truth q i , the Wing Loss is defined as:</p><formula xml:id="formula_1">L W = w ln(1 + v i / ) if v i &lt; w v i − C otherwise<label>(1)</label></formula><p>where v i = p i − q i , w controls the width of non-linear part to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Augmentation</head><p>Our method is based on <ref type="bibr">MANO [Romero et al., 2017]</ref> to synthesize training data. MANO renders a depth image containing a right hand using three parameters: a camera parameter c, a hand pose parameter a and a shape parameter s. The camera parameter c is a 8-dimensional camera parameter including scale c s ∈ R, translation c s ∈ R 3 along three camera axes, and global rotation c q ∈ R 4 (in quaternion). The hand pose parameter a is a 45-dimensional vector, and the shape parameter s is a 10-dimensional vector. To obtain MANO parameters, we use the HANDS19 dataset which uses gradient based optimization <ref type="bibr" target="#b0">[Baek et al., 2019]</ref> to estimate MANO parameters from real images ( <ref type="figure" target="#fig_3">Figure 6 (A)</ref>). Then we use these estimated MANO parameters to synthesize images ( <ref type="figure" target="#fig_3">Figure 6  (B)</ref>).</p><p>We have four strategies to prepare training data. The first two are to use real data and the original MANO parameters provided by HANDS19 directly. We do not add, remove or modify any data. These two dataset contain totally 170K images respectively. We call these two datasets as Real Dataset (RD) and Synthetic Dataset (SD) respectively.</p><p>The third strategy is to use a linear blending method to combine synthetic images with real images, that is because the distribution of SD and RD is different. An example is given in <ref type="figure" target="#fig_3">Figure 6</ref>. Given a synthetic image I s and its corresponding real image I r , the final mixed image is given:</p><formula xml:id="formula_2">I f (i, j) = I s (i, j), if I s (i, j) &gt; 0 I r (i, j), if I s (i, j) = 0<label>(2)</label></formula><p>We create totally 170K mixed synthetic images. This dataset is denoted as the Mixed synthetic Dataset (MD). Lastly, in order to generate more training data, we create new MANO parameters by add Gaussian noise to the original three MANO parameters c, a and s provided by HANDS 2019 dataset. The Gaussian distribution is obtained by assuming that each dimension of the three parameters are independent and the noise follow the same distribution as the original data. To generate the data, we add noise to only one of the three parameters (camera view, hand pose and shape parameters) or all of them. Totally, we create 400K images, 100K for camera view parameters, 100K for hand pose (articular) parameters, 100K for shape parameters and 100K for all of the three parameters. We denote this dataset as Noised synthetic Dataset (ND).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Preprocessing. Similar to the previous method <ref type="bibr" target="#b0">[Chen et al., 2019]</ref>, we extract a patch from the input depth image. The patch center and patch size is determined by the metacarpophalangeal (MCP) joints of middle finger. Notice, we use a provided bounding box to get input patches on hands 2019 experiments where MMCP is not available. The patches are then resized to 224 × 224. The depth values of input patches are first truncated by the depth of MCP joint and then normalized into [−1, 1]. These patches are then feed into neural networks. Training. We train our two networks on a workstation equipped with a Intel Xeon Platinum 8160 CPU and two NVIDIA GEFORCE RTX 2080 Ti GPUs. We implement the networks using pytorch. To train N et 1 , the batch size and learning rate are set 128 and 0.0006 respectively and Adamax is used to optimize. A step-wise learning rate scheduler is used. The network is trained using all the training data, including RD, SD, MD and ND, and we have 640K images in total. Then N et 2 is fine tuned from N et 1 . The batch size and learning rate are also set 128 and 0.0006 respectively. The optimizer is also Adamax. A step-wise learning rate scheduler is also used. N et 2 is trained using RD and SD. The parameter w and from Wing Loss are empirically set as 100 and 7.5 respectively in all experiments. All x of f set , y of f set , z of f set are set to 30 mm, and z thickness is set to 20 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We first introduce datasets and evaluation metrics used in our experiments. Afterwards we compare our method with state-of-the-art methods. Finally we conduct extensive experiments for ablation study to discuss the effectiveness and robustness of different components of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NYU Hand Pose Dataset <ref type="bibr" target="#b9">[Tompson et al., 2014]</ref>. The NYU hand pose dataset was collected using three Kinects from different views. The training set contains 72K images from 1 subject. And the test set contains 8.2K images from 2 subjects, while one of the subjects in test set doesn't appear in training set. The annotation of 3D hand pose contains 36 joints. Following the protocol of previous works <ref type="bibr" target="#b0">[Chen et al., 2019;</ref><ref type="bibr" target="#b4">Guo et al., 2017;</ref><ref type="bibr" target="#b5">Moon et al., 2018]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>Average 3D joint error is average euclidean distance between predicted joint location and ground-truth for each joint over all test frames. We use the average 3D joint error as main evaluation metric in HANDS 2019 and NYU experiment. Furthermore, in HANDS 2019 dataset, there are five evaluation axes are calculated: -Total/Extrapolation: viewpoints, articulations and hand shapes not present in the training set. We refer it as Extrapolation in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art methods</head><p>HANDS 2019 dataset: We compare our method with the state-of-the-art 3D hand pose estimation methods . The results is listed in <ref type="table" target="#tab_3">Table 2</ref>. It can be observed that: -On this challenging million-scale dataset, our method outperforms the other approaches in most of the score axes. This essentially verifies the superiority of our proposition. -Our method reaches the lowest average joint 3D error in extrapolation, shape and viewpoint score axis, simultaneously. It demonstrates the robustness and generalization ability of our method. -A2J and V2V are strong competitors to our method. And V2V even gets better score than our method in interpolation and articulation score axes. But their methods are the result of using carefully designed neural network architectures. As a consequence, it is much more complicated than our method which only uses a simple twostage neural network. NYU Hand Pose dataset: Our method is compared with state-of-the-art 3D hand pose estimation methods. The experiment result are given in <ref type="table">Table 3</ref>. We can summarize that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error (mm) DeepPrior++ <ref type="bibr" target="#b6">[Oberweger and Lepetit, 2017]</ref> 12.24 Pose-REN <ref type="bibr" target="#b0">[Chen et al., 2019]</ref> 11.81 HandPointNet <ref type="bibr" target="#b3">[Ge et al., 2018]</ref> 10.50 DenseReg <ref type="bibr" target="#b9">[Wan et al., 2018]</ref> 10.20 V2V <ref type="bibr" target="#b5">[Moon et al., 2018]</ref> 9.22 A2J <ref type="bibr" target="#b10">[Xiong et al., 2019]</ref> 8.61 SS(our baseline) 13.44 TS (ours) 9.02 <ref type="table">Table 3</ref>: Comparison Average 3D joint error with state-of-art methods on NYU dataset <ref type="bibr" target="#b9">[Tompson et al., 2014]</ref>.</p><p>-Our method is superior to the other methods in most cases. The exceptional case is that our method is slightly inferior to A2J method on NYU dataset. This is because NYU dataset provides ground truth annotations to extract hand regions for test data, but HANDS 2019 dataset only provides coarse bounding box to extract hand regions for test data. Therefore, using ground truth annotations for hand region extraction is more accurate than our proposed two-stage network scheme. However, it is impossible to get ground truth annotations in real application which makes A2J less practical than our method. This demonstrates the robustness of HandAugment. -Our proposed method decreases the error of baseline network (SS) from 13.44 mm to 9.02 mm with a 33% improvement. This verifies the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study Component Effectiveness Analysis</head><p>The component effectiveness analysis within HandAugment is executed on HANDS 2019 dataset. We will investigate the effectiveness of two stage network scheme and our synthesized data strategy. Firstly, We build a baseline model M odel 0 which is a single stage (SS) network and trained on real data (RD) only. Note that the single stage scheme only contains one EfficientNet-B0 network. Secondly, We add two stage scheme and synthetic data to baseline model and denote them as M odel 1 and M odel 2 , respectively. Finally, We add both two-stage scheme and synthetic data to get our final model M odel 3 . The result are given in <ref type="table" target="#tab_5">Table 4</ref>. It can be observed that: -The two-stage scheme remarkably improves the accuracy of hand pose estimation whether we use the data augmentation method or not. This verifies our observation that the extracted hand region is an important factor that affects the accuracy of predicted hand poses. Our proposed two-stage scheme can extract accurate hand regions for neural networks to estimate hand poses. -Using synthetic data generated by our proposed method can tremendously decrease the average 3d joint errors of both the single-stage scheme and the two-stage scheme. This demonstrates the importance of synthetic data, and the effectiveness of our data synthesis method. -By combining all components together, our method finally gets 27.84% improvement compared with the   baseline model. This essentially verifies the effectiveness of HandAugment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of The Fine-Tuning on The Second Stage Network</head><p>Our two-stage scheme contains two neural networks and the second stage network is fine-tuned on the first stage network.</p><p>To show how the fine-tuning can improve the performance, we give the results in <ref type="table" target="#tab_6">Table 5</ref>. Note that these results are obtained using real data only. Our two-stage scheme without fine-tuning performs worse than the single-stage scheme. This is probably because the first stage network is over-fitting to the input hand regions of the first stage network. This overfitting leads to the decreases of generalization. The distribution of input data of the second stage is a subspace of the distribution of input data of the first stage. Obviously, it is easier to train a neural network in a subset if this network has been already trained on a super-set. Therefore, we fine tune the second stage network from the weights of the first stage network. The results show how it greatly improves the performance in <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of The Method for Synthesized Data</head><p>We propose three strategies to synthesize training data as introduced in Section 3.3. We train neural networks by using different combination of the three strategies and the results are given in <ref type="table" target="#tab_8">Table 6</ref>. Note that the results are obtained on the single-stage scheme. We can see that the performance is gradually improved as we add SD, MD and ND into training. This demonstrates that using our proposed strategies to synthesize training data fills the gap between the real data and synthetic data. Furthermore, we show how different strategies of adding noise to synthesize data influence the performance. The results are listed in <ref type="table" target="#tab_9">Table 7</ref>. Note that these results are also obtained with the single-stage network. We add noise in one of the three parameters, including camera view point, hand articular pose and hand shape, or all of the three parameters. We can see that the performance is improved even we add   noise into only one parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose HandAugment, a method to synthesize image data to augment the training of hand pose estimation method. First, We propose a scheme of two-stage neural networks to tackle the hand region extraction. This scheme can gradually find the hand regions in the input depth maps. Second, we propose a data synthesis method based on MANO. We have three strategies to prepare the training data: using the original MANO parameters, mixed real and synthetic data and noised synthetic data. Finally, we conduct several experiments to demonstrate that HandAugment is effective to improve the performance and achieves stateof-the-art results compared to existing method in Hands 2019 challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(A) A wrist band is detected to determine the hand region. (B) The hand region is estimated from input, however it can introduce arm or other foreground regions. (C) Using groundtrudth to extract hand region for training, however this is impractical in real application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>There is a lot of noise near the hand area in coarse patch patch1, such as arm region (shown in blue dotted frame), human body and other background object (shown in magenta dotted frame). The influence of expanding range in region extraction. (A) The input depth image. (B) The coarse patch patch1. (C) The augmented patch w/o range expanding and z thickness . (D) The augmented patch w/o z thickness . (E) The augmented patch we proposed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>y of f set and z of f set We use the pose estimate from N et1 to augment the patch for N et2. (A) The input depth image. (B) The input of N et1. (C) The input of N et2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Data Synthesis. (A) A real image. (B) A synthetic image using corresponding MANO parameters. (C) The final synthetic image combining a real image and a synthetic image. be within [−w, w], limits the curvature of the nonlinear part, and C = w − w ln(1 + v i / ) links the linear and non-linear parts together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-Articulation: articulations not present in the training set. -Viewpoint: viewpoints not present in the training set. -Shape: shapes not present in the training set. -Interpolation: viewpoints, articulations and shapes present in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The summary of N et1 and N et2 architecture</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we only use images from the frontal view and pick 14 of the 36 joints for evaluation. Both annotations of training and test set are provided.HANDS 2019 Dataset[HANDS19, 2019]. This dataset is sampled from BigHand2.2M. The training set contains 175K images from 5 different subjects. Some hand articulations and viewpoints are strategically excluded in the training set. The test set contains 125K images from 10 different subjects, 5 subjects overlapping with the training set, exhaustive coverage of viewpoints and articulations. The annotations of hand poses contain 21 joints, with 4 joints for each finger and 1 joint for the palm. The hand annotations</figDesc><table><row><cell>Method</cell><cell>Main Error</cell><cell>I.</cell><cell>S.</cell><cell>A.</cell><cell>V.</cell></row><row><cell>BT [Yang et al., 2019]</cell><cell>23.62</cell><cell>18.78</cell><cell>21.84</cell><cell>16.73</cell><cell>19.48</cell></row><row><cell>IPR [Sun et al., 2018]</cell><cell>19.63</cell><cell>8.42</cell><cell>14.21</cell><cell>7.50</cell><cell>14.16</cell></row><row><cell>V2V [Moon et al., 2018]</cell><cell>13.76</cell><cell>3.93</cell><cell>11.75</cell><cell>3.65</cell><cell>7.50</cell></row><row><cell>A2J [Xiong et al., 2019]</cell><cell>13.74</cell><cell>6.33</cell><cell>11.23</cell><cell>6.05</cell><cell>8.78</cell></row><row><cell>Ours</cell><cell>13.66</cell><cell>4.10</cell><cell>10.27</cell><cell>4.74</cell><cell>7.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison Average joint 3D error(mm) and ranking result with state-of-art methods on HANDS 2019 dataset[HANDS19,  2019]. I., S., A. and V. stand for the errors of interpolation, shape, articulation and viewpoint, respectively. The main error is an extrapolation error on HANDS 2019 dataset. Details of the evaluation metric are described in section 4.2.</figDesc><table><row><cell>are only available for the training set. Instead, the bound-</cell></row><row><cell>ing boxes of the test set are provided. We use the HANDS</cell></row><row><cell>2019 official test tool to calculate test scores. This dataset</cell></row><row><cell>has large viewpoint, articulations and hand shape variations,</cell></row><row><cell>which makes it a rather challenging dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiments of different configuration of our method. SS and TS stand for the single stage and the two stage networks respectively. RD and SD stand for real data and synthetic data respectively.</figDesc><table><row><cell cols="3">Model Name Method Error (mm)</cell></row><row><cell>M odel 0</cell><cell>ST</cell><cell>18.93</cell></row><row><cell>M odel 10</cell><cell>TS  *</cell><cell>20.99</cell></row><row><cell>M odel 1</cell><cell>TS  †</cell><cell>16.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Experiments of different configuration of our method. * The two-stage scheme without fine-tuning on the second stage network. † The two-stage scheme with fine-tuning on the second stage network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Effect of synthetic data. Notice that the single stage network is trained. All these results are obtained on a single stage network. RD and SD stand for real data and synthetic data respectively. MD and ND stand for the mixed synthesis data and the noised synthetic data respectively.</figDesc><table><row><cell>Model Name</cell><cell>Method</cell><cell>Error (mm)</cell></row><row><cell>M odel21</cell><cell>No noise</cell><cell>16.22</cell></row><row><cell>M odel210</cell><cell>Viewpoint</cell><cell>15.94</cell></row><row><cell>M odel211</cell><cell>Articular</cell><cell>15.82</cell></row><row><cell>M odel212</cell><cell>Shape</cell><cell>16.13</cell></row><row><cell>M odel2</cell><cell>Viewpoint + Articulate + Shape</cell><cell>15.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Adding noise to MANO parameters to generate synthetic data. All these results are obtained using the single stage scheme.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Neurocomputing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1067" to="1076" />
		</imprint>
	</monogr>
	<note>Pose guided structured re</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8417" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4512" to="4516" />
		</imprint>
	</monogr>
	<note>HANDS19, 2019] HANDS19. Hands 2019 challenge</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinterstoisser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09606</idno>
		<ptr target="https://sites.google.com/view/hands2019/challenge" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Oberweger and Lepetit</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature mapping for learning fast and accurate 3d pose inference from synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">245</biblScope>
		</imprint>
	</monogr>
	<note>Embodied hands: Modeling and capturing hands and bodies together</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deephand: Robust hand pose estimation by completing a matrix imputed with deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V Le ;</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
	</analytic>
	<monogr>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">143</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
	<note>Dense 3d regression for hand pose estimation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Yuan et al., 2017] Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, and Tae-Kyun Kim. Bighand2.</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4866" to="4874" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
