<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dahua Lin †</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ ShanghaiTech University § SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cylinder Partition</term>
					<term>Asymmetric Residual Block</term>
					<term>Context Modeling *</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art methods for large-scale driving-scene LiDAR semantic segmentation often project and process the point clouds in the 2D space. The projection methods includes spherical projection, bird-eye view projection, etc. Although this process makes the point cloud suitable for the 2D CNN-based networks, it inevitably alters and abandons the 3D topology and geometric relations. A straightforward solution to tackle the issue of 3D-to-2D projection is to keep the 3D representation and process the points in the 3D space. In this work, we first perform an in-depth analysis for different representations and backbones in 2D and 3D spaces, and reveal the effectiveness of 3D representations and networks on LiDAR segmentation. Then, we develop a 3D cylinder partition and a 3D cylinder convolution based framework, termed as Cylinder3D, which exploits the 3D topology relations and structures of driving-scene point clouds. Moreover, a dimensiondecomposition based context modeling module is introduced to explore the highrank context information in point clouds in a progressive manner. We evaluate the proposed model on a large-scale driving-scene dataset, i.e. SematicKITTI. Our method achieves state-of-the-art performance and outperforms existing methods by 6% in terms of mIoU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D LiDAR sensor has become an indispensable device in modern autonomous driving vehicles. It captures more precise and further-away distance measurements of the surrounding environments than conventional visual cameras. The measurements of the sensor naturally form 3D point clouds that can be used to understand the overall scenes for autonomous driving planning and execution.</p><p>Semantic segmentation of 3D point clouds is crucial for driving-scene understanding. It aims to identify the pre-defined categories of each 3D point that belongs to, such as car, truck, pedestrian, etc, which provides point-wise perception information of the overall 3D scene.</p><p>Most existing point cloud-based segmentation algorithms focus more on indoor scenes, where the point clouds are generally dense and have mostly uniform densities. In contrast, only very few methods work on segmentation of LiDAR point clouds in outdoor or autonomous driving scenes, where the LiDAR points have varying densities according to their distances to the sensor and pose great challenges to the algorithms.</p><p>Recent methods usually pay much attention on point feature representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Point feature representation for LiDAR point clouds has three major categories: range image <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>, bird view image <ref type="bibr" target="#b1">[2]</ref> and voxel partition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. The range image is obtained via spherical projection of the irregularly distributed 3D point clouds to the 2D dense grids. The bird-view image squeezes point height information and shares a global height feature for each location on the bird view map. However, most of these approaches might lose certain accurate geometric information during the 3D-to-2D projection.  <ref type="figure">Figure 1</ref>: (Left) The detailed road map for network architecture search on SemanticKITTI, from 2D, 2.5D to 3D (Note that 2.5D means 3D grid representation and 2D backbone). (Right) The limitation of spherical projection, namely, abandons certain valuable 3D structures, where neighboring region in projection reflects significantly different locations in 3D space, which shows that spherical projection cannot maintain the 3D geometry structure.</p><p>In this paper, we reposition the focus of LiDAR segmentation in autonomous driving scenes.This paper conducts experiments to show effectiveness of different point feature representations and neural network architectures. Experiments reveal that 3D partition with 3D convolutional neural networks works better than other counterparts. A cylinder partition is proposed to process the driving-scene point clouds due to its varying densities, which balances the distribution of driving-scene point clouds. To match the cuboid objects in driving-scene LiDAR data, we propose the asymmetric residual block as a basic module to form the 3D backbone. In addition to the network search, we also propose a new dimension decomposition block to efficiently exploit the context information via a series of low-rank convolution kernels.</p><p>The contributions of this work can be summarized as three-fold. <ref type="bibr" target="#b0">(1)</ref> We study state-of-the-art network architectures and different point feature representations, which reveal directly processing point clouds without 3D-to-2D projection is crucial for achieving superior segmentation performance. <ref type="bibr" target="#b1">(2)</ref> We propose a cylinder partition, a point cloud encoding scheme, which better follows the inherent distribution of the 3D driving-scene point clouds, and develop a 3D convolution based framework, in which the asymmetric residual block is designed as the basic module and a new dimension decomposition block is proposed to explore the context in a progressive manner. (3) Our proposed LiDAR segmentation algorithm outperforms state-of-the-art algorithms on driving-scene semantic segmentation benchmarks with a large margin, i.e., 6% mIoU gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Indoor-scene Point Cloud Segmentation. Indoor-scene point clouds have some properties, including generally uniform density and small range of the scene. Hence, most indoor-scene segmentation methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> often learn the point features from the raw point directly. PointNet <ref type="bibr" target="#b5">[6]</ref> is a classical convolutional neural network on point sets and proposed a multi-layer perception to extract features from input points. Moreover, PointNet++ <ref type="bibr" target="#b13">[14]</ref> further proposed multi-scale sampling to aggregate global and local features. Another group of indoor-scene segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> utilizes the clustering (including KNN) to extract the point features. However, these methods are computationally costly and do not take varying sparsity (the property of outdoor-scene LiDAR) into consideration.</p><p>Outdoor-scene Point Cloud Segmentation. Most existing outdoor-scene point cloud segmentation focuses on converting the 3D point cloud to 2D grids to enable the use of 2D Convolutional Neural Networks. SqueezeSeg <ref type="bibr" target="#b0">[1]</ref>, Darknet <ref type="bibr" target="#b14">[15]</ref>, SqueezeSegv2 <ref type="bibr" target="#b15">[16]</ref>, and RangeNet++ <ref type="bibr" target="#b3">[4]</ref> utilize the spherical projection mechanism, which converts the point cloud to a frontal-view (range) image, and adopt the 2D convolution network on the pseudo image for segmentation. PolarNet <ref type="bibr" target="#b1">[2]</ref> follows the bird-view projection, which projects point cloud data into small grids from the bird view and takes the height as a whole. Instead of partitioning points in a Cartesian coordinate system, they use a polar coordinate system for encoding point clouds. However, this 3D-to-2D projection inevitably compresses the 3D topology and fails to model the geometric information.</p><p>3D Voxel Partition 3D voxel partition is another routine of point cloud encoding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. It converts a point cloud into 3D voxels. 3D U-Net <ref type="bibr" target="#b4">[5]</ref> proposes voxel partition and 3D U-Net on biomedical data and shows successful application on difficult microscopic datasets. OccuSeg <ref type="bibr" target="#b17">[18]</ref>, SSCN <ref type="bibr" target="#b2">[3]</ref> and SEGCloud <ref type="bibr" target="#b19">[20]</ref> follow this line to utilize the voxel partition and apply 3D convolutions for LiDAR segmentation. Our work also follows this routine, utilizing the 3D grid and 3D convolution networks, but with substantial differences. We use the 3D cylinder partition based on the cylinder coordinate system, which meets the varying sparsity of driving-scene LiDAR point cloud and balances point distribution. Specifically, distant region performs much sparse than closer one, and cylinder partition thus utilizes a larger cylinder to cover the distant region accordingly.</p><p>Network Architectures for Segmentation. Fully Convolutional Network <ref type="bibr" target="#b20">[21]</ref> is the fundamental work in the deep-learning era. U-Net <ref type="bibr" target="#b21">[22]</ref> built upon FCN and proposed a symmetric architecture to utilize the low-level features. Furthermore, many works explore the dilated convolution for multiscale context modeling, including DeepLab <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and PSP <ref type="bibr" target="#b24">[25]</ref>. Due to the great success of U-Net on 2D benchmarks, many studies for LiDAR segmentation adapt the U-Net to the 3D space and propose 3D U-Net <ref type="bibr" target="#b4">[5]</ref>. However, they often fail to explore the distribution and property of the driving-scene LiDAR point cloud. In this work, two modules, i.e., Asymmetric Residual Block and Dimension-decomposition based Context Modeling, are designed to match the cuboid objects and model the high-rank context information, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Study on 3D Point Representations</head><p>Outdoor-scene point clouds have significant differences with indoor-scene point clouds. (1) A driving-scene point cloud might cover a very large area, as far as over 100 meters. (2) It generally contains more points (&gt;100,000 points) but are much sparser than those of the indoor scenes. Hence, the indoor segmentation methods working on dense and fixed-number points are difficult to be adapted to the driving scenes with varying point densities.</p><p>Existing outdoor LiDAR segmentation methods mainly focus on transforming the 3D point clouds to 2D representations via projection, including spherical projection and bird-eye view projection, and then adopt 2D convolutions to process the 2D grid representations. However, as shown in <ref type="figure">Fig. 1(right)</ref>, the local spatial pattern in 2D grid representation cannot well capture 3D geometric structures. It can be observed that the red rectangle in 2D grid denotes the points distributing in different spatial locations. Hence, these 3D-to-2D projection methods may fail to encode certain 3D geometric structures and incur inaccurate pattern extraction. The detailed survey is shown in Section 4.2. We perform extensive experiments with various partition and networks among 2D, 2.5D and 3D. From the results, the consistent performance gain indicates the effectiveness of our technical road map, namely, 3D partition and 3D networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Overview</head><p>The outdoor point clouds are covering a large varieties of urban scenes. Our task is to assign a semantic label to each point in the point cloud. Based on our investigation on the distribution of 2D and 3D point-cloud representations, we discover that the 2D representation obtained from projection would abandon many available 3D structures. To this end, we propose a new outdoor LiDAR segmentation approach based on 3D representation and neural networks.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the framework consists of two components, including 3D cylinder partition (to obtain the 3D representation) and 3D U-Net (to process the 3D representation). Particularly, we design two modules to suit the properties of outdoor point clouds, i.e., Asymmetrical Residual Block to match these cuboid based objects often appearing in the driving scenes (cars, trucks, motorcycles, etc), and dimension-decomposition based context modeling module to exploit the high-rank context information in point clouds in a decomposition-aggregation manner. In the following sections, we will introduce these components in detail.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cylinder Partition</head><p>As mentioned above, outdoor-scene LiDAR point cloud possesses the property of varying density, where nearby region has much greater density than distant region. We thus use the cylinder coordinate system to replace the Cartesian grid partition. It utilizes the increasing grid to cover the further-away region, thus it more evenly distributes the points across different regions and matches the distribution of outdoor points. Moreover, unlike these projection-based methods project the point to the 2D view, we maintain the 3D grid representation to retain the geometric structure. The workflow is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We first transform the points on Cartesian coordinate system to the Cylinder coordinate system, where radius ρ and azimuth θ are calculated. This step transforms the points (x, y, z) to points (ρ, θ, z). Then cylinder partition is to split these three dimensions uniformly, note that this split indicates more further-away region, larger voxel. These cylinder grid representation is fed to a MLP-based pointnet to get the cylinder features. After these steps, we can get the 3D cylinder representation R ∈ C × H × W × L, where C denotes the feature dimension.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Asymmetric Residual Block</head><p>For the autonomous driving scenes, there exist a large amount of cuboid objects, including cars, trucks, buses and motorcycles. Inspired by text detection methods <ref type="bibr" target="#b25">[26]</ref>, where asymmetry convolutional kernels are used to match the rectangle target regions, we design the asymmetric residual block to meet the property of such cuboid objects. Moreover, this asymmetric residual block also significantly reduces the computational cost of conventional 3D convolutional kernels. Specifically, using a convolution with kernel=3×1×3 followed by a 1×3×3 convolution is equivalent to sliding a two layer network with the same receptive field as in a 3D convolution with kernel= 3 × 3 × 3, but it has 33% cheaper computational cost than a 3 × 3 × 3 convolution with same number of output filters. The proposed asymmetrical residual block is the basic component of downsample block and upsample block. For downsample block, it consists of a asymmetrical residual block and a 3D convolution with stride=2 to perform downsample. Upsample block incorporates the low-level features and processes the fused features with a asymmetrical residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dimension-decomposition based Context Modeling</head><p>Due to the large varieties of context (for 3D space, its context varies from point cloud to point cloud and should have large diversity), the context tensor should be high-rank <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> to have enough capacity for encoding context information. To model this context feature requires a huge cost, especially in the 3D space, because of the high-rank property of context. Inspired by the high-rank matrix decomposition theory, we can separate the high-rank context into several low-rank representation. In our task, this high-rank context can be divided into three dimensions, i.e., height, width and depth, where all three fragments are both low-rank. Then we build up the complete highrank context using these fragments. In this way, this decomposite-aggregate strategy tackles the high-rank difficulty from different views with low-rank constraints. As shown in <ref type="figure" target="#fig_1">Fig. 2(bottom)</ref>, three rank-1 kernels (i.e., 3 × 1 × 1, 1 × 3 × 1 and 1 × 1 × 3) are used to generate these low-rank encoding in all three dimensions. Then the Sigmoid function modulate the convolution results and generates weights for each dimension, in which the co-occurrence contextual information is mined based on the rank-1 tensors from different views. We aggregate all three low-rank activations to obtain the summation to represent the complete context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Network Optimization</head><p>In this section, we give the details of three parts: 3D cylinder partition, 3D segmentation backbone and segmentation head, as shown in <ref type="figure" target="#fig_1">Fig.2</ref>. Cylinder partition utilizes a 4-layer MLP network with BatchNorm and ReLU to extract point features for each point and select the maximum magnitude of the point features as voxel representation. Our 3D segmentation backbone is derived from U-Net, where 3D convolution is the sparse convolution adapted from <ref type="bibr" target="#b28">[29]</ref>. As mentioned above, we replace traditional residual block with asymmetrical residual block and insert a DDCM module before the final prediction. The input to segmentation backbone is C × H × W × L tensor. The third part is segmentation head, in which we adapts a 3d convolution layer with 3 × 3 × 3 kernel as a light-weight segmentation head. After the whole pipeline, voxel based prediction, whose size is</p><formula xml:id="formula_0">Class × H × W × L, is obtained.</formula><p>For network optimization, we use a weighted cross-entropy loss and a lovasz-softmax <ref type="bibr" target="#b29">[30]</ref> loss to maximize the point accuracy and the intersection-over-union score for classes. Two losses share the same weight. Thus, the total loss is: ζ all = ζ iou + ζ acc . For the optimizer, Adam with an initial learning rate of 0.001, is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metric</head><p>SemanticKITTI <ref type="bibr" target="#b14">[15]</ref> is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark and collected in Germany with a Velodyne-HDLE64 LiDAR. The dataset consists of 22 sequences, splitting sequences 00 to 10 as training set, and 11 to 21 as test set. Overall, the dataset provides 23201 point clouds for training and 20351 for testing. Following previous literature, sequence 08 is used as the validation set. The dataset has in total 28 classes, where 6 classes are duplicated with moving or non-moving attribute. After merging classes with different moving status and ignore classes with very few points, 19 classes are remained for training and evaluation. To evaluate the proposed method, we leverage mean intersection-overunion (mIoU) metric defined in <ref type="bibr" target="#b14">[15]</ref> over all classes, given by:</p><formula xml:id="formula_1">IoU i = T Pi</formula><p>T Pi+F Pi+F Ni where T P i , F P i , F N i represent true positive, false positive, and false negative predictions for class i and the mIoU is the mean value of IoU i over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Backbone and Representation Odyssey from 2D to 3D</head><p>For LiDAR segmentation in outdoor scene, there exists many previous literatures, in which various partitions and backbones are proposed among 2D and 3D space. We choose two published cuttingedge networks and some variants with different partitions and backbones (among 2D and 3D space) as a reference group, and conducts extensive experiments to show the odyssey of our network design.</p><p>Spherical Projection, RangeNet++ <ref type="bibr" target="#b3">[4]</ref> is one of typical methods of spherical projection, which projects point cloud onto a spherical surface surrounding the sensor. Compared with other analogous methods, such as Darknet <ref type="bibr" target="#b14">[15]</ref> and SqueezeSeg <ref type="bibr" target="#b0">[1]</ref>, it achieves the best performance on Semantic-Kitti test set. Thus, we choose RangeNet53 as spherical projection baseline and replace original rangenet53 with deeplab-resnet101. For a fairer comparison, we also adopt a KNN as postprocessing method to reduce spatial boundary effect for spherical projection.</p><p>Polar Bird View Projection, PolarNet <ref type="bibr" target="#b1">[2]</ref> is not traditional bird-view method defined in Cartesian coordinates. It introduces polar coordinates on radius-theta plane to effectively represent these points. Radius-theta encoding can reduce learning complexity due to its small input size. We follow the polar image setting in PolarNet and show the results with different network architectures.</p><p>Cuboid 3D Voxelization is a common point representation used in LiDAR segmentation. It converts point cloud into 3D voxels in Cartesian coordinates. These methods often possess huge computing costs because of the large cuboid voxel resolution and 3D convolution backbone.</p><p>Cylinder 3D Voxelization is proposed in this paper. It divides point cloud into small grids in Cylindrical coordinate system. As we claim in section 3, cylinder voxel partition meets the varying sparsity of driving-scene LiDAR point cloud and balances point distribution. <ref type="table" target="#tab_1">Table 1</ref>, we conduct extensive experiments to evaluate different projections with different segmentation backbones among 2D and 3D space. It can be observed that for 2D projections, polar projection outperforms spherical projection methods with different segmentation backbones, such as Resnet-50-FCN, DRN-DeepLab and Resnet101-DeepLab, which demonstrates the superiority of polar projection. It is worth noting that our 2D and 3D backbones share the same architecture as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The main difference between 2D and 3D backbones is convolution layer, and we instead use 2D Convolution in 2D backbone. Based on the same polar projection, our 2D backbone outperforms the polarnet by 2.8% mIoU, which demonstrates the scalability of the proposed model even in 2D space. When we replace the polar projection with our cylinder 3D voxelization, our model has a 1.7% gain because it retains the 3D topology, which indicates the effectiveness of 3D cylinder partition. After converting the 2D backbone to the 3D backbone, the proposed Cylinder3D obtains 4.2% gain and achieves 64.3% mIoU on val set. It can be observed that 3D convolution based framework significantly boosts the performance compared to the 2D backbone, which demonstrates the cooperation of 3D Cylinder partition and 3D convolution leads to the point cloud segmentation and verifies our conjecture 3D structure is a crucial aspect in LiDAR segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis. as shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on SemanticKitti</head><p>In this experiment, we report the results of our model on the SemanticKitti test set from official evaluation server. As shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of network components</head><p>In this experiment, we perform the ablation studies to investigate the effects of different network components in Cylinder3D, including Asymmetry residual block, Dimension-decomposition based context modeling and Flip test (a common technique to boost the performance). We use the Cylinder partition and 3D U-net (similar to our 3D networks, but use the common residual block and no dimension-decomposition context modeling) as the baseline method. Then we gradually add these network components to observe its effectiveness. By replacing residual block with asymmetry residual block, it can be found about 1.5% mIoU performance gain is achieved. When adding Dimension-decomposition based Context Modeling, our proposed Cylinder3D achieves 64.3% in terms of mIoU. Moreover, by further incorporating the Flip Test, i.e., flipping the original point cloud via x-axis, y-axis and x-y-axis, and averaging four predictions as the final results, the mIoU increases by another 0.9%. From the ablation, we can find that both two designed modules achieve the consistent performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization</head><p>Some of the results are visualized in <ref type="figure">Fig.5</ref>. It can be observe that the proposed Cylinder3D mainly achieves decent accuracy, and well separates the nearby objects because it maintains the 3D topology and utilizes the geometric information (we highlight corresponding regions with red rectangles). Baseline Asymmetry residual block DDCM Flip Test mIoU 0.615 0.630 0.643 0.652 <ref type="figure">Figure 5</ref>: Visualization on validation set. The left is ground-truth and right is our prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we follow the 3D nature of lidar point cloud to reposition the focus of lidar segmentation, from 2D to 3D representation and network. We design a 3D pointcloud representation, named Cylinder partition, which suits for the varying sparsity of driving-scene lidar point cloud, and propose a 3D convolution based network, where two basic network modules,called Asymmetric Residual Block and Dimension-decomposition based Context Modeling, are introduced to reduce computational cost and explore the high-rank context. With the cooperation of Cylinder partition and 3D convolution networks, our method achieves the state-of-the-art on the SemanticKITTI test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture. Top part is the full workflow of the proposed 3D LiDAR segmentation network, Cylinder3D. Bottom parts are the details of the Downsample block and UpSample block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The pipeline of Cylinder Partition. It first transforms points on Cartesian coordinate to Cylinder coordinate. Then a cylinder partition is introduced to perform the voxelization. Finally, cylinder features are produced by a simplified pointnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The detailed framework of asymmetry residual block and dimension-decomposition based context modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[4] 0.528 0.910 0.250 0.471 0.407 0.255 0.452 0.629 0.000 0.938 0.465 0.819 0.002 0.858 0.542 0.842 0.529 0.727 0.532 0.400 Deeplab-Resnet101 0.474 0.874 0.188 0.311 0.480 0.213 0.378 0.644 0.000 0.915 0.270 0.758 0.000 0.778 0.386 0.805 0.518 0.708 0.462 0.310 Polar Projection U-Net [2] 0.556 0.928 0.292 0.353 0.608 0.272 0.545 0.676 0.000 0.936 0.473 0.797 0.073 0.899 0.497 0.861 0.628 0.718 0.607 0.406 Deeplab-Resnet101 0.526 0.910 0.321 0.327 0.549 0.145 0.449 0.720 0.00 0.924 0.420 0.771 0.043 0.885 0.439 0.834 0.614 0.665 0.576 0.393 our 2D backbone 0.584 0.936 0.334 0.412 0.811 0.391 0.540 0.748 0.000 0.933 0.450 0.784 0.027 0.897 0.532 0.869 0.648 0.728 0.625 0.436 Cuboid 3D Voxelization our 3D backbone 0.609 0.946 0.448 0.563 0.756 0.379 0.672 0.906 0.005 0.916 0.432 0.762 0.024 0.910 0.586 0.860 0.661 0.695 0.609 0.446 Cylinder 3D Voxelization our 2D backbone 0.601 0.941 0.420 0.580 0.748 0.438 0.557 0.758 0.00 0.935 0.468 0.793 0.009 0.902 0.516 0.878 0.680 0.757 0.592 0.463 our 3D backbone 0.643 0.963 0.498 0.694 0.843 0.506 0.719 0.880 0.000 0.944 0.394 0.809 0.01 0.905 0.589 0.881 0.681 0.755 0.632 0.502</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.868 0.013 0.127 0.116 0.102 0.171 0.202 0.005 0.829 0.152 0.617 0.090 0.828 0.442 0.755 0.425 0.555 0.302 0.222 Darknet53[15] 0.499 0.864 0.245 0.327 0.255 0.226 0.362 0.336 0.047 0.918 0.648 0.746 0.279 0.841 0.55 0.783 0.501 0.640 0.389 0.522 RandLA-Net[32] 0.503 0.940 0.198 0.214 0.427 0.387 0.475 0.488 0.046 0.904 0.569 0.679 0.155 0.811 0.497 0.783 0.603 0.590 0.442 0.381 RangeNet++[4] 0.522 0.914 0.257 0.344 0.257 0.230 0.383 0.388 0.048 0.918 0.650 0.752 0.278 0.874 0.586 0.805 0.551 0.646 0.479 0.559 PolarNet[2] 0.543 0.938 0.403 0.301 0.229 0.285 0.432 0.402 0.056 0.908 0.617 0.744 0.217 0.900 0.613 0.840 0.655 0.678 0.518 0.575 SqueezeSegv3[31] 0.559 0.925 0.387 0.365 0.296 0.330 0.456 0.462 0.201 0.917 0.634 0.748 0.264 0.89 0.594 0.82 0.587 0.654 0.496 0.589 Cylinder3D 0.618 0.961 0.542 0.476 0.386 0.450 0.651 0.635 0.136 0.912 0.622 0.752 0.187 0.896 0.616 0.854 0.697 0.693 0.626 0.647</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of backbone on SemanticKITTI val set.</figDesc><table><row><cell>Projections</cell><cell>Backbones</cell><cell>mIoU</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic</cell></row><row><cell>Spherical Projection</cell><cell>RangeNet53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Net<ref type="bibr" target="#b31">[32]</ref>, et al. The proposed method outperforms other state-of-the-art methods by at least 6% in terms of mIoU.</figDesc><table><row><cell>, our method achieves the state-of-the-art on SemanticKitti test set</cell></row><row><cell>in comparison with existing methods, including RangeNet++ [4], PolarNet [2], SqueezeSegv3 [31],</cell></row><row><cell>RandLA-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results by our proposed method and state-of-the-art LiDAR Segmentation methods on SemantciKitti test set.</figDesc><table><row><cell>Methods</cell><cell>mIoU</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic</cell></row><row><cell>PointNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effects of network components on SemanticKITTI val set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to segment 3d point clouds in 2d image space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12255" to="12264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9031" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusion-aware point convolution for online semantic 3d scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4534" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02724</idno>
		<title level="m">Reconfigurable voxels: A new representation for lidar-based point clouds</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2940" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ssn: Shape signature networks for multiclass object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02774</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tensor low-rank reconstruction for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00490</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Squeeze-segv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
