<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SampleFix: Learning to Correct Programs by Sampling Diverse Fixes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hajipour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<country>Saarland Informatics Campus</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<email>fritz@cispa.saarland</email>
							<affiliation key="aff0">
								<orgName type="institution">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<country>Saarland Informatics Campus</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SampleFix: Learning to Correct Programs by Sampling Diverse Fixes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic program correction is an active topic of research, which holds the potential of dramatically improving productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning and NLP have rekindled the hope to eventually fully automate the process of repairing programs. A key challenge is ambiguity, as multiple codes -or fixes -can implement the same functionality. In addition, datasets by nature fail to capture the variance introduced by such ambiguities. Therefore, we propose a deep generative model to automatically correct programming errors by learning a distribution of potential fixes. Our model is formulated as a deep conditional variational autoencoder that samples diverse fixes for the given erroneous programs. In order to account for ambiguity and inherent lack of representative datasets, we propose a novel regularizer to encourage the model to generate diverse fixes. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-theart approaches by fixing up to 65% of the mistakes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Software development is a time-consuming and expensive process. Unfortunately, programs written by humans typically come with bugs, so that significant effort needs to be invested to obtain code that is only likely to be correct. Debugging is typically performed by humans and therefore tedious and can also contain mistakes. This is neither desirable nor acceptable in many critical applications. Therefore, automatically locating and correcting program errors offers the potential to increase productivity as well as improve correctness of software.</p><p>Advances in machine learning, deep learning <ref type="bibr" target="#b6">(Lecun et al. 1998;</ref><ref type="bibr" target="#b7">Lee et al. 2011;</ref><ref type="bibr" target="#b6">Krizhevsky, Sutskever, and Hinton 2012)</ref>, computer vision <ref type="bibr" target="#b2">(Girshick 2015;</ref><ref type="bibr" target="#b10">Simonyan and Zisserman 2015)</ref> and NLP <ref type="bibr" target="#b12">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio 2015)</ref> has dramatically boosted the machine's ability to automatically learn representations of natural data such as images and natural language contents for various tasks. Deep learning models also have been successful in learning the distribution over continuous <ref type="bibr" target="#b11">(Sohn, Lee, and Yan 2015;</ref><ref type="bibr" target="#b5">Kingma and Welling 2014)</ref> and discrete data <ref type="bibr" target="#b8">(Maddison, Mnih, and Teh ;</ref><ref type="bibr" target="#b5">Jang, Gu, and Poole 2017)</ref>, to generate new and diverse data points <ref type="bibr" target="#b2">(Gottschlich et al. 2018)</ref>. These advances in machine learning and the advent of large corpora of source code <ref type="bibr" target="#b0">(Allamanis et al. 2018)</ref> provide new opportunities toward harnessing deep learning methods to understand, generate or debug programs.</p><p>There has been an increasing interest to carry over the success stories of deep learning in NLP and related techniques to employed deep learning-based approaches to tackle the "common programming errors" problem <ref type="bibr" target="#b3">(Gupta et al. 2017;</ref><ref type="bibr" target="#b4">Gupta, Kanade, and Shevade 2019)</ref>. Such investigations have included compile-time errors such as missing scope delimiters, adding extraneous symbols, using incompatible operators as "common programming errors". Novice programmers and even experienced developers often struggled with these type of errors <ref type="bibr" target="#b10">(Seo et al. 2014)</ref>, which is usually due to lack of attention to the details of programs and/or programmer's inexperience <ref type="bibr" target="#b4">(Gupta, Kanade, and Shevade 2019)</ref>.</p><p>Recently, Gupta et al. proposes a deep sequence to sequence model called Deepfix where, given an erroneous program, the model predicts the locations of the errors and a potential fix for each predicted location. The problem is for-mulated as a one-to-one mapping task, where the model is trained to predict a single fix for each location with an error. No ambiguity is taken into account. However, different codes -and therefore also their fixes -can express the same functionality <ref type="figure" target="#fig_0">(Figure 1</ref>). The inherent ambiguity argues for a one-to-many mapping in order to account for ambiguity as well as uncertainty of the model and limited training data.</p><p>Hence, we propose a deep generative framework to automatically correct programming errors by learning the distribution of potential fixes. At the core of our approach is a conditional variational autoencoder that is trained to sample accurate and diverse fixes for the given erroneous programs, and interacts with a compiler that evaluates the sampled candidate fixes in the context of the given programs. The interplay of sampler and compiler allows for an iterative refinement procedure. A key contribution of our approach is a novel regularizer which encourages diversity by penalizing the distance among candidate samples, which thereby significantly increases the effectiveness by producing more diverse samples.</p><p>To summarize, the contributions of this paper are as follows, 1. We propose a generative framework to automatically correct common programming errors by learning the distribution over potential fixes and interacting with a compiler in an iterative procedure. 2. We propose a novel regularizer to encourage the model to generate diverse fixes. 3. Our conditional generative model together with the diversity regularizer shows an increase in the diversity of samples and strong improvement over the state-of-the-art approaches on real world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds on the general idea of sequence to sequence models as well as ideas from neural machine translation. We phrase our approach as a variational auto-encoder and compare it to prior work learning-based program repair. We review the related work in order below Neural Machine Translation. Sutskever, Vinyals, and Le introduces neural machine translation and casts it as a sequence to sequence learning problem. The popular encoderdecoder architecture is introduced to map the source sentences into target sentences. One the major drawback of this model is that the sequence encoder needs to compress all of the extracted information into a fixed length vector. <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> addresses this issue by using attention mechanism in the encoder-decoder architecture, where it focuses on the most relevant part of encoded information by learning to search over the encoded vector. In our work, we employ sequence to sequence model with attention to parameterize our generative model. This model gets an incorrect program as input, and maps it to many potential fixes by drawing samples on estimated distribution of the fixes.</p><p>Variational Autoencoders. The variational autoencoder (Kingma and Welling 2014; Rezende and Mohamed 2015) is a generative model designed to learn deep directed latent variable based graphical models of large datasets. The model is trained on the data distribution by maximizing the variational lower bound of the log-likelihood as the objective function. Bowman et al. extends this framework by introducing a RNN-based variational autoencoder to enable the learning of latent variable based generative models on text data. The proposed model is successful at generating diverse and coherent sentences. To model conditional distributions for the structured output representation Sohn, Lee, and Yan extended variational autoencoders by introducing an objective which maximizes the conditional data log-likelihood. In our approach, we use an RNN-based conditional variational autoencoder to model the distribution of the potential fixes given erroneous programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning-based Program Repair.</head><p>Recently there has been a growing interest in using learning-based approaches to automatically repair the programs (Monperrus 2018). Long and Rinard proposed a probabilistic model by explicitly designing code features to rank potential fixes for a given program. Pu et al. employ an encoder-decoder neural architecture to automatically correct programs, where they use beam search to generate candidate programs. In these works, and many learning-based programming repair approaches, enumerative search over programs is required to resolve all errors. However, our proposed framework is capable of predicting the location and potential fixes by passing the whole program to the model. Besides this, unlike our approach, which only generates fixes for the given erroneous program, <ref type="bibr">Pu et al. need</ref> to predict whole program statements to resolve bugs.</p><p>DeepFix <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref> and RLAssist <ref type="bibr" target="#b4">(Gupta, Kanade, and Shevade 2019)</ref> uses neural representations to repair syntax errors in programs. In detail, DeepFix uses a sequence to sequence model to directly predict a fix for incorrect programs. In contrast, our generative framework is able to generate and evaluate multiple fixes by learning the distribution of potential correctness. Therefore, our model does not penalize, but rather encourages diverse fixes. RLAssist repairs the programs by employing a reinforcement learning approach. They train an agent which navigate over the program to locate and resolve the syntax errors. In this work, they only address the typographic errors, rely on a hand designed action space and meet problems due to the increasing size of the action space. In contrast, our method shows improved performance on typographic errors and also generalizes to issues with missing variable declaration errors.</p><p>Harer et al. use generative adversarial networks to repair software vulnerabilities. They propose an adversarial approach to map from bad code domain to the correct code domain in an unsupervised fashion. They propose an unsupervised approach to repair the programs, however, there is no guarantee that their outputs do not introduce more errors to the input programs. In contrast, our generative framework automatically reject the sampled fixes which are introducing more errors to the input programs by using a compiler during inference time and directly models the distribution over potential fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SampleFix: Generative Model for Code Fixes</head><p>Repairing the common program errors is a challenging task due to ambiguity in potential corrections and lack of representative data. Given a single broken program, there are multiple ways to fix the program. Therefore, alternative correct solutions -that do not appear in the training set -should not be penalized during training but rather encouraged. Therefore, we propose a deep generative framework to automatically repair common programming errors by learning the distribution of potential fixes -rather than a single fix -given the broken program. In other words, we model the problem of repairing programs as a one-to-many problem, where, our model learns the distribution of potential fixes through a generative model conditioned on the erroneous input program. We model the distribution of fixes using a conditional latent variable model, where the latent variables are learned using pairs of erroneous programs and the corresponding fixes. As mentioned before, to resolve programming errors, the generation of diverse fixes is crucial. The main reason is that by nature programs and their potential fixes are redundant and ambiguous. In addition, datasets are deprived of the large diversity of potential mistakes that can occur in programs. Hence, we need to model and encourage diverse samples. In order to address this issue, we propose a novel regularizer which encourages the model to generate diverse samples by penalizing the distance among drawn fixes. <ref type="figure" target="#fig_1">Figure 2</ref> provides an overview of our proposed approach. For a given errorous program, the generative model draws candidate fixeŝ ( = 4 in <ref type="figure" target="#fig_1">Figure 2</ref>) from the learned conditional distribution. In order to select one out of candidate fixes, we employ an compiler which evaluate the candidate fixes. The compiler evaluates each fix by compiling the updated program. Out of drawn fixes, the compiler selects the fix which resolves the largest number of error messages. In <ref type="figure" target="#fig_1">Figure 2</ref>, compiler selectŝ 2 , as the best fix for the given program. After updating the program, to resolve the remaining error(s), we follow Gupta et al. and iteratively input the updated program to our generative model. In the following, we formulate our generative model using the Conditional Variational Autoencoder framework and describe in detail our proposed novel regularizer. Finally, we provide details of our training and inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Variational Autoencoders for Generating Fixes</head><p>Conditional Variational Autoencoders (CVAE) <ref type="bibr" target="#b11">(Sohn, Lee, and Yan 2015)</ref>, model conditional distributions (y|x) using latent variables z. The conditioning introduced through z enables the modelling of complex multi-model distributions.</p><p>As powerful transformations can be learned using neural networks, z itself can have a simple distribution which allows for efficient sampling. This model allows for sampling from (y|x) given an input sequence x, by first sampling latent variablesẑ from the prior distribution (z). During training, amortized variational inference is used and the latent variables z are learned using a recognition network (z|x, y), parametrized by . In detail, the variational lower bound of the model (Equation 1) is maximized, log( (y|x)) ≥ (z|x,y) log( (y|z, x)) − KL ( (z|x, y) || (z|x)).</p><p>( <ref type="formula">1)</ref> Penalizing the divergence of (z|x, y) to the prior in Equation 1 allows for sampling from the prior (z) during inference. In practice, the variational lower bound is estimated using Monte-Carlo integration, <ref type="figure">y)</ref>, (z|x)) .</p><formula xml:id="formula_0"> CVAE = 1 ∑ i=1 log( (y|ẑ i , x)) − KL ( (z|x,</formula><p>( <ref type="formula">2)</ref> where,ẑ i ∼ (z|x, y), and is the number of samples. We cast our generative model for resolving program errors in the Conditional Variational Autoencoder framework. In our formulation, the input x is the erroneous program and y is the potential fix. Note that x, y are sequences in our formulation. However, the plain CVAE as described in <ref type="bibr" target="#b11">(Sohn, Lee, and Yan 2015)</ref> suffers from diversity issues. Usually, the drawn samples do not reflect the true variance of the posterior (y|x). This would amount to the correct fix potentially missing from our candidate fixes. To mitigate this problem, next we introduce a novel objective that aims to enhance the diversity of our candidate fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DS-SampleFix: Encouraging Diversity with a Diversity-sensitive Regularizer</head><p>Casting our model in the Conditional Variational Autoencoder framework would enable us to sample a set of candidate fixes for a given erroneous program. However as pointed out in (Bhattacharyya, Schiele, and Fritz 2018), the standard variational lower bound objective does not encourage diversity in the candidate fixes. This is because the average likelihood of the candidate fixes is considered. In detail, as the average likelihood is considered, all candidate fixes must explain the "true" fix in training set well. This discourages diversity and severely constrains the recognition network, which is already constrained to maintain a Gaussian latent variable distribution. In practice, the learned distribution is pushed to the mean and fails to fully capture the variance of the true distribution. To encourage diversity, we employ the lower bound proposed by (Bhattacharyya, Schiele, and Fritz 2018) and further include a novel regularizer. Our novel regularizer further encourages diversity and does not require a large number of candidate fixes to be drawn at training time. In practice, we observe considerable gains even with the use of only = 2 candidate fixes,</p><formula xml:id="formula_1">log( (y|x)) ≥ log ∫ (y|z, x) (z|x, y) − KL ( (z|x, y) || (z|x)).<label>(3)</label></formula><p>In comparison to Equation 1, this lower bound (Equation 3) encourages diversity in the model by allowing for multiple chances to draw highly likely candidate fixes. This enables the model to generate diverse candidate fixes, while maintaining high likelihood. In practice, due to numerical stability issues, the log-sum-exp on the right of (Equation 3) is approximated using the max <ref type="formula" target="#formula_2">(Equation 4</ref>). This approximate objective retains the diversity enhancing nature of Equation 3 while being easy to train,</p><formula xml:id="formula_2"> BMS = max i log( (y|ẑ i , x)) − KL ( (z|x, y), (z|x)) .<label>(4)</label></formula><p>While prior work uses around = 10 samples during training, this is computationally prohibitive especially for large models, as it requires 10 times the memory or 10 times the number of forward passes. If is decreased, the objective behaves similarly to the standard CVAE objective as the recognition network has fewer and fewer chances to draw highly likely samples/candidate fixes, thus limiting diversity. Therefore, in order to encourage the model to generate diverse fixes even when high values of are computationally prohibitive, we propose a novel regularizer which penalizes the distance of two closest candidate fixes (Equation 5), thus forcing the candidate fixes to be diverse. <ref type="figure">, y)</ref>, (z|x)) .</p><formula xml:id="formula_3"> DS-BMS = max i log( (y|ẑ i , x)) + min i,j (ŷ i ,ŷ j ) − KL ( (z|x</formula><p>(5) Distance Metric. Here, we discuss the distance metric in Equation 5. Note, that the samples ŷ i ,ŷ j can be of different lengths. Therefore, we consider the distance over the maximum length of the two samples and pad the shorter sample to equalize lengths. Next, we assume that our predicted distributions are point masses over each token. Under this assumption, the Euclidean distance is equivalent to the Wasserstein distance between the distributions. We also experimented with the commonly used cosine similarity. However, our approach performed better using the Euclidean (Wasserstein) distance. This is mainly because, in practice, Euclidean distance is easier to optimize. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture and Implementation Details</head><p>To ensure fair comparison, our generative model is based on the popular sequence to sequence architecture with attention, similar to <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref>. We employ LSTMs encoders and decoders. <ref type="figure" target="#fig_2">Figure 3</ref> shows the architecture of our model in detail. Note that the recognition network is available to encode the fixes to latent variables z only during training. Another LSTM encoder is used to encode the conditioning input erroneous program x to a code vector v. The decoder is conditioned on the code vector v and a sample of distribution with latent variables z.</p><p>All of the networks in our framework consists of 4-layers of LSTM cells with 300 units. We apply dropout at rate 0.2 for each of these layers. To process the program through the networks, we tokenize the programs similar to the setting which used by Gupta et al. and process them into 50dimensional vectors using an embedding layer. The network is optimized using ADAM optimizer (Kingma and Ba 2015) with the default learning rate and weight decay. We train the network up to 20 epochs and select the model with best validation performance. For the evaluation and comparisons we train two models, one for repairing the typographic errors, and another one for missing variable declaration errors. We use = 2 samples to train our models, and = 100 samples during inference time. All of these experiments are conducted on a Nvidia Tesla P40 with 22GB memory.</p><p>During inference, the conditioning erroneous program x is input to the encoder, which encodes the program to the vector v. To generate multiple fixes using our decoder, the code vector v along with a sample of z from the prior (z) is input to the decoder. For simplicity, we use a standard Gaussian  (0, I) prior, although more complex priors can be easily leveraged. The decoder is unrolled in time and output logits <ref type="figure">( (y|ẑ i , x)</ref>). To sample from these logits we use computationally efficient Gibbs sampling strategy. We approximate a Gumbel distribution over the predicted logits <ref type="bibr" target="#b5">(Jang, Gu, and Poole 2017;</ref><ref type="bibr" target="#b8">Maddison, Mnih, and Teh )</ref> and predict each token by sampling over this distribution. We additionally experiment with a Beam search decoding scheme (Deshpande et al. 2019) to further enhance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Iterative Repair</head><p>We adopt the iterative repair procedure <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref> in the context of our proposed generative model, where the iterative procedure now leverages multiple candidate fixes. Given an erroneous program, each of our candidate fixes contain a line number and the corresponding fix for that line. To select the best fix, we take the candidate fixes and the input erroneous program, reconcile them to create updated programs. Our oracle which is a compiler takes these programs and selects a fix out of candidate fixes which have a lowest number of error messages. After applying a fix on the input program, we feed the updated program to the network to resolve the other errors. This procedure is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In order to allow for a fair comparison to the stateof-the-art methods, we use the same setting for applying iterative repair strategy as it is described in <ref type="bibr">Gupta et al..</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our approach on the task of repairing common programming errors. We evaluate the diversity and accuracy of our sampled error corrections as well as compare our proposed method with the state-of-the-art. Dataset Details. Unfortunately, the choice of dataset in this domain is still quite limited. We use the dataset published by Gupta et al. as it's sizable and also includes real-world data for test. It contains programs written in C. The dataset consists of 93 different tasks which were written by students in an introductory programming course. The programs were collected using a web-based tutoring system <ref type="bibr">(Das et al. )</ref>. The collected programs have token lengths in the range <ref type="bibr">[75,</ref><ref type="bibr">450]</ref>, and contain typographic and missing variable declaration errors. To tokenize the programs and generate training and test data we follow the procedure which is used by <ref type="bibr">Gupta et al.,</ref> where they tokenized programs by considering different type of tokens, such as types, keywords, special characters, functions, literals and variables. Furthermore, the dataset contains two sets of data which are called synthetic and realworld data. The synthetic data contains the erroneous program which are synthesized by mutating correct programs written by students. The real-world data contains the realworld erroneous programs written by students. In line with prior work, we use the synthetic data to train and test our model, and use real-world data to evaluate the performance of our model in repairing the real-world erroneous programs. In order to stay comparable with prior work, we follow the same training and evaluation procedure as in <ref type="bibr">Gupta et al..</ref> The dataset is divided into 5 cross-validation folds. In each fold, 1 ∕5 ℎ of the programming tasks are held out as the test set, and the training data is generated by mutating the correct programs from the remaining 4 ∕5 ℎ of the programming tasks. To generate the training pairs, correct programs are mutated to introduce the errors. Mutated programs are then paired with the fix for the first incorrect line, where each fix contains the line number and the corrected line for that program. We refer the reader to <ref type="bibr">Gupta et al.</ref> for more details about the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We evaluate our approach on synthetic and real-world erroneous programs. To evaluate our approach on the synthetic test set we randomly select 10k pairs of the data for resolving typographic and missing variable declaration errors. This data contains pairs of erroneous programs with the intended fixes for the first incorrect line in programs. To evaluate our approach on real-world data we use a real-world set of erroneous programs which contains 6975 erroneous programs with 16766 error messages. Unlike synthetic test set, we don't have access to intended fix(es) in the real-world data. Synthetic Data. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of our proposed approaches, SampleFix and DS-SampleFix, with DeepFix <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref> on synthetic data in the first iteration. In this table <ref type="table" target="#tab_0">(Table 1)</ref>, we can see that our approaches outperform DeepFix in generating intended fixes for the typographic and missing variable declaration errors. Sample-Fix and DS-SampleFix generate 88.7% and 90.4% of the intended fixes for typographic and missing variable declaration errors respectively. This <ref type="table" target="#tab_0">(Table 1)</ref> shows that DS-SampleFix outperforms SampleFix in generating fixes for typographic errors. However, DS-SampleFix is not as effective as Sam-pleFix in correcting the missing variable declaration errors, this is mainly due to the lack of diversity of this dataset. In other words, DeepFix and SampleFix (without our diversity regularizer) memorizes the fixes from the training set. However, as we demonstrate in the following, this performance advantage does not translate to real world scenarios. Real-World Data. In <ref type="table" target="#tab_1">Table 2</ref> (top four rows) we show the comparison of our approaches, SampleFix and DS-SampleFix, with DeepFix <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref> and RLAssist (Gupta, Kanade, and Shevade 2019) on the real-world test set . We also refer to the results of RLAssist, however, it has to be noted that it is not directly comparable as this approach relies on handcrafted actions. Furthermore, it has only been shown to resolve typographic errors. <ref type="table" target="#tab_1">Table 2</ref> (top four rows) shows that our approaches (with 100 samples obtained using Gumbel sampling) outperform DeepFix in resolving both typographic and missing variable declaration error messages. This shows that generating multiple diverse fixes can lead to substantial improvement in performance. SampleFix and DS-SampleFix resolve 38.8% and 40.9% of all of the typographic error messages and 22.8%, and 24.7% of missing variable declaration respectively. These results show that DS-SampleFix outperforms DeepFix by a large margin of 32.7% and 91.4% (relative improvement) in resolving typographic and missing variable declaration errors -almost double the performance of the latter. We also competitive with RLAssist approach while our approach does not require any additional supervision in term of actions. Overall, DS-SampleFix is able to fix 61.0% of all errors -a 20% improvement over DeepFix. Furthermore, the performance advantage of DS-SampleFix over SampleFix shows the effectiveness of our novel regularizer. Beam Search Decoding. So far we have considered only the Top-1 sample of DeepFix (as in <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref>). For fairness, in <ref type="table" target="#tab_1">Table 2</ref> we consider sampling multiple fixes also from DeepFix (this setting was not considered by <ref type="bibr" target="#b3">(Gupta et al. 2017)</ref>). We use beam search to estimate the Top-100 (unique) samples from DeepFix with the highest posterior probability <ref type="bibr" target="#b1">(Deshpande et al. 2019)</ref>. However, also note that beam search is computationally expensive algorithm and its significantly slower than generative models <ref type="bibr" target="#b1">(Deshpande et al. 2019)</ref>. In <ref type="table" target="#tab_1">Table 2</ref> (bottom two rows) we show that our proposed approach (DS-SampleFix) can also employ beam search to improve performance. In order to employ beam search decoding with DS-SampleFix, we consider beam width of size 5 for each sample and in total 20 samples. Again we see that DS-SampleFix outperforms Deep-Fix, showing that our DS-SampleFix not only samples diverse but also accurate fixes. Qualitative Examples. We provide qualitative examples in <ref type="figure" target="#fig_4">Figure 4</ref>. We illustrate diverse fixes generated by our DS-SampleFix in <ref type="figure" target="#fig_4">Figure 4a</ref> with a code example with typograhic errors, with the corresponding outputs of DeepFix and 5 output samples of 100 drawn samples of DS-SampleFix. We see that DS-SampleFix generates multiple correct fixes for the given program and in contrast to DeepFix -which only predicts a single fix -our model is able to generate multiple diverse fixes. This indicate that our approach is capable of handling inherent ambiguity and uncertainty in predicting fixes for the erroneous programs. Note that DeepFix and DS-SampleFix output a program location along with the corresponding fix, and these models output _eos_ whenever they could not provide any fix for the given erroneous program. <ref type="figure" target="#fig_4">Figure 4b</ref> shows a code example where DS-SampleFix repairs the typographic and missing variable declaration errors. In <ref type="figure" target="#fig_4">Figure 4b</ref>, we can see that our approach resolves one missing variable declaration error, by defining variable value at line 3, and remove a bracket at line 11, and insert a bracket at line 15 to resolve typographic errors. More code examples can be found in the Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Sampling Strategy</head><p>We further analyze the effectiveness of our proposed DS-SampleFix in drawing multiple diverse samples for a given incorrect program. <ref type="figure" target="#fig_5">Figure 5a</ref> shows in detail the effectiveness of drawing multiple samples for repairing 1426 unseen programs from a held out fold of real-world data. In <ref type="figure" target="#fig_5">Figure 5a</ref> x and y axis refer to number of drawn samples and number of resolved error messages respectively. These programs contain typographic and missing variable declaration errors. We see that although DeepFix outperforms Sample-Fix and DS-SampleFix when we only one sample is drawn to resolve each program, by drawing more samples Sample-Fix and DS-SampleFix are able to resolve larger number error messages, and both of our approaches outperform Deep-Fix after drawing 100 samples. Furthermore, <ref type="figure" target="#fig_5">Figure 5a</ref> also further illustrates the effectiveness of our novel regularizer, as DS-SampleFix outperforms SampleFix at every step. <ref type="figure">Figure</ref> 5a also shows the effectiveness of using beam search with DeepFix and DS-SampleFix . We see that using beam search on top the models improve the performance of DeepFix and DS-SampleFix by finding top and unique fixes. This figure shows that after drawing 100 fixes DS-SampleFix with beam search outperforms DeepFix + beam search. This shows that our proposed regularizer not only enables the model to draw more diverse fixes but also more accurate fixes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Iterative Repair</head><p>To resolve the multiple errors in a program we use the iterative repair strategy as introduced by <ref type="bibr" target="#b3">(Gupta et al. 2017</ref>).</p><p>Here, we analyze the effectiveness of our proposed approaches in resolving multiple errors in a program using iterative repair strategy. <ref type="figure" target="#fig_5">Figure 5b</ref> show the number of remaining errors messages in 5 iterations for DeepFix, Deep-Fix with beam search, SampleFix , DS-SampleFix, and DS-SampleFix with beam search. This figure shows the original number of error messages for 1426 programs (a held out fold of real-world data) marked as iteration 0, and then shows the number of remaining error messages after end of every iteration for different approaches. We use up to 5 iterations to resolve multiple error messages. We can see that after 5 iterations, SampleFix and DS-SampleFix resolve more error messages than DeepFix, and DS-SampleFix with beam search outperforms all of the other approaches. One of the reason that our approaches outperform DeepFix over the different iterations is that, DeepFix is only capable of predicting one fix for a given program, and if the network fails to produce a fix for a program in an iteration, then feeding the same program into the network in the subsequent iterations will not change the outcome. However, our proposed approaches are capable of drawing multiple fixes in each iteration, which means that our methods can resolve more error messages in each iteration, and we will have larger number of programs to feed it to the network in next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel framework to correct common programming errors. We recognize and model the inherent ambiguity and uncertainty when predicting fixes. In contrast to previous approaches, our novel framework is able to learn the distribution of candidate fixes rather than the most likely fix. The key to our success is a novel diversity-sensitive regularizer. This helps us overcome the inherent limitations of supervised datasets in this challenging domain and generalize to unseen real world test sets. Furthermore, our experiments show that the steady increase in discovery of correct programs as we draw more candidate fixes is a strong indication that our proposed model can be the basis of further advances in debugging by sampling based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Additional Qualitative Examples</head><p>Here we provide additional examples of correcting erroneous programs using our proposed method. <ref type="figure" target="#fig_6">Figure 6</ref> shows a code example which indicate our approach is capable of resolving multiple missing variable declaration errors. <ref type="figure" target="#fig_7">Figures 7 and  8</ref> shows code examples with typographic errors and missing variable declaration errors, with the corresponding outputs of DeepFix and DS-SampleFix given these erroneous programs.</p><p>In these examples, DeepFix fails to resolve these errors. However, our proposed approach (DS-SampleFix) is able to successfully resolve the errors by sampling diverse fixes. Here we show 5 samples out of 100 drawn samples. Note that each model outputs a program location along with the corresponding fix, and a model outputs _eos_ whenever it could not provide any fix for the given erroneous program. <ref type="figure" target="#fig_7">Figure 7</ref> shows an example of erroneous program with typographic errors, and the corresponding outputs of DeepFix and DS-SampleFix given that program. <ref type="figure" target="#fig_0">Figures 9 and 10</ref> show two examples of resolving multiple errors using itrerative repair strategy, here we show 5 samples out of 100 drawn samples. In <ref type="figure">Figure 9a</ref> we can see that in the first iteration our approach resolves a typographic error at line 9, and in the second iteration <ref type="figure">(Figure 9b</ref>), after updating the program, it resolves another typographic error at line 11. <ref type="figure" target="#fig_0">Figure 10</ref>, show an example of iteratively resolving multiple missing variable declaration errors. <ref type="figure" target="#fig_0">Figures 10a  and 10b</ref> show resolving multiple errors by defining variables x and i respectively.   <ref type="figure">Figure 8</ref>: Example programs with missing variable declaration errors. The potential location of the error is highlighted. Our DS-SampleFix generates multiple candidate fixes which attempt to declare variables (correct fix with green tick). DeepFix is unable to generate any fix.</p><p>(a) (b) <ref type="figure">Figure 9</ref>: An example of iteratively resolving multiple typographic errors using our approach (DS-SampleFix).</p><p>(a) (b) <ref type="figure" target="#fig_0">Figure 10</ref>: An example of iteratively resolving multiple missing variable declaration errors using our approach (DS-SampleFix).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example highlighting inherent ambiguity of possible fixes of erroneous programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of SampleFix at inference time, highlighting the generation of diverse fixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overview of network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Diverse fixes generated by our DS-SampleFix.(b) Iterative repair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a) Left: Example program with a typographic error. The error is highlighted at line 19. (a) Right: Our DS-SampleFix generates multiple correct fixes (line number and fix), highlighting the ability of DS-SampleFix to generate diverse fixes (correct fixes with green tick). DeepFix is unable to generate any fix. (b) Examples of resolving typographic errors, and missing variable declaration errors using our approach (DS-SampleFix). (a) Sampling strategy. (b) Iterative repair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) Number of resolved error messages with drawing 1, 10 and 100 samples for SampleFix , DS-SampleFix , DS-SampleFix + beam search, and DeepFix + beam search in comparison to number of resolved error messages by DeepFix. (b) Number of error messages after each iterations for DeepFix, DeepFix + beam search, SampleFix, DS-SampleFix, and DS-SampleFix + beam search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Additional example of resolving missing variable declaration errors using our approach (DS-SampleFix).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Example program with a typographic error. The error is highlighted at line 5. Our DS-SampleFix generates multiple candidate fixes (line number and fix) which attempt the fix the correct bug (correct fix with green tick).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of performance comparison of DeepFix, SampleFix and DS-SampleFix on synthetic data. Typo , Miss Dec and All refer to typographic, missing variable declarations, and all of the errors respectively.</figDesc><table><row><cell>Models</cell><cell>Typo</cell><cell>Miss Dec</cell><cell>All</cell></row><row><cell>DeepFix</cell><cell>86.5%</cell><cell>81.3%</cell><cell>84.1%</cell></row><row><cell>Our SampleFix</cell><cell>87.1%</cell><cell>90.8%</cell><cell>88.7%</cell></row><row><cell cols="2">Our DS-SampleFix 96.1%</cell><cell>85.6%</cell><cell>90.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top: Results of performance comparison of DeepFix, RLAssist, SampleFix and DS-SampleFix. Bottom: Results of performance comparison of DeepFix and DS-SampleFix with beam search decoding. Typo, Miss Dec and All refer to, typographic, missing variable declarations, and all of the errors respectively. and ◐ denote completely fixed programs and partially fixed programs, while denote resolved error messages.<ref type="bibr" target="#b3">Gupta et al. 2017)</ref> 23.3% 16.2% 30.8% 10.1% 12.2% 12.9% 33.4% 22.3% 40.8%</figDesc><table><row><cell>Models</cell><cell>Typo</cell><cell></cell><cell>Miss Dec</cell><cell></cell><cell></cell><cell>All</cell></row><row><cell></cell><cell>◐</cell><cell></cell><cell>◐</cell><cell></cell><cell></cell><cell>◐</cell></row><row><cell cols="2">DeepFix (RLAssist (Gupta, Kanade, and Shevade 2019) 26.6% 20.4% 39.7%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Our SampleFix</cell><cell cols="7">24.8% 22.3% 38.8% 16.1% 19.0% 22.8% 40.9% 33.4% 56.3%</cell></row><row><cell>Our DS-SampleFix</cell><cell cols="7">27.7% 21.5% 40.9% 16.7% 21.2% 24.7% 44.4% 35.5% 61.0%</cell></row><row><cell>Our DeepFix + Beam Search</cell><cell cols="7">25.9% 22.7% 42.2% 20.3% 34.5% 47.0% 44.7% 36.8% 63.9%</cell></row><row><cell>Our DS-SampleFix + Beam Search</cell><cell cols="7">27.8% 24.9% 45.6% 19.2% 35.6% 47.9% 45.2% 37.6% 65.2%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate and diverse sampling of sequences based on a &quot;best of many&quot; sample objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allamanis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A system for tutoring CS1 and collecting student programs for analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<idno>abs/1608.03828</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10695" to="10704" />
		</imprint>
	</monogr>
	<note>Fast, diverse and accurate image captioning guided by part-of-speech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gottschlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Gottschlich et al. 2018. The three pillars of machine programming</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepfix: Fixing common c language errors by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to repair software vulnerabilities with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanade</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lazovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>ICLR. [Kingma and Welling</editor>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Automatic patch generation by learning correct code</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnih</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Teh ] Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno>abs/1611.00712</idno>
	</analytic>
	<monogr>
		<title level="m">Monperrus 2018] Monperrus, M</title>
		<imprint>
			<publisher>CSUR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Automatic software repair: a bibliography</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN</title>
		<editor>Rezende, D. J., and Mohamed</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Programmers&apos; build errors: a case study (at google). In ICSE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><forename type="middle">;</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
