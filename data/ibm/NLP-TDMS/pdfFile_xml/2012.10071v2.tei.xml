<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TDN: Temporal Difference Networks for Efficient Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<email>07wanglimin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
							<email>tongzhan@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
							<email>gswu@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TDN: Temporal Difference Networks for Efficient Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal modeling still remains challenging for action recognition in videos. To mitigate this issue, this paper presents a new video architecture, termed as Temporal Difference Network (TDN), with a focus on capturing multiscale temporal information for efficient action recognition. The core of our TDN is to devise an efficient temporal module (TDM) by explicitly leveraging a temporal difference operator, and systematically assess its effect on short-term and long-term motion modeling. To fully capture temporal information over the entire video, our TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation. TDN provides a simple and principled temporal modeling framework and could be instantiated with the existing CNNs at a small extra computational cost. Our TDN presents a new state of the art on the Something-Something V1 &amp; V2 datasets and is on par with the best performance on the Kinetics-400 dataset. In addition, we conduct in-depth ablation studies and plot the visualization results of our TDN, hopefully providing insightful analysis on temporal difference modeling. We release the code at https://github.com/MCG-NJU/TDN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have witnessed great progress for action recognition in videos <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b41">38,</ref><ref type="bibr" target="#b34">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">26,</ref><ref type="bibr" target="#b40">37]</ref>. Temporal modeling is crucial for capturing motion information in videos for action recognition, and this is usually achieved by two kinds of mechanisms in the current deep learning approaches. One common method is to use a two-stream network <ref type="bibr" target="#b32">[29]</ref>, where one stream is on RGB frames to extract appearance information, and the other is to leverage optical flow as an input to capture movement information. This method turns out to be effective for improving action recognition accuracy but requires high computational con-  <ref type="figure">Figure 1</ref>. Video classification performance comparison on Something-Something V1 <ref type="bibr" target="#b7">[8]</ref> in terms of Top1 accuracy, computational cost, and model size. Our proposed TDN achieves the best trade-off between accuracy and efficiency, when compared with previous methods such as NL I3D <ref type="bibr" target="#b43">[40]</ref>, ECO <ref type="bibr" target="#b49">[46]</ref>, TSM <ref type="bibr" target="#b20">[19]</ref> and TEINet <ref type="bibr" target="#b21">[20]</ref>. sumption for optical flow calculation. Another alternative approach is to use 3D convolutions <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b34">31]</ref> or temporal convolutions <ref type="bibr" target="#b36">[33,</ref><ref type="bibr" target="#b44">41,</ref><ref type="bibr" target="#b28">25]</ref> to implicitly learn motion features from RGB frames. However, 3D convolutions often lack specific consideration in the temporal dimension and might bring higher computational cost as well. Therefore, designing an effective temporal module of high motion modeling power and low computational consumption is still a challenging problem for video recognition. This paper aims to present a new temporal modeling mechanism by introducing a temporal difference based module (TDM). Temporal derivative (difference) is highly relevant with optical flow <ref type="bibr" target="#b12">[11]</ref>, and has shown effectiveness in action recognition by using RGB difference as an approximate motion representation <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b46">43]</ref>. However, these approaches simply treat RGB difference as another video modality and train a different network to fuse with the RGB network. Instead, we aim to present a unified framework to capture appearance and motion information jointly, by generalizing the idea of temporal difference into a principled and efficient temporal module for end-to-end network design.</p><p>In addition, we argue that both short-term and long-term temporal information are crucial for action recognition, in the sense that they are able to capture the distinctive and complementary properties of an action instance. Therefore, in our proposed temporal modeling mechanism, we present a unique two-level temporal modeling framework based on a holistic and sparse sampling strategy <ref type="bibr" target="#b41">[38]</ref>, termed as Temporal Difference Network (TDN). Specifically, in TDN, we consider two efficient forms of TDMs for motion modeling at different scales. For local motion modeling, we present a light-weight and low-resolution difference module to supply a single RGB with motion patterns via lateral connections, while for long-range motion modeling, we propose a multi-scale and bidirectional difference module to capture cross-segment variations for motion excitation. These two TDMs are systematically studied as modular building blocks for short-term and long-rang temporal structure extraction.</p><p>Our TDN provides a simple and general video-level motion modeling framework and could be instantiated with existing CNNs at a small extra computational cost. To demonstrate the effectiveness of TDN, we implement it with ResNets and perform experiments on two datasets: Kinetics and Something-Something. The evaluation results show that our TDN is able to yield a new state-of-the-art performance on both motion relevant Something-Something dataset and scene relevant Kinetics dataset, under the setting of using similar backbones. As shown in <ref type="figure">Figure 1</ref>, our best result is significantly better than previous methods on the dataset of Something-Something V1. We also perform detailed ablation studies to demonstrate the importance of temporal difference operation and investigate the effect of a specific design of TDM. In summary, our main contribution lies in the following three aspects:</p><p>• We generalize the idea of RGB difference to devise an efficient temporal difference module (TDM) for motion modeling in videos and provide an alternative to 3D convolutions by systematically presenting effective and detailed module design. • Our TDN presents a video-level motion modeling framework with the proposed temporal difference module, with a focus on capturing both short-term and long-term temporal structure for video recognition. • Our TDN obtains the new state-of-the-art performance on the datasets of Kinetics and Something-Something under the setting of using the same backbones. We also perform in-depth ablation study on TDM to provide some insights on our temporal difference modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Short-term temporal modeling. Action recognition has attracted lots of research attention in the past few years.</p><p>These methods could be categorized into two types: <ref type="bibr" target="#b0">(1)</ref> two-stream CNNs <ref type="bibr" target="#b32">[29]</ref> or its variants <ref type="bibr" target="#b6">[7]</ref>: it used two inputs of RGB and optical flow to separately model appearance and motion information in videos with a late fusion; (2) 3D-CNNs <ref type="bibr" target="#b34">[31,</ref><ref type="bibr" target="#b13">12]</ref>: it proposed 3D convolution and pooling to directly learn spatiotemporal features from videos. Several variants tried to reduce the computation cost of 3D convolution by decomposing it into a 2D convolution and a 1D temporal convolution, for example R(2+1)D <ref type="bibr" target="#b36">[33]</ref>, S3D <ref type="bibr" target="#b44">[41]</ref>, P3D <ref type="bibr" target="#b28">[25]</ref>, and CT-Net <ref type="bibr" target="#b17">[16]</ref>. Following this research line, several works focused on designing more powerful temporal modules and inserted them into a 2D CNN for efficient action recognition, such as TSM <ref type="bibr" target="#b20">[19]</ref>, TIN <ref type="bibr" target="#b31">[28]</ref>, TEINet <ref type="bibr" target="#b21">[20]</ref>, TANet <ref type="bibr" target="#b22">[21]</ref>, and TEA <ref type="bibr" target="#b19">[18]</ref>. In addition, some methods tried to leverage the idea of two stream network to design a multi-branch architecture to capture both appearance and motion or context information, with a carefully designed temporal module or two RGB inputs sampled at different FPS, including Non-local Net <ref type="bibr" target="#b42">[39]</ref>, ARTNet <ref type="bibr" target="#b39">[36]</ref>, STM <ref type="bibr" target="#b14">[13]</ref>, SlowFast <ref type="bibr" target="#b5">[6]</ref>, and CorrelationNet <ref type="bibr" target="#b38">[35]</ref>. Some recent works <ref type="bibr" target="#b4">[5]</ref> tried network architecture search for video recognition. These works were clip-based architecture with a focus on short-term motion modeling by learning from a small portion of the whole video (e.g., 64 frames).</p><p>Long-term temporal modeling. Short-term clip-based networks fail to capture the long-range temporal structure. Several methods were proposed to overcome this limitation by stacking more frames with RNN <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b2">3]</ref> or long temporal convolution <ref type="bibr" target="#b37">[34]</ref>, or using a sparse sampling and aggregation strategy <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b47">44,</ref><ref type="bibr" target="#b45">42,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b19">18]</ref>. Among these methods, temporal segment network (TSN) <ref type="bibr" target="#b41">[38]</ref> turned out to be an effective long-range modeling framework and obtained the state-of-the-art performance with 2D CNNs on several benchmarks. However, TSN with 2D CNNs only performed temporal fusion at the last stage and failed to capture the finer temporal structure. StNet <ref type="bibr" target="#b10">[9]</ref> proposed a local and global module to model temporal information hierarchically. V4D <ref type="bibr" target="#b45">[42]</ref> extended the TSN framework by proposing a principled 4D convolutional operator to aggregate long-range information from different stages.</p><p>Temporal difference representation. Temporal difference operations appeared in several previous works for motion extraction, such as RGB Difference <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b46">43,</ref><ref type="bibr" target="#b24">23]</ref> and Feature Difference <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b19">18]</ref>. RGB difference turned out to be an efficient alternative modality to optical flow as motion representation <ref type="bibr" target="#b41">[38,</ref><ref type="bibr" target="#b46">43,</ref><ref type="bibr" target="#b24">23]</ref>. However, they only treated RGB differently with another video modality and trained a separate network to fuse with RGB stream. The work of TEINet <ref type="bibr" target="#b21">[20]</ref>, TEA <ref type="bibr" target="#b19">[18]</ref>, and STM <ref type="bibr" target="#b14">[13]</ref> employed a difference operation for network design. However, these two methods simply used a simple difference operator for single-level motion extraction and received less research attention than 3D convolutions. . . <ref type="figure">Figure 2</ref>. Temporal Difference Network. We present a video-level framework for learning action models from the entire video, coined as TDN. Based on the sparse sampling from multiple segments, our TDN aims to model both short-term and long-term motion information in our framework. The key contribution is to design an efficient short-term temporal difference module (S-TDM) and a long-term temporal difference module (L-TDM), to supply a 2D CNN with local motion information and enable long-range modeling across segments, respectively. CNNs share the same parameters on all segments. Details on both modules could be found in <ref type="figure">Figure 3</ref>.</p><p>Different from the existing methods, our proposed temporal difference network (TDN) is a video-level architecture of capturing both short-term and long-term information for end-to-end action recognition. Our key contribution is to introduce a temporal difference module (TDM) to explicitly compute motion information, and efficiently leverage it into our two-level motion modeling paradigm. We hope to improve and popularize these temporal difference-based modeling alternatives, which turns out to generally outperform 3D convolutions on two benchmarks with smaller FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Difference Networks</head><p>In this section, we describe our Temporal Difference Network (TDN) in detail. First, we give an overview of the TDN framework, which is composed of a short-term and long-term temporal difference module (TDM). Then, we give a technical description of both modules. Finally, we provide the implementation detail to instantiate TDN with a ResNet backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As shown in <ref type="figure">Figure 2</ref>, our proposed temporal difference network (TDN) is a video-level framework for learning action models by using the entire video information. Due to the limit of GPU memory, following TSN framework <ref type="bibr" target="#b41">[38]</ref>, we present a sparse and holistic sampling strat-egy for each video. Our key contribution is to leverage the temporal difference operator into the network design to explicitly capture both short-term and long-term motion information. Efficiency is our core consideration in temporal difference module (TDM) design, and we investigate two specific forms to accomplish the tasks of motion supplement in a local window and motion enhancement across different segments, respectively. These two modules are incorporated into the main network via a residual connection.</p><p>Specifically, each video V is divided into T segments of equal duration without overlapping. We randomly sample a frame from each segment and totally obtain T frames</p><formula xml:id="formula_0">I = [I 1 , · · · , I T ], where the shape of I is [T, C, H, W ].</formula><p>These frames are separate fed into a 2D CNN to extract frame-wise features F = [F 1 , · · · , F T ], where F denotes the feature representation in the hidden layer and its dimension is [T, C , H , W ]. The short-term TDM aims to supply these frame-wise representation F of early layers with local motion information to improve its representation power:</p><formula xml:id="formula_1">Short term TDM :F i = F i + H(I i ),<label>(1)</label></formula><p>whereF i denotes the enhanced representation by TDM, H denotes our short-term TDM, and it extracts local motion from adjacent frames around I i . The long-term TDM aims at leveraging cross-segment temporal structure to enhance frame-level feature representation: <ref type="figure">Figure 3</ref>. An illustration of the short-term TDM and long-term TDM. Left: our S-TDM operates on the stacked RGB difference and is fused with single RGB CNN via residual connection to capture the short-term motion. Right: our L-TDM presents a bi-directional and multi-scale attention mechanism to leverage cross-segment information to enhance the frame-wise representations. More details could be found in the text.</p><formula xml:id="formula_2">Long term TDM :F i = F i + F i G(F i , F i+1 ),<label>(2)</label></formula><p>where G represents our long-term TDM, and in the current implementation, we only consider adjacent segment-level information for long-range temporal modeling in each longterm TDM. By stacking multiple long-term TDMs, we are able to capture temporal structure over a long scale. Details will be described in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Short-term TDM</head><p>We argue that adjacent frames are very similar in a local temporal window and directly stacking multiple frames for subsequent processing is inefficient. On the other hand, sampling a single frame from each window is able to extract appearance information, but fails to capture local motion information. Therefore, our short-term TDM chooses to supply a single RGB frame with a temporal difference to yield an efficient video representation, explicitly encoding both appearance and motion information.</p><p>Specifically, our short-term TDM operates at early layers of networks for low-level feature extraction and enables a single frame RGB to be aware of local motion via fusing temporal difference information. As shown in <ref type="figure">Figure</ref> 3, for each sampled frame I i , we extract several temporal RGB difference in a local window centered at I i , and then stack them along channel dimension D(</p><formula xml:id="formula_3">I i ) = [D −2 , D −1 , D 1 , D 2 ].</formula><p>Based on this representation, we present an efficient form of TDM:</p><formula xml:id="formula_4">H(I i ) = Upsample(CNN(Downsample(D(I i )))), (3)</formula><p>where D represents the RGB difference around I i , and CNN is the specific network for different stages. To keep the efficiency, we design a light-weight CNN module to operate on the stacked RGB difference D(I i ). It generally follows a low-resolution processing strategy: (1) downsample RGB difference by half with an average pooling, (2) extract motion features with a 2D CNN, (3) upsample motion features to match RGB features. This design comes from our observation that RGB difference exhibits very small values for most areas and only contains high response in motion salient regions. So, it is enough to use low-resolution architecture for this sparse signal without much loss of accuracy.</p><p>The information of short-term TDM is fused with the single RGB frame, so that the original frame-level representation is aware of motion pattern and able to better describe a local temporal window. We implement this fusion with lateral connections. We attach a fusion connection from short-term TDM to frame-level representation for each early stage (i.e., Stage 1-2 in our experiments). In practice, we also compare the residual connection with other fusion strategies as shown in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Long-term TDM</head><p>The frame-wise representation equipped with short-term TDM is powerful for capturing spatiotemporal information within a local segment (window). However, this representation is limited in terms of the temporal receptive field and thus fails to explore the long-range temporal structure for learning action models. Thus, our long-term TDM tries to use cross-segment information to enhance the original representation via a novel bidirectional and multi-scale temporal difference module.</p><p>In additional to efficiency, the missing-alignment of spatial location between long-range frames is another issue. Consequently, we devise a multi-scale architecture to smooth difference in large a receptive field before difference calculation. As shown in <ref type="figure">Figure 3</ref>, we first compress the feature dimension by a ratio r with a convolution for efficiency, and calculate the aligned temporal difference through adjacent segments:</p><formula xml:id="formula_5">C(F i , F i+1 ) = F i − Conv(F i+1 ),<label>(4)</label></formula><p>where C(F i , F i+1 ) represents the aligned temporal difference for segment F i , Conv is the channel-wise convolution for spatially smoothing and thus relieving the missingalignment issue. Then, the aligned temporal difference undergoes through a multi-scale module for long-range motion information extraction:</p><formula xml:id="formula_6">M (Fi, Fi+1) = Sigmd(Conv( N j=1</formula><p>CNNj(C(Fi, Fi+1)))), <ref type="bibr" target="#b4">(5)</ref> where CNN j at different spatial scales aims at extracting motion information from different receptive field, and N = 3 in practice. Their fusion could be more robust for the missing-alignment issue. In implementation, it involves three branches: (1) short connection, (2) a 3×3 convolution, and (3) a average pooling, a 3×3 convolution, and a bilinear upsampling. Finally, we utilize bidirectional crosssegment temporal difference to enhance fame level features as follows: <ref type="bibr" target="#b5">(6)</ref> where is the element-wise multiplication. We also combine the original frame level representation and enhance representation via a residual connection as in Eq. (2). Slightly different from short-term TDM, we employ the difference representation as an attention map to enhance frame level features, which is partially based on the observation that attention modeling is more effective for latter stage of CNNs. We also compare this implementation with other forms in the ablation study.</p><formula xml:id="formula_7">Fi G(Fi, Fi+1) = Fi 1 2 [M (Fi, Fi+1) + M (Fi+1, Fi)],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Exemplar: TDN-ResNet</head><p>As discussed above, our TDN framework is based on sparse sampling of TSN <ref type="bibr" target="#b41">[38]</ref>, which operates on a sequence of frames uniformly distributed over the entire video. Our TDN presents a two-level motion modeling mechanism, with a focus on capturing temporal information in a localto-global fashion. In particular, as shown in <ref type="figure">Figure 2</ref>, we insert short-term TDMs (S-TDM) in early stages for finer and low-level motion extraction, and long-term TDMs (L-TDM) into latter stages for coarser and high-level temporal structure modeling.</p><p>We instantiate our TDN with a ResNet backbone <ref type="bibr" target="#b11">[10]</ref>. Following the practice in V4D <ref type="bibr" target="#b45">[42]</ref>, the first two stages of ResNet are for short-term temporal information extraction within each segment by using S-TDMs, and the latter three stages of ResNet are equipped with L-TDMs for capturing long-range temporal structure across segments. For local motion modeling, we add both residual connections between S-TDM and the main network for Stage 1 and Stage 2. For long term motion modeling, we add L-TDM and a temporal convolution in each residual block of Stages 3-5. In practice, the final TDN-ResNet only increases the FLOPs over the original 2D TSN-ResNet by around 9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the experiment results of our TDN framework. First, we describe the evaluation datasets and implementation details. Then, we perform ablation studies on the design of our TDN. Next, we compare our TDN with the state-of-the-art methods. Finally, we show some visualization results to further analyze our TDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and implementation details</head><p>Video datasets. We evaluate our TDN on two video datasets, which pay attention to different aspects of an action instance for recognition. Kinetics-400 <ref type="bibr" target="#b16">[15]</ref> is a largescale YouTube video dataset and has around 300k trimmed videos covering 400 categories. The Kinetics dataset contains activities in daily life and some categories are highly correlated with interacting objects or scene context. We train our TDN on the training data (around 240k videos) and report performance on the validation data (around 20k videos). Something-Something <ref type="bibr" target="#b7">[8]</ref> is a large-scale dataset created by crowdsourcing. The videos are collected by performing the same action with different objects so that action recognition is expected to focus on the motion property instead of objects or scene context. The first version contains around 100k videos over 174 categories, while the second version is with more videos, containing around 169k videos in training set and 25k videos in validation set. We report performance on the validation set of Something-Something V1 &amp; V2. Training and testing. In experiments, we use ResNet50 and ResNet101 to implement our TDN framework, and sample T = 8 or T = 16 frames from each video. Following common practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">39]</ref>, during training, each video frame is resized to have shorter side in [256, 320] and a crop of 224 × 224 is randomly cropped. We pre-train our TDN on the ImageNet dataset <ref type="bibr" target="#b1">[2]</ref>. The batch size is 128 and the initial learning rate is 0.02. The total training epoch is set as 100 in the Kinetics dataset and 60 in the Something-Something dataset. The learning rate will be divided by a factor of 10 when the performance on validation set saturates. For testing, the shorter side of each video is resized to 256. We implement two kinds of testing scheme: 1-clip and center-crop where only a center crop of 224 × 224 from a single clip is used for evaluation, and 10-clip and 3-crop where three crops of 256 × 256 and 10 clips are used for testing. The first testing scheme is with high efficiency while the second one is for improving accuracy with a denser prediction scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>We perform ablation studies on the Something-Something V1 dataset. For these evaluations, we use the testing scheme of 1 clip and center crop, and report the Top1 We compare TDM with several temporal modules: Temporal convolution, TSM, and TEINet. For fair comparison, we report the result with 8 frame and 40 for each temporal module (++ for 40 frames). Our TDM is better than previous temporal modules. <ref type="table">Table 1</ref>. Ablations on Something-Something V1 with 8-frame TDN-ResNet50. We show top-1 classification accuracy (%), and computational cost measured in FLOPs (floating-point operations, in # of multiply-adds ) for a 1-clip and center-crop input of size 224×224. accuracy. We also compare with other temporal modeling modules to demonstrate the effectiveness of TDM.</p><p>Study on the effect of difference operation. We begin our oblation study by exploring the effectiveness of temporal difference operation in our TDM. We implement fairly comparable baselines by simply removing temporal difference operation in S-TDM and replacing temporal difference with taking average in L-TDM. <ref type="table">Table 1a</ref> shows the results of various settings with temporal difference or without temporal difference. It can be seen that simply stacking and taking average to fuse temporal information will greatly decrease the recognition accuracy by around 10%. We analyze that these temporal fusion strategies without difference operation would make the network to over-fit static information and fail to capture temporal variation in videos. Adding temporal difference in both S-TDM and L-TDM contributes to better accuracy and their combination obtains the best performance.</p><p>Study on short-term TDM. We compare different forms of short-term TDM (S-TDM). We add long-term TDM (L-TDM) for all latter stages and place variations of S-TDM in early stages. As shown in <ref type="table">Table 1b</ref>, we first compare different fusion strategies to combine difference representation with RGB features in S-TDM: (1) attention with elementwise multiplication, (2) addition with attention, (3) only addition. We can see that our S-TDM with simply addition yields the best performance and the other attention-based fusion might destroy the pre-trained feature correspondence. In addition, we try to use RGB difference representation to learn the channel attention weight just as TEINet <ref type="bibr" target="#b22">[21]</ref>, and its performance is also worse than our proposed S-TDM (47.3% vs. 52.3%). In the remaining study, we use the addition form of S-TDM by default.</p><p>Study on long-term TDM. We employ short-term TDM for the early stages, and compare with different forms of long-term TDM (L-TDM) placed on the latter stages. The results are reported in <ref type="table">Table 1c</ref>. For L-TDM design, we first compare with two baseline architecture: (1) no attention modeling in Eq. (2) and directly adding the difference representation into frame-level features; (2) channel attention modeling just as TEA <ref type="bibr" target="#b19">[18]</ref>. It is observed that our proposed spatiotemporal attention form of L-TDM is better than no attention (52.3% vs.44.1%) and channel attention (52.3% vs. 50.9%). Then, we investigate the effectiveness of multiscale architecture in difference feature extraction and it is able to improve performance from 49.7% to 52.3%, which confirms its effectiveness of large receptive field for difference feature extraction. Finally, we compare the performance of bidirectional difference with one-directional difference, and it helps to improve performance by 2.3%.</p><p>Study on the location of S-TDM and L-TDM . We perform the ablation study on which stage to use short-term TDM (S-TDM) or long-term TDM (L-TDM). The results are shown in <ref type="table">Table 1d</ref>. From these results, we see that  adding more S-TDMs into the main network will increase the network computational cost slightly due to its feature extraction for temporal difference representation. The setting of using S-TDM in stages 1-2 and L-TDM in stages 3-5 obtains the best recognition accuracy and the computational cost is also reasonable.</p><p>Short-term vs. long-term modeling. We conduct comparative study to separately investigate the effectiveness of S-TDM and L-TDM. The results are summarized in <ref type="table">Table 1e</ref>. We first report the performance of baseline without  S-TDM or L-TDM, namely only with 1D temporal convolutions in latter stages for temporal modeling, and its accuracy is 46.6%. Then we separately add S-TDM and L-TDM into the baseline, and they obtain the performance of 51.5% and 49.9%. The superior performance of S-TDM to L-TDM might be ascribed to the fact that local motion information is crucial for action recognition. Finally, combining S-TDM and L-TDM could boost performance to 52.3%, which implies the complementarity of two modules.</p><p>Comparison with other temporal modules. Finally, we compare our proposed TDM with other temporal modeling methods, and the results are reported in <ref type="table">Table 1f</ref>. We compare our TDM with three temporal modules: temporal convolution <ref type="bibr" target="#b36">[33]</ref>, TSM <ref type="bibr" target="#b20">[19]</ref>, and TEINet <ref type="bibr" target="#b21">[20]</ref>. First, these methods all use the ResNet50 as backbones and 8 frames as input. In this setting, their FLOPs are similar to our TDN. We find that the performance of our TDN is much better than those baselines with similar FLOPs, demonstrating the effectiveness of explicit temporal difference operation. Then, for fair comparison, we also implement other tem-poral modules taking the same number of frames as ours (i.e., 40 frames denoted by ++), and we observe that simply inputting more frames will not contribute much to improve recognition accuracy. We analyze that these temporal modules still lack sufficient modeling capacity to well capture fine-grained motion information and thus more frames will make them over-fit with appearance more seriously. On the other hand, thanks to temporal difference operation, our TDM is able to focus more on the motion information and thus improve the recognition accuracy with more frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state of the art</head><p>After the ablation study of 8-frame TDN on Something-Somthing V1 dataset, we directly transfer its optimal setting to the datasets of Something-Something V2 and Kinetics-400. In this subsection, we compare our TDN with those state-of-the-art methods on these benchmarks. As expected, sampling more frames can further improve the accuracy, but also increases the FLOPs. We report the performance of both 8-frame TDN and 16-frame TDN. For fair comparison, we simply list the performance of methods solely using RGB without pre-training on extra video datasets.</p><p>The results are summarized in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>. For fair comparison with previous methods, we use 1 clip and center crop testing scheme on the Something-Something dataset and 10 clips and 3 crops for testing on the Kinetics-400 dataset. We first compare with 2D CNN based baselines with late fusion for long-range temporal modeling such as TSN <ref type="bibr" target="#b41">[38]</ref> and TRN <ref type="bibr" target="#b47">[44]</ref>, and see that our TDN outperforms these baseline methods significantly on both datasets. Then, we compare our TDN with 2D CNN with temporal modules for all stages, such as S3D <ref type="bibr" target="#b44">[41]</ref>, R(2+1)D <ref type="bibr" target="#b36">[33]</ref>, TSM <ref type="bibr" target="#b20">[19]</ref>, TEINet <ref type="bibr" target="#b21">[20]</ref>, TANet <ref type="bibr" target="#b22">[21]</ref>, TAM <ref type="bibr" target="#b3">[4]</ref>, and GSM <ref type="bibr" target="#b33">[30]</ref>, and our TDN consistently outperforms them on both datasets, demonstrating the effectiveness of TDM in temporal modeling for action recognition. After this, we compare with more recent 3D CNNs based methods, such as I3D <ref type="bibr" target="#b0">[1]</ref>, Non-local I3D <ref type="bibr" target="#b42">[39]</ref>, and SlowFast <ref type="bibr" target="#b5">[6]</ref>, and our TDN can still obtain slightly better performance than those methods, with a relatively smaller computational cost. Finally, we compare more recent video recognition networks, such as SmallBigNet <ref type="bibr" target="#b18">[17]</ref>, V4D <ref type="bibr" target="#b45">[42]</ref>, CorrelationNet <ref type="bibr" target="#b38">[35]</ref>, and X3D <ref type="bibr" target="#b4">[5]</ref>. Our best result significantly outperforms previous methods on Something-Something V1 and is on par with the previous best performance on the Kinetics dataset. The best performance on the Kinetics dataset is the combination of SlowFast and Non-local Net, which is slightly better than ours for Top1 accuracy yet with lower Top5 accuracy and higher FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of activation maps</head><p>We visualize the class activation maps with Grad-CAM <ref type="bibr" target="#b48">[45,</ref><ref type="bibr" target="#b30">27]</ref> and results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. In this visualization, we take 8 frames as input and only plot the activation maps in the center frames. These visualization results indicate that baseline with only temporal convolutions fails to focus on motion-salient regions, while our TDN is able to localize more action-relevant regions, thanks to our proposed TDMs for short-term and long-term temporal modeling. For example, our TDN pays more attention to the hand motion with interaction objects, while the temporal convolution may only focus on the background. More visualization examples and analysis could be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a new video-level framework, termed as TDN, for learning action models from the entire video. The core contribution of TDN is to generalize temporal difference operator into efficient and general temporal modules (TDM) with specific designs, for capturing both short-term and long-term temporal information in a video. We present two customized forms for the implementation of TDMs and systematically assess their effects on temporal modeling. As demonstrated on the Kinetics-400 and Something-Something dataset, our TDN is able to yield superior performance to previous state-of-the-art methods of using similar backbones.</p><p>In addition, we present an in-depth ablation study on TDMs to investigate the effect of temporal difference operation, and demonstrate that it is more effective to extract fine-grained temporal information than a standard 3D convolution with more frames. We hope our analysis provides more insights about temporal difference operation, and TDM might provide an alternative to 3D convolution for temporal modeling in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results on the UCF101 and HMDB51</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pretrain Backbone UCF101 HMDB51 TSN <ref type="bibr" target="#b41">[38]</ref> ImageNet Inception V2 86.4% 53.7% P3D <ref type="bibr" target="#b28">[25]</ref> ImageNet ResNet50 88.6% -C3D <ref type="bibr" target="#b34">[31]</ref> Sports-1M ResNet18 85.8% 54.9% I3D <ref type="bibr" target="#b0">[1]</ref> ImageNet+Kinetics Inception V2 95.6% 74.8% ARTNet <ref type="bibr" target="#b39">[36]</ref> Kinetics ResNet18 94.3% 70.9% S3D <ref type="bibr" target="#b44">[41]</ref> ImageNet+Kinetics Inception V2 96.8% 75.9% R(2+1)D <ref type="bibr" target="#b36">[33]</ref> Kinetics ResNet34 96.8% 74.5% TSM <ref type="bibr" target="#b20">[19]</ref> Kinetics ResNet50 96.0% 73.2% STM <ref type="bibr" target="#b14">[13]</ref> ImageNet + Kinetics ResNet50 96.2% 72.2% TEA <ref type="bibr" target="#b19">[18]</ref> ImageNet + Kinetics ResNet50 96.9% 73.3% TDN(Ours) ImageNet + Kinetics ResNet50 97.4% 76.3% <ref type="table">Table 4</ref>. Comparison with the state-of-the-art methods on UCF101 and HMDB51.</p><p>To further verify the generalization ability of TDN, we transfer the learned 16-frame TDN models from the Kinetics-400 dataset to the UCF101 and HMDB51. These two datasets are relatively small and the action recognition performance on them already saturates. We follow the standard evaluation scheme on these two datasets and report the mean accuracy over three splits. The results are summarized in <ref type="table">Table 4</ref>. We compare our TDN with previous stateof-the-art methods such as 2D baselines of TSN <ref type="bibr" target="#b41">[38]</ref>, 3D CNNs of I3D <ref type="bibr" target="#b0">[1]</ref> and C3D <ref type="bibr" target="#b34">[31]</ref>, R(2+1)D <ref type="bibr" target="#b36">[33]</ref>, and other temporal modeling methods <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b14">13]</ref>. From the results, we can see that our TDN is able to outperform these methods, and the performance improvement is more evident on the dataset of HMDB51 by around 2.5%. The action classes in HMDB51 are more relevant with motion information, and thus temporal modeling is more important on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Running time analysis</head><p>We report the inference time of our TDN with on Tesla V100 as follows. The testing batchsize is set as 16 and the running time include all evaluation, including loading data and network inference. The results are reported in <ref type="table">Table 5</ref>. From these results, we see that our TDN is slower than previous method but still could run in real-time (i.e. ≥25 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization analysis</head><p>To further investigate the performance the TDN models, we use the technique of Grad-CAM <ref type="bibr" target="#b30">[27]</ref> to visualize the feature representation of different models. Specifically, to better understand the effect of short-term TDM, we visualize the the features in Res2 stage of baseline model  <ref type="table">Table 5</ref>. Running time analysis on a Tesla V100.</p><p>(corresponding to the first row in <ref type="table">Table 1</ref>(e) of main article) and the TDM model only with S-TDM (corresponding to third row in in <ref type="table">Table 1</ref>(e) of main article), and the results are shown in <ref type="figure">Figure 5</ref>. Note that, these visualizations only are performed on the center frame of 8-frame models. From these results, the models equipped with S-TDM focuses more on motion-relevant information. Then, we give more visualization examples of activation maps in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>. In these results, we give the visualization results on 8 frames and compare our TDM models with the baseline method (corresponding to the first row in <ref type="table">Table 1</ref>(e) of main article). We could see that our TDN is able to yield more reasonable class activation maps than the baseline method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of activation maps with CAM. Left: video, Middle: baseline, Right: TDN. In this visualization, we train a 8frame network with TDMs (TDN) or temporal convolutions (Baseline). For simplicity, we only visualize the CAM on the center frames. More visualization examples on 8 frames could be found in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state-of-the-art methods on Something-Something V1 and V2. We instantiate our TDN with the backbones of ResNet50 and ResNet101 for evaluation. We compare with other methods with similar backbones under the 1clip and center crop setting. "-" indicates the numbers are not available for us.</figDesc><table /><note>1 Pre-trained on Sports1M.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art methods on the validation set of Kinetics-400. We instantiate our TDN with the backbones of ResNet50 and ResNet101. For fair comparison, we compare with the other methods by using the similar backbones without pre-training on extra videos. "-" indicates the numbers are not available for us.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix contains the results on the UCF101 and HMDB51, running time analysis of TDN, and more visualization results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu (</forename><surname>Richard) Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2261" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Figure 5. Visualization of Res2 features with Grad-CAM. We use 8-frame TDN models to visualize on the Something-Something V1 dataset. Left: video, Middle: baseline, Right: TDN with S-TDM. Note that we only show visualization on the center frame of sampled 8 frames</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8401" to="8408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">STM: spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CT-net: Channel tensorization network for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TSM: temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">TAM: temporal adaptive module for video recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5512" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal difference networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rajat Monga, and George Figure 6. Visualization of activation maps with Grad-CAM. We use 8-frame TDN models to visualize on the Something-Something V1 dataset. In the first row, we plot the 8 RGB frames. In the second row, we plot the activation maps of the baseline method without temporal difference module (TDM)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In the third row, we plot the activation maps of the TDN models</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">We use 8-frame TDN models to visualize on the Something-Something V1 dataset. In the first row, we plot the 8 RGB frames. In the second row, we plot the activation maps of the baseline method without temporal difference module (TDM)</title>
	</analytic>
	<monogr>
		<title level="m">Visualization of activation maps with Grad-CAM</title>
		<imprint/>
	</monogr>
	<note>In the third row, we plot the activation maps of the TDN models</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal representation with pseudo-3d residual networks. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12056" to="12065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Temporal interlacing network. AAAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11966" to="11973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gate-shift networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">V4d: 4d convolutional neural networks for video-level representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020. 2, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recognize actions by disentangling components of dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="831" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ECO: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
