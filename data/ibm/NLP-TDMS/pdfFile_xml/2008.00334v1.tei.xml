<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yu</surname></persName>
							<email>qi.yu@rit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
							<email>yu.kong@rit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA 2020; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413827</idno>
					<note>Rochester Institute of Technology Rochester, New York, USA Rochester Institute of Technology Rochester, New York, USA Rochester Institute of Technology Rochester, New York, USA ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 ACM Reference Format:. ACM, New York, NY, USA, 9 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision tasks</term>
					<term>Bayesian network models</term>
					<term>Neural networks KEYWORDS Accident anticipation</term>
					<term>graph convolution</term>
					<term>bayesian neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of Uncertainty-based Accident Anticipation. This paper presents a novel model to predict the probabilities (black curve) of a future accident (ranges from 90-th to 100-th frame). Our goal is to achieve early anticipation (large Time-to-Accident) giving a threshold probability (horizontal dashed line), while estimating two kinds of predictive uncertainties, i,e., aleatoric uncertainty (wheat color region) and epistemic uncertainty (blue region).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Traffic accident anticipation aims to predict accidents from dashcam videos as early as possible, which is critical to safety-guaranteed self-driving systems. With cluttered traffic scenes and limited visual cues, it is of great challenge to predict how long there will be an accident from early observed frames. Most existing approaches are developed to learn features of accident-relevant agents for accident anticipation, while ignoring the features of their spatial and temporal relations. Besides, current deterministic deep neural networks could be overconfident in false predictions, leading to high risk of traffic accidents caused by self-driving systems. In this paper, we propose an uncertainty-based accident anticipation model with spatio-temporal relational learning. It sequentially predicts the probability of traffic accident occurrence with dashcam videos. Specifically, we propose to take advantage of graph convolution and recurrent networks for relational feature learning, and leverage Bayesian neural networks to address the intrinsic variability of latent relational representations. The derived uncertainty-based ranking loss is found to significantly boost model performance by improving Permission to make digital the quality of relational features. In addition, we collect a new Car Crash Dataset (CCD) for traffic accident anticipation which contains environmental attributes and accident reasons annotations. Experimental results on both public and the newly-compiled datasets show state-of-the-art performance of our model. Our code and CCD dataset are available at: https://github.com/Cogito2012/UString.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Accident anticipation aims to predict an accident from dashcam video before it happens. It is one of the most important tasks for safety-guaranteed autonomous driving applications and has been receiving increasing attentions in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref>. Thanks to accident anticipation, the safety level of intelligent systems on vehicles could be significantly enhanced. For example, even a successful anticipation made with only a few seconds earlier before the accident happens can help a self-driving system to make urgent safety control, avoiding a possible car crash accident.</p><p>However, accident anticipation is still an extremely challenging task due to noisy and limited visual cues in an observed dashcam video. Take <ref type="figure">Fig. 1</ref> as an example, a traffic scene captured in egocentric view is typically crowded with multiple cars, pedestrians, motorcyclists, and so on. In this scenario, accident-relevant visual cues could be overwhelmed by objects that are not relevant to the accident, making an intelligent system insensible to a car crash accident happened at the road intersection. Nevertheless, traffic accidents are foreseeable by training a powerful uncertainty-based model to distinguish the accident-relevant cues from noisy video data. For example, the inconsistent motions of multiple vehicles may indicate high risk of possible future accidents.</p><p>In this paper, we propose a novel uncertainty-based accident anticipation model with spatio-temporal relational learning. The model aims to learn accident-relevant cues for accident anticipation by considering both spatial and temporal relations among candidate agents. The candidate agents are a group of moving objects like vehicles and their relational features are indicative of future unobserved accidents. The spatial relations of candidate agents are learned from their spatial distance, visual appearance features, as well as historical visual memory. The temporal relations of agents provide learnable patterns to indicate how the agents evolve and end with an accident in temporal context. It can be recurrently learned by updating historical memory with agent-specific features and the spatial relational representation. To address the variability of the spatio-temporal relational representations, a probabilistic module is incorporated to simultaneously predict accident scores and estimate how much uncertainty when making the prediction.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, on one hand, we propose to learn spatial relations with graph convolutional networks (GCN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> by considering the hidden states from recurrent neural network (RNN)) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref> cell. On the other hand, we propose to build temporal relations with RNNs by considering both spatial relational and agent-specific features. The cyclic process of the coupled GCNs and RNNs could generate representative latent spatio-temporal relational features. Besides, we propose to incorporate Bayesian deep neural networks (BNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> into our model to address the predictive uncertainty. With the Bayesian formulation, our derived epistemic uncertaintybased ranking loss is effective to improve the quality of the learned relational features and significantly leads to performance gain. At last, to further consider the global guidance of all hidden states in training stage, we propose a self-attention aggregation layer as shown in <ref type="figure">Fig. 4</ref>, from which an auxiliary video-level loss is obtained and demonstrated beneficial to our model. Compared with existing RNN-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, our model captures not only agent-specific features but also relational features for accident anticipation. Compared with the recent approach <ref type="bibr" target="#b24">[25]</ref> which is developed with 3D CNNs, our model is developed with GCNs and RNNs so that both spatial and temporal relations can be learned. Moreover, our method is capable of estimating the predictive uncertainty while all existing methods are deterministic.</p><p>The proposed model is evaluated on two public dashcam video datasets, i.e., DAD <ref type="bibr" target="#b3">[4]</ref> and A3D <ref type="bibr" target="#b31">[32]</ref>, and our collected Car Crash Dataset (CCD). Experimental results show that our model can outperform existing methods on all datasets. For DAD datasets, our method can anticipate traffic accident 3.53 seconds on average earlier before an accident happens. With best precision setting, our model can achieve 72.22% average precision. Compared with DAD and A3D datasets, our CCD dataset includes diversified environmental annotations and accident reason descriptions, which could promote research on traffic accident reasoning.</p><p>The main contributions of this paper are summarized below:</p><p>• We propose a traffic accident anticipation model by considering both agent-specific features and their spatio-temporal relations, as well as the predictive uncertainty. • With Bayesian formulation, the spatio-temporal relational representations can be learned with high quality by a novel uncertainty-based ranking loss. • We propose a self-attention aggregation layer to generate video-level prediction in the training stage, which serves as global guidance and is demonstrated beneficial to our model. • We release a new dataset containing real traffic accidents, in which diversified environmental annotations and accident reasons are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Traffic Accident Anticipation</head><p>To anticipate traffic accidents that happened in future frames, an intuitive solution is to iteratively predict accident confidence score for each time step. Chan et al. <ref type="bibr" target="#b3">[4]</ref> recently proposed DSA framework to leverage candidate objects appeared in each frame to represent the traffic status. They applied spatial-attention on these objects to get weighted feature representation for each LSTM cell. Based on this work, Suzuki et al. <ref type="bibr" target="#b31">[32]</ref> proposed an adaptive loss for early anticipation with quasi-recurrent neural networks <ref type="bibr" target="#b1">[2]</ref>. Similar to DSA that implements dynamic-spatial attention to focus on accident-relevant objects, Corcoran and James <ref type="bibr" target="#b6">[7]</ref> proposed a two-stream approach to traffic risk assessment. They utilized features of candidate objects as spatial stream and optical flow as temporal stream, and the two-stream features are fused for risk level classification. Instead of using dashcam videos, Shah et al. <ref type="bibr" target="#b29">[30]</ref> proposed to use surveillance videos to anticipate traffic accidents by using the framework DSA. Different from previous works, recently Neumann and Zisserman <ref type="bibr" target="#b24">[25]</ref> used 3D convolutional networks to predict the sufficient statistics of a mixture of 1D Gaussian distributions. In addition to using only dashcam video data, Takimoto et al. <ref type="bibr" target="#b32">[33]</ref> proposed to incorporate physical location data to predict the occurrence of traffic accidents. Closely related to traffic accident anticipation, the traffic accident detection is recently studied by Yao et al. <ref type="bibr" target="#b35">[36]</ref>. They proposed to detect traffic anomalies by predicting the future locations on video frames using ego-motion information. To anticipate both spatial risky regions and temporal accident occurrence, Zeng et al. <ref type="bibr" target="#b37">[38]</ref> proposed a soft-attention RNN by considering event agent such as human that triggers the event. However, existing work typically ignores the relations between accident-relevant agents which capture important cues to anticipate accidents in future frames. Besides, none of them considers the uncertainty estimation in developing their models, which is critical to safety-guaranteed systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty in Sequential Modeling</head><p>Uncertainty estimation is crucial to sequential relational modeling. One way is to directly formulate the latent representations of relational observations at each time step as random variables, which follow posterior distributions that can be approximated by deep neural networks. This is similar to variational auto-encoder (VAE) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Inspired by VAE, Chung et al. <ref type="bibr" target="#b5">[6]</ref> proposed variational recurrent neural network (VRNN) which formulates the hidden states of RNN as random variables and uses neural networks to approximate the posterior distributions of the variables. To further consider the relational representation of sequential data, Hajiramezanali et al. <ref type="bibr" target="#b13">[14]</ref> proposed variational graph recurrent neural networks (VGRNN) for dynamic link prediction problem by combining the graph RNN and variational graph auto-encoder (VGAE) <ref type="bibr" target="#b17">[18]</ref>. Another way to address uncertainty estimation is to formulate the weights of neural network as random variables such as Bayesian neural networks (BNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. Recently, Zhao et al. <ref type="bibr" target="#b38">[39]</ref> proposed a Bayesian graph convolution LSTM model for skeleton-based action recognition. In this paper, we also use graph convolution and BNNs but the difference is that their method uses stochastic gradient Hamiltonian Monte Carlo (SGHMC) sampling for posterior approximation, while we use Bayes-by-backprop <ref type="bibr" target="#b0">[1]</ref> as our approximation method. Compared with SGHMC, Bayes-by-backprop can be seamlessly integrated into deep learning optimization process so that it is more flexible to handle the learning tasks with large-scale dataset, i.e., dashcam videos used in traffic accident anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>Problem Setup. In this paper, the goal of accident anticipation is to predict an accident from dashcam videos before it happens. Formally, given a video with current time step t, the model is expected to predict the probability a t that an accident event will happen in the future. Furthermore, suppose an accident will happen at time step y where t &lt; y, the Time-to-Accident (TTA) is defined as τ = y − t when t is the first time that a t is larger than given threshold (see <ref type="figure">Fig. 1</ref>). For any t ≥ y with a positive video that contains an accident, we define τ = 0 which means the model fails to anticipate the accident. In this paper, our goal is to predict a t and expect τ to be as large as possible for dashcam videos that contain accidents. Similar to <ref type="bibr" target="#b3">[4]</ref>, the ground truth of a t is expressed with 2-dimensional one-hot encoding so that prediction target is</p><formula xml:id="formula_0">a t = (a (p) t , a (n) t ) T , where a (p)</formula><p>t and a (n) t represent the positive and negative predictions, respectively, meaning an accident will happen or not happen in the given video.</p><p>Framework Overview. The framework of our model is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. With a dashcam video as input, a graph is constructed with detected objects and corresponding features at each time step. To learn the spatio-temporal relations of these objects, we use graph convolutional networks (GCNs) to learn the spatial relations and leverage the hidden state h t of recurrent neural network (RNN) cell to enhance the input of the last GCN layer. Besides, the latent relational features are fused with corresponding object features as input of an RNN cell to update the hidden state at next time step. The cyclic process encourages our model to learn the latent relational features Z t from both spatial and temporal aspects. Furthermore, we propose to use Bayesian neural network (BNN) to predict accident scores a t so that predictive uncertainties are naturally formulated. During the training stage, we propose a self-attention aggregation (SAA, in <ref type="figure">Fig. 4</ref>) layer to predict video-level score, which can globally guide the learning of the proposed model.</p><p>In the following sections, each part of our model will be introduced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatio-Temporal Relational Learning</head><p>The spatio-temporal relations of traffic accident-relevant agents are informative to predict future accidents. In our model, we propose to use graph structured data to represent the observation at each time step. Then, the feature learning of spatial and temporal relations are coupled into a cyclic process.</p><p>Graph Representation. Graph representation for traffic scene has the advantages over full-frame feature embedding in that the impact of cluttered traffic background can be reduced and informative relations of traffic agents can be discovered for accident anticipation. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, we exploit object detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> to obtain a fixed number of candidate objects. These objects are treated as graph nodes so that a complete graph can be formed. However, the computational cost of graph convolution could be tremendous if the node features are with high dimensions.</p><p>In this paper, to construct low-dimensional but representative features for graph nodes X t , we introduce fully-connected (FC) layers to embed both the features of full-frame and candidate objects into the same low-dimensional space. Then, the frame-level and all object-level features are concatenated to enhance the feature representation capability:</p><formula xml:id="formula_1">X (i) t = Φ O (i) t , Φ (F t ) ,<label>(1)</label></formula><p>where Φ denotes FC layer, O (i) t and F t are high-dimensional features of the i-th object and corresponding frame at time t, respectively. The operator <ref type="bibr">[, ]</ref> represents concatenation in feature dimension and is used throughout this paper for simplicity.</p><p>The graph edge at time t is expressed as an adjacent matrix A t of a complete graph since we do not have information on which candidate object will be involved in an accident. Typically, an object with closer distance to others has higher possibility to be involved in an future accident. Therefore, the spatial distance between objects should be considered in edge weights such that we define A t as</p><formula xml:id="formula_2">A (i j) t = exp{−d(r i , r j )} i j exp{−d(r i , r j )} ,<label>(2)</label></formula><p>where d(r i , r j ) measures the Euclidean distance between two candidate object regions r i and r j . By this formulation, closer distance leads to larger A (i j) t . This means the two objects i and j will be applied with larger weight when we use graph convolution to learn their relational features for accident anticipation. Note that due to object occlusions, small distance defined in pixel space does not necessarily indicate close distance in physical world. It is possible to use 3D real-world distance if camera intrinsics are known. Nevertheless, the adjacency matrix defined in Eq. 2 has advantage to suppress the impact of irrelevant objects with significant large pixel distance to relevant objects.</p><p>Temporal Relational Learning. To build temporal relations at different time steps, RNN methods such as LSTM <ref type="bibr" target="#b14">[15]</ref> and GRU <ref type="bibr" target="#b4">[5]</ref> are widely adopted in existing works. However, traffic objects may not always be remained in each frame, the node features of the statically structured graph will be dynamically changing over time. Thanks to the recent graph convolutional recurrent network (GCRN) <ref type="bibr" target="#b28">[29]</ref>, it can handle the node dynamics defined over a static graph structure <ref type="bibr" target="#b13">[14]</ref>. Therefore, we propose to adapt GCRN for temporal relational feature learning. Specifically, the hidden states h t of RNN cell at each time step are recurrently updated by</p><formula xml:id="formula_3">h t +1 = GCRN ([Z t , X t ] , h t ) ,<label>(3)</label></formula><p>where Z t is the relational feature generated by the last GCN layer. The feature fusion between Z t and X t ensures our model to make fully use of both agent-specific and relational features. Spatial Relational Learning. To capture spatial relations of detected objects, we follow the graph convolution defined by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> for each GCN layer. In this paper, we use two stacked GCN layers and consider the hidden state h t learned by RNNs to learn the spatial relational features:</p><formula xml:id="formula_4">Z t = GCN ([GCN (X t , A t ) , h t ] , A t ) .<label>(4)</label></formula><p>The fusion with h t enables the latent relational representation aware of temporal contextual information. This fusion method is demonstrated to be effective to boost the performance of accident anticipation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BNNs for Accident Anticipation</head><p>To predict traffic accident score a t , a straightforward way is to utilize neural networks (NNs) as shown in <ref type="figure">Fig. 3(a)</ref>. However, the output of NNs is a point estimate which cannot address the intrinsic variability of the input relational features at each time step. Moreover, NNs could be overconfident in false model predictions when the model suffers from over-fitting problem.</p><p>To this end, we incorporate Bayesian neural networks (BNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> into our framework for accident score prediction. The architecture is shown in <ref type="figure">Fig. 3(b)</ref>. The BNNs module consists of two BNN layers with latent representation Z t given by Eq. 4 as input to predict accident score a t . To best of our knowledge, we are the first to incorporate BNNs into video-based traffic accident anticipation such that predictive uncertainty can be achieved. The predictive ⋮ ⋮ (a) Neural Networks ⋮ ⋮ (b) Bayesian Neural Networks <ref type="figure">Figure 3</ref>: Compared with NNs ( <ref type="figure">Fig.. 3(a)</ref>), network parameters of BNNs ( <ref type="figure">Fig. 3(b)</ref>) are sampled from Gaussian distributions so that both a t and its uncertainty can be obtained.</p><p>uncertainty could be utilized to not only guide the relational features learning (see Section 3.3), but also provide tools to interpret the model performance.</p><p>As we formulate the accident anticipation part as BNNs, the network parameters of BNNs such as weights and biases are all random variables, denoted as θ . Each entry of θ is drawn from a Gaussian distribution determined by a mean and variance, i.e., θ (j) ∼ N (µ, σ ), in which α (j) = (µ, σ ) need to be learned with dataset D = (Z t , a t ). Therefore, the likelihood of prediction can be expressed as p(a t |Z t , θ ) = N (f (Z t ; θ ), β), where β is the predictive variance. However, according to Bayesian rule, to obtain the true posterior of model parameters, i.e., p(θ |D), in addition to the likelihood and prior of θ , the marginal distribution ∫ p(a t |Z t , θ )dθ is required, which is intractable since a t = f (Z t , θ ) is modeled by a complex neural network. To estimate p(θ |D), existing variational inference methods (VI) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> could be used.</p><p>In this paper, we adopt the VI method Bayes-by-Backprop [1] to approximate p(θ |D) since it can be seamlessly incorporated in standard gradient-based optimization to learn from large-scale video dataset. According to <ref type="bibr" target="#b0">[1]</ref>, the variational approximation aims to minimize the following objective:</p><formula xml:id="formula_5">arg min α J i=1 log q (θ i |α ) − log p (θ i ) − log (p (D |θ i )) ,<label>(5)</label></formula><p>where J is the number of Monte Carlo samplings for θ . The first term q (θ i |α ) is the variational posterior distribution parameterized by α . The distribution parameters α can be efficiently learned by using reparameterization trick and standard gradient descent methods <ref type="bibr" target="#b0">[1]</ref>. We denote this loss term as L V POS . The second term p(θ i ) is the prior distribution of θ . It is typically modeled with a spike-and-slab distribution, i.e., a mixture of two Gaussian density functions with zero means but different variances. We denote this loss term as L P RI . The third term in Eq. 5 is the negative log-likelihood of model predictions. Since minimizing this term is equivalent to minimizing the mean squared error (MSE), in this paper, we propose to use exponential binary cross entropy to achieve this objective: where f is the constant frame rate for the given video, and y is the beginning time of an accident provided by training set. The exponential weighted factor applies larger penalty to the time step that is closer to the beginning time of an accident.</p><formula xml:id="formula_6">L EX P = T t =1 −e − max 0, y−t f log a (p) t + T t =1 − log 1 − a (n) t ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty-guided Ranking Loss</head><p>With the Bayesian formulation for accident anticipation, we can perform multiple forward passes at each time step such that an assembled prediction could be obtained by taking the average of these multiple outputs. Furthermore, as suggested by <ref type="bibr" target="#b15">[16]</ref>, the predictive uncertainty (variance) can be decomposed as aleatoric uncertainty and epistemic uncertainty <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>:</p><formula xml:id="formula_7">U t = 1 M M i=1 diag (â i ) −â iâ T i Aleatoric Uncertainty(U al t t ) + 1 M M i=1 (â i −ā) (â i −ā) T Epistemic Uncertainty(U ept t ) ,<label>(7)</label></formula><formula xml:id="formula_8">whereā = 1 M M i=1â i andâ i = (â (n) t ,â (p) t ) T i .</formula><p>They are the predictions of the i-th forward pass at time step t with total M forward passes. The first term in Eq. 7 is the aleatoric uncertainty, which measures the input variability (noise) of BNNs. In our model, the aleatoric uncertainty serves as an indicator to the quality of the learned relational features from GCNs and RNNs.</p><p>The second term in Eq. 7 is epistemic uncertainty which is determined by the BNNs model itself. Inspired by Ma et al. <ref type="bibr" target="#b22">[23]</ref>, ideally the epistemic uncertainties of sequential predictions should be monotonically decreasing, since as more frames the model observes, the more confident of the learned model (smaller epistemic uncertainty) will be. Therefore, we propose a novel ranking loss:</p><formula xml:id="formula_9">L RAN K = max 0, trace U ept t − U ept t −1 ,<label>(8)</label></formula><p>where U ept t −1 and U ept t are epistemic uncertainties of successive frames t − 1 and t defined in Eq. 7. Note that U t as well as the two terms in Eq. 7 are matrices with size 2 × 2, therefore in practice we propose to use matrix trace to quantify the uncertainties, which is similar to the method adopted in <ref type="bibr" target="#b30">[31]</ref>. Our proposed ranking loss aims to apply penalty to the predictions that do not follow the epistemic uncertainty ranking rule.</p><p>For aleatoric uncertainty U al t t , it is not necessary to satisfy the monotonic ranking requirement since the noise ratio of accumulated data in video sequence is intrinsically not monotonic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal Self-Attention Aggregation</head><p>Recurrent network can naturally build temporal relations of observations. However, the drawback of RNNs is that inaccurate hidden states in early temporal stages could be accumulated in iterative procedure and mislead the model to give false predictions in latter temporal stages. Besides, the hidden states in different time steps should be adaptive to anticipate the occurrence of a future accident.</p><p>To this end, motivated by recent self-attention design <ref type="bibr" target="#b33">[34]</ref>, we propose a self-attention aggregation (SAA) layer in the training stage by adaptively aggregating hidden states of all time steps. Then, we use the aggregated representation to predict video-level accident score. The architecture of SAA layer is shown in <ref type="figure">Fig. 4</ref>.</p><p>Specifically, we first aggregate hidden states of N individual objects at each time step by applying the concatenation between mean-and max-pooling results. Then, the self-attention <ref type="bibr" target="#b33">[34]</ref> is adapted to weigh the representation of all T time steps. In this module, the embedding layers are not used. Lastly, instead of using simple average pooling, we introduce an FC layer with T learnable parameters to adaptively aggregate the T temporal hidden states. The aggregated video-level representation is used to predict the video-level accident score a by two FC layers. This network is trained with binary cross-entropy (BCE) loss:</p><formula xml:id="formula_10">L BC E = − log a (p) − log 1 − a (n) ,<label>(9)</label></formula><p>where a = (a (n) , a (p) ) T normalized by softmax function. This auxiliary learning objective encourages the model to learn better hidden states even though SAA layer is not used in testing stage. Finally, the complete learning objective of our model is to minimize the following weighted loss:</p><formula xml:id="formula_11">L = L EX P +w 1 ·(L V POS − L P RI )+w 2 ·L RAN K +w 3 ·L BC E (10)</formula><p>where the L V POS and L P RI are loss functions of variational posterior and prior. The constants w 1 , w 2 and w 3 are set to 0.001, 10 and 10, respectively, to balance the magnitudes of these loss terms. The second penalty term (L V POS − L P RI ) is also termed as complexity loss and has similar effect to overcome over-fitting problem. The third penalty term L BC E introduces video-level classification guidance while the fourth term L RAN K brings uncertainty ranking guidance to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate our model on three real-world datasets, including our collected Car Crash Dataset (CCD) and two public datasets, i.e., Dashcam Accident Dataset (DAD) <ref type="bibr" target="#b3">[4]</ref> and AnAn Accident Detection (A3D) dataset <ref type="bibr" target="#b35">[36]</ref>. State-of-the art methods are compared and ablation studies are performed to validate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>CCD dataset 1 . In this paper, we collect a challenging Car Crash Dataset (CCD) for accident anticipation. We ask annotators to label YouTube accident videos with temporal annotations, diversified environmental attributes (day/night, snowy/rainy/good weather conditions), whether ego-vehicles involved, accident participants, and accident reason descriptions. For temporal annotations, the   <ref type="table">Table 1</ref>: Comparison between CCD dataset and existing datasets. Information about DAD and A3D is obtained from their released sources. Temporal Annos. means the temporal accident time annotations. Random ABT. means accidents beginning times are randomly placed. Ego-involved means the ego-vehicles are involved in accidents. Day/Night indicates the data is collected in day or night. Weather includes rainy, snowy, and sunny conditions. Participants Bbox means the bounding boxes tracklets for accident participants. Accident Reasons contains multiple possible reasons for each accident participant. accident beginning time is labeled at the time when a car crash actually happens. To get trimmed videos with 5 seconds long, the accident beginning times are further randomly placed in last 2 seconds, generating 1,500 traffic accident video clips. We also collected 3,000 normal dashcam videos from BDD100K <ref type="bibr" target="#b36">[37]</ref> as negative samples. The dataset is divided into 3,600 training videos and 900 testing videos. Examples are shown in <ref type="figure" target="#fig_2">Fig. 5</ref> and comparison details with existing datasets are reported in <ref type="table">Table 1</ref>. Compared with DAD <ref type="bibr" target="#b3">[4]</ref> and A3D <ref type="bibr" target="#b35">[36]</ref>, our CCD is larger with diversified annotations. DAD dataset. DAD <ref type="bibr" target="#b3">[4]</ref> contains dashcam videos collected in six cities in Taiwan. It provides 620 accident videos and 1,130 normal videos. Each video is trimmed and sampled into 100 frames with totally 5 seconds long. For accident videos, accidents are placed in the last 10 frames. The dataset has been divided into 1,284 training videos (455 positives and 829 negatives) and 466 testing videos (165 positives and 301 negatives).</p><p>A3D dataset. A3D <ref type="bibr" target="#b35">[36]</ref> is also a dashcam accident video dataset. It contains 1,500 positive traffic accident videos. In this paper, we only keep the 587 videos in which ego-vehicles are not involved in accidents. We sampled each A3D video with 20 fps to get 100 frames in total and placed the beginning time of each accident at the last 20 frames similar to DAD. The dataset is divided into 80% training set and 20% testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Average Precision. This metric evaluates the correctness of identifying an accident from a video. Following the same definition as <ref type="bibr" target="#b3">[4]</ref>, at time step t, if a (p) t is larger than a threshold, then the prediction at frame t is positive to contain an accident, otherwise it is negative. For accident videos, all frames are labeled with ones (positive), otherwise the labels are zeros (negative). By this way, the precision, recall, as well as the derived Average Precision (AP) can be adopted to evaluate models.</p><p>Time-to-Accident. This metric evaluates the earliness of accident anticipation based on positive predictions. For a range of threshold values, multiple TTA results as well as corresponding recall rates can be obtained. Then, we use mTTA and TTA@0.8 to evaluate the earliness, where mTTA is the average of all TTA values and TTA@0.8 is the TTA value when recall rate is 80%. Note that if a large portion of predictions are false positives, very high TTA results can still be achieved while corresponding AP would be low. That means the model is overfitting on accident video and may give positive predictions for arbitrary input. Therefore, except for fair comparison with existing methods, we mainly report TTA metrics when the highest AP is achieved, because it is meaningless to obtain high TTA if high AP cannot be guaranteed.</p><p>Predictive Uncertainty. Based on Eq. 7, we introduce to use the mean aleatoric uncertainty (mAU) and mean epistemic uncertainty (mEU) to evaluate the predictive uncertainties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We implement our model with PyTorch <ref type="bibr" target="#b25">[26]</ref>. For DAD dataset, we use the candidate objects and corresponding features provided <ref type="table">Table 2</ref>: Evaluation results on DAD, A3D, and CCD datasets. Results of baselines on DAD are obtained from <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b31">[32]</ref>. The notation "-" means the metric is not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Methods  <ref type="bibr" target="#b2">[3]</ref> with ResNeXt-101 <ref type="bibr" target="#b34">[35]</ref> backbone and FPN <ref type="bibr" target="#b20">[21]</ref> neck as our object detector on KITTI 2D detection dataset <ref type="bibr" target="#b11">[12]</ref>. The trained detector is used to detect candidate objects and then extract VGG-16 features of full-frame and all objects. As suggested by Bayes-by-backprop <ref type="bibr" target="#b0">[1]</ref>, we set the number of forward passes M to 2 in training stage and 10 for testing stage. For the hyper-parameters of prior distribution, we set the mixture ratio π to 0.5 and the variances of the two Gaussian distributions σ 1 to 1 and σ 2 to exp(−6). The dimensions of both hidden state of RNN and output of GCNs are set to 256. In the training stage, we set batch size to 10 and initial learning rate to 0.0005 with ReduceLROnPlateau as learning rate scheduler. The model is trained by Adam optimizer for totally 70 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Evaluation</head><p>Compare with State-of-the-art Methods. Existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> are compared and results are reported in <ref type="table">Table 2</ref>. For fair comparison, we use the model at the last training epoch for evaluation on DAD datasets. Nevertheless, the trained model with best AP is kept for evaluation on other two datasets since high AP is important to suppress impact of false positives on TTA evaluation. Note that these two metrics currently are only applicable to our model, since we are the first to introduce uncertainty formulation for accident anticipation. From <ref type="table">Table 2</ref>, our model on DAD dataset achieves the best mTTA which means the model anticipates on average 3.53 seconds earlier before an accident happens, while keeping competitive AP performance at 53.7% compared with L-RAI and adaLEA. Note that the video lengths of the three datasets are all 5 seconds, our high performance on A3D and CCD demonstrate that our model is easier to be trained on different datasets. This can be explained by the mAU results due to their consistence with TTA evaluation results in <ref type="table">Table 2</ref>. The low mAU values on A3D and DAD datasets reveal that our model has learned relational representations with high quality on these datasets.</p><p>We further report TTA results with different recall rates from 10% to 90% in <ref type="table" target="#tab_3">Table 3</ref>. It shows that our model outperform DSA in most of recall rate requirements. For recall rates larger than 80%,  our method performs poorly compared with DSA. However, high recall rate may also lead to too much false alarm so that AP cannot be guaranteed to be high. This finding also supports our motivation to use the trained model with best AP for evaluation. Visualization We visualized accident anticipation results with samples in DAD dataset (see <ref type="figure" target="#fig_3">Fig. 6</ref>). The uncertainty regions indicate that in both early and late stages, the model is quite confident on prediction (low uncertainties), while in the middle stage when accident scores start are increasing, the model is uncertain to give predictions. Note that the predicted epistemic uncertainty (blue region) is not necessary to be monotonically decreasing since we only use Eq. 8 as training regularizer rather than strict guarantee on predictions. The results are with good interpretability, in that driving system is typically quite sure about the accident risk level when the self-driving car is far from or almost being involved in an accident, while it is uncertain about it when accumulated accident cues are insufficient to make decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, to validate the effectiveness of the several main components, the following components are replaced or removed, and compared with our model based on best AP setting. (1) BNNs: The BNNs are replaced with vanilla FC layers. Note that in this case, L V POS − L P RI and our proposed ranking loss L RAN K in Eq. 10, as well as mAU are not applicable. (2) SAA: The SAA layer is removed so that L BC E in Eq. 10 is not used. (3) GCN: We replace GCNs with vanilla FC layers in Eq. 3 and Eq. 4. (4) Fusion: For this variant, the fusion in Eq. 3 and Eq. 4 are removed such that only Z t and GCN(X t , A t ) are used, respectively. (5) RankLoss: The epistemic uncertainty-based ranking loss is removed so that L RAN K in Eq. 10 is not applicable. Results are shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We can clearly see that the uncertainty-based ranking loss contributes most to our model by comparing variant (2)(6) with (1), with about 7.6% performance gain. Though the BNNs module leads to small performance gain, we attribute the benefit of BNNs to its derived uncertainty ranking loss as well as the interpretable results. Furthermore, the lowest mAU and highest AP for variant <ref type="bibr" target="#b0">(1)</ref> demonstrate that the learned relational features are of highest quality (smallest uncertainty) compared with other variants.  The results of variants (3) validate the effectiveness of our selfattention aggregation (SAA) layer, while the results of variant (4) validate the superiority GCN over naive FC layers. The results of variant <ref type="bibr" target="#b4">(5)</ref> show that the feature fusion between GCN outputs and hidden states, and the fusion between relational features and agentspecific features are important to accident anticipation, leading to approximately 7% performance gain.</p><p>Model Size Comparison. The number of network parameters are counted and reported in <ref type="table" target="#tab_5">Table 5</ref>. It shows that the proposed model is much light-weighted than DSA, and only slightly increases the model size when compared with other variants of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an uncertainty-based traffic accident anticipation with spatio-temporal relational learning. Our model can handle the challenges of relational feature learning and anticipation uncertainty from video data. Moreover, the introduced Bayesian formulation not only significantly boosts anticipation performance by using the uncertainty-based ranking loss, but also provides interpretation on predictive uncertainty. In addition, we release a Car Crash Dataset (CCD) for accident anticipation which contains rich environmental attributes and accident reason annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Framework of the proposed model. With graph embedded representations G(X t , A t ) at time step t, our model learns the latent relational representations Z t by the cyclic process of graph convolutional networks (GCNs) and recurrent neural network (RNN) cell, and predicts the accident score a t by Bayesian neural networks (BNNs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 ⋮Figure 4 :</head><label>14</label><figDesc>SAA Layer. First, all N ×T hidden states are gathered and pooled by max-avg concatenation. Then, the simplified self-attention and adaptive aggregation are proposed to predict video-level accident score a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>⋰Figure 5 :</head><label>5</label><figDesc>Annotation samples of our Car Crash Dataset (CCD). The gray box on top-left contains video-level annotations, while the other three white boxes provide instance-level annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Examples of our predictions on DAD datasets. The red curves indicate smoothed accident scores as observed frames increase. The ground truth (beginning time of accident) are labeled at 90-th frame. We plot one time of squared epistemic (blue region) and aleatoric uncertainties (wheat color region). The horizontal line indicates probability threshold 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>TTA with different recall rates on DAD dataset.Recall 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 DSA [4] 0.28 0.50 0.73 0.87 0.92 1.02 1.24 1.35 2.28 Ours 0.59 0.75 0.84 0.96 1.07 1.16 1.33 1.56 1.99</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies results on DAD dataset.</figDesc><table><row><cell cols="8">Variants BNNs SAA GCN Fusion RankLoss AP(%) mAU</cell></row><row><cell>(1)</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="2">72.22 0.0731</cell></row><row><cell>(2)</cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>70.38</cell><cell>-</cell></row><row><cell>(3)</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="2">67.34 0.1150</cell></row><row><cell>(4)</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>✓</cell><cell cols="2">67.10 0.1250</cell></row><row><cell>(5)</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell cols="2">65.50 0.1172</cell></row><row><cell>(6)</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell cols="2">64.60 0.0950</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Model size comparison. Our model variants (2), (4), and (5) are included for comparison. Unit M means a million.</figDesc><table><row><cell>Methods</cell><cell>DSA</cell><cell>Ours</cell><cell>v(2)</cell><cell>v(4)</cell><cell>v(5)</cell></row><row><cell># Params. (M)</cell><cell>4.40</cell><cell>1.97</cell><cell>1.66</cell><cell>1.97</cell><cell>1.90</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">CCD dataset is available at: https://github.com/Cogito2012/CarCrashDataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">DSA: https://github.com/smallcorgi/Anticipating-Accidents 3 MMDetection: https://github.com/open-mmlab/mmdetection</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank NVIDIA for GPU donation and Haiting Hao for organizing dataset collection. This research is supported by an ONR Award N00014-18-1-2875. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weight Uncertainty in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quasi-Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into High Quality Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anticipating Accidents in Dashcam Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Fu-Hsiang Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Traffic Risk Assessment: A Two-Stream Approach Using Dynamic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer and Robot Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transforming Neural-Net Output Levels to Probability Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical Variational Inference for Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational Graph Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (Workshop)</title>
		<meeting>Neural Information Processing Systems (Workshop)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty Quantification Using Bayesian Neural Networks in Classification: Application to Ischemic Stroke Lesion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchan</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Ho</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beom Joon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunghee Cho</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Feature Pyramid Networks for Object Detection</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<title level="m">A Critical Review of Recurrent Neural Networks for Sequence Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Activity Progression in LSTMs for Activity Detection and Early Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Future Event Prediction: If and When</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (Workshop)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (Workshop)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<editor>Junjie Bai, and Soumith Chintala</editor>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structured Sequence Modeling with Graph Convolutional Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Baptiste Lamare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Nguyen Anh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Traffic and Street Surveillance for Safety and Security</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05978</idno>
		<title level="m">Uncertainty Estimations by Softplus Normalization in Bayesian Convolutional Neural Networks with Variational Inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting Traffic Accidents with Event Recorder Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Takimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Kurashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGSPATIAL International Workshop on Prediction of Human Mobility</title>
		<meeting>ACM SIGSPATIAL International Workshop on Prediction of Human Mobility</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised Traffic Accident Detection in First-person Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><forename type="middle">M</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Han</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Hsiang</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
