<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Depth Learning in Challenging Indoor Video: Weak Rectification to Rescue</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision 3 TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision 3 TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision 3 TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision 3 TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision 3 TuSimple</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Depth Learning in Challenging Indoor Video: Weak Rectification to Rescue</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-view depth estimation using CNNs trained from unlabelled videos has shown significant promise. However, the excellent results have mostly been obtained in street-scene driving scenarios, and such methods often fail in other settings, particularly indoor videos taken by handheld devices, in which case the ego-motion is often degenerate, i.e., the rotation dominates the translation. In this work, we establish that the degenerate camera motions exhibited in handheld settings are a critical obstacle for unsupervised depth learning. A main contribution of our work is fundamental analysis which shows that the rotation behaves as noise during training, as opposed to the translation (baseline) which provides supervision signals. To capitalise on our findings, we propose a novel data pre-processing method for effective training, i.e., we search for image pairs with modest translation and remove their rotation via the proposed weak image rectification. With our pre-processing, existing unsupervised models can be trained well in challenging scenarios (e.g., NYUv2 dataset), and the results outperform the unsupervised SOTA by a large margin (0.147 vs. 0.189 in the AbsRel error).</p><p>Based on epipolar geometry [7-9], learning depth without requiring the ground-truth supervision has been explored. Garg et al. [18]  show that the single-view depth CNN can be trained from stereo image pairs with known baseline via photometric loss. Zhou et al. [19]  further explored the unsupervised framework and proposed to train the depth CNN from unlabelled videos. They additionally introduced a Pose CNN to estimate the relative camera pose between consecutive frames, and they still use photometric loss for supervision. Following that, a number of of unsupervised methods have been proposed, which can be categorised into stereo-based [20-23] and video-based [24-33], according to the type of training data. Our work follows the latter paradigm, since unlabelled videos are easier to obtain in real-world scenarios.</p><p>Unsupervised methods have shown promising results in driving scenes, e.g., KITTI [34]  and Cityscapes <ref type="bibr" target="#b34">[35]</ref>. However, as reported in <ref type="bibr" target="#b35">[36]</ref>, they usually fail in generic scenarios such as the Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inferring 3D geometry from 2D images is a long-standing problem in robotics and computer vision. Depending on the specific use case, it is usually solved by Structure-from-Motion <ref type="bibr" target="#b0">[1]</ref> or Visual SLAM <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Underpinning these traditional pipelines is searching for correspondences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> across multiple images and triangulating them via epipolar geometry <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> to obtain 3D points. Following the growth of deep learning-based approaches, Eigen et al. <ref type="bibr" target="#b9">[10]</ref> show that the depth map can be inferred from a single color image by a CNN, which is trained with the ground-truth depth supervisions captured by range sensors. Subsequently a series of supervised methods <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> have been proposed and the accuracy of estimated depth is progressively improved.</p><p>indoor scenes in NYUv2 dataset <ref type="bibr" target="#b36">[37]</ref>. For example, GeoNet <ref type="bibr" target="#b25">[26]</ref>, which achieves state-of-the-art performance in KITTI, is unable to obtain reasonable results in NYUv2. To this end, <ref type="bibr" target="#b35">[36]</ref> proposes to use optical flow as the supervision signal to train the depth CNN, and very recent <ref type="bibr" target="#b37">[38]</ref> uses optical flow for estimating ego-motion to replace the Pose CNN. However, the reported depth accuracy <ref type="bibr" target="#b37">[38]</ref> is still limited, i.e., 0.189 in terms of AbsRel-see also qualitative results in <ref type="figure">Fig. 3</ref>.</p><p>Our work investigates the fundamental reasons behind poor results of unsupervised depth learning in indoor scenes. In addition to the usual challenges such as non-Lambertian surfaces and low-texture scenes, we identify the camera motion profile in the training videos as a critical factor that affects the training process. To develop this insight, we conduct an in-depth analysis of the effects of camera pose to current unsupervised depth learning framework. Our analysis shows that (i) fundamentally the camera rotation behaves as noise to training, while the translation contributes effective gradients; (ii) the rotation component dominates the translation component in indoor videos captured using handheld cameras, while the opposite is true in autonomous driving scenarios.</p><p>To capitalise on our findings, we propose a novel data pre-processing method for unsupervised depth learning. Our analysis (described in Sec. <ref type="bibr">2.3)</ref> indicates that image pairs with small relative camera rotation and moderate translation should be favoured. Therefore, we search for image pairs that fall into our defined translation range, and we weakly rectify the selected pairs to remove their relative rotation. Note that the processing requires no ground truth depth and camera pose. With our proposed data pre-processing, we demonstrate that existing state-of-the-art (SOTA) unsupervised methods can be trained well in the challenging indoor NYUv2 dataset <ref type="bibr" target="#b36">[37]</ref>. The results outperform the unsupervised SOTA <ref type="bibr" target="#b37">[38]</ref> by a large margin (0.147 vs. 0.189 in the AbsRel error).</p><p>To summarize, our main contributions are three-fold:</p><p>• We theoretically analyze the effects of camera motion on current unsupervised frameworks for depth learning, and reveal that the camera rotation behaves as noise for training depth CNNs, while the translation contributes effective supervisions.</p><p>• We calculate the distribution of camera motions in different scenarios, which, along with the analysis above, helps to answer the question why it is challenging to train unsupervised depth CNNs from indoor videos captured using handheld cameras.</p><p>• We propose a novel method to select and weakly rectify image pairs for better training. It enables existing unsupervised methods to show competitive results with many supervised methods in the challenging NYUv2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analysis</head><p>We first overview the unsupervised framework for depth learning. Then, we revisit the depth and camera pose based image warping and demonstrate the relationship between camera motion and depth network training. Finally, we compare the statistics of camera motions in different datasets to verify the impact of camera motion on depth learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of video-based unsupervised depth learning framework</head><p>Following SfMLearner <ref type="bibr" target="#b18">[19]</ref>, plenty of video-based unsupervised frameworks for depth estimation have been proposed. SC-SfMLearner <ref type="bibr" target="#b32">[33]</ref>, which is the current SOTA framework, additionally constrains the geometry consistency over <ref type="bibr" target="#b18">[19]</ref>, leading to more accurate and scale-consistent results.</p><p>In this paper, we use SC-SfMLearner as our framework, and overview its pipeline in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward.</head><p>A training image pair (I a , I b ) is first passed into a weight-shared depth CNN to obtain the depth maps (D a , D b ), respectively. Then, the pose CNN takes the concatenation of two images as input and predicts their 6D relative camera pose P ab . With the predicted depth D a and pose P ab , the warping flow between two images is generated according to Sec. 2.2.</p><p>Loss. First, the main supervision signal is the photometric loss L P . It calculates the color difference in each pixel between I a with its warped position on I b using a differentiable bilinear interpolation <ref type="bibr" target="#b38">[39]</ref>. Second, depth maps are regularized by the geometric inconsistency loss L GC , where it enforces the consistency of predicted depths between different frames. Besides, a weighting mask M is derived from L GC to handle dynamics and occlusions, which is applied on L P to obtain the Forward Loss <ref type="figure">Figure 1</ref>: Overview of SC-SfMLearner <ref type="bibr" target="#b32">[33]</ref>. Firstly, in the forward pass, training images (I a , I b ) are passed into the network to predict depth maps (D a , D b ) and relative camera pose P ab . With D a and P ab , we obtain the warping flow between two views according to Eqn. 2. Secondly, given the warping flow, the photometric loss L P and the geometry consistency loss L GC are computed. Also, the weighting mask M is derived from L GC and applied over L P to handle dynamics and occlusions. Moreover, an edge-aware smoothness loss L S is used to regularize the predicted depth map. See <ref type="bibr" target="#b32">[33]</ref> for more details.</p><p>weighted L M P . Third, depth maps are also regularized by a smoothness loss L S , which ensures that depth smoothness is guided by the edge of images. Overall, the objective function is:</p><formula xml:id="formula_0">L = αL M P + βL S + γL GC ,<label>(1)</label></formula><p>where α, β, and γ are hyper-parameters to balance different losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Depth and camera pose based image warping</head><p>The image warping builds the link between networks and losses during training, i.e., the warping flow is generated by network predictions (depth and camera motion) in forward pass, and the gradients are back-propagated from the losses via the warping flow to networks. Therefore, we investigate the warping to analyze the camera pose effects on the depth learning, which avoids involving image content factors, such as illumination changes and low-texture scenes.</p><p>Full transformation. The camera pose is composed of rotation and translation components. For one point (u1, v1) in the first image that is warped to (u2, v2) in the second image. It satisfies:</p><formula xml:id="formula_1">K −1 d 2 u 2 v 2 1 = RK −1 d 1 u 1 v 1 1 + t,<label>(2)</label></formula><p>where d i is the depth of this point in two images and K is the 3x3 camera intrinsic matrix. R is a 3x3 rotation matrix and t is a 3x1 translation vector. We decompose the full warping flow and discuss each component below.</p><p>Pure-rotation transformation. If two images are related by a pure-rotation transformation (i.e., t = 0), based on Eqn. 2, the warping satisfies:</p><formula xml:id="formula_2">d 2 u 2 v 2 1 = KRK −1 d 1 u 1 v 1 1 ,<label>(3)</label></formula><p>where [KRK −1 ] is as known as the homography matrix H [8], and we have</p><formula xml:id="formula_3">u 2 v 2 1 = d 1 d 2 H u 1 v 1 1 = c h 11 h 12 h 13 h 21 h 22 h 23 h 31 h 32 h 33 u 1 v 1 1 ,<label>(4)</label></formula><p>where c = d1 d2 , standing for the depth relation between two views, is determined by the third row of the above equation, i.e., c = 1/(h 31 u 1 + h 32 v 1 + h 33 ). It indicates that we can obtain (u 2 , v 2 ) without d 1 . Specifically, solving the above equation, we have</p><formula xml:id="formula_4">u 2 = (h 11 u 1 + h 12 v 1 + h 13 )/(h 31 u 1 + h 32 v 1 + h 33 ) v 2 = (h 21 u 1 + h 22 v 1 + h 23 )/(h 31 u 1 + h 32 v 1 + h 33 ).<label>(5)</label></formula><p>This demonstrates that the rotational flow in image warping is independent to the depth, and it is only determined by K and R. Consequently, the rotational motion in image pairs cannot contribute effective gradients to supervise the depth CNN during training, even when it is correctly estimated. More importantly, if the estimated rotation is inaccurate 1 , noisy gradients will arise and harm the depth CNN in backpropagation. Therefore, we conclude that the rotational motion behaves as the noise to unsupervised depth learning.</p><p>Pure-translation transformation. A pure-translation transformation means that R is an identity matrix in Eqn. 2. Then we have</p><formula xml:id="formula_5">d 2 u 2 v 2 1 = d 1 u 1 v 1 1 + Kt = d 1 u 1 v 1 1 + f x 0 c x 0 f y c y 0 0 1 t 1 t 2 t 3 ,<label>(6)</label></formula><p>where (f x f y ) are camera focal lengths, and (c x , c y ) are principal point offsets. Solving the above equation, we have</p><formula xml:id="formula_6">   d 2 u 2 = d 1 u 1 + f x t 1 + c x t 3 d 2 v 2 = d 1 v 1 + f y t 2 + c y t 3 d 2 = d 1 + t 3 u 2 = (d 1 u 1 + f x t 1 + c x t 3 )/(d 1 + t 3 ) v 2 = (d 1 v 1 + f y t 2 + c y t 3 )/(d 1 + t 3 ).<label>(7)</label></formula><p>It shows that the translation vector t is coupled with the depth d 1 during the warping from (u 1 , v 1 ) to (u 2 , v 2 ). This builds the link between the depth CNN and the warping, so that gradients from the photometric loss can flow to the depth CNN via the warping. Therefore, we conclude that the translational motion provides effective supervision signals to depth learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distribution of decomposed camera motions in different scenarios</head><p>Inter-frame camera motions and warping flows. <ref type="figure">Fig. 2(a)</ref> shows the camera motion statistics on KITTI <ref type="bibr" target="#b33">[34]</ref> and NYUv2 <ref type="bibr" target="#b36">[37]</ref> datasets. KITTI is pre-processed by removing static images, as in done <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. We pick one image of every 10 frames in NYUv2, which is denoted as Original NYUv2. Then we apply the proposed pre-processing (Sec. 3) to obtain Rectified NYUv2. For all datasets, we compare the decomposed camera pose of their training image pairs w.r.t. the absolute magnitude and inter-frame warping flow 2 . Specifically, we compute the averaged warping flow of randomly sample points in the first image using the ground-truth depth and pose. For each point ( <ref type="figure">Fig. 2</ref>(a) shows that the rotational flow dominates the translational flow in Original NYUv2 but it is opposite in KITTI. Along with the conclusion in Sec. 2.2 that the depth is supervised by the translation while the rotation behaves as the noise, this answers the question why unsupervised depth learning methods that obtain state-of-the-art results in driving scenes often fail in indoor videos. Besides, the results on Rectified NYUv2 demonstrate that our proposed data pre-processing can address this issue.</p><formula xml:id="formula_7">u 1 , v 1 ) that is warped to (u 2 , v 2 ), the flow magnitude is (u 2 − u 1 ) 2 + (v 2 − v 1 ) 2 .</formula><p>Warping error sensitivity to depth error. Besides the above statistics, we investigate the relation between warping error and depth error. As the network is supervised via the warping, we expect the warping error (px) to be sensitive to depth errors. For investigation, we manually generate wrong depths for randomly sampled points and then analyze their warping errors in all datasets. <ref type="figure">Fig. 2(b)</ref> shows the results, which shows that the warping error in Original NYUv2 is about 5 times smaller than that in KITTI when the sampled points have the same relative error. This indicates another challenge in indoor videos against driving scenes. Indeed, the issue is due to the fact that the sensitivity will be KITTI <ref type="bibr" target="#b33">[34]</ref> Original NYUv2 <ref type="bibr" target="#b36">[37]</ref> Rectified NYUv2 R=0.25 • , T=0.99m R=2.28 • , T=0.05m R=0.68 • , T=0.24m  <ref type="figure">Figure 2</ref>: Camera motion statistics (a) and warping error sensitivity investigation (b). "Rectified" stands for the proposed pre-processing described in Sec. 3. In (a), the first row shows the averaged magnitude of camera poses, i.e., R for rotation and T for translation. The plot shows the distribution of decomposed warping flow magnitudes (px) over randomly sampled points. In (b), we manually generate wrong depths for randomly sampled points using the ground-truth depths for investigating the warping errors. Note different scale in vertical axis.</p><p>significantly decreased when the camera translation is small. Formally, when t is close to 0, based on Eqn. 7, we have:</p><formula xml:id="formula_8">u 2 = (f x t 1 + c x t 3 + d 1 u 1 )/(t 3 + d 1 ) ≈ d 1 u 1 /d 1 = u 1 v 2 = (f y t 2 + c y t 3 + d 1 v 1 )/(t 3 + d 1 ) ≈ d 1 v 1 /d 1 = v 1 .<label>(8)</label></formula><p>This causes the warping error hard to separate accurate/inaccurate depth estimates, confusing the depth CNN. We address this issue by translation-based image pairing (see Sec. 3.1). The results on Rectified NYUv2 demonstrate that the efficacy of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed data processing</head><p>The above analysis suggests that unsupervised depth learning frameworks favour image pairs those have small rotational and sufficient translational motions for training. However, unlike driving sequences, videos captured by handheld cameras tend to have more rotational while less translational motions, as shown in <ref type="figure">Fig. 2</ref>. In this section, we describe the proposed method to select image pairs with appropriate translation in Sec. 3.1, and reduce the rotation of selected pairs in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translation-based image pairing</head><p>For high frame rate videos (e.g., 30fps in NYUv2 <ref type="bibr" target="#b36">[37]</ref>), we first downsample the raw videos temporally to remove redundant images, i.e., extract one key frame from every m frames. Here, m = 10 is used in NYUv2. The resulting data is denoted as the Original NYUv2 in all experiments. Then, instead of only considering adjacent frames as a pair, we pair up each image with its following k frames We also let k = 10 in NYUv2 <ref type="bibr" target="#b36">[37]</ref>. For each image pair candidate, we compute the relative camera pose by searching for feature correspondences and using the epipolar geometry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. As the estimated pose is up-to-scale <ref type="bibr" target="#b7">[8]</ref>, we use the translational flow (i.e., as the same as in <ref type="figure">Fig. 2(a)</ref>) instead of absolute translation distance for pairing. No ground-truth data is required in the proposed method.</p><p>First, we generate correspondences by using SIFT <ref type="bibr" target="#b4">[5]</ref> features. Then we apply the ratio test <ref type="bibr" target="#b4">[5]</ref> and GMS <ref type="bibr" target="#b32">[33]</ref> to find good ones. Second, with the selected correspondences, we estimate the essential matrix using the five-point algorithm <ref type="bibr" target="#b41">[42]</ref> within a RANSAC <ref type="bibr" target="#b42">[43]</ref> framework, and then we recover the relative camera pose. Third, for each image pair candidate, we compute the averaged magnitude of translational flows overall all inlier correspondences, which is as the same as in <ref type="figure">Fig. 2(a)</ref>. Based on the distribution of warping flows on KITTI that is a good example for us, we empirically set the expected range as (10, 50) pixels. The out-of-range pairs are removed.</p><p>Although running Struecture-from-Motion (e.g., COLMAP <ref type="bibr" target="#b0">[1]</ref>) or VSLAM (e.g., ORB-SLAM <ref type="bibr" target="#b3">[4]</ref>) to compute relative camera poses is also possible, we argue that it is overkill for our problem. More importantly, these pipelines are often brittle, especially when processing videos with pure rotational motions and low-texture contents <ref type="bibr" target="#b43">[44]</ref>. Compared with them, our method does not require a 3D map, and hence avoiding issues such as incomplete reconstruction and tracking lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3-DoF weak rectification</head><p>In order to remove the rotational motion of selected pairs, we propose a weak rectification method. It warps two images to a common plane using the pre-computed rotation matrix R. Specifically, (i) we fist convert R to rotation vector r using Rodrigues formula <ref type="bibr" target="#b44">[45]</ref> to obtain half rotation vectors for two images (i.e., r 2 and − r 2 ), and then we convert them back to rotation matrices R 1 and R 2 . (ii) Given R 1 , R 2 , and camera intrinsic K, we warp images to a new common image plane according to Eqn. 5. Then in the common plane, we crop their overlapped rectangular regions to obtain the weakly rectified pairs. See the Matlab pseudo code in the supplementary material.</p><p>Compared with the standard rectification <ref type="bibr" target="#b45">[46]</ref>, our method only uses the rotation R for image warping and deliberately ignores the translation T, so our weakly rectified pairs have 3-DoF translational motions, while the rigorously rectified pairs have 1-DoF translational motions, i.e., corresponding points have identical vertical coordinates. The reason is that we have different input settings (i.e., temporal frames from arbitrary-motion videos vs. left and right images from two horizontal cameras) and different purposes (i.e., depth learning vs. stereo matching) with the latter.</p><p>On the one hand, due to the rigours 1-DoF requirement in stereo matching, the standard rectification <ref type="bibr" target="#b45">[46]</ref> suffers in forward-motion pairs, where the epipoles lie inside the image and cause heavy deformation, e.g., resulting in extremely large images <ref type="bibr" target="#b45">[46]</ref>. Although polar rectification <ref type="bibr" target="#b46">[47]</ref> can mitigate the issue to some extent, the results are still deformed. However, this issue is avoided in our 3-DoF weak rectification, as we do not constrain the translational motion. On the other hand, the rigorous 1-DoF rectification is indeed unnecessary for depth learning. For example, related methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> work well in KITTI videos, where image pairs have 3-DoF translational motions, and the results are comparable to methods those training on KITTI stereo pairs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Moreover, these methods show that the Pose CNN predicted 3-DoF translation is quite accurate, which even outperforms ORB-SALM [4] on short sequences (i.e., 5-frame segments).</p><p>Due to above reasons, we propose the 3-DoF weak rectification, which reduces the rectification requirement and more suits the unsupervised depth learning problem. In practice, we still let the Pose CNN to predict 6-DoF motions as all related works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, where we use the predicted 3-DoF rotational motion to compensate the rotation residuals (see <ref type="figure">Fig. 2</ref>) caused by the imperfect rectification, and use the predicted 3-DoF translational motion to help train the depth CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method, dataset, and metrics</head><p>Method. We use the updated SC-SfMLearner <ref type="bibr" target="#b32">[33]</ref>, publicly available on GitHub, as our unsupervised learning framework. Compared with the original version, it replaces the encoder of depth and pose CNNs with a ResNet-18 <ref type="bibr" target="#b47">[48]</ref> backbone to enable training from the Imagenet <ref type="bibr" target="#b48">[49]</ref> pre-trained model. Besides, to demonstrate that our proposed pre-processing is universal to different methods, we also experiment with Monodepth2 <ref type="bibr" target="#b28">[29]</ref> (ResNet-18 backbone) in ablation studies. For all methods, we use the default hyper-parameters, and train models for 50 epochs. NYUv2 depth dataset. The NYUv2 depth dataset <ref type="bibr" target="#b36">[37]</ref> is composed of indoor video sequences recorded by a handheld Kinect RGB-D camera at 640 × 480 resolution. The dataset contains 464 scenes taken from three cities. We use the officially provided 654 densely labeled images for testing, and use the rest 335 sequences (no overlap with testing scenes) for training (302) and validation <ref type="bibr" target="#b32">(33)</ref>. The raw training sequences contain 268K images. It is first downsampled 10 times to remove redundant frames, and then processed by using our proposed method, resulting in total 67K rectified image pairs. The images are resized to 320 × 256 resolution for training.</p><p>RGB-D 7 Scenes dataset. The dataset <ref type="bibr" target="#b56">[57]</ref> contains 7 scenes, and each scene contains several video sequences (500-1000 frames per sequence), which are captured by a Kinect camera at 640×480 <ref type="table">Table 1</ref>: Single-view depth estimation results on NYUv2 <ref type="bibr" target="#b36">[37]</ref>. As reported in <ref type="bibr" target="#b35">[36]</ref>, unsupervised methods like GeoNet <ref type="bibr" target="#b25">[26]</ref> fail to show reasonable results in this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Supervision Error ↓ Accuracy ↑ AbsRel Log10 RMS δ 1 δ 2 δ 3 Make3D <ref type="bibr" target="#b49">[50]</ref> 0.349 -1.214 0.447 0.745 0.897 Depth Transfer <ref type="bibr" target="#b50">[51]</ref> 0.349 0.131 1.21 ---Liu et al. <ref type="bibr" target="#b51">[52]</ref> 0.335 0.127 1.06 ---Ladicky et al. <ref type="bibr" target="#b52">[53]</ref> ---0.542 0.829 0.941 Li et al. <ref type="bibr" target="#b53">[54]</ref> 0.232 0.094 0.821 0.621 0.886 0.968 Roy et al. <ref type="bibr" target="#b54">[55]</ref> 0.187 0.078 0.744 ---Liu et al. <ref type="bibr" target="#b10">[11]</ref> 0.213 0.087 0.759 0.650 0.906 0.976 Wang et al. <ref type="bibr" target="#b55">[56]</ref> 0.220 0.094 0.745 0.605 0.890 0.970 Eigen et al. <ref type="bibr" target="#b11">[12]</ref> 0.158 -0.641 0.769 0.950 0.988 Chakrabarti et al. <ref type="bibr" target="#b12">[13]</ref> 0.149 -0.620 0.806 0.958 0.987 Laina et al. <ref type="bibr" target="#b13">[14]</ref> 0.127 0.055 0.573 0.811 0.953 0.988 Li et al. <ref type="bibr" target="#b14">[15]</ref> 0.143 0.063 0.635 0.788 0.958 0.991 DORN <ref type="bibr" target="#b15">[16]</ref> 0.115 0.051 0.509 0.828 0.965 0.992 VNL <ref type="bibr" target="#b16">[17]</ref> 0.108 0.048 0.416 0.875 0.976 0.994 Zhou et al. <ref type="bibr" target="#b35">[36]</ref> 0.208 0.086 0.712 0.674 0.900 0.968 Zhao et al. <ref type="bibr" target="#b37">[38]</ref> 0.189 0.079 0.686 0.701 0.912 0.978 Ours 0.147 0.062 0.536 0.804 0.950 0.986 <ref type="table">Table 2</ref>: Ablation studies on NYUv2 <ref type="bibr" target="#b36">[37]</ref>. Rectified stands for the proposed data processing. Note that Monodepth2 <ref type="bibr" target="#b28">[29]</ref> models often collapse when training from scratch, especially on original data.</p><p>Here, we report the results for their successful case.  <ref type="table">Table 3</ref>: Single-view depth estimation results on 7 Scenes <ref type="bibr" target="#b56">[57]</ref>. The model is pre-trained on NYUv2 <ref type="bibr" target="#b36">[37]</ref>, and on each scene, we fine-tune models for three epochs. As the training data is limited, the fine-tuning consumes less than 10 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenes</head><p>Training pairs Before Fine-tuning After Fine-tuning AbsRel Acc (δ 1 ) AbsRel Acc (δ 1 resolution. We follow the official train/test split for each scene. For training, we use the proposed preprocessing, and for testing, we simply extract one image from every 30 frames. We first pre-train the model on NYUv2 dataset, and then fine-tune the model on this dataset to demonstrate the universality of the proposed method.</p><p>Evaluation metrics. We follow previous methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to evaluate depth estimators. Specifically, we use the mean absolute relative error (AbsRel), mean log10 error (Log10), root mean squared error (RMS), and the accuracy under threshold (δ i &lt; 1.25 i , i = 1, 2, 3). As unsupervised methods cannot recover the absolute scale, we multiply the predicted depth maps by a scalar that matches the median with the ground truth, as done in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours GT Zhao et al. <ref type="bibr" target="#b37">[38]</ref> Figure 3: Qualitative comparison of single-view depth estimation on NYUv2 <ref type="bibr" target="#b36">[37]</ref>. More results are attached in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Comparing with the state-of-the-art (SOTA) methods. Tab. 1 shows the results on NYUv2 <ref type="bibr" target="#b36">[37]</ref>. It shows that our method outperforms previous unsupervised SOTA method <ref type="bibr" target="#b37">[38]</ref> by a large margin. <ref type="figure">Fig. 3</ref> shows the qualitative depth results. Note that NYUv2 dataset is so challenging that previous unsupervised methods such as GeoNet <ref type="bibr" target="#b25">[26]</ref> is unable to get reasonable results, as reported in <ref type="bibr" target="#b35">[36]</ref>. Besides, our method also outperforms a series of fully supervised methods <ref type="bibr">[11, 50-56, 12, 13]</ref>. However, it still has a gap between the SOTA supervised approach <ref type="bibr" target="#b16">[17]</ref>.</p><p>Ablation studies. Tab. 2 summarizes the results. First, for both SC-SfMLearner <ref type="bibr" target="#b32">[33]</ref> and Mon-odepth2 <ref type="bibr" target="#b28">[29]</ref>, training on our rectified data leads to significantly better results than on original data. It also demonstrates that the proposed pre-processing is independent to method chosen. Besides, note that the training is easy to collapse in original data, especially when starting from scratch. We here report the results for their successful case.</p><p>Generalization. Tab. 3 shows the depth estimation results on 7 Scenes dataset <ref type="bibr" target="#b56">[57]</ref>. It shows that our model can generalize to previously unseen data, and fine-tuning on a few new data can boost the performance significantly. This has huge potentials to real-world applications, e.g., we can quickly adapt our pre-trained model to a new scene.</p><p>Timing. It takes 25 (28) hours to train SC-SfMLearner <ref type="bibr" target="#b32">[33]</ref> models for 50 epochs on rectified (original) data, measured in a single 16GB NVIDIA V100 GPU. Learning curves are provided in the supplementary material, which show that our pre-processing enables faster convergence. The inference speed of models is about 210fps on 320 × 256 images in a NVIDIA RTX 2080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigate the degenerate motion in indoor videos, and theoretically analyze its impact on the unsupervised monocular depth learning. We conclude that (i) rotational motion dominates translational motion in videos taken by handheld devices, and (ii) rotation behaves as noises while translation contributes effective signals to learning. Moreover, we propose a novel data pre-processing method, which searches for modestly translational pairs and remove their relative rotation for effective training. Comprehensive results in different datasets and learning frameworks demonstrate the efficacy of proposed method, and we establish a new unsupervised SOTA performance in challenging indoor NYUv2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional details</head><p>Experimental details in <ref type="figure">Fig. 2</ref>. First, we follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> to pre-process KITTI <ref type="bibr" target="#b33">[34]</ref> dataset, where static frames that are manually labelled by Eigen et al. <ref type="bibr" target="#b9">[10]</ref> are removed from the raw video. The images are resized to 832 × 256. The accurate ground truth depth and camera poses are provided by a Velodyne laser scanner and a GPS localization system. Second, as the NYUv2 <ref type="bibr" target="#b36">[37]</ref> dataset does not provide the ground truth camera pose, we use the ORB-SLAM2 <ref type="bibr" target="#b3">[4]</ref> (RGB-D mode with the ground truth depth) to compute the camera trajectory. The image resolution is 640 × 480. We down-sample the raw videos by picking first image of every 10 times. Third, we randomly select one long sequence from the dataset for analysis. Given a sequence, we randomly sample 1, 000 valid points (with a good depth range) per image and compute their projection magnitudes <ref type="figure">(Fig. 2(a)</ref>) and projection errors ( <ref type="figure">Fig. 2(b)</ref>) using the ground truth. For box visualization, we randomly sample 1, 000 points that are collected from the entire sequence.</p><p>It demonstrates that training on our rectified data leads to better results and faster convergence, compared with the original dataset. Visualization of single-view depth estimation. <ref type="figure">Fig. 4</ref> shows more results on NYUv2 <ref type="bibr" target="#b36">[37]</ref>.</p><p>Visualization of rectification and fine-tuning effects. <ref type="figure">Fig. 5</ref> shows results. In NYUv2 <ref type="bibr" target="#b36">[37]</ref>, we train models on both original data and our rectified data. In 7 Scenes <ref type="bibr" target="#b56">[57]</ref>, we fine-tune the model that is pre-trained on NYUv2. The qualitative evaluation results demonstrate the efficacy and universality of our proposed pre-processing, and it demonstrates the generalization ability of pre-trained depth CNN in previously unseen scenes.</p><p>Visualization of depth and converted point cloud. <ref type="figure" target="#fig_2">Fig. 6</ref> shows the video screenshot. We predict depth using our trained model on one sequence (i.e., office) from 7 Scenes <ref type="bibr" target="#b56">[57]</ref>. The top shows the textured point cloud generated by the predicted depth map (bottom right) and source image (bottom left). The full video is attached along with this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours GT Zhao et al. <ref type="bibr" target="#b37">[38]</ref> Figure 4: More qualitative comparison of single-view depth estimation on NYUv2 <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUv2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Train. on original data Train. on rectified data GT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Scenes</head><p>Input Before fine-tuning After fine-tuning GT <ref type="figure">Figure 5</ref>: Qualitative results for ablation studies. In NYUv2 <ref type="bibr" target="#b36">[37]</ref>, we train models on both original data and our rectified data. In 7 Scenes <ref type="bibr" target="#b56">[57]</ref>, we fine-tune the model that is pre-trained on NYUv2. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Warping error with depth error</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Depth and point cloud visualization on 7 Scenes [57]. The top shows the textured point cloud generated by the predicted depth map (bottom right) with the source image (bottom left). The full video is also attached.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Related work<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> shows that the Pose CNN enables more accurate translation estimation than ORB-SALM<ref type="bibr" target="#b3">[4]</ref>, but its predicted rotation is much worse than the latter, as demonstrated in<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>.<ref type="bibr" target="#b1">2</ref> We first compute the rotational flow using Eqn. 5, and then we obtain the translational flow by subtracting the rotational flow from the overall warping flow. Here, the translational flow is also called residual parallax in<ref type="bibr" target="#b40">[41]</ref>, where it is used to compute depth from correspondences and relative camera poses.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Implementation details in Sec. <ref type="bibr" target="#b2">3</ref>. First, for computing the feature correspondence, we use the SIFT <ref type="bibr" target="#b4">[5]</ref> implementation by VLFeat library. The default parameters are used. Second, we use the built-in function in Matlab library to compute the essential matrix and relative camera pose. The maximum RANSAC <ref type="bibr" target="#b42">[43]</ref> iterations are 10K, and the inlier threshold is 1px. we use the following pseudo Matlab code to compute the weakly rectified images.  <ref type="bibr" target="#b36">[37]</ref>. "Rectified" stands for the proposed pre-processing, and "pt" stands for pre-training on ImageNet <ref type="bibr" target="#b48">[49]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monoslam: Real-time single camera slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Andrew J Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Nicholas D Molton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Determining the epipolar geometry and its uncertainty: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An evaluation of feature matchers for fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning for single view depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Competitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihuai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision meets Robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Moving indoor: Unsupervised video depth learning in challenging environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihuai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards better generalization: Joint depth-pose learning without posenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhi</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual odometry revisited: What should be learnt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mannequinchallenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual slam: Why bundle adjust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra Bustos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Introductory techniques for 3-D computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Prentice Hall Englewood Cliffs</publisher>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A compact algorithm for rectification of stereo pairs. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fusiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple and efficient rectification method for general motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
