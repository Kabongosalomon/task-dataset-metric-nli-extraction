<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Multi-Frame Monocular Scene Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Darmstadt 2 hessian.AI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Darmstadt 2 hessian.AI</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Multi-Frame Monocular Scene Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D scene flow from a sequence of monocular images has been gaining increased attention due to the simple, economical capture setup. Owing to the severe illposedness of the problem, the accuracy of current methods has been limited, especially that of efficient, real-time approaches. In this paper, we introduce a multi-frame monocular scene flow network based on self-supervised learning, improving the accuracy over previous networks while retaining real-time efficiency. Based on an advanced twoframe baseline with a split-decoder design, we propose (i) a multi-frame model using a triple frame input and convolutional LSTM connections, (ii) an occlusion-aware census loss for better accuracy, and (iii) a gradient detaching strategy to improve training stability. On the KITTI dataset, we observe state-of-the-art accuracy among monocular scene flow methods based on self-supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene flow estimation, that is the task of estimating 3D structure and 3D motion of a dynamic scene, has been receiving increased attention together with a growing interest and demand for autonomous navigation systems. Many approaches have been proposed, based on various input data such as stereo images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b67">68]</ref>, RGB-D sequences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>, or 3D point clouds <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>Recently, monocular scene flow approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b72">73]</ref> have shown the possibility of estimating 3D scene flow from a pair of temporally consecutive monocular frames only, obviating complicated, expensive sensor setups such as a stereo rig, RGB-D sensors, or a LiDAR scanner. Only a simple, affordable monocular camera is needed. The availability of ground-truth data has been another key challenge for scene flow estimation in general. To address this, methods based on self-supervised learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> have shown it possible to train CNNs for jointly estimating depth and scene flow without expensive 3D annotations. Yet, their accuracy is bounded by the limitation of only using two frames as input, their underlying proxy loss, and training instabilities due to the difficulty of optimizing CNNs for mul-} } 3D Scene Flow  <ref type="figure" target="#fig_2">Figure 1</ref>. Our multi-frame monocular scene flow approach inputs three frames and estimates depth and scene flow (visualized in 3D <ref type="bibr" target="#b77">[78]</ref> with scene flow color coding in Appendix A). tiple tasks, particularly in a self-supervised manner <ref type="bibr" target="#b35">[36]</ref>.</p><p>Semi-supervised methods have demonstrated promising accuracy by combining CNNs with energy minimization <ref type="bibr" target="#b6">[7]</ref> or sequentially estimating optical flow and depth to infer 3D scene flow <ref type="bibr" target="#b72">[73]</ref>. Those methods, however, do not reach real-time efficiency due to iterative energy minimization <ref type="bibr" target="#b6">[7]</ref> or the additional processing time from pre-computing depth and optical flow <ref type="bibr" target="#b72">[73]</ref> beforehand. Yet, computational efficiency is important for autonomous navigation applications.</p><p>In this paper, we introduce a self-supervised monocular scene flow approach that substantially advances the previously most accurate real-time method of Hur et al. <ref type="bibr" target="#b22">[23]</ref>, while keeping its advantages (e.g., computational efficiency and training on unlabeled data). We first analyze the technical design, revealing some limitations, and propose an improved two-frame backbone network to overcome them. Next, we introduce a multi-frame formulation that temporally propagates the estimate from the previous time step for more accurate and reliable results in consecutive frames. Previous monocular methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b72">73]</ref> utilize only two frames as input, which is a minimal setup for demonstrating the underlying ideas. In contrast, our approach is the first to demonstrate how to exploit multiple consecutive frames, which are naturally available in most real-world scenarios.</p><p>We make the following main contributions: (i) We uncover some limitations of the baseline architecture of <ref type="bibr" target="#b22">[23]</ref> and introduce an advanced two-frame basis with a split-decoder design. Contradicting the finding of <ref type="bibr" target="#b22">[23]</ref> on using a single joint decoder, our split decoder is not only faster and more stable to train, but delivers competitive accuracy.</p><p>(ii) Next, we introduce our multi-frame network based on overlapping triplets of frames as input and temporally propagating the previous estimate via a convolutional LSTM <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b58">59]</ref> (cf . <ref type="figure" target="#fig_2">Fig. 1</ref>). (iii) Importantly, we propagate the hidden states using forward warping, which is especially beneficial for handling occlusion; it is also more stable to train than using backward warping in a self-supervised setup. (iv) We propose an occlusion-aware census transform to take occlusion cues into account, providing a more robust measure for brightness difference as a self-supervised proxy loss. (v) Lastly, we introduce a gradient detaching strategy that improves not only the accuracy but also the training stability, which self-supervised methods for multi-task learning can benefit from. We successfully validate all our design choices through an ablation study.</p><p>Training on KITTI raw <ref type="bibr" target="#b10">[11]</ref> in a self-supervised manner, our model improves the accuracy of the direct baseline of <ref type="bibr" target="#b22">[23]</ref> by 15.4%. Owing to its self-supervised design, our approach also generalizes to different datasets. We can optionally perform (semi-)supervised fine-tuning on KITTI Scene Flow Training <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, where we also outperform <ref type="bibr" target="#b22">[23]</ref>, reducing the accuracy gap to semi-supervised methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b72">73]</ref> while remaining many times more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene flow. First introduced by Vedula et al. <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, scene flow aims to estimate dense 3D motion for each 3D point in the scene. Depending on the available input data, approaches differ in their objectives and formulations.</p><p>Stereo-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b76">77</ref>] estimate a disparity map between stereo pairs to recover the 3D scene structure as well as a dense 3D scene flow field between a temporal frame pair. Earlier work mainly uses variational formulations or graphical models, which yields limited accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> and/or slow runtime <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>. Recent CNN-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref> overcome these limitations: they attain state-of-the-art accuracy in real time by training CNNs on large synthetic datasets followed by fine-tuning on the target domain in a supervised manner. Un-/self-supervised approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b68">69]</ref> aim to overcome the dependency on accurate, diverse labeled data, which is not easy to obtain. Approaches using sequences of RGB-D images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> or 3D point clouds <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72]</ref> have been also proposed, exploiting an already given 3D sparse point input.</p><p>In contrast, our approach jointly estimates both 3D scene structure and dense 3D scene flow from a monocular image sequence alone, which is a more practical, yet much more challenging setup. Monocular scene flow. Estimating scene flow using a monocular image sequence has been gaining increased attention. Multi-task CNN approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref> jointly predict optical flow, depth, and camera motion from a monocular sequence; scene flow can be reconstructed from those outputs. However, such approaches have a critical limitation in that they cannot recover scene flow for occluded pixels. Brickwedde et al. <ref type="bibr" target="#b6">[7]</ref> propose to combine CNNs for monocular depth prediction with an energy-based formulation for estimating scene flow from the given depth cue. Yang et al. <ref type="bibr" target="#b72">[73]</ref> introduce an integrated pipeline that obtains scene flow from given optical flow and depth cues via determining motion in depth from observing changes in object sizes. Using a single network, Hur et al. <ref type="bibr" target="#b22">[23]</ref> directly estimate depth and scene flow with a joint decoder design, trained in a self-supervised manner.</p><p>All above methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b72">73]</ref> are limited to using two frames. In contrast, we demonstrate how to leverage multiple consecutive frames for more accurate and consistent results, which is desirable in real applications. Multi-frame estimation. Multi-frame approaches to optical flow typically exploit a constant velocity or acceleration assumption <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">55]</ref> to encourage temporally smooth and reliable estimates. Using CNNs, propagation approaches have shown how to exploit previous predictions for the current time step, either by explicitly fusing the two outputs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b52">53]</ref> or using them as input for the current estimation <ref type="bibr" target="#b45">[46]</ref>. Better temporal consistency has also been achieved using a bi-directional cost volume <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref> and convolutional LSTMs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. Overall, these multi-frame approaches improve the optical flow accuracy, especially for occluded or out-of-bound areas.</p><p>For scene flow (stereo or RGB-D based), relatively few multi-frame methods have been introduced so far, all using classical energy minimization. A consistent rigid motion assumption has been proposed for temporal consistency <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b64">65]</ref>. Other approaches include jointly estimating camera pose and motion segmentation <ref type="bibr" target="#b61">[62]</ref>, matching and visibility reasoning among multiple frames <ref type="bibr" target="#b57">[58]</ref>, or an integrated energy formulation <ref type="bibr" target="#b14">[15]</ref>. These methods are robust against outliers and occlusion, improving the accuracy; yet, their runtime is slow due to iterative energy minimization.</p><p>We introduce a CNN-based multi-frame scene flow approach in the challenging monocular setup, ensuring realtime efficiency. Building on the two-frame network of <ref type="bibr" target="#b22">[23]</ref>, our method utilizes a bi-directional cost volume with convolutional LSTM connections, ensuring temporal consistency through overlapping frame triplets and temporal propagation of intermediate outputs (cf . <ref type="figure" target="#fig_2">Fig. 1</ref>). Moreover, we propose a forward-warping strategy for LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-frame Monocular Scene Flow</head><p>Given N temporally consecutive frames, {I t−(N −2) , I t−(N −1) , ..., I t , I t+1 }, our main objective is to estimate 3D surface points P = (P x , P y , P z ) for each pixel p = (p x , p y ) in the reference frame I t and the 3D scene flow s = (s x , s y , s z ) of each 3D point to the target frame I t+1 . <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Refined backbone architecture</head><p>Advanced two-frame baseline. Our network architecture is based on the integrated two-frame network of Hur et al. <ref type="bibr" target="#b22">[23]</ref>, which uses PWC-Net <ref type="bibr" target="#b60">[61]</ref> as a basis and runs in real time. The network constructs a feature pyramid for each input frame, calculates the cost volume, and estimates the residual scene flow and disparity with a joint decoder over the pyramid levels. While maintaining the core backbone, we first investigate whether recent advances in selfsupervised optical flow can be carried over to monocular 3D scene flow.</p><p>Jonschkowski et al. <ref type="bibr" target="#b27">[28]</ref> systematically analyze the key factors for highly accurate self-supervised optical flow, identifying crucial steps such as cost volume normalization, level dropout, data distillation, using a square resolution, etc. While we do not aim for a comprehensive review of such factors in the context of monocular scene flow, we performed a simple empirical study of their key findings. <ref type="bibr" target="#b0">1</ref> We found cost volume normalization and using one less pyramid level (i.e. 6 instead of 7) to be helpful, and employ them for our advanced baseline. Other findings were less effective for monocular scene flow; hence we do not adopt them.</p><p>Moreover, we observed that the context network, a postprocessing module with dilated convolutions <ref type="bibr" target="#b60">[61]</ref>, is a source of training instability in the self-supervised setup. <ref type="bibr" target="#b0">1</ref> We thus discard the context network for stable convergence.</p><p>Split-decoder design. We further probe the decoder design in detail and introduce a split-decoder model that converges faster and more stably. Hur et al. <ref type="bibr" target="#b22">[23]</ref> propose to use a single decoder (cf . <ref type="figure" target="#fig_0">Fig. 2a</ref>) that jointly predicts both scene flow and disparity based on the observation that separating the decoder for each task leads to balancing issues. This can result in a trivial prediction for the disparity (e.g., outputting a constant value for all pixels). However, we observe that this issue mainly stems from the context network, which we discard (see above) due to stability concerns.</p><p>After discarding the context network ( <ref type="figure" target="#fig_0">Fig. 2b</ref>), we find a better decoder configuration. We gradually split the decoder starting from the last layer into two separate decoders for each task and compare the scene flow accuracy in the experimental setting of <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_0">Table 1</ref> reports the result (lower is better). Discarding the context network degrades the accuracy by 4.1%, but in the end, splitting the decoder at the 2 nd -to-last layer yields an accuracy competitive to the one of <ref type="bibr" target="#b22">[23]</ref>. We choose this configuration (i.e. <ref type="figure" target="#fig_0">Fig. 2c</ref>) as our decoder design. Our findings suggest that the conclusions of <ref type="bibr" target="#b22">[23]</ref> regarding the decoder design only hold in the presence of a context network. The benefit of our split decoder is that competitive accuracy is achieved more stably and in fewer training iterations (at 56% of the full training schedule), with a lighter network (∼ 10% fewer parameters). 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-frame estimation</head><p>Three-frame estimation. Toward temporally consistent estimation over multiple frames, we first utilize three frames at each time step <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="figure" target="#fig_7">Fig. 3</ref> illustrates our network for multi-frame estimation in detail. For simplicity, we only visualize one pyramid level (i.e. the dashed square in <ref type="figure" target="#fig_7">Fig. 3</ref>), noting that we iterate this across all pyramid levels. Given the feature maps from each frame at times t − 1, t, and t + 1, the forward cost volume (from t to t + 1) and the backward cost volume (from t to t − 1) are calculated from the correlation layer and fed into the decoder that estimates the forward scene flow s f t and disparity d f t . The remaining inputs of the decoders are the feature map x feat t from the encoder, upsampled estimates, and the hidden states in the convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b58">[59]</ref> module, see below, both from the previous pyramid level. For backward scene flow s b t with disparity d b t , the same decoder with shared weights is used by switching the order of the inputs. We average the two disparity predictions for the final estimate, i.e. d t = (d f t + d b t )/2, as they correspond to the same view and should be consistent forward and backward in time.</p><p>LSTM with forward warping. To further encourage temporal consistency, we employ a convolutional LSTM <ref type="bibr" target="#b58">[59]</ref> in the decoder so that it can temporally propagate the hidden state across overlapping frame triplets (cf . <ref type="figure" target="#fig_2">Fig. 1</ref>) and implicitly exploit the previous estimates for the current time step. <ref type="figure" target="#fig_14">Fig. 5</ref> shows our decoder in detail, visualizing only the forward scene flow case s f t,l at pyramid level l for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation from the previous level</head><p>Forward Cost Vol.      Inside the decoder, we place the ConvLSTM module right before splitting into two separate decoders so that we can temporally propagate the joint intermediate representation of scene flow and depth. The ConvLSTM is fed the feature map from previous layers, as well as cell state c warp t−1,l and hidden state h warp t−1,l , both from the previous time step at the same pyramid level l. The module outputs the current cell state c t,l and hidden state h t,l , which are fed into the subsequent split decoder that outputs residual scene flow and (non-residual) disparity, respectively. We use a leaky ReLU activation instead of tanh in the ConvLSTM, aiding faster convergence in our case.</p><formula xml:id="formula_0">= " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q S b z O w f o A 2 R + 7 m V w z E / s 3 N M b l m l N Y S w S u y Q N V K L l 6 V / O I C Z p S C M g H E v Z t 6 0 E 3 A w L Y I T T v O a k k i a Y j P G Q 9 h W N c E i l m 0 2 f y I 1 D p Q y M I B a q I j C m 6 u + J D I d S T k J f d R Y n y 3 m v E P / z + i k E l 2 7 G o i Q F G p H Z o i D l B s R G k Y g x Y I I S 4 B N F M B F M 3 W q Q E R a Y g</formula><formula xml:id="formula_1">K U G x A b R S L G g A l K g E 8 U w U Q w d a t B R l h g A i q 3 m g r B n n 9 5 k X R O T f v c t G 7 O G k 2 z j K O K 6 u g A H S E b X a A m u k Y t 1 E Y E P a J n 9 I r e t C f t R X v X P m a t F a 2 c 2 U d / o H 3 + A F Y B m H U = &lt; / l a t e x i t &gt; h f t,l 1</formula><p>Importantly, we forward-warp the previous states (i.e. c t−1,l and h t−1,l ) using the estimated scene flow at the previous time step s f t−1 so that the coordinates of the states properly correspond. Without warping, each pixel in the current frame will attend to the previous state from mismatched pixels, which does not ensure proper propagation of corresponding states. Using backward warping based on backward scene flow at the previous pyramid level s b t,l−1 may also be possible, but exhibits a challenge: using backward flow (at l−1) to warp the previous states to update itself (at l), which is not easy if the initial estimate is unreliable.</p><p>As an example, <ref type="figure" target="#fig_12">Fig. 4</ref> shows the forward-warping result (I warp t−1 , <ref type="figure" target="#fig_12">Fig. 4c</ref>) of the previous frame I t−1 <ref type="figure" target="#fig_12">(Fig. 4a)</ref>, which matches the current frame I t <ref type="figure" target="#fig_12">(Fig. 4b</ref>) well. When a pixel p 1 moves to p 2 in the next frame, the pixel p 2 in I t should       attend to the previous state of the corresponding pixel in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>j V U d J X A K z s A F 8 I A P W u A G t E E H E C D B A 3 g C z 0 7 m P D o v z u t y d M 1 Z 7 Z y A H 3 D e P g G S J J I D &lt; / l a t e x i t &gt;</head><formula xml:id="formula_2">V 6 K B x X D h R u p u O F Q Y 1 h M i p F G / c = " &gt; A A A C C X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 q C E R R Y 8 F L x 4 r 2 A 9 o Y 9 l s N + 3 S z S b s T t Q S c v X i X / H i Q R G v / g N v / h s 3 b Q / a + m D g 8 d 4 M M / P 8 W H A N j v N t L S w u L a + s F t a K 6 x u b W 9 u l n d 2 G j h J F W Z 1 G I l I t n 2 g m u G R 1 4 C B Y K 1 a M h L 5 g T X 9 4 m f v N O 6 Y 0 j + Q N j G L m h a Q v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e c A p A S N 1 S 7 g T E h j 4 Q T r I b j v A H i A N K v d E x V k 3 h W O 3 I r J u q e z Y z h h 4 n r h T U k Z T 1 L q l r 0 4 v o k n I J F B B t G 6 7 T g x e S h R w K l h</head><formula xml:id="formula_3">Q = " &gt; A A A C C X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 q C E R R Y 8 F L x 4 r 2 A 9 o Y 9 l s N + 3 S z S b s T t Q S c v X i X / H i Q R G v / g N v / h s 3 b Q / a + m D g 8 d 4 M M / P 8 W H A N j v N t L S w u L a + s F t a K 6 x u b W 9 u l n d 2 G j h J F W Z 1 G I l I t n 2 g m u G R 1 4 C B Y K 1 a M h L 5 g T X 9 4 m f v N O 6 Y 0 j + Q N j G L m h a Q v e c A p A S N 1 S 7 g T E h j 4 Q U q z 2 w 6 w B 0 i D y j 1 R c d Z N 4 d i t i K x b K j u 2 M w a e J + 6 U l N E U t W 7 p q 9 O L a B I y C V Q Q r d u u E 4 O X E g W c C p Y V O 4 l m M a F D 0 m d t Q y U J m f b S 8 S c Z P j R K D w e R M i U B j 9 X f E y k J t R 6 F v u n M 7 9 a z X i 7 + 5 7 U T C C 6 8 l M s 4 A S b p Z F G Q C A w R z m P B P a 4 Y B T E y h F D F z a 2 Y D o g i F E x 4 R R O C O / v y P G m c 2 O 6 Z 7 V y f l q v 2 N I 4 C 2 k c H 6 A i 5 6 B x V 0 R W q o T q i 6 B E 9 o 1 f 0 Z j 1 Z L 9 a 7 9 T F p X b C m M 3 v o D 6 z P H y K m m o g = &lt; / l a t e x i t &gt; c f,warp t 1,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C w p Q v J 9 D e V U 0 U h g D p O U 9 K r T X E P c = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V</formula><formula xml:id="formula_4">I t−1 , i.e. h t−1,l (p 1 ).</formula><p>To do so, we forward-warp the previous states using the estimated scene flow and disparity. Furthermore, we use a validity mask M to filter out states from mismatched pixels based on the affinity score of CNN feature vectors from corresponding pixels:</p><formula xml:id="formula_5">h warp t−1,l = f w (h t−1,l )M (x feat t−1 , x feat t ) (1a) c warp t−1,l = f w (c t−1,l )M (x feat t−1 , x feat t ) (1b) with M (x feat t−1 , x feat t ) = (conv 1×1 (f w (x feat t−1 )·x feat t ) &gt; 0.5), (1c)</formula><p>where f w is the forward-warping operation with (implicitly) given estimated scene flow and disparity. To generate the per-pixel mask M , we forward-warp the previous (normalized) feature map (x feat t−1 ), dot-product with the current (normalized) feature map (x feat t ) to calculate the affinity score, pass it through a 1 × 1 convolution layer, and apply a fixed threshold. Here, the 1 × 1 convolution is used to learn to scale the affinity score before thresholding.</p><p>For forward warping, we adopt the softmax splatting strategy of Niklaus et al. <ref type="bibr" target="#b47">[48]</ref>, which resolves conflicts between multiple pixels mapped into the same pixel location when forward-warping. In our case, we utilize the estimated disparity as a cue to compare the depth orders, determine visible pixels, and preserve their hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-supervised loss</head><p>Given the scene flow and disparity estimates over the multiple frames, we apply a self-supervised loss on each pair of temporally neighboring estimates that establish a bi-directional relationship. This allows us to exploit occlusion cues. As shown in <ref type="figure" target="#fig_18">Fig. 6</ref>, given the estimates for two time steps as</p><formula xml:id="formula_6">{s f t−1 , s b t−1 , d t−1 } and {s f t , s b t , d t }, we apply the proxy loss to {s f t−1 , d t−1 , s b t , d t }.</formula><p>We adopt the selfsupervised loss from Hur et al. <ref type="bibr" target="#b22">[23]</ref>, which consists of a view synthesis loss and a 3D reconstruction loss, guiding the disparity and scene flow output to be consistent with the given input images. The total self-supervised loss is a weighted sum of disparity loss L d and scene flow loss L sf ,     The main difference to <ref type="bibr" target="#b22">[23]</ref> is that we newly propose an occlusion-aware census loss for penalizing the photometric difference. We only introduce our novel contribution here and provide details on the losses from <ref type="bibr" target="#b22">[23]</ref> with our modifications in Appendix C.</p><formula xml:id="formula_7">L total = L d + λ sf L sf .<label>(2)</label></formula><formula xml:id="formula_8">D q o X o t y 0 k o K Y = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 1 g E N 5 Z E F F 0 W 3 b i s Y B / Q x D C Z T t</formula><formula xml:id="formula_9">U = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y E y y C q 5 K I o s u i G 5 c V 7 A O a G C b T S T t 0 M g k z N 2 I J A X / F j Q t F 3 P o d 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 2 D b 3 0 Z l a X l l d a 2 6 X t v Y 3 N r e M X f 3 O i p O J a F t E v N Y 9 g K s K G e C t o E B p 7 1 E U h w F n H a D 8 X X h d x + o V C w W d z B J q B f h o W A h I x i 0 5 J s H b o R h F I S Z y u 9 d o I + Q B b k P v l m 3 G / Y U 1 i J x S l J H J V q + + e U O Y p J G V A D h W K m + Y y f g Z V g C I 5 z m N T d V N M F k j I e 0 r 6 n A E V V e N o 2 f W 8 d a G V h h L P U T Y E 3 V 3 x s Z j p S a R I G e L M K q e a 8 Q / / P 6 K Y S X X s Z E k g I V Z H Y o T L k F s V V 0 Y Q 2 Y p A T 4 R B N M J N N Z L T L C E h P Q j d V 0 C</formula><formula xml:id="formula_10">i G 5 c V 7 A O a G C b T S T t 0 M g k z N 2 I J A X / F j Q t F 3 P o d 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 2 D b 3 0 Z l a X l l d a 2 6 X t v Y 3 N r e M X f 3 O i p O J a F t E v N Y 9 g K s K G e C t o E B p 7 1 E U h w F n H a D 8 X X h d x + o V C w W d z B J q B f h o W A h I x i 0 5 J s H b o R h F I S Z y u 9 d o I + Q h b k P v l m 3 G / Y U 1 i J x S l J H J V q + + e U O Y p J G V A D h W K m + Y y f g Z V g C I 5 z m N T d V N M F k j I e 0 r 6 n A E V V e N o 2 f W 8 d a G V h h L P U T Y E 3 V 3 x s Z j p S a R I G e L M K q e a 8 Q / / P 6 K Y S X X s Z E k g I V Z H Y o T L k F s V V 0 Y Q 2 Y p A T 4 R B N M J N N Z L T L C E h P Q j d V</formula><formula xml:id="formula_11">R i X T n b E l 8 / A b t A x o k R 4 S v s T M I u I = " &gt; A A A B + H i c b V D L S g N B E J y N r x g f W f X o Z T A I n s K u K H o M e t B j B P O A 7 L L M T m a T I b M P Z n r E u O R L v H h Q x K u f 4 s 2 / c Z L s Q R M L G o q q b r q 7 w k x w B Y 7 z b Z V W V t f W N 8 q b l a 3 t n d 2 q v b f f V q m W l L V o K l L Z D Y l i g i e s B R w E 6 2 a S k T g U r B O O r q d + 5 4 F J x d P k H s Y Z 8 2 M y S H j E K Q E j B X b V U z r G U e A B e 4 T 8 Z h L Y N a f u z I C X i V u Q G i r Q D O w v r 5 9 S H b M E q C B K 9 V w n A z 8 n E j g V b F L x t G I Z o S M y Y D 1 D E x I</formula><p>Occlusion-aware census transform. Carefully designing the proxy loss function matters for the accuracy of selfsupervised learning <ref type="bibr" target="#b27">[28]</ref>. For penalizing the photometric difference for the view-synthesis proxy task, the census transform <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b75">76]</ref> has demonstrated its robustness to illumination changes, e.g., in outdoor scenes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">66]</ref>. The conventional (ternary) census transform computes the local census patch <ref type="figure" target="#fig_13">(Fig. 7a</ref>) and calculates the Hamming distance between them to evaluate the brightness difference <ref type="figure" target="#fig_13">(Fig. 7b</ref>). However, it is vulnerable to outlier pixels (e.g., occlusions) present in the patch, yielding a higher distance. Taking into account the occlusion cue, we introduce an occlusion-aware census transform between images I andĨ, which calculates the Hamming distance only for visible pixels: </p><formula xml:id="formula_12">(I(p + y) − I(p)) 2 + σ 2 T (3b) f G (t 1 , t 2 ) = (t 1 − t 2 ) 2 (t 1 − t 2 ) 2 + σ G ,<label>(3c)</label></formula><p>where σ G = 0.1 and σ T = 0.9. To facilitate differentiability, T (I, p, y) in Eq. (3b) calculates a continuous approximation to the ternary value at pixel p with an offset y in image I. f G in Eq. (3c) is the Geman-McClure function <ref type="bibr" target="#b5">[6]</ref> that scores the difference of the two input ternary values. As shown in <ref type="figure" target="#fig_13">Fig. 7c</ref>, our occlusion-aware formulation can prevent from having a high score caused by occlusions in the census patch, thus providing a measure for the brightness difference that is more robust against outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Improving the training stability</head><p>As shortly discussed in Sec. 3.1, discarding the context network improves the training stability. However, we found that integrating a ConvLSTM module <ref type="bibr" target="#b58">[59]</ref> may still yield unstable training, resulting in trivial disparity predictions in the early stages of training.</p><p>To resolve the issue, we propose to detach the gradient from the scene flow loss (L sf in Eq. (2)) to the disparity decoder in the early stages of training so that each split decoder focuses on its own task first. We conjecture that the gradient back-propagated from the scene flow loss to the disparity decoder strongly affects the disparity estimate, yielding a trivial prediction in the end. To prevent the scene flow from dominating, we detach the gradients, but only for the first 2 epochs of the training schedule as illustrated in <ref type="figure" target="#fig_20">Fig. 8a</ref>, and then continue to train in the normal setting. <ref type="figure" target="#fig_20">Figures 8b and 8c</ref> demonstrate the effect of detaching the gradient in terms of the training loss and the scene flow outlier rate. Without detachment, the model outputs a constant disparity map in the early stage of training, thus yields higher scene flow error rates. In contrast, applying our gradient detaching strategy demonstrates faster and stable convergence, with much better accuracy (39.82% vs. 49.69%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>For a fair comparison with the most closely related prior work <ref type="bibr" target="#b22">[23]</ref>, we use the same dataset (i.e. KITTI raw <ref type="bibr" target="#b10">[11]</ref>) and the same training protocol, assuming a fixed stereo baseline. We use the KITTI Split <ref type="bibr" target="#b12">[13]</ref> by splitting the 32 scenes total into 25 scenes for training and the remaining 7 for validation. Unlike <ref type="bibr" target="#b22">[23]</ref>, we divide the training/validation split at the level of entire scenes in order to exploit more continuous frames for our multi-frame setup and completely remove possible overlaps between the two splits. Then, we evaluate our model on KITTI Scene Flow Training <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, using the provided scene flow ground truth. Note that KITTI Split and KITTI Scene Flow Training do not overlap. After our self-supervised training on KITTI Split, we optionally fine-tune our model on KITTI Scene Flow Training in a semi-supervised manner and compare with previous stateof-the-art monocular scene flow methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>Given that we use the network of <ref type="bibr" target="#b22">[23]</ref> as the basis, we use the same augmentation schemes and training configurations (e.g., learning rate, training schedule, optimizer, etc.), except for the following changes. For training, we use one sequence of 5 temporally consecutive frames as a mini-batch. To ensure training stability, we detach the gradient between the scene flow loss and the disparity decoder during the first 2 epochs, as discussed in Sec. 3.4. 2,3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>We conduct a series of ablations to study the accuracy gain from our contributions over the two-frame baseline. We use our multi-frame train split of KITTI Split and evaluate on KITTI Scene Flow Training <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> using the scene flow evaluation metric. The metric reports the outlier rate (in %, lower is better) among pixels with ground truth; a pixel is regarded an outlier if exceeding a threshold of 3 pixels or 5% w.r.t. the ground-truth disparity or motion. After evaluating the outlier rate of the disparity (D1-all), disparity change (D2-all), and optical flow (Fl-all), the scene flow outlier rate (SF-all) is obtained by checking if a pixel is an outlier on either of them. Advanced two-frame baseline. In <ref type="table" target="#tab_2">Table 2</ref>, we first conduct an ablation study of our advanced baseline described in Sec. <ref type="bibr" target="#b2">3</ref>   <ref type="table">Table 3</ref>. Ablation study of our main contributions: Our key contributions (multi-frame extension, occlusion-aware census loss) consistently improve the accuracy over our advanced baseline. significantly drops by 29.4% (relative change) compared to training on their data split. This difference comes down to our train split containing less diverse scenes, which suggests that <ref type="bibr" target="#b22">[23]</ref> may be somewhat sensitive to the choice of training data. Simply removing the context network already significantly improves the accuracy up to 17.1% by improving the training stability, almost closing the gap. As discussed in Sec. 3.1, we follow <ref type="bibr" target="#b27">[28]</ref> and use one less pyramid level and feature normalization, which further improves the accuracy, even slightly beyond that of the original baseline trained on the split of <ref type="bibr" target="#b22">[23]</ref> (covering more diverse scenes).</p><p>Major contributions. In <ref type="table">Table 3</ref>, we validate our major contributions, i.e. multi-frame estimation (Sec. 3.2) and the occlusion-aware census transform (Sec. 3.3), compared to our advanced two-frame baseline. Both contributions yield a relative accuracy improvement of 9.3% (multi-frame extension) and 6.1% (occlusion-aware census transform) over the baseline. Overall, our final model achieves 14.7% more accurate results than our baseline. Note that our advanced baseline already significantly outperforms the original implementation of <ref type="bibr" target="#b22">[23]</ref> on our train split by 23.3%.</p><p>Multi-frame extension. We further ablate the technical details of our multi-frame estimation in <ref type="table" target="#tab_4">Table 4</ref>. Inputting bidirectional cost volumes to the decoder (i.e. three-frame estimation) only marginally improves the accuracy. Adding a ConvLSTM that temporally propagates the hidden states (without using warping) only shows limited improvement as well. Using backward warping in the LSTM for temporal propagation with backward flow, interestingly, degrades the accuracy, possibly due to using less accurate backward flow from the previous level to warp the hidden states.   <ref type="table">Table 5</ref>. Ablation study of the occlusion-aware census transform: Our occlusion-aware census loss provides a more discriminative proxy by taking occlusion cues into account.</p><p>However, when propagating the hidden states with forward warping with the estimated scene flow from the previous frame, we observe a significant relative accuracy improvement of up to 9.3% compared to two-frame estimation. Notably, this is the only setting in which there is a clear gain from going to more than two frames. This highlights the importance of propagating the hidden states and choosing a suitable warping method inside the ConvLSTM. Occlusion-aware census transform. Lastly in <ref type="table">Table 5</ref>, we compare our occlusion-aware census transform to the standard census transform and the widely used basic photometric loss consisting of brightness difference and SSIM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. We conduct this experiment on top of our multiframe architecture. Our occlusion-aware census further improves the scene flow accuracy by 6.0% (relative improvement) over the basic photometric loss and by 4.5% over the standard census transform <ref type="bibr" target="#b27">[28]</ref>. <ref type="table">Table 6</ref> compares our method with state-of-the-art monocular scene flow methods on KITTI Scene Flow Training. Our multi-frame architecture achieves the best scene flow accuracy among monocular methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b73">74]</ref> based on purely self-supervised learning. Our contributions yield a relative improvement over the direct baseline of [23] of 15.4%. Also note that our method closes a substantial part of the gap to the semi-supervised method of <ref type="bibr" target="#b6">[7]</ref>, but is significantly faster. Despite using multiple frames and having better accuracy, the runtime of our approach per time step is actually notably shorter (0.063s) than the direct baseline <ref type="bibr" target="#b22">[23]</ref> (0.09s), benefiting from removing the context network and using one less pyramid level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Monocular scene flow</head><p>We further evaluate our model on the KITTI Scene Flow benchmark and compare with monocular scene flow approaches based on self-or semi-supervised learning in Ta-  <ref type="table">Table 7</ref>. KITTI 2015 Scene Flow Test <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>: Our method consistently outperforms <ref type="bibr" target="#b22">[23]</ref> and moves closer to the accuracy of semi-supervised methods with significantly faster runtime. ble 7. Our model consistently outperforms <ref type="bibr" target="#b22">[23]</ref>. Optionally fine-tuning (-ft) with 200 annotated pairs, our approach reduces the gap to semi-supervised methods that use a large amount of 3D LiDAR data <ref type="bibr" target="#b6">[7]</ref> or multiple synthetic datasets for optical flow <ref type="bibr" target="#b72">[73]</ref>. Yet, our model offers significantly faster runtime (e.g., 650× faster than <ref type="bibr" target="#b6">[7]</ref> and 4× faster than <ref type="bibr" target="#b72">[73]</ref>). Our model can thus exploit available labeled datasets for accuracy gains while keeping the same runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Temporal consistency</head><p>We evaluate the temporal consistency of our model, comparing to the direct baseline of <ref type="bibr" target="#b22">[23]</ref>. Lacking multiframe metrics, we subtract two temporally consecutive scene flow estimates, project to optical flow, and visualize the result in <ref type="figure">Fig. 9</ref>. This shows how scene flow changes over time at the same pixel location. Our model produces visibly more temporally consistent scene flow, especially near out-of-bound regions and foreground objects. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Both methods correct</head><p>Ours is correct, <ref type="bibr" target="#b22">[23]</ref> is not <ref type="bibr" target="#b22">[23]</ref> is correct, ours is not Both failed <ref type="bibr">(a)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative comparison</head><p>In <ref type="figure" target="#fig_2">Fig. 10</ref>, we qualitatively compare our model with the direct baseline of <ref type="bibr" target="#b22">[23]</ref> by visualizing where the accuracy gain mainly originates from. Our method outputs more accurate scene flow especially on planar road surfaces (1 st and 2 nd row), out-of-bound regions (2 nd row), and foreground objects (1 st and 3 rd row). Especially the accuracy gain on foreground objects and planar road surfaces originates from more accurate estimates on the disparity and disparity change map. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Generalization to other datasets</head><p>We test the generalization of our model trained on KITTI <ref type="bibr" target="#b10">[11]</ref> to other datasets, such as the nuScenes <ref type="bibr" target="#b7">[8]</ref>, Monkaa <ref type="bibr" target="#b39">[40]</ref>, and DAVIS <ref type="bibr" target="#b48">[49]</ref> datasets. <ref type="figure" target="#fig_2">Fig. 11</ref> provides visual examples. Our method demonstrates good generalization to the real-world nuScenes dataset, but shows visually less accurate results on the synthetic domain (i.e. Monkaa), as can be expected. Interestingly on DAVIS, our method demonstrates reasonable performance on completely unseen domains (e.g., indoor, ego-centric). <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a multi-frame monocular scene flow network based on self-supervised learning. Starting from the recent self-supervised two-frame network of <ref type="bibr" target="#b22">[23]</ref>, we first pointed out limitations of the decoder design and introduced an advanced two-frame baseline, which is stable to train and already improves the accuracy. Our multi-frame model then exploits the temporal coherency of 3D scene flow using overlapping triplets of input frames and temporally propagating previous estimates via a convolutional LSTM. Using forward warping in the ConvLSTM turned out to be crucial for accurate temporal propagation. An occlusion-aware census loss and a gradient detaching strategy further boost the accuracy and training practicality. Our model achieves state-of-the-art scene flow accuracy among self-supervised methods, while even yielding faster runtime, and reduces the accuracy gap to less efficient semi-supervised methods. Future work should consider extending our selfsupervised approach to challenging, uncontrolled capture setups with variable training baselines for even broader applicability. Also, while we do not explicitly model camera ego-motion mainly for the simplicity of the pipeline, additionally exploiting ego-motion can yield further benefits, e.g., for driving scenarios where rigid motion dominates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Multi-Frame Monocular Scene Flow -Supplementary Material -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Junhwa Hur 1</head><p>Stefan Roth 1,2 1 Department of Computer Science, TU Darmstadt 2 hessian.AI In this supplementary material, we introduce our 3D scene flow color coding scheme and provide details on our refined backbone architecture, self-supervised loss function, implementation, and computational cost. Then, we demonstrate additional results on temporal consistency, qualitative comparisons with the direct two-frame baseline (Self-Mono-SF <ref type="bibr" target="#b22">[23]</ref>), and more experimental results for the generalization to other datasets. Lastly, we provide preliminary results of our model trained on a vast amount of unlabeled web videos in a self-supervised manner. We discuss the results as well as a current limitation of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scene Flow Color Coding</head><p>For visualizing 3D scene flow in 2D image coordinates, we use the CIE-LAB color space, as visualized in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Refined Backbone Architecture</head><p>We provide a more in-depth analysis of our refined backbone two-frame architecture introduced in Sec. 3.1 of the main paper. We first present a simple empirical study of key findings from <ref type="bibr" target="#b27">[28]</ref> and discuss which key factors can be carried over to monocular scene flow estimation. Afterward, we demonstrate an accuracy analysis and the improved training stability of our refined architecture by discarding the context network and splitting the decoder. Empirical study on key findings from <ref type="bibr" target="#b27">[28]</ref>.</p><p>Jonschkowski et al. <ref type="bibr" target="#b27">[28]</ref> provide a systematical analysis of the key design factors for highly accurate self-supervised optical flow. We conduct an empirical study on which of their key findings are beneficial in the context of monocular 3D scene flow. We report results on KITTI Scene Flow Training <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> using the scene flow metrics (cf . Sec. 4.2 in the main paper). <ref type="table" target="#tab_8">Table 8</ref> provides our empirical study on adopting each key factor on top of our baseline and reports the scene flow accuracy. We follow the training setup from <ref type="bibr" target="#b22">[23]</ref>. We first apply cost volume normalization (CV. Norm.) on the model from <ref type="bibr" target="#b22">[23]</ref> without the context network (Cont. Net.). <ref type="bibr" target="#b0">1</ref>      <ref type="figure" target="#fig_2">Figure 13</ref>. An analysis of training stability: We compare the training loss and the accuracy of three models (i.e., direct baseline <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b22">[23]</ref> without context network, and <ref type="bibr" target="#b22">[23]</ref> without context network and with split decoder), training on two different splits (i.e. the original training split from <ref type="bibr" target="#b22">[23]</ref> and our multi-frame train split). Discarding the context network offers more stable, faster convergence of the accuracy on both splits. Further applying the split decoder improves the accuracy.  <ref type="table">Table 9</ref>. Comparison of different occlusion estimation strategies: Using disocclusion produces more accurate scene flow estimates than using a forward-backward consistency check.</p><p>We find the census loss and using one less pyramid level to be beneficial for the monocular scene flow setup as well. On the other hand, using data distillation or a 640 × 640 pixel resolution only brings marginal improvements, but requires a much longer training time; thus we decide not to apply them here. We observe that using a square resolution improves the optical flow accuracy but hurts the disparity accuracy; this offsets the improvement in the end. When using level dropout, which randomly skips some pyramid levels when training, training unfortunately did not converge.</p><p>In <ref type="table">Table 9</ref>, we also compare different occlusion estimation techniques, specifically disocclusion detection and the forward-backward consistency check as described in <ref type="bibr" target="#b27">[28]</ref>. We use our final model with multi-frame estimation for the study. Unlike the conclusion from <ref type="bibr" target="#b27">[28]</ref>, we find that using disocclusion information for occlusion detection produces more accurate scene flow than using a forward-backward consistency check.</p><p>Improved training stability. As discussed in Sec. 3.1 of the main paper, discarding the context network and splitting the decoder improve the training stability with faster convergence. <ref type="figure" target="#fig_2">Fig. 13</ref> plots the training loss and the scene flow outlier rate of the direct baseline <ref type="bibr" target="#b22">[23]</ref>, the baseline without the context network, and additionally applying our split decoder design. We demonstrate the results on using two different train splits, the original split from <ref type="bibr" target="#b22">[23]</ref> and our multi-frame train split (see Sec. 4.1).</p><p>We make the following main observations: (i) The direct baseline <ref type="bibr" target="#b22">[23]</ref> shows a significant accuracy drop when using our train split. (ii) Discarding the context network resolves the issue and offers more stable, faster convergence regarding the accuracy on both train splits. (iii) After applying our split decoder design, the model further improves the accuracy, showing more stable and faster convergence regarding the training loss on both train splits.</p><p>Note that a lower training loss does not always directly translate to better accuracy, since the model optimizes the self-supervised proxy loss. For this reason we also plot the scene flow outlier rate (on KITTI Scene Flow Training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-Supervised Loss</head><p>We provide further details on the self-supervised proxy loss introduced in Sec. 3.3 in the main paper. The weighting constant λ sf in Eq. (2) in the main paper is calculated at every iteration step to make the scene flow loss L sf equal to the disparity loss L d , which previous work <ref type="bibr" target="#b22">[23]</ref> empirically found to be better than using a fixed constant.</p><p>Disparity loss. Following Godard et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we use the right view of a stereo image pair for the guidance of dispar-ity estimation at training time; the second view is not used at test time. The disparity loss consists of a photometric loss L d,ph and a smoothness loss L d,sm with regularization constant λ d,sm = 0.1,</p><formula xml:id="formula_13">L d = L d,ph + λ d,sm L d,sm (4a) with L d,ph = ρ census I t ,Ĩ disp t , O disp t .<label>(4b)</label></formula><p>The photometric loss L d,ph in Eq. (4b) penalizes the photometric difference between the left view I t and the synthesized left viewĨ disp t , obtained from the output disparity d t and the given right view I r t via backward warping <ref type="bibr" target="#b80">[81]</ref>. To calculate the photometric difference, we use our new occlusion-aware census loss ρ census in Eq. (3a) in the main paper. As in <ref type="bibr" target="#b22">[23]</ref>, we obtain the disparity occlusion mask O disp t from forward-warping the right disparity map by inputting the right view I r t into the network. We use an edge-aware 2 nd -order smoothness term <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref> to define the disparity smoothness L d,sm in Eq. (4a).</p><formula xml:id="formula_14">L d,sm = 1 N p i∈{x,y} ∇ 2 i d t (p) · e −β ∇iIt(p) 1 ,<label>(5)</label></formula><p>with β = 150, divided by the number of pixels N <ref type="bibr" target="#b27">[28]</ref>. Scene flow loss. The scene flow loss consists of three terms <ref type="bibr" target="#b22">[23]</ref>: a photometric loss L sf,ph , a 3D point reconstruction loss L sf,pt , and a scene flow smoothness loss L sf,sm , L sf = L sf,ph + λ sf,pt L sf,pt + λ sf,sm L sf,sm ,</p><p>with L sf,ph = ρ census I t ,Ĩ sf t , O sf t ,</p><p>and regularization weights λ sf,pt = 0.2, λ sf,sm = 1000. The scene flow photometric loss in Eq. (6b) penalizes the photometric difference between the reference image I t and the synthesized reference imageĨ sf t , obtained from the camera intrinsics K, estimated disparity d t , and the scene flow s f t (cf . <ref type="figure" target="#fig_12">Fig. 4a</ref> in <ref type="bibr" target="#b22">[23]</ref>). Here we also apply our novel occlusion-aware census transform ρ census from Eq. (3a). The scene flow occlusion mask O sf t is obtained by the disocclusion from the backward scene flow s b t+1 . The 3D reconstruction loss L sf,pt in Eq. (6a) penalizes the Euclidean distance between the corresponding 3D points, P t and P t+1 , however only for visible pixels:</p><formula xml:id="formula_17">L sf,pt = 1 q O sf t (q) p O sf t (p) · P t − P t+1 2 P t 2<label>(7a)</label></formula><p>with P t =d t (p) · K −1 p + s f t (p) (7b)</p><formula xml:id="formula_18">P t+1 =d t+1 (p ) · K −1 p ,<label>(7c)</label></formula><p>and p = K d t (p) · K −1 p + s f t (p) ,</p><p>where p is the corresponding pixel of p given the scene flow and disparity estimate.d t andd t+1 are the depth maps at time t and t + 1 respectively. The depthd is trivally converted from the disparity estimates given the camera focal length f focal and the baseline of the stereo rig b, specificallŷ d = f focal · b/d. Here, we assume that the camera focal length and the stereo baseline is given so that the network outputs disparity (or depth) on a certain, fixed scale. The loss is normalized by the 3D distance of each point P t to the camera to penalize the relative distance to camera.</p><p>The same edge-aware smoothness loss is applied to 3D scene flow, yielding L sf,sm in Eq. (6a), also normalized by its 3D distance to camera:</p><formula xml:id="formula_20">L sf,sm = 1 N p i∈{x,y} ∇ 2 i s f t (p) · e −β ∇iIt(p) 1 P t 2 ,<label>(8)</label></formula><p>with β = 150 and N being the number of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>As briefly discussed in Sec. 4.1 of the main paper, we use the augmentation scheme and training configuration suggested by <ref type="bibr" target="#b22">[23]</ref>. The geometric augmentation consists of horizontal flips (with 50% probability), random scaling, cropping, and resizing into 256 × 832 pixels. Then, a photometric augmentation is applied with 50% probability, consisting of gamma adjustment, random brightness and color changes. The augmentation parameters are uniformly sampled from the ranges given in <ref type="table" target="#tab_0">Table 10</ref>. We use the same augmentation parameters for all consecutive frames included in the same mini-batch.</p><p>For training, we use the Adam optimizer <ref type="bibr" target="#b83">[82]</ref> with β 1 = 0.9 and β 2 = 0.999. We do not apply weight decay because we found that it harms the accuracy. The learning rate schedule from <ref type="bibr" target="#b22">[23]</ref> is used. That is, for self-supervised training for 400k iterations, the initial learning rate starts at 2 × 10 −4 , being halved at 150k, 250k, 300k, and 350k iteration steps. Afterwards, for semi-supervised fine-tuning for 45k iterations, the learning rate starts from 4 × 10 −5 , being halved at 10k, 20k, 30k, 35k, and 40k iteration steps.  <ref type="figure" target="#fig_2">Figure 14</ref>. Qualitative comparison of temporal consistency: Each scene shows (a) overlayed input images and scene flow difference maps of (b) our method, and (c) the direct two-frame baseline of <ref type="bibr" target="#b22">[23]</ref>, visualized using optical flow color coding. Our method provides more temporally consistent estimates near moving objects and out-of-bound regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Mono-SF [23]</head><p>Multi-Mono-SF (ours) AEPE 0.0969 0.0911 <ref type="table" target="#tab_0">Table 11</ref>. Temporal consistency evaluation: Our method shows lower average end-point error (AEPE) between two temporally consecutive estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Cost and Training Time</head><p>Our model takes 153.1G FLOPS of computation per frame pair, with a model size of 7.537M parameters. It requires only one GPU to train, consumes only 4.89G GPU memory, and trains for 4.5 days (on a single NVIDIA GTX 1080 Ti GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Temporal Consistency</head><p>We provide an additional analysis of the temporal consistency, continuing from Sec. 4.4 in the main paper. <ref type="figure" target="#fig_2">Fig. 14</ref> visualizes additional comparisons of the scene flow difference map, comparing to the direct two-frame baseline <ref type="bibr" target="#b22">[23]</ref>. Our model produces visibly more temporally consistent scene flow, especially near moving objects and out-ofbound regions.</p><p>In <ref type="table" target="#tab_0">Table 11</ref>, we also quantitatively evaluate the temporal consistency on KITTI Scene Flow Training by calculating the average Euclidean distance of two corresponding scene flow vectors between the two temporally consecutive estimates. The corresponding scene flow is found using the provided ground truth labels. While this is not an ideal way for measuring temporal consistency, it shows how much each corresponding scene flow vector changes over time, assuming constant velocity. Comparing to the direct two-frame baseline <ref type="bibr" target="#b22">[23]</ref>, our method gives lower AEPE between two temporally corresponding scene flow vectors, which indicates more temporally consistent estimates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Comparison</head><p>In <ref type="figure" target="#fig_2">Fig. 15</ref>, we provide additional qualitative comparisons with the direct two-frame baseline <ref type="bibr" target="#b22">[23]</ref> as in Sec. 4.5 in the main paper. Supporting the same conclusion as in the main paper, our approach produces more accurate 3D scene flow on out-of-bound regions, foreground objects, and planar road surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Generalization to Other Datasets</head><p>Continuing from Sec. 4.5 of the main paper, we provide more qualitative results on the nuScenes <ref type="bibr" target="#b7">[8]</ref>, DAVIS <ref type="bibr" target="#b48">[49]</ref>, Driving and Monkaa <ref type="bibr" target="#b39">[40]</ref> datasets. <ref type="figure" target="#fig_2">Fig. 16</ref> provides both successful cases and failure cases on those three datasets, respectively. Our model, trained only on the KITTI dataset, generalizes well to the nuScenes dataset <ref type="bibr" target="#b7">[8]</ref>, which is reasonably close in domain (i.e., driving scenes). However, there exist some failure cases with inaccurate depth estimation as well as occasional artifacts on the image boundary. On the DAVIS <ref type="bibr" target="#b48">[49]</ref> dataset, our model generalizes surprisingly well to completely unseen domains, yet depth estimation on unseen objects (e.g., horse, cat) can sometimes fail.</p><p>In the synthetic domain (Driving and Monkaa <ref type="bibr" target="#b39">[40]</ref> datasets), however, our model demonstrates less accurate results, as can be expected. Typical failure cases are again inaccurate depth estimation on completely unseen synthetic objects or reflective road surfaces. In <ref type="table" target="#tab_0">Table 12</ref>, we eval-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Successful cases</head><p>Failure cases nuScenes <ref type="bibr" target="#b7">[8]</ref> DAVIS <ref type="bibr" target="#b48">[49]</ref> Driving <ref type="bibr" target="#b39">[40]</ref> Monkaa <ref type="bibr" target="#b39">[40]</ref>  Result and discussion. <ref type="figure" target="#fig_2">Fig. 17</ref> demonstrates the test result on the DAVIS <ref type="bibr" target="#b48">[49]</ref> dataset. Comparing to our KITTItrained model (cf . <ref type="figure" target="#fig_2">Fig. 11</ref>), our model trained on the WSVD dataset <ref type="bibr" target="#b84">[83]</ref> is able to correctly estimate depth on diverse scenes from the DAVIS <ref type="bibr" target="#b48">[49]</ref> dataset. This also confirms our observation from the generalization analysis in Appendix H that better generalization can be achieved by training on a dataset with diverse scenes and objects. However, our model unfortunately fails to correctly estimate scene flow: the network outputs z components of the scene flow that are nearly zero and only estimates x and y components (refer the scene flow color coding in <ref type="figure" target="#fig_0">Fig. 12</ref>). We also observe that the 3D reconstruction loss L sf,pt in Eq. (7a) does not converge at all. We conjecture that the failure comes from the issue that the model does not es-timate scale-consistent depth but instead normalized depth for each frame, which makes it difficult to determine the correct z component of the scene flow for each sequence. This connects to a current limitation that our approach requires a fixed (or given) focal length and a stereo baseline for training. However, we expect that this can be overcome once the network is able to output scale-consistent depth across sequences or videos while being trained on videos with diverse camera settings. We leave this for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I t− 2 I</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Decoder configuration: (a) A single joint decoder [23], (b) removing the context network, and (c) our split decoder design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>&lt; l a t e x i t s h a 1</head><label>1</label><figDesc>_ b a s e 6 4 = " V P q + 6 z 4 h r X K 7 E f S s s X X X g Y H y g 1 w = " &gt; A A A C B X i c b V A 9 S w N B E N 2 L X z F + n V p q c R g E C z n u R N E y Y G M Z w X x A E s P e Z i 5 Z s v f B 7 p w k H N f Y + F d s L B S x 9 T / Y + W / c J F d o 4 o O B x 3 s z z M z z Y s E V O s 6 3 U V h a X l l d K 6 6 X N j a 3 t n f M 3 b 2 6 i h L J o M Y i E c m m R x U I H k I N O Q p o x h J o 4 A l o e M P r i d 9 4 A K l 4 F N 7 h O I Z O Q P s h 9 z m j q K W u e d g O K A 4 8 P x 1 l 9 2 2 E E a Y + U M y 6 K Z 6 K r G u W H d u Z w l o k b k 7 K J E e 1 a 3 6 1 e x F L A g i R C a p U y 3 V i 7 K R U I m c C s l I 7 U R B T N q R 9 a G k a 0 g B U J 5 1 + k V n H W u l Z f i R 1 h W h N 1 d 8 T K Q 2 U G g e e 7 p z c r O a 9 i f i f 1 0 r Q v + q k P I w T h J D N F v m J s D C y J p F Y P S 6 B o R h r Q p n k + l a L D a i k D H V w J R 2 C O / / y I q m f 2 e 6 F 7 d y e l y t 2 H k e R H J A j c k J c c k k q 5 I Z U S Y 0 w 8 k i e y S t 5 M 5 6 M F + P d + J i 1 F o x 8 Z p / 8 g f H 5 A + 4 l m W s = &lt; / l a t e x i t &gt; x feat t,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w r V 6 A j M B 0 8 j m K K D G Y J m 0 m / n j v A k = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q S b z O w f o A 2 R B 7 m V w z E / s 3 N M b l m l N Y S w S u y Q N V K L l 6 V / O I C Z p S C M g H E v Z t 6 0 E 3 A w L Y I T T v O a k k i a Y j P G Q 9 h W N c E i l m 0 2 f y I 1 D p Q y M I B a q I j C m 6 u + J D I d S T k J f d R Y n y 3 m v E P / z + i k E l 2 7 G o i Q F G p H Z o i D l B s R G k Y g x Y I I S 4 B N F M B F M 3 W q Q E R a Y g M q t p k K w 5 1 9 e J J 1 T 0 z 4 3 r Z u z R t M s 4 6 i i O j p A R 8 h G F 6 i J r l E L t R F B j + g Z v a I 3 7 U l 7 0 d 6 1 j 1 l r R S t n 9 t E f a J 8 / Z 3 O Y g A = = &lt; / l a t e x i t &gt; s f t,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L v P G I K 5 z C i m Q w k L L g j p a + V w a j O I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>M q t p k K w 5 1 9 e J J 1 T 0 z 4 3 r Z u z R t M s 4 6 i i O j p A R 8 h G F 6 i J r l E L t R F B j + g Z v a I 3 7 U l 7 0 d 6 1 j 1 l r R S t n 9 t E f a J 8 / Y T + Y f A = = &lt; / l a t e x i t &gt; s b t,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 + / o n T F J 6 O K v O 5 2 i Y R 6 x o j I S L p s = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 0 J C I o s u C G 5 c V 7 A P a E C a T S T t 0 8 m B m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 z x U 8 6 k s u 1 v Y 2 V 1 b X 1 j s 7 Z V 3 9 7 Z 3 d s 3 D x p d m W S C 0 A 5 J e C L 6 P p a U s 5 h 2 F F O c 9 l N B c e R z 2 v M n t 6 X f e 6 R C s i R + U N O U u h E e x S x k B C s t e W Z j G G E 1 9 s M 8 K L x c n f F z p / D M p m 3 Z M 6 B l 4 l S k C R X a n v k 1 D B K S R T R W h G M p B 4 6 d K j f H Q j H C a V E f Z p K m m E z w i A 4 0 j X F E p Z v P s h f o R C s B C h O h X 6 z Q T P 2 9 k e N I y m n k 6 8 k y q V z 0 S v E / b 5 C p 8 M b N W Z x m i s Z k f i j M O F I J K o t A A R O U K D 7 V B B P B d F Z E x l h g o n R d d V 2 C s / j l Z d K 9 s J w r y 7 6 / b L a s q o 4 a H M E x n I I D 1 9 C C O 2 h D B w g 8 w T O 8 w p t R G C / G u / E x H 1 0 x q p 1 D + A P j 8 w f S i Z Q 6 &lt; / l a t e x i t &gt; dt,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o e 9 e 4 Q 6 r z c j E r i D N w n p y M w A h x y 8 = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q T b K 7 x y g D 5 A F u Z f B M T + x c 0 9 v W K Y 1 h b F I 7 J I 0 U I m W p 3 8 5 g 5 i k I Y 2 A c C x l 3 7 Y S c D M s g B F O 8 5 q T S p p g M s Z D 2 l c 0 w i G V b j Z 9 I j c O l T I w g l i o i s C Y q r 8 n M h x K O Q l 9 1 V m c L O e 9 Q v z P 6 6 c Q X L o Z i 5 I U a E R m i 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt; l a t e x i t s h a 1 1 &lt;</head><label>11</label><figDesc>_ b a s e 6 4 = " V L p p T n C M F u Y c T 1 n x o T R G p N C C d t 0 = " &gt; A A A C A n i c b V D L S s N A F J 3 4 r P U V d S V u g k V w I S E R R Z c F N y 4 r 2 A c 0 M U w m k 3 b o 5 M H M j V h C c O O v u H G h i F u / w p 1 / 4 6 T N Q l s P X D i c c y / 3 3 u O n n E m w r G 9 t Y X F p e W W 1 t l Z f 3 9 j c 2 t Z 3 d j s y y Q S h b Z L w R P R 8 L C l n M W 0 D A 0 5 7 q a A 4 8 j n t + q O r 0 u / e U y F Z E t / C O K V u h A c x C x n B o C R P 3 3 c i D E M / z I P i z g H 6 A H l Y e D m c 8 M L T G 5 Z p T W D M E 7 s i D V S h 5 e l f T p C Q L K I x E I 6 l 7 N t W C m 6 O B T D C a V F 3 M k l T T E Z 4 Q P u K x j i i 0 s 0 n L x T G k V I C I 0 y E q h i M i f p 7 I s e R l O P I V 5 3 l w X L W K 8 X / v H 4 G 4 a W b s z j N g M Z k u i j M u A G J U e Z h B E x Q A n y s C C a C q V s N M s Q C E 1 C p 1 V U I 9 u z L 8 6 R z a t r n p n V z 1 m i a V R w 1 d I A O 0 T G y 0 Q V q o m v U Q m 1 E 0 C N 6 R q / o T X v S X r R 3 7 W P a u q B V M 3 v o D 7 T P H 2 J V l / 8 = &lt; / l a t e x i t &gt; d f t,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R e q x m db E F Q k z Y x r t W a z 1 b 0 c Z 9 T Y = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 a q 4 8 4 A + Q B 4 V Q Q 4 n v A j M m m M 7 Y 1 j z x J 2 S G p q i E Z h f X i 8 h W U w F E I 6 V 6 r p O C n 6 O J T D C a V H 1 M k V T T Ia 4 T 7 u a C h x T 5 e f j F w r r S C s 9 K 0 q k L g H W W P 0 9 k e N Y q V E c 6 s 7 y Y D X r l e J / X j e D 6 N L P m U g z o I J M F k U Z t y C x y j y s H p O U A B 9 p g o l k + l a L D L D E B H R q V R 2 C O / v y P G m d 2 u 6 5 7 d y c 1 e r 2 N I 4 K O k C H 6 B i 5 6 A L V 0 T V q o C Y i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F p X T C m M 3 v o D 4 z P H 3 o B m A 4 = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " q d i t x c 6 7 L t / v I 9 L 6 8 8 W o u C z h F 7 s = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B H c W B J R d F l 0 o 7 s K 9 g F t C J P p p B 0 6 m Y S Z S a G E / I k b F 4 q 4 9 U / c + T d O 2 i y 0 9 c D A 4 Z x 7 u W d O k H C m t O N 8 W y u r a + s b m 5 W t 6 v b O 7 t 6 + f X D Y V n E q C W 2 R m M e y G 2 B F O R O 0 p Z n m t J t I i q O A 0 0 4 w v i v 8 z o R K x W L x p K c J 9 S I 8 F C x k B G s j + b b d j 7 A e B W H 2 k P u Z P n d z 3 6 4 5 d W c G t E z c k t S g R N O 3 v / q D m K Q R F Z p w r F T P d R L t Z V h q R j j N q / 1 U 0 Q S T M R 7 S n q E C R 1 R 5 2 S x 5 j k 6 N M k B h L M 0 T G s 3 U 3 x s Z j p S a R o G Z L H K q R a 8 Q / / N 6 q Q 5 v v I y J J N V U k P m h M O V I x 6 i o A Q 2 Y p E T z q S G Y S G a y I j L C E h N t y q q a E t z F L y + T 9 k X d v a o 7 j 5 e 1 x m 1 Z R w W O 4 Q T O w I V r a M A 9 N K E F B C b w D K / w Z m X W i / V u f c x H V 6 x y 5 w j + w P r 8 A X F N k 4 c = &lt; / l a t e x i t &gt; I t l a t e x i t s h a 1 _ b a s e 6 4 = " j S V i D m 2 p Q R a u O 5 / v r S q P N S m B h a I = " &gt; A A A B 9 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q U X R b d 6 K 6 C f U A b y 2 Q 6 a Y d O H s z c K C X k P 9 y 4 U M S t / + L O v 3 H S Z q G t B w Y O 5 9 z L P X O 8 W A q N t v 1 t L S 2 v r K 6 t l z b K m 1 v b O 7 u V v f 2 W j h L F e J N F M l I d j 2 o u R c i b K F D y T q w 4 D T z J 2 9 7 4 O v f b j 1 x p E Y X 3 O I m 5 G 9 B h K H z B K B r p o R d Q H H l + e p v 1 U 8 z 6 l a p d s 6 c g i 8 Q p S B U K N P q V r 9 4 g Y k n A Q 2 S S a t 1 1 7 B j d l C o U T P K s 3 E s 0 j y k b 0 y H v G h r S g G s 3 n a b O y L F R B s S P l H k h k q n 6 e y O l g d a T w D O T e U o 9 7 + X i f 1 4 3 Q f / S T U U Y J 8 h D N j v k J 5 J g R P I K y E A o z l B O D K F M C Z O V s B F V l K E p q m x K c O a / v E h a p z X n v G b f n V X r V 0 U d J T i E I z g B B y 6 g D j f Q g C Y w U P A M r / B m P V k v 1 r v 1 M R t d s o q d A / g D 6 / M H F a + S 5 A = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>14</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " q H + r m d e C / e g L P P o K q m A u A 3 r j A p w = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B E E o S S i 6 L L o R n c V 7 A P a E C b T S T t 0 M g k z k 0 I J + R M 3 L h R x 6 5 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I O F M a c f 5 t l Z W 1 9 Y 3 N i t b 1 e 2 d 3 b 1 9 + + C w r e J U E t o i M Y 9 l N 8 C K c i Z o S z P N a T e R F E c B p 5 1 g f F f 4 n Q m V i s X i S U 8 T 6 k V 4 K F j I C N Z G 8 m 2 7 H 2 E 9 C s L s I f c z f e 7 m v l 1 z 6 s 4 M a J m 4 J a l B i a Z v f / U H M U k j K j T h W K m e 6 y T a y 7 D U j H C a V / u p o g k m Y z y k P U M F j q j y s l n y H J 0 a Z Y D C W J o n N J q p v z c y H C k 1 j Q I z W e R U i 1 4 h / u f 1 U h 3 e e B k T S a q p I P N D Y c q R j l F R A x o w S Y n m U 0 M w k c x k R W S E J S b a l F U 1 J b i L X 1 4 m 7 Y u 6 e 1 V 3 H i 9 r j d u y j g o c w w m c g Q v X 0 I B 7 a E I L C E z g G V 7 h z c q s F + v d + p i P r l j l z h H 8 g f X 5 A 2 5 B k 4 U = &lt; / l a t e x i t &gt; " 3 CV 6 K B x X D h R u p u O F Q Y 1 h M i p F G / c = " &gt; A A A C C X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 q C E R R Y 8 F L x 4 r 2 A 9 o Y 9 l s N + 3 S z S b s T t Q S c v X i X / H i Q R G v / g N v / h s 3 b Q / a + m D g 8 d 4 M M / P 8 W H A N j v N t L S w u L a + s F t a K 6 x u b W 9 u l n d 2 G j h J F W Z 1 G I l I t n 2 g m u G R 1 4 C B Y K 1 a M h L 5 g T X 9 4 m f v N O 6 Y 0 j + Q N j G L m h a Q v e c A p A S N 1 S 7 g T E h j 4 Q T r I b j v A H i A N K v d E x V k 3 h W O 3 I r J u q e z Y z h h 4 n r h T U k Z T 1 L q l r 0 4 v o k n I J F B B t G 6 7 T g x e S h R w K l h W 7 C S a x Y Q O S Z + 1 D Z U k Z N p L x 5 9 k + N A o P R x E y p Q E P F Z / T 6 Q k 1 H o U + q Y z v 1 v P er n 4 n 9 d O I L j w U i 7 j B J i k k 0 V B I j B E O I 8 F 9 7 h i F M T I E E I V N 7 d i O i C K U D D h F U 0 I 7 u z L 8 6 R x Y r t n t n N 9 W q 7 a 0 z g K a B 8 d o C P k o n N U R V e o h u q I o k f 0 j F 7 R m / V k v V j v 1 s e k d c G a z u y h P 7 A + f w A q r Z q N &lt; / l a t e x i t &gt; h f,warp t 1,l Forward Warping &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 9 y b 4 2 o O p L d s b C g 4 j 7 H y A O u x a H Q = " &gt; A A A C C X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 q C E R R Y 8 F L x 4 r 2 A 9 o Y 9 l s N + 3 S z S b s T t Q S c v X i X / H i Q R G v / g N v / h s 3 b Q / a + m D g 8 d 4 M M / P 8 W H A N j v N t L S w u L a + s F t a K 6 x u b W 9 u l n d 2 G j h J F W Z 1 G I l I t n 2 g m u G R 1 4 C B Y K 1 a M h L 5 g T X 9 4 m f v N O 6 Y 0 j + Q N j G L m h a Q v e c A p A S N 1 S 7 g T E h j 4 Q U q z 2 w 6 w B 0 i D y j 1 R c d Z N 4 d i t i K x b K j u 2 M w a e J + 6 U l N E U t W 7 p q 9 O L a B I y C V Q Q r d u u E 4 O X E g W c C p Y V O 4 l m M a F D 0 m d t Q y U J m f b S 8 S c Z P j R K D w e R M i U B j 9 X f E y k J t R 6 F v u n M 7 9 a z X i 7 + 5 7 U T C C 6 8 l M s 4 A S b p Z F G Q C A w R z m P B P a 4 Y B T E y h F D F z a 2 Y D o g i F E x 4 R R O C O / v y P G m c 2 O 6 Z 7 V y f l q v 2 N I 4 C 2 k c H 6 A i 5 6 B x V 0 R W q o T q i 6 B E 9 o 1 f 0 Z j 1 Z L 9 a 7 9 T F p X b C m M 3 v o D 6 z P H y K m m o g = &lt; / l a t e x i t &gt; c f,warp t 1,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C w p Q v J 9 D e V U 0 U h g D p O U 9 K r T X E P c = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 a S 4 8 4 A + Q B 4 V Q Q 4 n v A j M m m M 7 Y 1 j z x J 2 S G p q i E Z h f X i 8 h W U w F E I 6 V 6 r p O C n 6 O J T D C a V H 1 M k V T T I a 4 T 7 u a C h x T 5 e f j F w r r S C s 9 K 0 q k L g H W W P 0 9 k e N Y q V E c 6 s 7 y Y D X r l e J / X j e D 6 N L P m U g z o I J M F k U Z t y C x y j y s H p O U A B 9 p g o l k + l a L D L D E B H R q V R 2 C O / v y P G m d 2 u 6 5 7 d y c 1 e r 2 N I 4 K O k C H 6 B i 5 6 A L V 0 T V q o C Y i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F p X T C m M 3 v o D 4 z P H 2 D B l / 4 = &lt; / l a t e x i t &gt; c f t,l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Forward Warping &lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>14</label><figDesc>" g p E s 6 t W V O g I h M B X V J a m s A s T n m U c = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 Y P i z g P 6 A H l U B D m c 8 C I w a 4 7 t j G H N E 3 d K a m i K R m B + e b 2 E Z D E V Q D h W q u s 6 K f g 5 l s A I p 0 X V y x R N M R n i P u 1 q K n B M l Z + P X y i s I 6 3 0 r C i R u g R Y Y / X 3 R I 5 j p U Z x q D v L g 9 W s V 4 r / e d 0 M o k s / Z y L N g A o y W R R l 3 I L E K v O w e k x S A n y k C S a S 6 V s t M s A S E 9 C p V X U I 7 u z L 8 6 R 1 a r v n t n N z V q v b 0 z g q 6 A A d o m P k o g t U R 9 e o g Z q I o E f 0 j F 7 R m / F k v B j v x s e k d c G Y z u y h P z A + f w B o p Z g D &lt; / l a t e x i t &gt; h f t,l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Our network architecture for multi-frame monocular scene flow based on PWC-Net<ref type="bibr" target="#b60">[61]</ref>: At each pyramid level (dashed square), the two cost volumes are input to the decoder (shared for bi-directional estimation) to estimate the residual scene flow and disparity (averaged from the two decoders).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>&lt; l a t</head><label></label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " m W I k 9 n S H 8 4 s D 5 o T c W 8 / Q 7 h w i l 4 k = " &gt; A A A B 8 3 i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 0 Z a p 9 0 V 3 L i s Y B / Q G U o m z b S h m U x I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a S u o 6 I H A 4 Z x 7 u S c n k p x p g 9 C H s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 6 7 O s 0 U o R 2 S 8 l T 1 I 6 w p Z 4 J 2 D D O c 9 q W i O I k 4 7 U X T 6 8 L v 3 V O l W S r u z E z S M M F j w W J G s L F S E C T Y T K I 4 l / O h N 6 x U k d t o 1 u t + D V p S 8 x B C l q C m 7 1 9 5 0 H P R A l W w Q n t Y e Q 9 G K c k S K g z h W O u B h 6 Q J c 6 w M I 5 z O y 0 G m q c R k i s d 0 Y K n A C d V h v s g 8 h + d W G c E 4 V f Y J A x f q 9 4 0 c J 1 r P k s h O F h n 1 b 6 8 Q / / I G m Y k b Y c 6 E z A w V Z H k o z j g 0 K S w K g C O m K D F 8 Z g k m i t m s k E y w w s T Y m s q 2 h K + f w v 9 J 9 9 L 1 6 i 6 6 r V V b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>p 1 &lt; 2 ( a )</head><label>12a</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " F I 2 / V l 9 N F M 9 J / F b s V b i T N / Y W u T A = " &gt; A A A B 8 3 i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K p n S O u 2 u 4 M Z l B f u A z l A y a a Y N z W R C k h H K 0 N 9 w 4 0 I R t / 6 M O / / G T F t B R Q 8 E D u f c y z 0 5 o e R M G 4 Q + n M L G 5 t b 2 T n G 3 t L d / c H h U P j 7 p 6 S R V h H Z J w h M 1 C L G m n A n a N c x w O p C K 4 j j k t B / O r n O / f 0 + V Z o m 4 M 3 N J g x h P B I s Y w c Z K v h 9 j M w 2 j T C 5 G t V G 5 g q r N V q P h 1 a E l d R c h Z A l q e d 6 V C 9 0 q W q I C 1 u i M y u / + O C F p T I U h H G s 9 d J E 0 Q Y a V Y Y T T R c l P N Z W Y z P C E D i 0 V O K Y 6 y J a Z F / D C K m M Y J c o + Y e B S / b 6 R 4 V j r e R z a y T y j / u 3 l 4 l / e M D V R M 8 i Y k K m h g q w O R S m H J o F 5 A X D M F C W G z y 3 B R D G b F Z I p V p g Y W 1 P J l v D 1 U / g / 6 d W q b q O K b u u V d n N d R x G c g X N w C V z g g T a 4 A R 3 Q B Q R I 8 A C e w L O T O o / O i / O 6 G i 0 4 6 5 1 T 8 A P O 2 y e T q J I E &lt; / l a t e x i t &gt; p Previous frame It−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>p 1 p 2 (</head><label>2</label><figDesc>b) Current frame It</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>p 1 p 2 (</head><label>2</label><figDesc>c) Warping result I warp t−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 .</head><label>4</label><figDesc>An example of forward warping: (a) Previous frame, (b) current frame, and (c) forward-warped previous frame. t e x i t s h a 1 _ b a s e 6 4 = " q H + r m d e C / e g L P P o K q m A u A 3 r j A p w = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B E E o S S i 6 L L o R n c V 7 A P a E C b T S T t 0 M g k z k 0 I J + R M 3 L h R x 6 5 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I O F M a c f 5 t l Z W 1 9 Y 3 N i t b 1 e 2 d 3 b 1 9 + + C w r e J U E t o i M Y 9 l N 8 C K c i Z o S z P N a T e R F E c B p 5 1 g f F f 4 n Q m V i s X i S U 8 T 6 k V 4 K F j I C N Z G 8 m 2 7 H 2 E 9 C s L s I f c z f e 7 m v l 1 z 6 s 4 M a J m 4 J a l B i a Z v f / U H M U k j K j T h W K m e 6 y T a y 7 D U j H C a V / u p o g k m Y z y k P U M F j q j y s l n y H J 0 a Z Y D C W J o n N J q p v z c y H C k 1 j Q I z W e R U i 1 4 h / u f 1 U h 3 e e B k T S a q p I P N D Y c q R j l F R A x o w S Y n m U 0 M w k c x k R W S E J S b a l F U 1 J b i L X 1 4 m 7 Y u 6 e 1 V 3 H i 9 r j d u y j g o c w w m c g Q v X 0 I B 7 a E I L C E z g G V 7 h z c q s F + v d + p i P r l j l z h H 8 g f X 5 A 2 5 B k 4 U = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " 3 C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>W 7 C</head><label>7</label><figDesc>S a x Y Q O S Z + 1 D Z U k Z N p L x 5 9 k + N A o P R x E y p Q E P F Z / T 6 Q k 1 H o U + q Y z v 1 v P e r n 4 n 9 d O I L j w U i 7 j B J i k k 0 V B I j B E O I 8 F 9 7 h i F M T I E E I V N 7 d i O i C K U D D h F U 0 I 7 u z L 8 6 R x Y r t n t n N 9 W q 7 a 0 z g K a B 8 d o C P k o n N U R V e o h u q I o k f 0 j F 7 R m / V k v V j v 1 s e k d c G a z u y h P 7 A + f w A q r Z q N &lt; / l a t e x i t &gt; h f,warp t 1,l Forward Warping &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 9 y b 4 2 o O p L d s b C g 4 j 7 H y A O u x a H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 .</head><label>5</label><figDesc>y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 a S 4 8 4 A + Q B 4 V Q Q 4 n v A j M m m M 7 Y 1 j z x J 2 S G p q i E Z h f X i 8 h W U w F E I 6 V 6 r p O C n 6 O J T D C a V H 1 M k V T T I a 4 T 7 u a C h x T 5 e f j F w r r S C s 9 K 0 q k L g H W W P 0 9 k e N Y q V E c 6 s 7 y Y D X r l e J / X j e D 6 N L P m U g z o I J M F k U Z t y C x y j y s H p O U A B 9 p g o l k + l a L D L D E B H R q V R 2 C O / v y P G m d 2 u 6 5 7 d y c 1 e r 2 N I 4 K O k C H 6 B i 5 6 A L V 0 T V q o C Y i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F p X T C m M 3 v o D 4 z P H 2 D B l / 4 = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " g p E s 6 t W V O g I h M B X V J a m s A s T n m U c = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 Y P i z g P 6 A H l U B D m c 8 C I w a 4 7 t j G H N E 3 d K a m i K R m B + e b 2 E Z D E V Q D h W q u s 6 K f g 5 l s A I p 0 X V y x R N M R n i P u 1 q K n B M l Z + P X y i s I 6 3 0 r C i R u g R Y Y / X 3 R I 5 j p U Z x q D v L g 9 W s V 4 r / e d 0 M o k s / Z y L N g A o y W R R l 3 I L E K v O w e k x S A n y k C S a S 6 V s t M s A S E 9 C p V X U I 7 u z L 8 6 R 1 a r v n t n N z V q v b 0 z g q 6 A A d o m P k o g t U R 9 e o g Z q I o E f 0 j F 7 R m / F k v B j v x s e k d c G Y z u y h P z A + f w B o p Z g D &lt; / l a t e x i t &gt; A detailed view of the decoder at pyramid level l, based on a convolutional LSTM with forward warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 3 9 W 7 X N c c r F 5 1 g 5 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>b t 1 ForwardScene Flow s f t− 1 &lt;f t 1 &lt; 1 Forward 1 &lt;</head><label>11111</label><figDesc>q h k w c z N 2 I J w Y 2 / 4 s a F I m 7 9 C n f + j Z O 2 C 2 0 9 c O F w z r 3 c e 4 + f C K 7 A s r 6 N 0 s L i 0 v J K e b W y t r 6 x u W V u 7 7 R U n E r K m j Q W s e z 4 R D H B I 9 Y E D o J 1 E s l I 6 A v W 9 o d X h d + + Z 1 L x O L q F U c L c k P Q j H n B K Q E u e u e e E B A Z + k K n 8 z g H 2 A J m f e x k c 2 7 l n V q 2 a N Q a e J / a U V N E U D c / 8 c n o x T U M W A R V E q a 5 t J e B m R A K n g u U V J 1 U s I X R I + q y r a U R C p t x s / E K O D 7 X S w 0 E s d U W A x + r v i Y y E S o 1 C X 3 c W B 6 t Z r x D / 8 7 o p B B d u x q M k B R b R y a I g F R h i X O S B e 1 w y C m K k C a G S 6 1 s x H R B J K O j U K j o E e / b l e d I 6 q d l n N e v m t F q / n M Z R R v v o A B 0 h G 5 2 j O r p G D d R E F D 2 i Z / S K 3 o w n 4 8 V 4 N z 4 m r S V j O r O L / s D 4 / A E h u J f k &lt; / l a t e x i t &gt; s l a t e x i t s h a 1 _ b a s e 6 4 = " S i U C Y r 9 6 P p v 1 z M T I + W M b N s 2 K U y E = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 1 g E N 5 Z E F F 0 W 3 b i s Y B / Q x D C Z T t q h k w c z N 2 I J w Y 2 / 4 s a F I m 7 9 C n f + j Z O 2 C 2 0 9 c O F w z r 3 c e 4 + f C K 7 A s r 6 N 0 s L i 0 v J K e b W y t r 6 x u W V u 7 7 R U n E r K m j Q W s e z 4 R D H B I 9 Y E D o J 1 E s l I 6 A v W 9 o d X h d + + Z 1 L x O L q F U c L c k P Q j H n B K Q E u e u e e E B A Z + k K n 8 z g H 2 A F m Q e x k c 2 7 l n V q 2 a N Q a e J / a U V N E U D c / 8 c n o x T U M W A R V E q a 5 t J e B m R A K n g u U V J 1 U s I X R I + q y r a U R C p t x s / E K O D 7 X S w 0 E s d U W A x + r v i Y y E S o 1 C X 3 c W B 6 t Z r x D / 8 7 o p B B d u x q M k B R b R y a I g F R h i X O S B e 1 w y C m K k C a G S 6 1 s x H R B J K O j U K j o E e / b l e d I 6 q d l n N e v m t F q / n M Z R R v v o A B 0 h G 5 2 j O r p G D d R E F D 2 i Z / S K 3 o w n 4 8 V 4 N z 4 m r S V j O r O L / s D 4 / A E n 5 J f o &lt; / l a t e x i t &gt; s l a t e x i t s h a 1 _ b a s e 6 4 = " B H U V K I J / E A i W j g 5 V 0 K s u k t M b M 3 k = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w Y 0 l E 0 W X R j c s K 9 g F t C J P p p B 0 6 m Y S Z m 0 I J + R M 3 L h R x 6 5 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c g + N 8 W 5 W 1 9 Y 3 N r e p 2 b W d 3 b / / A P j z q 6 D h V l L V p L G L V C 4 h m g k v W B g 6 C 9 R L F S B Q I 1 g 0 m 9 4 X f n T K l e S y f Y J Y w L y I j y U N O C R j J t + 1 B R G A c h N k w 9 z O 4 c H P f r j s N Z w 6 8 S t y S 1 F G J l m 9 / D Y Y x T S M m g Q q i d d 9 1 E v A y o o B T w f L a I N U s I X R C R q x v q C Q R 0 1 4 2 T 5 7 j M 6 M M c R g r 8 y T g u f p 7 I y O R 1 r M o M J N F T r 3 s F e J / X j + F 8 N b L u E x S Y J I u D o W p w B D j o g Y 8 5 I p R E D N D C F X c Z M V 0 T B S h Y M q q m R L c 5 S + v k s 5 l w 7 1 u O I 9 X 9 e Z d W U c V n a B T d I 5 c d I O a 6 A G 1 U B t R N E X P 6 B W 9 W Z n 1 Y r 1 b H 4 v R i l X u H K M / s D 5 / A J r 2 k 6 I = &lt; / l a t e x i t &gt; d t l a t e x i t s h a 1 _ b a s e 6 4 = " u c u 3 b T A U K + S k 9 g u j S 1 b 9 j m z 4 B +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>c 7 8 l x d J 5 7 T h n D f s 2 7 N 6 8 6 q s o 4 o O 0 R E 6 Q Q 6 6 Q E 1 0 g 1 q o j Q j K 0 D N 6 R W / G k / F i v B s f s 9 G K U e 7 s o z 8 w P n 8 A Y O S W Z g = = &lt; / l a t e x i t &gt; s b t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M W p K 7 b 9 j P 4 D z C Q g J S F f I s q S Y e 0 s = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R J R d F l 0 4 7 K C f U A T y m Q y a Y d O J m H m R i i h v + H G h S J u / R l 3 / o 2 T N g t t P T B w O O d e 7 p k T p I J r d J x v q 7 K 2 v r G 5 V d 2 u 7 e z u 7 R / U D 4 + 6 O s k U Z R 2 a i E T 1 A 6 K Z 4 J J 1 k K N g / V Q x E g e C 9 Y L J X e H 3 n p j S P J G P O E 2 Z H 5 O R 5 B G n B I 3 k e T H B c R D l 4 W y I w 3 r D a T p z 2 K v E L U k D S r S H 9 S 8 v T G g W M 4 l U E K 0 H r p O i n x O F n A o 2 q 3 m Z Z i m h E z J i A 0 M l i Z n 2 8 3 n m m X 1 m l N C O E m W e R H u u / t 7 I S a z 1 N A 7 M Z J F R L 3 u F + J 8 3 y D C 6 8 X M u 0 w y Z p I t D U S Z s T O y i A D v k i l E U U 0 M I V d x k t e m Y K E L R 1 F Q z J b j L X 1 4 l 3 Y u m e 9 V 0 H i 4 b r d u y j i q c w C m c g w v X 0 I J 7 a E M H K K T w D K / w Z m X W i / V u f S x G K 1 a 5 c w x / Y H 3 + A H K y k f M = &lt; / l a t e x i t &gt; d t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 5 c o N S X V K k D 9 w N V x X m b F W f + r 4 H 8 = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y E y y C q 5 K I o s u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 6 .</head><label>6</label><figDesc>0 C c 7 8 l x d J 5 7 T h n D f s 2 7 N 6 8 6 q s o 4 o O 0 R E 6 Q Q 6 6 Q E 1 0 g 1 q o j Q j K 0 D N 6 R W / G k / F i v B s f s 9 G K U e 7 s o z 8 w P n 8 A Z w C W a g = = &lt; / l a t e x i t &gt; Loss scheme. The self-supervised loss is applied between each pair of temporally neighboring estimates. t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>/ 9 ⇡ 0. 34 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =&lt; l a t e x i t s h a 1 _ b a s e 6 4 =/ 9 ⇡ 0. 34 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =!&lt; l a t e x i t s h a 1 / 9 ⇡ 0. 34 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =Figure 7 .</head><label>9341414934141934147</label><figDesc>z 5 e e z w y f 4 2 C h 9 H K X S V A J 4 p v 6 e y E m s 1 D g O T W d M Y K g W v a n 4 n 9 f T E F 3 6 O U 8 y D S y h 8 0 W R F h h S P E 0 B 9 7 l k F M T Y E E I l N 7 d i O i S S U D B Z V U w I 7 u L L y 6 R 9 W n f P 6 8 7 d W a 1 x V c R R R o f o C J 0 g F 1 2 g B r p F T d R C F G n 0 j F 7 R m / V k v V j v 1 s e 8 t W Q V M w f o D 6 z P H + C R k z s = &lt; / l a t e x i t &gt; X f G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V y s J l e v B T / B 6 7 w I X U K x b e X O S 7 U o = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u B o v g K i Z a U R G k 4 M Z l B f u A J p T J d N I O n W S G m Y l Y Q v 0 V N y 4 U c e u H u P N v n L Z Z a O u B C 4 d z 7 u X e e 0 L B q N K u + 2 0 V l p Z X V t e K 6 6 W N z a 3 t H X t 3 r 6 l 4 K j F p Y M 6 4 b I d I E U Y T 0 t B U M 9 I W k q A 4 Z K Q V D m 8 m f u u B S E V 5 c q 9 H g g Q x 6 i c 0 o h h p I 3 X t 8 j H 0 r + A l 9 J E Q k j 9 C 1 z m t d u 2 K 6 7 h T w E X i 5 a Q C c t S 7 9 p f f 4 z i N S a I x Q 0 p 1 P F f o I E N S U 8 z I u O S n i g i E h 6 h P O o Y m K C Y q y K b H j + G h U X o w 4 t J U o u F U / T 2 R o V i p U R y a z h j p g Z r 3 J u J / X i f V 0 U W Q 0 U S k m i R 4 t i h K G d Q c T p K A P S o J 1 m x k C M K S m l s h H i C J s D Z 5 l U w I 3 v z L i 6 R 5 4 n h n j n t X r d S u 8 z i K Y B 8 c g C P g g X N Q A 7 e g D h o A g x F 4 B q / g z X q y X q x 3 6 2 P W W r D y m T L 4 A + v z B 1 w U k q M = &lt; / l a t e x i t &gt; " w 6 G s + f D F U t A U R S 7 5 p J y q H W Z b G C Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h p 5 K I o i c p e v F Y w b S F N p T N d t M u 3 W z C 7 k Q o p b / B i w d F v P q D v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C 1 M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 b M a j 2 y h W 3 5 s 5 B V o m X k w r k a P T K X 9 1 + w r K Y K 2 S S G t P x 3 B S D C d U o m O T T U j c z P K V s R A e 8 Y 6 m i M T f B Z H 7 s l J x Z p U + i R N t S S O b q 7 4 k J j Y 0 Z x 6 H t j C k O z b I 3 E / / z O h l G 1 8 F E q D R D r t h i U Z R J g g m Z f U 7 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n x K N g R v + e V V 0 j y v e Z c 1 9 + G i U r / J 4 y j C C Z x C F T y 4 g j r c Q w N 8 Y C D g G V 7 h z V H O i / P u f C x a C 0 4 + c w x / 4 H z + A C 8 H j k I = &lt; / l a t e x i t &gt; ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N U F h Q H y k 3 T e E o 0 P O 6 x x y 0 q Y c 4 m M = " &gt; A A A B 6 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w T w g W c L s Z D Y Z M j O 7 z E M I S 3 7 B i w d F v P p D 3 v w b Z 5 M 9 a G J B Q 1 H V T X d X l H K m j e 9 / e 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 1 o l V h L Z I w h P V j b C m n E n a M s x w 2 k 0 V x S L i t B N N 7 n K / 8 0 S V Z o l 8 N N O U h g K P J I s Z w S a X + t q K Q b X m 1 / 0 5 0 C o J C l K D A s 1 B 9 a s / T I g V V B r C s d a 9 w E 9 N m G F l G O F 0 V u l b T V N M J n h E e 4 5 K L K g O s / m t M 3 T m l C G K E + V K G j R X f 0 9 k W G g 9 F Z H r F N i M 9 b K X i / 9 5 P W v i m z B j M r W G S r J Y F F u O T I L y x 9 G Q K U o M n z q C i W L u V k T G W G F i X D w V F 0 K w / P I q a V / U g 6 u 6 / 3 B Z a 9 w W c Z T h B E 7 h H A K 4 h g b c Q x N a Q G A M z / A K b 5 7 w X r x 3 7 2 P R W v K K m W P 4 A + / z B z I J j l M = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R C s 2 t b O y + G t s M v U 9 D o + t G 3 I g b a 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 E 0 U o C N p Y R T A w k R 9 j b z C V L 9 n b P 3 T 0 h h P w J G w t F b P 0 7 d v 4 b N 8 k V m v h g 4 P H e D D P z o l R w Y 3 3 / 2 y u s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p l G Z Z t h g S i j d i q h B w S U 2 L L c C W 6 l G m k Q C H 6 L h z d R / e E J t u J L 3 d p R i m N C + 5 D F n 1 D q p 1 c H U c K F k t 1 z x q / 4 M Z J k E O a l A j n q 3 / N X p K Z Y l K C 0 T 1 J h 2 4 K c 2 H F N t O R M 4 K X U y g y l l Q 9 r H t q O S J m j C 8 e z e C T l x S o / E S r u S l s z U 3 x N j m h g z S i L X m V A 7 M I v e V P z P a 2 c 2 v g r H X K a Z R c n m i + J M E K v I 9 H n S 4 x q Z F S N H K N P c 3 U r Y g G r K r I u o 5 E I I F l 9 e J s 2 z a n B R 9 e / O K 7 X r P I 4 i H M E x n E I A l 1 C D W 6 h D A x g I e I Z X e P M e v R f v 3 f u Y t x a 8 f O Y Q / s D 7 / A F M 7 5 A g &lt; / l a t e x i t &gt; ✏ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d w k D N R Q T s C Q I L V f A 3 T 5 q g V d h f S I = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U R V d S c O O y g n 1 A Z y i Z N N O G Z p K Q Z M R S + h t u X C j i 1 p 9 x 5 9 + Y t r P Q 1 g O X e z j n X n J z Y s W Z s b 7 / 7 R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R y 8 h M E 9 o k k k v d i b G h n A n a t M x y 2 l G a 4 j T m t B 2 P b m d + + 5 F q w 6 R 4 s G N F o x Q P B E s Y w d Z J Y Y i V 0 v I J + V W / 1 i t X X J s D r Z I g J x X I 0 e i V v 8 K + J F l K h S U c G 9 M N f G W j C d a W E U 6 n p T A z V G E y w g P a d V T g l J p o M r 9 5 i s 6 c 0 k e J 1 K 6 E R X P 1 9 8 Y E p 8 a M 0 9 h N p t g O z b I 3 E / / z u p l N r q M J E y q z V J D F Q 0 n G k Z V o F g D q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r I u p 5 E I I l r + 8 S l q 1 a n B Z 9 e 8 v K v W b P I 4 i n M A p n E M A V 1 C H O 2 h A E w g o e I Z X e P M y 7 8 V 7 9 z 4 W o w U v 3 z m G P / A + f w C w T J D I &lt; / l a t e x i t &gt; ⇡ 0.02 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v R g t E O / r i 1 4 S A O V v U x + T Z Z 1 y / g = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w c R I s o T Z 2 d l k y D y W m V k h h H y F F w + K e P V z v P k 3 z i Z 7 0 M S C h q K q m + 6 u K O X M W N / / 9 k o r q 2 v r G + X N y t b 2 z u 5 e d f + g b V S m C W 0 R x Z X u R N h Q z i R t W W Y 5 7 a S a Y h F x + h C N b n L / 4 Y l q w 5 S 8 t + O U h g I P J E s Y w d Z J j 7 2 I D V S s b K V f r f l 1 f w a 0 T I K C 1 K B A s 1 / 9 6 s W K Z I J K S z g 2 p h v 4 q Q 0 n W F t G O J 1 W e p m h K S Y j P K B d R y U W 1 I S T 2 c F T d O K U G C V K u 5 I W z d T f E x M s j B m L y H U K b I d m 0 c v F / 7 x u Z p O r c M J k m l k q y X x R k n F k F c q / R z H T l F g + d g Q T z d y t i A y x x s S 6 j P I Q g s W X l 0 n 7 r B 5 c 1 P 2 7 8 1 r j u o i j D E d w D K c Q w C U 0 4 B a a 0 A I C A p 7 h F d 4 8 7 b 1 4 7 9 7 H v L X k F T O H 8 A f e 5 w 9 o o p A i &lt; / l a t e x i t &gt; K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 G T h CL r L C G x o D i K u Z k b b t y 8 7 P E Q = " &gt; A A A C B X i c b V D L S s N A F J 3 4 r P U V d a m L w S J 0 V R J R d C V F F 7 q s Y B / Q h D C Z T t q h k w c z N 2 I J 3 b j x V 9 y 4 U M S t / + D O v 3 G a Z q G t B y 4 c z r m X e + / x E 8 E V W N a 3 s b C 4 t L y y W l o r r 2 9 s b m 2 b O 7 s t F a e S s i a N R S w 7 P l F M 8 I g 1 g Y N g n U Q y E v q C t f 3 h 1 c R v 3 z O p e B z d w S h h b k j 6 E Q 8 4 J a A l z z x w V B p i 5 5 L 3 + 1 U c e A 6 w B 8 i u x 7 l S 9 c y K V b N y 4 H l i F 6 S C C j Q 8 8 8 v p x T Q N W Q R U E K W 6 t p W A m x E J n A o 2 L j u p Y g m h Q 9 J n X U 0 j E j L l Z v k X Y 3 y k l R 4 O Y q k r A p y r v y c y E i o 1 C n 3 d G R I Y q F l v I v 7 n d V M I z t 2 M R 0 k K L K L T R U E q M M R 4 E g n u c c k o i J E m h E q u b 8 V 0 Q C S h o I M r 6 x D s 2 Z f n S e u 4 Z p / W r N u T S v 2 i i K O E 9 t E h q i I b n a E 6 u k E N 1 E Q U P a J n 9 I r e j C f j x X g 3 P q a t C 0 Y x s 4 f + w P j 8 A e x r l 4 w = &lt; / l a t e x i t &gt; X f G ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s B 5 C O w C c N G t P d L J C / l O g n C u E F u A = " &gt; A A A B 7 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B L 2 V X K n q S o h e P F e w H t E v J p t l t b D Z Z k q x Q l v 4 H L x 4 U 8 e r / 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 0 g 4 0 8 Z 1 v 5 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a G m Z K k K b R H K p O g H W l D N B m 4 Y Z T j u J o j g O O G 0 H o 9 u p 3 3 6 i S j M p H s w 4 o X 6 M I 8 F C R r C x U q t 3 w 6 L o r F + u u F V 3 B r R M v J x U I E e j X / 7 q D S R J Y y o M 4 V j r r u c m x s + w M o x w O i n 1 U k 0 T T E Y 4 o l 1 L B Y 6 p 9 r P Z t R N 0 Y p U B C q W y J Q y a q b 8 n M h x r P Y 4 D 2 x l j M 9 S L 3 l T 8 z + u m J r z y M y a S 1 F B B 5 o v C l C M j 0 f R 1 N G C K E s P H l m C i m L 0 V k S F W m B g b U M m G 4 C 2 + v E x a 5 1 X v o u r e 1 y r 1 6 z y O I h z B M Z y C B 5 d Q h z t o Q B M I P M I z v M K b I 5 0 X 5 9 3 5 m L c W n H z m E P 7 A + f w B 8 2 i O t A = = &lt; / l a t e x i t &gt; ! " R i X T n b E l 8 / A b t A x o k R 4 S v s T M I u I = " &gt; A A A B + H i c b V D L S g N B E J y N r x g f W f X o Z T A I n s K u K H o M e t B j B P O A 7 L L M T m a T I b M P Z n r E u O R L v H h Q x K u f 4 s 2 / c Z L s Q R M L G o q q b r q 7 w k x w B Y 7 z b Z V W V t f W N 8 q b l a 3 t n d 2 q v b f f V q m W l L V o K l L Z D Y l i g i e s B R w E 6 2 a S k T g U r B O O r q d + 5 4 F J x d P k H s Y Z 8 2 M y S H j E K Q E j B X b V U z r G U e A B e 4 T 8 Z h L Y N a f u z I C X i V u Q G i r Q D O w v r 5 9 S H b M E q C B K 9 V w n A z 8 n E j g V b F L x t G I Z o S M y Y D 1 D E x Iz 5 e e z w y f 4 2 C h 9 H K X S V A J 4 p v 6 e y E m s 1 D g O T W d M Y K g W v a n 4 n 9 f T E F 3 6 O U 8 y D S y h 8 0 W R F h h S P E 0 B 9 7 l k F M T Y E E I l N 7 d i O i S S U D B Z V U w I 7 u L L y 6 R 9 W n f P 6 8 7 d W a 1 x V c R R R o f o C J 0 g F 1 2 g B r p F T d R C F G n 0 j F 7 R m / V k v V j v 1 s e 8 t W Q V M w f o D 6 z P H + C R k z s = &lt; / l a t e x i t &gt; X f G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V y s J l e v B T / B 6 7 w I X U K x b e X O S 7 U o = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u B o v g K i Z a U R G k 4 M Z l B f u A J p T J d N I O n W S G m Y l Y Q v 0 V N y 4 U c e u H u P N v n L Z Z a O u B C 4 d z 7 u X e e 0 L B q N K u + 2 0 V l p Z X V t e K 6 6 W N z a 3 t H X t 3 r 6 l 4 K j F p Y M 6 4 b I d I E U Y T 0 t B U M 9 I W k q A 4 Z K Q V D m 8 m f u u B S E V 5 c q 9 H g g Q x 6 i c 0 o h h p I 3 X t 8 j H 0 r + A l 9 J E Q k j 9 C 1 z m t d u 2 K 6 7 h T w E X i 5 a Q C c t S 7 9 p f f 4 z i N S a I x Q 0 p 1 P F f o I E N S U 8 z I u O S n i gi E h 6 h P O o Y m K C Y q y K b H j + G h U X o w 4 t J U o u F U / T 2 R o V i p U R y a z h j p g Z r 3 J u J / X i f V 0 U W Q 0 U S k m i R 4 t i h K G d Q c T p K A P S o J 1 m x k C M K S m l s h H i C J s D Z 5 l U w I 3 v z L i 6 R 5 4 n h n j n t X r d S u 8 z i K Y B 8 c g C P g g X N Q A 7 e g D h o A g x F 4 B q / g z X q y X q x 3 6 2 P W W r D y m T L 4 A + v z B 1 w U k q M = &lt; / l a t e x i t &gt; " w 6 G s + f D F U t A U R S 7 5 p J y q H W Z b G C Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h p 5 K I o i c p e v F Y w b S F N p T N d t M u 3 W z C 7 k Q o p b / B i w d F v P q D v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C 1 M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 b M a j 2 y h W 3 5 s 5 B V o m X k w r k a P T K X 9 1 + w r K Y K 2 S S G t P x 3 B S D C d U o m O T T U j c z P K V s R A e 8 Y 6 m i M T f B Z H 7 s l J x Z p U + i R N t S S O b q 7 4 k J j Y 0 Z x 6 H t j C k O z b I 3 E / / z O h l G 1 8 F E q D R D r t h i U Z R J g g m Z f U 7 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n x K N g R v + e V V0 j y v e Z c 1 9 + G i U r / J 4 y j C C Z x C F T y 4 g j r c Q w N 8 Y C D g G V 7 h z V H O i / P u f C x a C 0 4 + c w x / 4 H z + A C 8 H j k I = &lt; / l a t e x i t &gt; ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N U F h Q H y k 3 T e E o 0 P O 6 x x y 0 q Y c 4 m M = " &gt; A A A B 6 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w T w g W c L s Z D Y Z M j O 7 z E M I S 3 7 B i w d F v P p D 3 v w b Z 5 M 9 a G J B Q 1 H V T X d X l H K m j e 9 / e 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 1 o l V h L Z I w h P V j b C m n E n a M s x w 2 k 0 V x S L i t B N N 7 n K / 8 0 S V Z o l 8 N N O U h g K P J I s Z w S a X + t q K Q b X m 1 / 0 5 0 C o J C l K D A s 1 B 9 a s / T I g V V B r C s d a 9 w E 9 N m G F l G O F 0 V u l b T V N M J n h E e 4 5 K L K g O s / m t M 3 T m l C G K E + V K G j R X f 0 9 k W G g 9 F Z H r F N i M 9 b K X i / 9 5 P W v i m z B j M r W G S r J Y F F u O T I L y x 9 G Q K U o M n z q C i W L u V k T G W G F i X D w V F 0 K w / P I q a V / U g 6 u 6 / 3 B Z a 9 w W c Z T h B E 7 h H A K 4 h g b c Q x N a Q G A M z / A K b 5 7 w X r x 3 7 2 P R W v K K m W P 4 A + / z B z I J j l M = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R C s 2 t b O y + G t s M v U 9 D o + t G 3 I g b a 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 E 0 U o C N p Y R T A w k R 9 j b z C V L 9 n b P 3 T 0 h h P w J G w t F b P 0 7 d v 4 b N 8 k V m v h g 4 P H e D D P z o l R w Y 3 3 / 2 y u s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p l G Z Z t h g S i j d i q h B w S U 2 L L c C W 6 l G m k Q C H 6 L h z d R / e E J t u J L 3 d p R i m N C + 5 D F n 1 D q p 1 c H U c K F k t 1 z x q / 4 M Z J k E O a l A j n q 3 / N X p K Z Y l K C 0 T 1 J h 2 4 K c 2 H F N t O R M 4 K X U y g y l l Q 9 r H t q O S J m j C 8 e z e C T l x S o / E S r u S l s z U 3 x N j m h g z S i L X m V A 7 M I v e V P z P a 2 c 2 v g r H X K a Z R c n m i + J M E K v I 9 H n S 4 x q Z F S N H K N P c 3 U r Y g G r K r I u o 5 E I I F l 9 e J s 2 z a n B R 9 e / O K 7 X r P I 4 i H M E x n E I A l 1 C D W 6 h D A x g I e I Z X e P M e v R f v 3 f u Y t x a 8 f O Y Q / s D 7 / A F M 7 5 A g &lt; / l a t e x i t &gt; ✏ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d w k D N R Q T s C Q I L V f A 3 T 5 q g V d h f S I = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U R V d S c O O y g n 1 A Z y i Z N N O G Z p K Q Z M R S + h t u X C j i 1 p 9 x 5 9 + Y t r P Q 1 g O X e z j n X n J z Y s W Z s b 7 / 7 R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R y 8 h M E 9 o k k k v d i b G h n A n a t M x y 2 l G a 4 j T m t B 2 P b m d + + 5 F q w 6 R 4 s G N F o x Q P B E s Y w d Z J Y Y i V 0 v I J + V W / 1 i t X X J s D r Z I g J x X I 0 e i V v 8 K + J F l K h S U c G 9 M N f G W j C d a W E U 6 n p T A z V G E y w g P a d V T g l J p o M r 9 5 i s 6 c 0 k e J 1 K 6 E R X P 1 9 8 Y E p 8 a M 0 9 h N p t g O z b I 3 E / / z u p l N r q M J E y q z V J D F Q 0 n G k Z V o F g D q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r I u p 5 E I I l r + 8 S l q 1 a n B Z 9 e 8 v K v W b P I 4 i n M A p n E M A V 1 C H O 2 h A E w g o e I Z X e P M y 7 8 V 7 9 z 4 W o w U v 3 z m G P / A + f w C w T J D I &lt; / l a t e x i t &gt; ⇡ 0.02 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v R g t E O / r i 1 4 S A O V v U x + T Z Z 1 y / g = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w c R I s o T Z 2 d l k y D y W m V k h h H y F F w + K e P V z v P k 3 z i Z 7 0 M S C h q K q m + 6 u K O X M W N / / 9 k o r q 2 v r G + X N y t b 2 z u 5 e d f + g b V S m C W 0 R x Z X u R N h Q z i R t W W Y 5 7 a S a Y h F x + h C N b n L / 4 Y l q w 5 S 8 t + O U h g I P J E s Y w d Z J j 7 2 I D V S s b K V f r f l 1 f w a 0 T I K C 1 K B A s 1 / 9 6 s W K Z I J K S z g 2 p h v 4 q Q 0 n W F t G O J 1 W e p m h K S Y j P K B d R y U W 1 I S T 2 c F T d O K U G C V K u 5 I W z d T f E x M s j B m L y H U K b I d m 0 c v F / 7 x u Z p O r c M J k m l k q y X x R k n F k F c q / R z H T l F g + d g Q T z d y t i A y x x s S 6 j P I Q g s W X l 0 n 7 r B 5 c 1 P 2 7 8 1 r j u o i j D E d w D K c Q w C U 0 4 B a a 0 A I C A p 7 h F d 4 8 7 b 1 4 7 9 7 H v L X k F T O H 8 A f e 5 w 9 o o p A i &lt; / l a t e x i t &gt; K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 G T h C L r L C G x o D i K u Z k b b t y 8 7 P E Q = " &gt; A A A C B X i c b V D L S s N A F J 3 4 r P U V d a m L w S J 0 V R J R d C V F F 7 q s Y B / Q h D C Z T t q h k w c z N 2 I J 3 b j x V 9 y 4 U M S t / + D O v 3 G a Z q G t B y 4 c z r m X e + / x E 8 E V W N a 3 s b C 4 t L y y W l o r r 2 9 s b m 2 b O 7 s t F a e S s i a N R S w 7 P l F M 8 I g 1 g Y N g n U Q y E v q C t f 3 h 1 c R v 3 z O p e B z d w S h h b k j 6 E Q 8 4 J a A l z z x w V B p i 5 5 L 3 + 1 U c e A 6 w B 8 i u x 7 l S 9 c y K V b N y 4 H l i F 6 S C C j Q 8 8 8 v p x T Q N W Q R U E K W 6 t p W A m x E J n A o 2 L j u p Y g m h Q 9 J n X U 0 j E j L l Z v k X Y 3 y k l R 4 O Y q k r A p y r v y c y E i o 1 C n 3 d G R I Y q F l v I v 7 n d V M I z t 2 M R 0 k K L K L T R U E q M M R 4 E g n u c c k o i J E m h E q u b 8 V 0 Q C S h o I M r 6 x D s 2 Z f n S e u 4 Z p / W r N u T S v 2 i i K O E 9 t E h q i I b n a E 6 u k E N 1 E Q U P a J n 9 I r e j C f j x X g 3 P q a t C 0 Y x s 4 f + w P j 8 A e x r l 4 w = &lt; / l a t e x i t &gt; X f G ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s B 5 C O w C c N G t P d L J C / l O g n C u E F u A = " &gt; A A A B 7 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B L 2 V X K n q S o h e P F e w H t E v J p t l t b D Z Z k q x Q l v 4 H L x 4 U 8 e r / 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 0 g 4 0 8 Z 1 v 5 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a G m Z K k K b R H K p O g H W l D N B m 4 Y Z T j u J o j g O O G 0 H o 9 u p 3 3 6 i S j M p H s w 4 o X 6 M I 8 F C R r C x U q t 3 w 6 L o r F + u u F V 3 B r R M v J x U I E e j X / 7 q D S R J Y y o M 4 V j r r u c m x s + w M o x w O i n 1 U k 0 T T E Y 4 o l 1 L B Y 6 p 9 r P Z t R N 0 Y p U B C q W y J Q y a q b 8 n M h x r P Y 4 D 2 x l j M 9 S L 3 l T 8 z + u m J r z y M y a S 1 F B B 5 o v C l C M j 0 f R 1 N G C K E s P H l m C i m L 0 V k S F W m B g b U M m G 4 C 2 + v E x a 5 1 X v o u r e 1 y r 1 6 z y O I h z B M Z y C B 5 d Q h z t o Q B M I P M I z v M K b I 5 0 X 5 9 3 5 m L c W n H z m E P 7 A + f w B 8 2 i O t A = = &lt; / l a t e x i t &gt; _ b a s e 6 4 = " R i X T n b E l 8 / A b t A x o k R 4 S v s T M I u I = " &gt; A A A B + H i c b V D L S g N B E J y N r x g f W f X o Z T A I n s K u K H o M e t B j B P O A 7 L L M T m a T I b M P Z n r E u O R L v H h Q x K u f 4 s 2 / c Z L s Q R M L G o q q b r q 7 w k x w B Y 7 z b Z V W V t f W N 8 q b l a 3 t n d 2 q v b f f V q m W l L V o K l L Z D Y l i g i e s B R w E 6 2 a S k T g U r B O O r q d + 5 4 F J x d P k H s Y Z 8 2 M y S H j E K Q E j B X b V U z r G U e A B e 4 T 8 Z h L Y N a f u z I C X i V u Q G i r Q D O w v r 5 9 S H b M E q C B K 9 V w n A z 8 n E j g V b F L x t G I Z o S M y Y D 1 D E x I z 5 e e z w y f 4 2 C h 9 H K X S V A J 4 p v 6 e y E m s 1 D g O T W d M Y K g W v a n 4 n 9 f T E F 3 6 O U 8 y D S y h 8 0 W R F h h S P E 0 B 9 7 l k F M T Y E E I l N 7 d i O i S S U D B Z V U w I 7 u L L y 6 R 9 W n f P 6 8 7 d W a 1 x V c R R R o f o C J 0 g F 1 2 g B r p F T d R C F G n 0 j F 7 R m / V k v V j v 1 s e 8 t W Q V M w f o D 6 z P H + C R k z s = &lt; / l a t e x i t &gt; X f G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V y s J l e v B T / B 6 7 w I X U K x b e X O S 7 U o = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u B o v g K i Z a U R G k 4 M Z l B f u A J p T J d N I O n W S G m Y l Y Q v 0 V N y 4 U c e u H u P N v n L Z Z a O u B C 4 d z 7 u X e e 0 L B q N K u + 2 0 V l p Z X V t e K 6 6 W N z a 3 t H X t 3 r 6 l 4 K j F p Y M 6 4 b I d I E U Y T 0 t B U M 9 I W k q A 4 Z K Q V D m 8 m f u u B S E V 5 c q 9 H g g Q x 6 i c 0 o h h p I 3 X t 8 j H 0 r + A l 9 J E Q k j 9 C 1 z m t d u 2 K 6 7 h T w E X i 5 a Q C c t S 7 9 p f f 4 z i N S a I x Q 0 p 1 P F f o I E N S U 8 z I u O S n i g i E h 6 h P O o Y m K C Y q y K b H j + G h U X o w 4 t J U o u F U / T 2 R o V i p U R y a z h j p g Z r 3 J u J / X i f V 0 U W Q 0 U S k m i R 4 t i h K G d Q c T p K A P S o J 1 m x k C M K S m l s h H i C J s D Z 5 l U w I 3 v z L i 6 R 5 4 n h n j n t X r d S u 8 z i K Y B 8 c g C P g g X N Q A 7 e g D h o A g x F 4 B q / g z X q y X q x 3 6 2 P W W r D y m T L 4 A + v z B 1 w U k q M = &lt; / l a t e x i t &gt; " w 6 G s + f D F U t A U R S 7 5 p J y q H W Z b G C Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h p 5 K I o i c p e v F Y w b S F N p T N d t M u 3 W z C 7 k Q o p b / B i w d F v P q D v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C 1 M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 b M a j 2 y h W 3 5 s 5 B V o m X k w r k a P T K X 9 1 + w r K Y K 2 S S G t P x 3 B S D C d U o m O T T U j c z P K V s R A e 8 Y 6 m i M T f B Z H 7 s l J x Z p U + i R N t S S O b q 7 4 k J j Y 0 Z x 6 H t j C k O z b I 3 E / / z O h l G 1 8 F E q D R D r t h i U Z R J g g m Z f U 7 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n x K N g R v + e V V 0 j y v e Z c 1 9 + G i U r / J 4 y j C C Z x C F T y 4 g j r c Q w N 8 Y C D g G V 7 h z V H O i / P u f C x a C 0 4 + c w x / 4 H z + A C 8 H j k I = &lt; / l a t e x i t &gt; ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 9 7 z J L e M X u t 6 H W C Y T 6 e z D b P 7 g U E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B L y U R R U 9 S 9 O K x g m k L b S i b 7 b R d u t m E 3 Y 1 Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y a C a + O 6 3 0 5 h Z X V t f a O 4 W d r a 3 t n d K + 8 f N H S c K o Y + i 0 W s W i H V K L h E 3 3 A j s J U o p F E o s B m O 7 q Z + 8 w m V 5 r F 8 N O M E g 4 g O J O 9 z R o 2 V / M 4 t H 5 x 1 y x W 3 6 s 5 A l o m X k w r k q H f L X 5 1 e z N I I p W G C a t 3 2 3 M Q E G V W G M 4 G T U i f V m F A 2 o g N s W y p p h D r I Z s d O y I l V e q Q f K 1 v S k J n 6 e y K j k d b j K L S d E T V D v e h N x f + 8 d m r 6 1 0 H G Z Z I a l G y + q J 8 K Y m I y / Z z 0 u E J m x N g S y h S 3 t x I 2 p I o y Y / M p 2 R C 8 x Z e X S e O 8 6 l 1 W 3 Y e L S u 0 m j 6 M I R 3 A M p + D B F d T g H u r g A w M O z / A K b 4 5 0 X p x 3 5 2 P e W n D y m U P 4 A + f z B z C L j k M = &lt; / l a t e x i t &gt; ⌘ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N U F h Q H y k 3 T e E o 0 P O 6 x x y 0 q Y c 4 m M = " &gt; A A A B 6 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w T w g W c L s Z D Y Z M j O 7 z E M I S 3 7 B i w d F v P p D 3 v w b Z 5 M 9 a G J B Q 1 H V T X d X l H K m j e 9 / e 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 1 o l V h L Z I w h P V j b C m n E n a M s x w 2 k 0 V x S L i t B N N 7 n K / 8 0 S V Z o l 8 N N O U h g K P J I s Z w S a X + t q K Q b X m 1 / 0 5 0 C o J C l K D A s 1 B 9 a s / T I g V V B r C s d a 9 w E 9 N m G F l G O F 0 V u l b T V N M J n h E e 4 5 K L K g O s / m t M 3 T m l C G K E + V K G j R X f 0 9 k W G g 9 F Z H r F N i M 9 b K X i / 9 5 P W v i m z B j M r W G S r J Y F F u O T I L y x 9 G Q K U o M n z q C i W L u V k T G W G F i X D w V F 0 K w / P I q a V / U g 6 u 6 / 3 B Z a 9 w W c Z T h B E 7 h H A K 4 h g b c Q x N a Q G A M z / A K b 5 7 w X r x 3 7 2 P R W v K K m W P 4 A + / z B z I J j l M = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R C s 2 t b O y + G t s M v U 9 D o + t G 3 I g b a 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 E 0 U o C N p Y R T A w k R 9 j b z C V L 9 n b P 3 T 0 h h P w J G w t F b P 0 7 d v 4 b N 8 k V m v h g 4 P H e D D P z o l R w Y 3 3 / 2 y u s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p l G Z Z t h g S i j d i q h B w S U 2 L L c C W 6 l G m k Q C H 6 L h z d R / e E J t u J L 3 d p R i m N C + 5 D F n 1 D q p 1 c H U c K F k t 1 z x q / 4 M Z J k E O a l A j n q 3 / N X p K Z Y l K C 0 T 1 J h 2 4 K c 2 H F N t O R M 4 K X U y g y l l Q 9 r H t q O S J m j C 8 e z e C T l x S o / E S r u S l s z U 3 x N j m h g z S i L X m V A 7 M I v e V P z P a 2 c 2 v g r H X K a Z R c n m i + J M E K v I 9 H n S 4 x q Z F S N H K N P c 3 U r Y g G r K r I u o 5 E I I F l 9 e J s 2 z a n B R 9 e / O K 7 X r P I 4 i H M E x n E I A l 1 C D W 6 h D A x g I e I Z X e P M e v R f v 3 f u Y t x a 8 f O Y Q / s D 7 / A F M 7 5 A g &lt; / l a t e x i t &gt; ✏ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d w k D N R Q T s C Q I L V f A 3 T 5 q g V d h f S I = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s w U R V d S c O O y g n 1 A Z y i Z N N O G Z p K Q Z M R S + h t u X C j i 1 p 9 x 5 9 + Y t r P Q 1 g O X e z j n X n J z Y s W Z s b 7 / 7 R X W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R y 8 h M E 9 o k k k v d i b G h n A n a t M x y 2 l G a 4 j T m t B 2 P b m d + + 5 F q w 6 R 4 s G N F o x Q P B E s Y w d Z J Y Y i V 0 v I J + V W / 1 i t X X J s D r Z I g J x X I 0 e i V v 8 K + J F l K h S U c G 9 M N f G W j C d a W E U 6 n p T A z V G E y w g P a d V T g l J p o M r 9 5 i s 6 c 0 k e J 1 K 6 E R X P 1 9 8 Y E p 8 a M 0 9 h N p t g O z b I 3 E / / z u p l N r q M J E y q z V J D F Q 0 n G k Z V o F g D q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r I u p 5 E I I l r + 8 S l q 1 a n B Z 9 e 8 v K v W b P I 4 i n M A p n E M A V 1 C H O 2 h A E w g o e I Z X e P M y 7 8 V 7 9 z 4 W o w U v 3 z m G P / A + f w C w T J D I &lt; / l a t e x i t &gt; ⇡ 0.02 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v R g t E O / r i 1 4 S A O V v U x + T Z Z 1 y / g = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o i c J e P E Y w c R I s o T Z 2 d l k y D y W m V k h h H y F F w + K e P V z v P k 3 z i Z 7 0 M S C h q K q m + 6 u K O X M W N / / 9 k o r q 2 v r G + X N y t b 2 z u 5 e d f + g b V S m C W 0 R x Z X u R N h Q z i R t W W Y 5 7 a S a Y h F x + h C N b n L / 4 Y l q w 5 S 8 t + O U h g I P J E s Y w d Z J j 7 2 I D V S s b K V f r f l 1 f w a 0 T I K C 1 K B A s 1 / 9 6 s W K Z I J K S z g 2 p h v 4 q Q 0 n W F t G O J 1 W e p m h K S Y j P K B d R y U W 1 I S T 2 c F T d O K U G C V K u 5 I W z d T f E x M s j B m L y H U K b I d m 0 c v F / 7 x u Z p O r c M J k m l k q y X x R k n F k F c q / R z H T l F g + d g Q T z d y t i A y x x s S 6 j P I Q g s W X l 0 n 7 r B 5 c 1 P 2 7 8 1 r j u o i j D E d w D K c Q w C U 0 4 B a a 0 A I C A p 7 h F d 4 8 7 b 1 4 7 9 7 H v L X k F T O H 8 A f e 5 w 9 o o p A i &lt; / l a t e x i t &gt; K &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 G T h C L r L C G x o D i K u Z k b b t y 8 7 P E Q = " &gt; A A A C B X i c b V D L S s N A F J 3 4 r P U V d a m L w S J 0 V R J R d C V F F 7 q s Y B / Q h D C Z T t q h k w c z N 2 I J 3 b j x V 9 y 4 U M S t / + D O v 3 G a Z q G t B y 4 c z r m X e + / x E 8 E V W N a 3 s b C 4 t L y y W l o r r 2 9 s b m 2 b O 7 s t F a e S s i a N R S w 7 P l F M 8 I g 1 g Y N g n U Q y E v q C t f 3 h 1 c R v 3 z O p e B z d w S h h b k j 6 E Q 8 4 J a A l z z x w V B p i 5 5 L 3 + 1 U c e A 6 w B 8 i u x 7 l S 9 c y K V b N y 4 H l i F 6 S C C j Q 8 8 8 v p x T Q N W Q R U E K W 6 t p W A m x E J n A o 2 L j u p Y g m h Q 9 J n X U 0 j E j L l Z v k X Y 3 y k l R 4 O Y q k r A p y r v y c y E i o 1 C n 3 d G R I Y q F l v I v 7 n d V M I z t 2 M R 0 k K L K L T R U E q M M R 4 E g n u c c k o i J E m h E q u b 8 V 0 Q C S h o I M r 6 x D s 2 Z f n S e u 4 Z p / W r N u T S v 2 i i K O E 9 t E h q i I b n a E 6 u k E N 1 E Q U P a J n 9 I r e j C f j x X g 3 P q a t C 0 Y x s 4 f + w P j 8 A e x r l 4 w = &lt; / l a t e x i t &gt; X f G ⇣ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s B 5 C O w C c N G t P d L J C / l O g n C u E F u A = " &gt; A A A B 7 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B L 2 V X K n q S o h e P F e w H t E v J p t l t b D Z Z k q x Q l v 4 H L x 4 U 8 e r / 8 e a / M W 3 3 o K 0 P B h 7 v z T A z L 0 g 4 0 8 Z 1 v 5 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a G m Z K k K b R H K p O g H W l D N B m 4 Y Z T j u J o j g O O G 0 H o 9 u p 3 3 6 i S j M p H s w 4 o X 6 M I 8 F C R r C x U q t 3 w 6 L o r F + u u F V 3 B r R M v J x U I E e j X / 7 q D S R J Y y o M 4 V j r r u c m x s + w M o x w O i n 1 U k 0 T T E Y 4 o l 1 L B Y 6 p 9 r P Z t R N 0 Y p U B C q W y J Q y a q b 8 n M h x r P Y 4 D 2 x l j M 9 S L 3 l T 8 z + u m J r z y M y a S 1 F B B 5 o v C l C M j 0 f R 1 N G C K E s P H l m C i m L 0 V k S F W m B g b U M m G 4 C 2 + v E x a 5 1 X v o u r e 1 y r 1 6 z y O I h z B M Z y C B 5 d Q h z t o Q B M I P M I z v M K b I 5 0 X 5 9 3 5 m L c W n H z m E P 7 A + f w B 8 2 i O t A = = &lt; / l a t e x i t &gt; ! (c) Our occlusion-aware approach Occlusion-aware census transform: (a) Computing the (continuous) census signature of two image patches and the corresponding occlusion mask, (b) standard approach of computing the Hamming distance of the census signature divided by the number of pixels, and (c) our occlusion-aware Hamming distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 8 .</head><label>8</label><figDesc>3,3] 2 f G T (I, p, y), T (Ĩ, p, y) O(p + y) y∈[−3,3] 2 O(p + y) + t e x i t s h a 1 _ b a s e 6 4 = " U 8 O D M 7 j 1 E F 4 x o F 5 v + a u Q z L 2 q d 1 I = " &gt; A A A B 8 X i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w p 0 o p g z Y W F h E M B + Y H G F v b 5 M s 2 d s 7 d u f E c O R f 2 F g o Y u u / s f P f u E m u 0 M Q H A 4 / 3 Z p i Z F y R S G H T d b 2 d l d W 1 9 Y 7 O w V d z e 2 d 3 b L x 0 c N k 2 c a s Y b L J a x b g f U c C k U b 6 B A y d u J 5 j Q K J G 8 F o + u p 3 3 r k 2 o h Y 3 e M 4 4 X 5 E B 0 r 0 B a N o p Y f b X h f 5 E 2 b h p F c q u x V 3 B r J M v J y U I U e 9 V / r q h j F L I 6 6 Q S W p M x 3 M T 9 D O q U T D J J 8 V u a n h C 2 Y g O e M d S R S N u / G x 2 8 Y S c W i U k / V j b U k h m 6 u + J j E b G j K P A d k Y U h 2 b R m 4 r / e Z 0 U + 1 U / E y p J k S s 2 X 9 R P J c G Y T N 8 n o d C c o R x b Q p k W 9 l b C h l R T h j a k o g 3 B W 3 x 5 m T T P K 9 5 l x b 2 7 K N e q e R w F O I Y T O A M P r q A G N 1 C H B j B Q 8 A y v 8 O Y Y 5 8 V 5 d z 7 m r S t O P n M E f + B 8 / g D Z b Z E A &lt; / l a t e x i t &gt; L d Scene Flow Loss &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z o v g q 4 A h o h w X b B P G V q F u p e j s m q s = " &gt; A A A B 8 n i c b V A 9 S w N B E N 2 L X z F + R S 1 t D o N g F e 5 E M W X A x s I i g v m A 5 A h 7 m 7 l k y d 7 u s T s n h i M / w 8 Z C E V t / j Z 3 / x k 1 y h S Y + G H i 8 N 8 P M v D A R 3 K D n f T u F t f W N z a 3 i d m l n d 2 / / o H x 4 1 D I q 1 Q y a T A m l O y E 1 I L i E J n I U 0 E k 0 0 D g U 0 A 7 H N z O / / Q j a c C U f c J J A E N O h 5 B F n F K 3 U v e v 3 E J 4 w M 9 G 0 X 6 5 4 V W 8 O d 5 X 4 O a m Q H I 1 + + a s 3 U C y N Q S I T 1 J i u 7 y U Y Z F Q j Z w K m p V 5 q I K F s T I f Q t V T S G E y Q z U + e u m d W G b i R 0 r Y k u n P 1 9 0 R G Y 2 M m c W g 7 Y 4 o j s + z N x P + 8 b o p R L c i 4 T F I E y R a L o l S 4 q N z Z / + 6 A a 2 A o J p Z Q p r m 9 1 W U j q i l D m 1 L J h u A v v 7 x K W h d V / 6 r q 3 V 9 W 6 r U 8 j i I 5 I a f k n P j k m t T J L W m Q J m F E k W f y S t 4 c d F 6 c d + d j 0 V p w 8 p l j 8 g f O 5 w + z 9 Z F / &lt; / l a t e x i t &gt; L sf (a) Detaching the gradient between scene flow loss and disparity decoder Our gradient detaching strategy: (a) Detaching the gradient between the scene flow loss and the disparity decoder in the early stages of the training improves (b) the training loss convergence and (c) the scene flow accuracy significantly. with occlusion state O (with O(p) = 1 if visible) and T (I, p, y) = I(p + y) − I(p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>( a )Figure 9 .</head><label>a9</label><figDesc>Overlayed input images (b) Ours (c) Direct baseline<ref type="bibr" target="#b22">[23]</ref> Qualitative comparison of temporal consistency: Each scene shows (a) overlayed input images and scene flow difference maps of (b) our method, and (c) the direct baseline of<ref type="bibr" target="#b22">[23]</ref>, visualized using an optical flow color coding. The dashed regions highlight our more temporally consistent estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 10 .</head><label>10</label><figDesc>Overlayed input images (b) Disparity error map (c) Disparity change error map (d) Optical flow error map (e) Scene flow error map Qualitative comparison with the direct baseline of [23]: Each scene shows (a) overlayed input images and error maps for (b) disparity, (c) disparity change, (d) optical flow, and (e) scene flow. Please refer to the color code above the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>( a )Figure 11 .</head><label>a11</label><figDesc>Overlayed input images (b) Depth map (c) Scene flow visualization Generalization to nuScenes (top), Monkaa (middle), and DAVIS (bottom) datasets: Each scene shows (a) overlayed input images, (b) depth, and (c) a 3D scene flow visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 12 .</head><label>12</label><figDesc>3D scene flow color coding scheme using the CIE-LAB color space: Each figure shows a sliced sphere along each plane for ease of visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>( a )Figure 16 .</head><label>a16</label><figDesc>Overlayed input images (b) Depth map (c) Scene flow visualization (a) Overlayed input images (b) Depth map (c) Scene flow visualization Generalization to nuScenes, DAVIS, Driving, and Monkaa datasets: Each scene shows (a) overlayed input images, (b) depth, and (c) 3D scene flow visualization. The left side demonstrates good generalization of our model to nuScenes, DAVIS, Driving, and Monkaa datasets. Nonetheless, failure cases do exist, provided on the right side and highlighted with dashed gray squares. A typical failure mode is inaccurate depth estimation of foreground objects that are not seen in the training set as well as reflective road surfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Configuration</cell><cell>D1-all D2-all Fl-all SF-all</cell></row><row><cell>Single joint decoder (Fig. 2a, [23])</cell><cell>31.25 34.86 23.49 47.05</cell></row><row><cell>Removing the context network (Fig. 2b)</cell><cell>33.04 36.45 36.45 48.97</cell></row><row><cell>Splitting at the last layer</cell><cell>34.11 37.28 24.49 49.92</cell></row><row><cell>Splitting at the 2 nd -to-last layer (Fig. 2c)</cell><cell>30.50 34.72 24.72 47.53</cell></row><row><cell>Splitting at the 3 rd -to-last layer</cell><cell>31.04 35.79 24.57 47.85</cell></row><row><cell>Splitting at the 4 th -to-last layer</cell><cell>32.21 36.28 24.65 48.84</cell></row><row><cell>Splitting into two separate decoders</cell><cell>32.12 36.14 25.02 48.66</cell></row></table><note>Scene flow accuracy of split-decoder designs: Remov- ing the context network degrades the accuracy (cf . Sec. 4.2 for a description of the metrics), but splitting the decoder at the 2 nd -to- last layer yields competitive accuracy while being stable to train.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.1. We first train the original implementation of Hur et al. [23] on our train split. Interestingly, the accuracy</figDesc><table><row><cell>Baseline type</cell><cell>D1-all</cell><cell>D2-all</cell><cell>Fl-all</cell><cell>SF-all</cell></row><row><cell>Hur et al. [23] (on original split)</cell><cell>31.25</cell><cell>34.86</cell><cell>23.49</cell><cell>47.05</cell></row><row><cell>Hur et al. [23] (on our train split)</cell><cell>36.70</cell><cell>45.50</cell><cell>24.65</cell><cell>60.88</cell></row><row><cell>[23] − Context Net</cell><cell>34.24</cell><cell>37.32</cell><cell>25.06</cell><cell>50.49</cell></row><row><cell>[23] − Context Net + [28]</cell><cell>33.56</cell><cell>34.49</cell><cell>22.92</cell><cell>46.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The accuracy of the original baseline of<ref type="bibr" target="#b22">[23]</ref> on our train split (second row) is significantly lower than on the original split (first row). Removing the context network (third row) and exploiting the findings from<ref type="bibr" target="#b27">[28]</ref> (fourth row, our advanced two-frame baseline) improves both the accuracy and the training stability.</figDesc><table><row><cell>Multi-frame extension</cell><cell>Occ-aware census loss</cell><cell cols="4">D1-all D2-all Fl-all SF-all</cell></row><row><cell cols="2">(Our advanced baseline)</cell><cell>33.56</cell><cell>34.49</cell><cell>22.92</cell><cell>46.70</cell></row><row><cell></cell><cell></cell><cell>28.68</cell><cell>32.58</cell><cell>21.11</cell><cell>42.37</cell></row><row><cell></cell><cell></cell><cell>30.35</cell><cell>31.92</cell><cell>21.86</cell><cell>43.87</cell></row><row><cell></cell><cell></cell><cell>27.33</cell><cell>30.44</cell><cell>18.92</cell><cell>39.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of our multi-frame estimation (Sec. 3.2): Using a ConvLSTM with forward-warping the hidden states improves the accuracy up to 9.3% (relative improvement).</figDesc><table><row><cell>Loss type</cell><cell cols="4">D1-all D2-all Fl-all SF-all</cell></row><row><cell>Brightness difference + SSIM</cell><cell>28.68</cell><cell>32.58</cell><cell>21.11</cell><cell>42.37</cell></row><row><cell>Standard census [28]</cell><cell>28.89</cell><cell>31.69</cell><cell>20.55</cell><cell>41.71</cell></row><row><cell>Occlusion-aware census (ours)</cell><cell>27.33</cell><cell>30.44</cell><cell>18.92</cell><cell>39.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Cost 1</head><label>1</label><figDesc>We use the model without the context network for more stable training, see main paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Cont. Net. 34.24 37.32 25.06 50.49 [23] − Cont. Net. + CV. Norm. (Baseline) 31.91 35.31 24.80 48.29</figDesc><table><row><cell>Model</cell><cell>D1-all D2-all Fl-all SF-all</cell></row><row><cell cols="2">[23] − Applying each row on top of the baseline above:</cell></row><row><cell>Census loss</cell><cell>32.52 34.54 21.79 45.61</cell></row><row><cell>Using one less pyramid level</cell><cell>33.68 35.03 23.98 47.77</cell></row><row><cell>Data distillation</cell><cell>34.62 35.74 24.09 48.11</cell></row><row><cell>Using 640 × 640 resolution</cell><cell>33.12 34.59 22.43 48.28</cell></row><row><cell>Level dropout</cell><cell>(not converged)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>An empirical study of the key findings from<ref type="bibr" target="#b27">[28]</ref>: We take<ref type="bibr" target="#b22">[23]</ref> after discarding the context network (Cont. Net.) and applying the cost volume normalization (CV. Norm.) as the baseline network. Then we apply each key factor to the baseline and compare the scene flow accuracy. Numbers colored in blue outperform the baseline accuracy. volume normalization clearly improves the accuracy on all metrics, up to 4.4% (relative improvement) in terms of the scene flow accuracy. We choose this model as the baseline and conduct further empirical study on top of it.</figDesc><table><row><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Augmentation parameters: The augmentation parameters are uniformly sampled from the given sampling ranges.</figDesc><table><row><cell>Augmentation type</cell><cell>Sampling range</cell></row><row><cell>Random scaling</cell><cell>[0.93, 1.0]</cell></row><row><cell>Random cropping</cell><cell>[−3.5%, 3.5%]</cell></row><row><cell>Gamma adjustment</cell><cell>[0.8, 1.2]</cell></row><row><cell>Brightness change multiplication factor</cell><cell>[0.5, 2.0]</cell></row><row><cell>Color change multiplication factor</cell><cell>[0.8, 1.2]</cell></row><row><cell>14</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Scene flow accuracy on Driving and Monkaa datasets<ref type="bibr" target="#b39">[40]</ref> using the End-Point Error (EPE) metric: The accuracy of our model is generally low in the synthetic domain but is better on Driving than on Monkaa.</figDesc><table><row><cell>Dataset</cell><cell>Optical flow EPE</cell><cell>Scene flow EPE</cell></row><row><cell>Driving</cell><cell>3.6613</cell><cell>0.8845</cell></row><row><cell>Monkaa</cell><cell>12.8881</cell><cell>4.2982</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V P q + 6 z 4 h r X K 7 E f S s s X X X g Y H y g 1 w = " &gt; A A A C B X i c b V A 9 S w N B E N 2 L X z F + n V p q c R g E C z n u R N E y Y G M Z w X x A E s P e Z i 5 Z s v f B 7 p w k H N f Y + F d s L B S x 9 T / Y + W / c J F d o 4 o O B x 3 s z z M z z Y s E V O s 6 3 U V h a X l l d K 6 6 X N j a 3 t n f M 3 b 2 6 i h L J o M Y i E c m m R x U I H k I N O Q p o x h J o 4 A l o e M P r i d 9 4 A K l 4 F N 7 h O I Z O Q P s h 9 z m j q K W u e d g O K A 4 8 P x 1 l 9 2 2 E E a Y + U M y 6 K Z 6 K r G u W H d u Z w l o k b k 7 K J E e 1 a 3 6 1 e x F L A g i R C a p U y 3 V i 7 K R U I m c C s l I 7 U R B T N q R 9 a G k a 0 g B U J 5 1 + k V n H W u l Z f i R 1 h W h N 1 d 8 T K Q 2 U G g e e 7 p z c r O a 9 i f i f 1 0 r Q v + q k P I w T h J D N F v m J s D C y J p F Y P S 6 B o R h r Q p n k + l a L D a i k D H V w J R 2 C O / / y I q m f 2 e 6 F 7 d y e l y t 2 H k e R H J A j c k J c c k k q 5 I Z U S Y 0 w 8 k i e y S t 5 M 5 6 M F + P d + J i 1 F o x 8 Z p / 8 g f H 5 A + 4 l m W s = &lt; / l a t e x i t &gt; x feat t,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w r V 6 A j M B 0 8 j m K K D G Y J m 0 m / n j v A k = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q S b z O w f o A 2 R B 7 m V w z E / s 3 N M b l m l N Y S w S u y Q N V K L l 6 V / O I C Z p S C M g H E v Z t 6 0 E 3 A w L Y I T T v O a k k i a Y j P G Q 9 h W N c E i l m 0 2 f y I 1 D p Q y M I B a q I j C m 6 u + J D I d S T k J f d R Y n y 3 m v E P / z + i k E l 2 7 G o i Q F G p H Z o i D l B s R G k Y g x Y I I S 4 B N F M B F M 3 W q Q E R a Y g M q t p k K w 5 1 9 e J J 1 T 0 z 4 3 r Z u z R t M s 4 6 i i O j p A R 8 h G F 6 i J r l E L t R F B j + g Z v a I 3 7 U l 7 0 d 6 1 j 1 l r R S t n 9 t E f a J 8 / Z 3 O Y g A = = &lt; / l a t e x i t &gt; s f t,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L v P G I K 5 z C i m Q w k L L g j p a + V w a j O I= " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q S b z O w f o A 2 R + 7 m V w z E / s 3 N M b l m l N Y S w S u y Q N V K L l 6 V / O I C Z p S C M g H E v Z t 6 0 E 3 A w L Y I T T v O a k k i a Y j P G Q 9 h W N c E i l m 0 2 f y I 1 D p Q y M I B a q I j C m 6 u + J D I d S T k J f d R Y n y 3 m v E P / z + i k E l 2 7 G o i Q F G p H Z o i D l B s R G k Y g x Y I I S 4 B N F M B F M 3 W q Q E R a Y gM q t p k K w 5 1 9 e J J 1 T 0 z 4 3 r Z u z R t M s 4 6 i i O j p A R 8 h G F 6 i J r l E L t R F B j + g Z v a I 3 7 U l 7 0 d 6 1 j 1 l r R S t n 9 t E f a J 8 / Y T + Y f A = = &lt; / l a t e x i t &gt; s b t,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 + / o n T F J 6 O K v O 5 2 i Y R 6 x o j I S L p s = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 0 J C I o s u C G 5 c V 7 A P a E C a T S T t 0 8 m B m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 z x U 8 6 k s u 1 v Y 2 V 1 b X 1 j s 7 Z V 3 9 7 Z 3 d s 3 D x p d m W S C 0 A 5 J e C L 6 P p a U s 5 h 2 F F O c 9 l N B c e R z 2 v M n t 6 X f e 6 R C s i R + U N O U u h E e x S x k B C s t e W Z j G G E 1 9 s M 8 K L x c n f F z p / D M p m 3 Z M 6 B l 4 l S k C R X a n v k 1 D B K S R T R W h G M p B 4 6 d K j f H Q j H C a V E f Z p K m m E z w i A 4 0 j X F E p Z v P s h f o R C s B C h O h X 6 z Q T P 2 9 k e N I y m n k 6 8 k y q V z 0 S v E / b 5 C p 8 M b N W Z x m i s Z k f i j M O F I J K o t A A R O U K D 7 V B B P B d F Z E x l h g o n R d d V 2 C s / j l Z d K 9 s J w r y 7 6 / b L a s q o 4 a H M E x n I I D 1 9 C C O 2 h D B w g 8 w T O 8 w p t R G C / G u / E x H 1 0 x q p 1 D + A P j 8 w f S i Z Q 6 &lt; / l a t e x i t &gt; dt,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o e 9 e 4 Q 6 r z c j E r i D N w n p y M w A h x y 8 = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u g k V w o S E R R Z c F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V h C F m 7 8 F T c u F H H r R 7 j z b 5 y 0 W W j r g Q u H c + 7 l 3 n v 8 h D M J l v W t V Z a W V 1 b X q u u 1 j c 2 t 7 R 1 9 d 6 8 j 4 1 Q Q 2 i Y x j 0 X P x 5 J y F t E 2 M O C 0 l w i K Q 5 / T r j + + K v z u P R W S x d E t T B L q h n g Y s Y A R D E r y 9 L o T Y h j 5 Q T b K 7 x y g D 5 A F u Z f B M T + x c 0 9 v W K Y 1 h b F I 7 J I 0 U I m W p 3 8 5 g 5 i k I Y 2 A c C x l 3 7 Y S c D M s g B F O 8 5 q T S p p g M s Z D 2 l c 0 w i G V b j Z 9 I j c O l T I w g l i o i s C Y q r 8 n M h x K O Q l 9 1 V m c L O e 9 Q v z P 6 6 c Q X L o Z i 5 I U a E R m i 4 K U G x A b R S L G g A l K g E 8 U w U Q w d a t B R l h g A i q 3 m g r B n n 9 5 k X R O T f v c t G 7 O G k 2 z j K O K 6 u g A H S E b X a A m u k Y t 1 E Y E P a J n 9 I r e t C f t R X v X P m a t F a 2 c 2 U d / o H 3 + A F Y B m H U = &lt; / l a t e x i t &gt; h f t,l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V L p p T n C M F u Y c T 1 n x o T R G p N C C d t 0 = " &gt; A A A C A n i c b V D L S s N A F J 3 4 r P U V d S V u g k V w I S E R R Z c F N y 4 r 2 A c 0 M U w m k 3 b o 5 M H M j V h C c O O v u H G h i F u / w p 1 / 4 6 T N Q l s P X D i c c y / 3 3 u O n n E m w r G 9 t Y X F p e W W 1 t l Z f 3 9 j c 2 t Z 3 d j s y y Q S h b Z L w R P R 8 L C l n M W 0 D A 0 5 7 q a A 4 8 j n t + q O r 0 u / e U y F Z E t / C O K V u h A c x C x n B o C R P 3 3 c i D E M / z I P i z g H 6 A H l Y e D m c 8 M L T G 5 Z p T W D M E 7 s i D V S h 5 e l f T p C Q L K I x E I 6 l 7 N t W C m 6 O B T D C a V F 3 M k l T T E Z 4 Q P u K x j i i 0 s 0 n L x T G k V I C I 0 y E q h i M i f p 7 I s e R l O P I V 5 3 l w X L W K 8 X / v H 4 G 4 a W b s z j N g M Z k u i j M u A G J U e Z h B E x Q A n y s C C a C q V s N M s Q C E 1 C p 1 V U I 9 u z L 8 6 R z a t r n p n V z 1 m i a V R w 1 d I A O 0 T G y 0 Q V q o m v U Q m 1 E 0 C N 6 R q / o T X v S X r R 3 7 W P a u q B V M 3 v o D 7 T P H 2 J V l / 8 = &lt; / l a t e x i t &gt; d f t,l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R e q x m d b E F Q k z Y x r t W a z 1 b 0 c Z 9 T Y = " &gt; A A A C A n i c b V B N S 8 N A E N 3 4 W e t X 1 J N 4 C R b B g 4 R E F D 0 W v H i s Y D + g i W G z 3 b R L N 5 u w O x F L C F 7 8 K 1 4 8 K O L V X + H N f + O m 7 U F b H w w 8 3 p t h Z l 6 Y c q b A c b 6 N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 2 p J J O E N k n C E 9 k J s a K c C d o E B p x 2 U k l x H H L a D o d X p d + + p 1 K x R N z C K K V + j P u C R Y x g 0 F J g 7 n s x h k E Y 5 a q 4 8 4 A + Q B 4 V Q Q 4 n v A j M m m M 7 Y 1 j z x J 2 S G p q i E Z h f X i 8 h W U w F E I 6 V 6 r p O C n 6 O J T D C a V H 1 M k V T T I a 4 T 7 u a C h x T 5 e f j F w r r S C s 9 K 0 q k L g H W W P 0 9 k e N Y q V E c 6 s 7 y Y D X r l e J / X j e D 6 N L P m U g z o I J M F k U Z t y C x y j y s H p O U A B 9 p g o l k + l a L D L D E B H R q V R 2 C O / v y P G m d 2 u 6 5 7 d y c 1 e r 2 N I 4 K O k C H 6 B i 5 6 A L V 0 T V q o C Y i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F p X T C m M 3 v o D 4 z P H 3 o B m A 4 = &lt; / l a t e x i t &gt;</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code is available at github.com/visinf/multi-mono-sf.<ref type="bibr" target="#b2">3</ref> See supplementary material for more details and analyses.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 866008).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b22">[23]</ref> <p>is correct, ours is not Both failed <ref type="bibr">(a)</ref>  uate the scene flow accuracy of our model on the Driving and Monkaa <ref type="bibr" target="#b39">[40]</ref> datasets, using the End-Point-Error (EPE) metric. Though the accuracy is quite low in general, the accuracy on Driving is much better than that on Monkaa as can be expected. These overall results suggest that the accuracy of our self-supervised model depends on the training domain as well as the presence of target objects in the training dataset. From this observation, we can conclude that better generalization requires to train the model on a dataset with both diverse domains and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Self-Supervised Learning in the Wild</head><p>Through self-supervised learning, our method can in principle leverage vast amounts of unlabeled stereo web videos. However unlike training on a single, calibrated dataset (e.g., KITTI), this comes with several new technical challenges. Each stereo video is captured with different camera intrinsics and stereo configurations, whose values are even unknown. Without knowing them, the selfsupervised loss in Eq. (2) in the main paper cannot be directly applied because it assumes a fixed (or given) focal length and stereo baseline. We provide preliminary experiments to assess the feasibility of this scenario.</p><p>To train the network despite these unknowns, we first assume all videos share the same focal length. Then, we normalize the output disparity (say, d norm ) to be in a fixed, normalized scale and use it for the scene flow loss. For the disparity loss, we further linearly transform the disparity d norm to match the actual disparity scale of each given stereo input:</p><p>To obtain the coefficients a scale and b scale , we estimate optical flow between the stereo pair using our network, take the horizontal flow as pseudo disparity d pseudo , and use the least squares between the pseudo disparity d pseudo and the normalized disparity d norm . Though our network now outputs disparity and scene flow on a normalized scale, it is still able to estimate optical flow (by projecting scene flow to image coordinates) on the correct scale due to being supervised by the 2D view-synthesis proxy loss. For our preliminary experiments, we use the WSVD dataset <ref type="bibr" target="#b84">[83]</ref>, which is a collection of stereo videos from YouTube, for training and test on the DAVIS <ref type="bibr" target="#b48">[49]</ref> dataset. Training dataset preparation. When training on such diverse data collected on the web, it is important to make sure that the dataset is free of outliers. For preparing the training data, we carefully pre-process the WVSD dataset by first discarding videos with low resolution, poor image quality, texts, or watermarks. We further discard videos having vertical disparity, lens distortion, and narrow stereo baselines for better stereo supervision. Then, we check every frame and remove black-colored edges on the image boundaries, if applicable. Also, we find that many videos contain static scenes; thus we sample every 4 th frame and 2 nd sequence for having more dynamic motion in the training sequences. This pre-processing step, in the end, results in 58k training images from about 1.5M raw frames. Given the pre-trained model on KITTI, we further train the model on this curated dataset for 300k iteration steps.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning end-to-end scene flow by distilling single tasks knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10435" to="10442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view scene flow estimation: A view centered variational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahum</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3D scene flow estimation in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Abu</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2574" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PointFlowNet: Learning representations for rigid motion estimation from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despoina</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Donné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7962" to="7971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the unification of line processes, outlier rejection, and robust statistics with applications in early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="91" />
			<date type="published" when="1996-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mono-SF: Multi-view geometry meets single-view depth for monocular scene flow estimation of dynamic traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Brickwedde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2780" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">STaRFlow: A spatiotemporal recurrent cell for lightweight multi-frame optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Godet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Plyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">Le</forename><surname>Besnerais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiframe scene flow with piecewise rigid motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piecewise rigid scene flow with implicit motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Görlitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1758" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HPLFlowNet: Hierarchical permutohedral lattice FlowNet for scene flow estimation on large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3254" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning for optical flow estimation using pyramid convolution LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxin</forename><surname>Shuosen Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kinecting the dots: Particle based scene flow from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2290" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SphereFlow: 6 DoF scene flow from RGB-D pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hornáček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3526" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A variational method for scene flow estimation from stereo sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Devernay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised monocular scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slow Flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1406" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SENSE: A shared encoder network for scene-flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMMCVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="364" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bridging stereo matching and optical flow via spatiotemporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ying</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning residual flow as dynamic motion from stereo videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1180" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of scene flow estimation fusing with local rigidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="876" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DDFlow: Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8770" to="8777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SelFlow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FlowNet3D: Learning scene flow in 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="529" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning rigidity in dynamic scenes with a moving camera for 3D motion field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep rigid instance scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3614" to="3622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ProFlow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7251" to="7259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint 3D estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Workshop on Image Sequence Analysis (ISA)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Just go with the flow: Self-supervised scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himangi</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Okorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11177" to="11185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Continual occlusions and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Šochman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object scene flow with temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Šochman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5436" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SF-Net: Learning scene flow from RGB-D images with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Lue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ze</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dense semi-rigid scene flow estimation from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Devernay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="567" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A fusion approach for multi-frame optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2077" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cascaded scene flow prediction using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dense Lagrangian motion estimation with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PWOC-3D: Deep occlusion-aware end-to-end scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SceneFlowFields: Dense interpolation of sparse scene flow correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kuschk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1056" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SceneFlowFields++: Multiframe matching, visibility prediction, and robust interpolation for scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kuschk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="546" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional LSTM Network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2015</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient computation of optical flow using the census transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fridtjof</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast multi-frame stereo scene flow with motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3939" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundar</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundar</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="480" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Viewconsistent 3D scene flow estimation over multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="263" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An evaluation of data costs for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Piecewise rigid scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1377" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">3D scene flow estimation with a piecewise rigid scene model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">UnOS: Unified unsupervised optical-flow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8071" to="8081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">FlowNet3D++: Geometric losses for deep scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Stereoscopic scene flow computation for 3D motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="51" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">PointPWC-Net: Cost volume on point clouds for (self-)supervised scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Zhi Yuan Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="88" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Upgrading optical flow to 3D scene flow through optical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3D motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On 3D scene flow and structure estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="3526" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Robustness meets deep learning: An end-to-end hybrid pipeline for unsupervised learning of egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2015</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Overlayed input images (b) Depth map (c) Scene flow visualization (a) Overlayed input images (b) Depth map (c) Scene flow visualization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Self-supervised learning in the wild and generalization to DAVIS dataset [49]: Our model trained on the WSVD dataset [83] (a large amount of web videos) generalizes well to diverse scenes from the DAVIS dataset</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Web stereo video supervision for depth prediction from dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
