<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Bao</surname></persName>
							<email>baosiqi@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>He</surname></persName>
							<email>hehuang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>wang.fan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenquan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-ofthe-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, task agnostic pre-training with largescale transformer models <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> and general text corpora has achieved great success in natural language understanding <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> as well as natural language generation, especially open-domain dialogue generation. For instance, based on the general language model GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>, DialoGPT <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> is further trained for response generation using Reddit comments. To obtain a human-like opendomain chatbot, <ref type="bibr">Meena (Adiwardana et al., 2020)</ref> scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender <ref type="bibr" target="#b25">(Roller et al., 2020)</ref> further fine-tunes the pre-trained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. * Equal contribution.</p><p>Besides the above attempts from model scale and data selection, PLATO <ref type="bibr" target="#b6">(Bao et al., 2020)</ref> aims to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for response generation <ref type="bibr" target="#b34">(Zhao et al., 2017;</ref><ref type="bibr" target="#b10">Chen et al., 2019)</ref>. PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation.</p><p>In this work, we will try to scale up PLATO to PLATO-2 and discuss its effective training schema via curriculum learning <ref type="bibr" target="#b8">(Bengio et al., 2009</ref>). There are two stages involved in the whole learning process, as sketched in <ref type="figure" target="#fig_0">Figure 1</ref>. In the first stage, under the simplified one-to-one mapping modeling, a coarse-grained generation model is trained for appropriate response generation under different conversation contexts. The second stage continues to refine the generation with a fine-grained generation model and an evaluation model. The fine-grained generation model explicitly models the one-to-many mapping relationship for diverse response generation. To select the most appropriate responses generated by the fine-grained generation model, the evaluation model is trained to estimate the coherence of the responses.</p><p>As for response selection, previous studies have employed variant scoring functions, including forward response generation probability <ref type="bibr">(Adiwardana et al., 2020)</ref>, backward context recover probability <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> and bi-directional coherence probability <ref type="bibr" target="#b6">(Bao et al., 2020)</ref>. However, the forward score favors safe and generic responses due to the property of maximum likelihood, while the backward score tends to select the response with a high overlap with the context, resulting in repetitive conversations. In order to ameliorate the above problems, we adopt the bi-directional coherence estimation in the evaluation model of PLATO-2, whose effectiveness is also verified in the experiments.</p><p>We trained PLATO-2 models with different sizes: 1.6 Billion parameters and 310 Million parameters. In addition to the English models, we also trained Chinese models with massive social media conversations. Comprehensive experiments on both English and Chinese datasets demonstrate that PLATO-2 outperforms the state-of-the-art models. We have released our English models and source codes at GitHub, hoping to facilitate the research in open-domain dialogue generation. 1 2 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>The infrastructure of PLATO-2 is shown in Figure 2(a), consisted of transformer blocks. The key components of the transform block include layer normalization, multi-head attention and feed forward layers. In the arrangement of these components, there are two options with regarding to the location of layer normalization. One way is the original post-normalization used in BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, where layer normalization is placed between residual connections. The other way is the recent pre-normalization used in GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>, where layer normalization is placed within residual connections. As reported in Megatron-LM <ref type="bibr" target="#b27">(Shoeybi et al., 2019)</ref>, postnormalization leads to performance degradation as the model size increases, and pre-normalization enables stable training in large-scale models. As such, pre-normalization is adopted in our model.</p><p>Besides, unlike conventional Seq2Seq, there are no separate encoder and decoder networks in our infrastructure. For the sake of training efficiency, PLATO-2 keeps the unified network for bidirectional context encoding and uni-directional response generation through flexible attention mechanism <ref type="bibr" target="#b16">(Dong et al., 2019;</ref><ref type="bibr" target="#b6">Bao et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Curriculum Learning</head><p>In this work, we carry out effective training of PLATO-2 via curriculum learning. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), there are two stages involved in the learning process: during stage 1, a coarsegrained baseline model is trained for general response generation under the simplified one-to-one mapping relationship; during stage 2, two models of fine-grained generation and evaluation are further trained for diverse response generation and response coherence estimation respectively. Although the backbones of these models are all transformer blocks, the self-attention masks are designed accordingly in order to fit the training objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">General Response Generation</head><p>It is well known that there exists a one-to-many relationship in conversations, where a piece of context may have multiple appropriate responses. Since the classical generation network is designed to fit oneto-one mapping, they tend to generate generic and dull responses. Whereas, it is still an efficient way to capture the general characteristics of response generation. As such, we first train a coarse-grained baseline model to learn general response generation under the simplified relationship of one-to-one mapping. Given one training sample of context and response (c, r), we need to minimize the following negative log-likelihood (NLL) loss:</p><formula xml:id="formula_0">L Baseline N LL = −E log p(r|c) = −E T t=1 log p(r t |c, r &lt;t ) ,<label>(1)</label></formula><p>where T is the length of the target response r and r &lt;t denotes previously generated words. Since the response generation is a uni-directional decoding process, each token in the response only attends to those before it, shown as dashed orange lines in <ref type="figure">Figure</ref> 2. As for the context, bi-directional attention is enabled for better natural language understanding, shown as blue lines in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Diverse Response Generation</head><p>Based upon the coarse-grained baseline model, diverse response generation is further trained under the relationship of one-to-many mapping. Following our previous work PLATO <ref type="bibr" target="#b6">(Bao et al., 2020)</ref>, the discrete latent variable is introduced for the one-to-many relationship modeling. It will first estimate the latent act distribution of the training sample p(z|c, r) and then generate the response with the sampled latent variable p(r|c, z). The NLL loss of diverse response generation is defined as follows:</p><formula xml:id="formula_1">L Generation N LL = −E z∼p(z|c,r) log p(r|c, z) = −E z∼p(z|c,r) T t=1 log p(r t |c, z, r &lt;t ) ,<label>(2)</label></formula><p>where z is the latent act sampled from p(z|c, r).</p><p>The posterior distribution over latent values is estimated through the task of latent act recognition:</p><formula xml:id="formula_2">p(z|c, r) = softmax(W 1 h [M ] + b 1 ) ∈ R K ,<label>(3)</label></formula><p>where h [M ] ∈ R D is the final hidden state of the special mask [M], W 1 ∈ R K×D and b 1 ∈ R K denote the weight matrices of one fully-connected layer.</p><p>Besides the classical NLL loss, the bag-of-words (BOW) loss <ref type="bibr" target="#b34">(Zhao et al., 2017)</ref> is also employed to facilitate the training process of discrete latent variables:</p><formula xml:id="formula_3">L Generation BOW = −E z∼p(z|c,r) T t=1 log p(r t |c, z) = −E z∼p(z|c,r) T t=1 log e fr t v∈V e fv ,<label>(4)</label></formula><p>where V refers to the whole vocabulary. The function f tries to predict the words within the target response in a non-autoregressive way:</p><formula xml:id="formula_4">f = softmax(W 2 h z + b 2 ) ∈ R |V | ,<label>(5)</label></formula><p>where h z is the final hidden state of the latent variable and |V | is the vocabulary size. f rt denotes the estimated probability of word r t . As compared with NLL loss, the BOW loss discards the order of words and forces the latent variable to capture the global information of the target response.</p><p>To sum up, the objective of the fine-grained generation model is to minimize the following integrated loss:</p><formula xml:id="formula_5">L Generation = L Generation N LL + L Generation BOW (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Response Coherence Estimation</head><p>Recently one strategy has been shown effective in boosting response quality, which first generates multiple candidate responses and then performs ranking according to a score function. While there are different definitions about this score function, which can be divided into three categories. Firstly, the length-average log-likelihood is employed in <ref type="bibr">Meena (Adiwardana et al., 2020)</ref> as the score function, which considers the forward generation probability of the response given the context p(r|c). Secondly, the maximum mutual information is utilized in DialoGPT <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref>, which considers the backward probability to recover the context given the candidate response p(c|r). Thirdly, a discriminative function is used in PLATO <ref type="bibr" target="#b6">(Bao et al., 2020)</ref> for coherence estimation between the context and response p(l r |c, r), where l r is the coherence label. Given that the forward score favors safe responses and the backward score produces repetitive conversations, PLATO-2 adopts the bi-directional coherence estimation in the evaluation model.</p><p>The loss of response coherence estimation (RCE) is defined as follows:</p><formula xml:id="formula_6">L Evaluation RCE = − log p(l r = 1|c, r) − log p(l r − = 0|c, r − )<label>(7)</label></formula><p>The positive training samples come from the dialogue context and corresponding target response (c, r), with coherence label l r = 1. And the negative samples are created by randomly selecting responses from the corpus (c, r − ), with coherence label l r − = 0.</p><p>To maintain the capacity of distributed representation, the task of masked language model (MLM) <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> is also included in the evaluation network. Within this task, 15% of the input tokens will be masked at random and the network needs to recover the masked ones. The MLM loss is defined as:</p><formula xml:id="formula_7">L Evaluation M LM = −E m∈M log p(x m |x \M ),<label>(8)</label></formula><p>where x refers to the input tokens of context and response. {x m } m∈M stands for masked tokens and x \M denotes the rest unmasked ones. To sum up, the objective of the evaluation model is to minimize the following integrated loss:</p><formula xml:id="formula_8">L Evaluation = L Evaluation RCE + L Evaluation M LM (9)</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data</head><p>PLATO-2 has English and Chinese models, with training data extracted from open-domain social media conversations. The details are elaborated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">English Data</head><p>The English training data is extracted from Reddit comments, which are collected by a third party and made publicly available on pushshift.io <ref type="bibr" target="#b7">(Baumgartner et al., 2020)</ref>. As the comments are formatted in message trees, any conversation path from the root to a tree node can be treated as one training sample, with the node as response and its former turns as context. To improve the generation quality, we carry out elaborate data cleaning. A message node and its sub-trees will be removed if any of the following conditions is met. 1) The number of BPE tokens is more than 128 or less than 2. 2) Any word has more than 30 characters or the message has more than 1024 characters.</p><p>3) The percentage of alphabetic characters is less than 70%. 4) The message contains URL. 5) The message contains special strings, such as r/, u/, &amp;amp. 6) The message has a high overlap with the parent's text. 7) The message is repeated more than 100 times. 8) The message contains offensive words. 9) The subreddit is quarantined. 10) The author is a known bot.</p><p>After filtering, the data is split into training and validation sets in chronological order. The training set contains 684M (context, response) samples, ranging from December 2005 to July 2019. For the validation set, 0.2M samples are selected from the rest data after July 2019. The English vocabulary contains 8K BPE tokens <ref type="bibr" target="#b26">(Sennrich et al., 2016)</ref>, constructed with SentencePiece library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Chinese Data</head><p>The Chinese training data is collected from public domain social medias, followed by a similar cleaning process. After filtering, there are 1.2B (context, response) samples in the training set and 0.1M samples in the validation set. As for the Chinese vocabulary, it contains 30K BPE tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>PLATO-2 has two model sizes: a small version of 310M parameters and a standard version of 1.6B parameters. The 310M parameter model has 24 transformer blocks and 16 attention heads, with the embedding dimension of 1024. The 1.6B parameter model has 32 transformer blocks and 32 attention heads, with the embedding dimension of 2048.</p><p>The hyper-parameters used in the training process are listed as follows. The maximum sequence lengths of context and response are all set to 128. We use Adam (Kingma and Ba, 2015) as the optimizer, with a learning rate scheduler including a linear warmup and an invsqrt decay <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. To train the large-scale model with a relatively large batch size, we employ gradient checkpointing <ref type="bibr" target="#b11">(Chen et al., 2016)</ref> to trade computation for memory. Detailed configurations are summarized in <ref type="table" target="#tab_2">Table 1</ref>. The training was carried out on 64 Nvidia Tesla V100 32G GPU cards. It takes about 3 weeks for the 1.6B parameter model to accomplish the curriculum learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Compared Methods</head><p>The following methods have been compared in the experiments.</p><p>• DialoGPT <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> is trained on the basis of GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation Metrics</head><p>We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model's capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 <ref type="bibr" target="#b18">(Li et al., 2016a)</ref>, which is defined as the number of distinct uni-or bi-grams divided by the total number of generated words.</p><p>In human evaluation, we employ four utterancelevel and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix.</p><p>• Coherence is an utterance-level metric, measuring whether the response is relevant and consistent with the context. • Informativeness is also an utterance-level metric, evaluating whether the response is informative or not given the context. • Engagingness is a dialogue-level metric, assessing whether the annotator would like to talk with the speaker for a long conversation. • Humanness is also a dialogue-level metric, judging whether the speaker is a human being or not.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">Stage Batch Size Learning Rate Warmup Steps Training Steps 310M</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>In the experiments, we include both static and interactive evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Self-Chat Evaluation</head><p>Self-chats have been widely used in the evaluation of dialogue systems <ref type="bibr" target="#b19">(Li et al., 2016b;</ref><ref type="bibr" target="#b5">Bao et al., 2019;</ref><ref type="bibr" target="#b25">Roller et al., 2020)</ref>, where a model plays the role of both partners in the conversation. As compared with human-bot conversations, self-chat logs can be collected efficiently at a cheaper price. As reported in <ref type="bibr" target="#b16">Li et al. (2019)</ref>, self-chat evaluations exhibit high agreement with the human-bot chat evaluations. In the experiments, we ask the bot to perform self-chats and then invite crowd-sourcing workers to evaluate the dialogue quality. The way to start the interactive conversation needs special attention. As pointed out by <ref type="bibr" target="#b25">Roller et al. (2020)</ref>, if starting with 'Hi!', partners tend to greet with each other and only cover some shallow topics in the short conversation. Therefore, to expose the model's weaknesses and explore the model's limits, we choose to start the interactive conversation with pre-selected topics. We use the classical 200 questions as the start topic <ref type="bibr" target="#b30">(Vinyals and Le, 2015)</ref> and ask the bot to performance selfchats given the context. There are 10 utterances in each dialogue, including the input start utterance. We carry out automatic evaluation on the 200 self-chat logs and randomly select 50 conversations from 200 self-chat logs for human evaluation.</p><p>The compared models are divided into two groups. In human evaluation, two self-chat logs, which are from the same group and have the same start topic, will be displayed to three crowd-sourcing workers. One example is given in <ref type="figure" target="#fig_2">Figure 3</ref>. As suggested in ACUTE-Eval , we ask crowd-sourcing workers to pay attention to only one speaker within a dialogue. In the evaluation, they need to give scores on coherence and informativeness for each P1's utterance, and assess P1's overall quality on engagingness and humanness. The self-chat evaluation results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. These results demonstrate that PLATO 1.6B model obtains the best performance across human and automatic evaluations. The gap of Blender and PLATO-2 on the corpus-level metric distinct-1/2 suggests that PLATO-2 has a better capacity on lexical diversity. In addition, the difference between these two groups indicates that enlarging model scales and exploiting human annotated conversations help improve the dialogue quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Human-Bot Chat Evaluation</head><p>In the Chinese evaluation, it is difficult to carry out self-chats for Microsoft XiaoIce, as there is no public available API. Therefore, we collect human-  founders is very good. they are based out of detroit but the brewery is in royal oak ohio yes they're in royal oak! have you ever been to founders? they have two breweries in pittsburgh and columbus i have been to founders brewery in columbus! great place to visit. bot conversations through their official Weibo platform. The interactive conversation also starts with a pre-selected topic and continues for 7-14 rounds, where 50 diverse topics are extracted from the highfrequency topics of a commercial chatbot, including travel, movie, hobby and so on. The collected human-bot conversations are distributed to crowdsourcing workers for evaluation. The human and automatic evaluation results are summarized in <ref type="table" target="#tab_6">Table 3</ref>. XiaoIce obtains higher distinct values, which may use a retrieval-based strategy in response generation. The human evaluations demonstrate that our PLATO-2 model achieves significant improvements over XiaoIce across all the human evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Static Evaluation</head><p>Besides the interactive evaluation, we also include static evaluation to analyze the model's performance. In static evaluation, each model will produce a response towards the given multi-turn context. To compare with Meena, we include their provided 60 static samples in the paper's Appendix  How did you learn it?</p><p>I learnt it in the river as a kid, with a bunch of friends.</p><p>That is great. I would like to learn swimming too.</p><p>Go for it. We can go swimming together then.</p><p>and generate corresponding responses with other models. We also include 60 test samples about daily life from Daily Dialog <ref type="bibr" target="#b21">(Li et al., 2017)</ref> and 60 test samples about in-depth discussion from Reddit. Given that the measurement of humanness usually needs multi-turn interaction, this metric is excluded from static evaluation. The evaluation results are summarized in <ref type="table" target="#tab_8">Table 4</ref>. It can be observed that PLATO-2 is able to produce coherent, informative and engaging responses under different chat scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Case Analysis</head><p>To further analyze the models' features, two selfchat examples of Blender and PLATO-2 are provided in <ref type="figure" target="#fig_2">Figure 3</ref>. Although both models are able to produce high-quality engaging conversations, they exhibit distinct discourse styles. Blender tends to switch topics quickly in the short conversation, including alcohol, hobbies, movies and work. The emergence of this style might be related with BST fine-tuning data. For instance, ConvAI2 is about the exchange of personal information between two partners, where topics need to switch quickly to know more about each other. Due to the task settings of data collection, some human annotated conversations might be a little unnatural. Nevertheless, fine-tuning with BST conversations is essential to mitigate undesirable toxic traits of large corpora and emphasize desirable skills of human conversations. Distinct with Blender, PLATO-2 can stick to the start topic and conduct in-depth discussions. The reasons might be two-fold. On the one hand, our model is able to generate diverse and informative responses with the accurate modeling of one-tomany relationship. On the other hand, the evaluation model helps select the coherent response and stick to current topic. We also asked crowdsourcing workers to compare the responses generated by these two models via pairwise ranking. The comparison result is shown in <ref type="table" target="#tab_10">Table 5</ref>, which also verifies our above analysis on discourse styles.</p><p>Besides, we also provide two human-bot chat examples of XiaoIce and PLATO-2 in <ref type="figure">Figure 4</ref>, with original interactive logs shown on the left and translated logs on the right. It can be observed that some responses produced by XiaoIce are not coherent with the contexts and there are some abrupt changes of topics. By contrast, the interaction with   PLATO-2 is more coherent and engaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Response Selection Comparison</head><p>We carry out more experiments to compare the performance of distinct score functions in response selection. Firstly, one Chinese response selection dataset is constructed: 100 dialogue contexts are selected from the test set and 10 candidate responses are retrieved for each context with a commercial chatbot. Secondly, we ask crowd-sourcing workers to annotate the label whether the candidate response is coherent with the context. Thirdly, we train three 333M parameter models as the score function, including the forward response generation probability p(r|c), the backward context recover probability p(c|r) and the bi-directional coherence probability p(l r |c, r). Their results on the annotated response selection dataset are summarized in <ref type="table" target="#tab_11">Table 6</ref>. The metrics of mean average precision (MAP) <ref type="bibr" target="#b4">(Baeza-Yates et al., 1999)</ref>, mean reciprocal rank (MRR) <ref type="bibr" target="#b31">(Voorhees et al., 1999)</ref> and precision at position 1 (P@1) are employed. These results indicate that PLATO-2's evaluation model is better at selecting appropriate responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Related works include large-scale language models and open-domain dialogue generation. Large-scale Language Models. Pre-trained largescale language models have brought many breakthroughs on various NLP tasks. GPT <ref type="bibr" target="#b22">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> are representative uni-directional and bi-directional language models, trained on general text corpora. By introducing pre-normalization and modifying weight initialization, GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref> successfully extends the model scale from 117M to 1.5B parameters. To cope with memory constraints, Megatron-LM <ref type="bibr" target="#b27">(Shoeybi et al., 2019</ref>) ex-ploits model parallelism to train an 8.3B parameter model on 512 GPUs. <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> further trains an 175B parameter autoregressive language model, demonstrating strong performance on many NLP tasks. The development of largescale language models is also beneficial to the task of dialogue generation.</p><p>Open-domain Dialogue Generation. On the basis of GPT-2, DialoGPT <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> is trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena (Adiwardana et al., 2020) scales up the network parameters to 2.6B and utilizes more social media conversations in the training process.</p><p>To emphasize desirable conversational skills of engagingness, knowledge, empathy and personality, Blender <ref type="bibr" target="#b25">(Roller et al., 2020)</ref> further fine-tunes the pre-trained model with human annotated conversations. Besides the above attempts on model scale and data selection, PLATO <ref type="bibr" target="#b5">(Bao et al., 2019)</ref> introduces discrete latent variable to tackle the inherent one-to-many mapping problem to improve response quality. In this work, we further scale up PLATO to PLATO-2 and discuss its effective training via curriculum learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we discuss the effective training of open-domain chatbot PLATO-2 via curriculum learning, where two stages are involved. In the first stage, one coarse-grained model is trained for general response generation. In the second stage, two models of fine-grained generation and evaluation are trained for diverse response generation and response coherence estimation. Experimental results demonstrate that PLATO-2 achieves substantial improvements over the state-of-the-art methods in both Chinese and English evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Curriculum learning process in PLATO-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>PLATO-2 illustration. (a) Network overview with the details of transformer blocks. (b) Curriculum learning process with self-attention visualization and training objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Self-chat examples by Blender and PLATO-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>These annotated conversations are referred as BST in short. Blender has three model sizes: 90M, 2.7B and 9.4B. Since the 2.7B parameter model obtains the best performance in their evaluations, we compare with it in the experiments. • Meena (Adiwardana et al., 2020) is an opendomain chatbot trained with social media conversations. Meena has 2.6B model parameters, similar to Blender. Given that Meena has not released the model or provided a service interface, In the experiments, we use the official Weibo platform to chat with XiaoIce. For the sake of comprehensive and fair comparisons, three versions of PLATO-2 are included in the experiments. • PLATO 1.6B parameter model is the standard version in English, which is first trained using Reddit comments and then fine-tuned with BST conversations. This model will be compared to the state-of-the-art open-domain chatbot Blender, to measure the effectiveness of PLATO-2.</figDesc><table><row><cell>• PLATO 310M parameter model is a small ver-</cell></row><row><cell>sion in English, which is trained with Reddit</cell></row><row><cell>comments. This model will be compared to Di-</cell></row><row><cell>aloGPT, as they have similar model scales and</cell></row><row><cell>training data.</cell></row><row><cell>us-</cell></row><row><cell>ing Reddit comments. There are three model</cell></row><row><cell>sizes: 117M, 345M and 762M. Since the 345M</cell></row><row><cell>parameter model obtains the best performance</cell></row><row><cell>in their evaluations, we compare with it in the</cell></row><row><cell>experiments.</cell></row><row><cell>• Blender (Roller et al., 2020) is firstly trained us-</cell></row><row><cell>ing Reddit comments and then fine-tuned with</cell></row><row><cell>human annotated conversations, to help empha-</cell></row><row><cell>size desirable conversational skills of engaging-</cell></row></table><note>ness, knowledge, empathy and personality. Dur- ing fine-tuning, there are four datasets included: ConvAI2 (Zhang et al., 2018; Dinan et al., 2020), Empathetic Dialogues (Rashkin et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and Blended Skill Talk (Smith et al., 2020).it is difficult to perform comprehensive compari- son. In the experiments, we include the provided samples in their paper for static evaluation.• Microsoft XiaoIce (Zhou et al., 2020) is a popu- lar social chatbot in Chinese.• PLATO 333M parameter Chinese model 2 will be compared to XiaoIce in the experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Training configurations of PLATO-2.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>Stage</cell><cell>Batch Size</cell><cell cols="2">Learning Rate</cell><cell>Warmup Steps</cell><cell cols="2">Training Steps</cell></row><row><cell></cell><cell></cell><cell cols="2">1. Baseline</cell><cell>524,288</cell><cell>1e-3</cell><cell></cell><cell>4000</cell><cell></cell><cell>20w</cell></row><row><cell cols="2">310M Parameter</cell><cell cols="2">2.1. Generation</cell><cell>262,144</cell><cell>4e-5</cell><cell></cell><cell>4000</cell><cell></cell><cell>10w</cell></row><row><cell></cell><cell></cell><cell cols="2">2.2. Evaluation</cell><cell>524,288</cell><cell>1e-4</cell><cell></cell><cell>4000</cell><cell></cell><cell>10w</cell></row><row><cell></cell><cell></cell><cell cols="2">1. Baseline</cell><cell>524,288</cell><cell>5e-4</cell><cell></cell><cell>3125</cell><cell></cell><cell>20w</cell></row><row><cell cols="2">1.6B Parameter</cell><cell cols="2">2.1. Generation</cell><cell>65,536</cell><cell>1e-5</cell><cell></cell><cell>3125</cell><cell></cell><cell>10w</cell></row><row><cell></cell><cell></cell><cell cols="2">2.2. Evaluation</cell><cell>524,288</cell><cell>5e-5</cell><cell></cell><cell>3125</cell><cell></cell><cell>10w</cell></row><row><cell>Training Data</cell><cell>Model</cell><cell></cell><cell cols="6">Human Evaluation Coherence Informativeness Engagingness Humanness Distinct-1 Distinct-2 Automatic Evaluation</cell><cell>Average Length</cell></row><row><cell></cell><cell cols="2">DialoGPT</cell><cell>0.720</cell><cell>0.712</cell><cell>0.340</cell><cell>0.100</cell><cell>0.143</cell><cell>0.504</cell><cell>9.906</cell></row><row><cell>Reddit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">PLATO 310M</cell><cell>1.572</cell><cell>1.620</cell><cell>1.300</cell><cell>1.160</cell><cell>0.064</cell><cell>0.432</cell><cell>23.536</cell></row><row><cell></cell><cell cols="2">Blender</cell><cell>1.856</cell><cell>1.816</cell><cell>1.820</cell><cell>1.540</cell><cell>0.115</cell><cell>0.387</cell><cell>17.677</cell></row><row><cell>Reddit + BST</cell><cell cols="2">PLATO 1.6B</cell><cell>1.920</cell><cell>1.892</cell><cell>1.840</cell><cell>1.740</cell><cell>0.163</cell><cell>0.603</cell><cell>16.540</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Coherence</cell><cell cols="3">Human Evaluation Informativeness Engagingness Humanness</cell><cell cols="2">Automatic Evaluation Distinct-1 Distinct-2</cell><cell>Average Length</cell></row><row><cell cols="2">Microsoft XiaoIce</cell><cell></cell><cell>0.869</cell><cell>0.822</cell><cell>0.560</cell><cell>0.260</cell><cell>0.289</cell><cell>0.764</cell></row><row><cell cols="2">PLATO 310M Chinese</cell><cell></cell><cell>1.737</cell><cell>1.683</cell><cell>1.600</cell><cell>1.480</cell><cell>0.212</cell><cell>0.713</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Self-chat evaluation results, with best value written in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Chinese interactive evaluation results, with best value written in bold.</figDesc><table><row><cell>do you drink alcohol ?</cell><cell>do you drink alcohol ?</cell></row><row><cell>No, I do not drink alcohol. I don't like the way it</cell><cell>i drink beer but not often</cell></row><row><cell>makes me feel.</cell><cell></cell></row><row><cell></cell><cell>that is fine, do you like sweet or bitter alcohol?</cell></row><row><cell>i know what you mean . i don't drink either . i</cell><cell></cell></row><row><cell>just don't see the point .</cell><cell>i like bitter alcohols more, but i do like both. i</cell></row><row><cell>Yeah, it's not for everyone. Some people like it,</cell><cell>don't drink often enough though.</cell></row><row><cell>but I'm not one of them.</cell><cell>i prefer bitter alcohol as well. what is your</cell></row><row><cell>i feel the same . it's just not for me . what do you</cell><cell>favorite beer?</cell></row><row><cell>like to do for fun ?</cell><cell>my favorite brewery that serves beer is founders.</cell></row><row><cell>I like to play video games, watch movies, and</cell><cell>they have excellent brown ales.</cell></row><row><cell>hang out with friends. What about you?</cell><cell></cell></row><row><cell>that's cool . i like to go to the movies too . what's</cell><cell></cell></row><row><cell>your favorite movie ?</cell><cell></cell></row><row><cell>I like a lot of movies, but my favorite of all time is</cell><cell></cell></row><row><cell>The Godfather Part II. What's yours?</cell><cell></cell></row><row><cell>i love the godfather too . i think my favorite is the</cell><cell></cell></row><row><cell>first one . what about you ?</cell><cell></cell></row><row><cell>I think the second one is my favorite. I love</cell><cell></cell></row><row><cell>Michael Corleone. What do you do for a living?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Static evaluation results, with best value written in bold.</figDesc><table><row><cell></cell><cell></cell><cell>Microsoft XiaoIce</cell><cell></cell></row><row><cell>Start</cell><cell>你会游泳吗</cell><cell>Start</cell><cell>Can you swim?</cell></row><row><cell></cell><cell>Bot</cell><cell>特别会游</cell><cell></cell><cell>Yes, I'm very good at swimming.</cell></row><row><cell>Human</cell><cell>这么厉害，我不会</cell><cell></cell><cell cols="2">Awesome. I can't swim.</cell></row><row><cell></cell><cell></cell><cell>没事 游泳圈 要不我教你</cell><cell></cell><cell>Don't worry. Swim ring. Or I can teach you.</cell></row><row><cell></cell><cell>好呀，你怎么学会的</cell><cell></cell><cell cols="2">Great. How did you learn it?</cell></row><row><cell></cell><cell></cell><cell>哪里有教学游泳的么</cell><cell></cell><cell>Is there any swimming course?</cell></row><row><cell></cell><cell>你教我呀</cell><cell></cell><cell cols="2">You can teach me.</cell></row><row><cell></cell><cell></cell><cell>我会游泳</cell><cell></cell><cell>I can swim.</cell></row><row><cell></cell><cell>你真厉害，我怕水</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>我教你</cell><cell></cell></row><row><cell></cell><cell>好，我们约个时间一起游泳</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>好的啊，一起吃饭</cell><cell></cell></row><row><cell></cell><cell>先游泳再吃饭</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>吃饭了</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Human</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Bot</cell><cell>A quick question, do you know how to insert the code of instant chat into the webpage?</cell></row><row><cell></cell><cell></cell><cell>Human</cell><cell>I don't know.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bot</cell><cell>Write me c++ code</cell></row><row><cell></cell><cell></cell><cell>PLATO 333M Chinese</cell><cell></cell></row><row><cell>Start</cell><cell>你会游泳吗</cell><cell>Start</cell><cell>Can you swim?</cell></row><row><cell></cell><cell>Bot</cell><cell>会游一点</cell><cell></cell><cell>Yes, but not so good at it.</cell></row><row><cell>Human</cell><cell>那你能教我么</cell><cell></cell><cell cols="2">Could you teach me?</cell></row><row><cell></cell><cell></cell><cell>我怕把你教的更不会游</cell><cell></cell><cell>I am afraid I am not a good teacher.</cell></row><row><cell></cell><cell>我很怕水，不敢游</cell><cell></cell><cell cols="2">I am afraid of water.</cell></row><row><cell></cell><cell></cell><cell>那我就更不敢教你</cell><cell></cell><cell>Then I am sure I can't teach you.</cell></row><row><cell></cell><cell>你怎么学会的</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>小时候和小伙伴去河里学的</cell><cell></cell></row><row><cell></cell><cell>那还挺好，我也打算学学</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>去吧去吧,学会了带我一起</cell><cell></cell></row><row><cell></cell><cell>好呀，到时候我们一起去游泳</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>你不怕我把你扔河里么</cell><cell></cell></row><row><cell></cell><cell>我会游泳了就不怕</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>那你要记得带着泳圈哦</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Thoroughness with regard to the start topic.</figDesc><table><row><cell>Blender Win</cell><cell></cell><cell>Tie</cell><cell>PLATO 1.6B Win</cell><cell>Score Function</cell><cell>MAP</cell><cell>MRR</cell><cell>P@1</cell></row><row><cell>8</cell><cell></cell><cell>18</cell><cell>24</cell><cell>!(#|%)</cell><cell>0.705</cell><cell>0.790</cell><cell>0.700</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>!(%|#)</cell><cell>0.672</cell><cell>0.737</cell><cell>0.610</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>! ' ( %, #)</cell><cell>0.754</cell><cell>0.819</cell><cell>0.750</cell></row><row><cell></cell><cell></cell><cell cols="2">Meena Samples</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Coherence</cell><cell cols="2">Informativeness Engagingness</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Meena</cell><cell>1.750</cell><cell>1.617</cell><cell>1.583</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DialoGPT</cell><cell>1.233</cell><cell>1.067</cell><cell>1.017</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blender</cell><cell>1.800</cell><cell>1.767</cell><cell>1.683</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PLATO 1.6B</cell><cell>1.900</cell><cell>1.917</cell><cell>1.850</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Daily Dialog Samples</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Coherence</cell><cell cols="2">Informativeness Engagingness</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DialoGPT</cell><cell>1.117</cell><cell>1.033</cell><cell>0.917</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blender</cell><cell>1.767</cell><cell>1.617</cell><cell>1.633</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PLATO 1.6B</cell><cell>1.867</cell><cell>1.850</cell><cell>1.833</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Reddit Samples</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Coherence</cell><cell cols="2">Informativeness Engagingness</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DialoGPT</cell><cell>1.283</cell><cell>1.283</cell><cell>1.183</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blender</cell><cell>1.767</cell><cell>1.550</cell><cell>1.583</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PLATO 1.6B</cell><cell>1.900</cell><cell>1.900</cell><cell>1.883</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different score functions in response selection, with best value written in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/PaddlePaddle/ Knover/tree/master/plato-2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This model has 24 transformer blocks and 16 attention heads, with the embedding dimension of 1024. As the Chinese vocabulary contains 30K BPE tokens, this model has 23M more parameters than the English small model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Jingzhou He, and Tingting Li for the help on resource coordination; Daxiang Dong, and Pingshuo Ma for the support on Pad-dlePaddle; Yu Sun, Yukun Li, and Han Zhang for the assistance with infrastructure and implementation. This work was supported by the Natural Key Research and Development Project of China (No. 2018AAA0101900).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Scoring Criteria</head><p>Score Coherence 0</p><p>• The response is not related with the context.</p><p>• The response simply repeats the context.</p><p>• The response has obvious conflicts with the context.</p><p>• There are serious logic conflicts within the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>• The response has minor conflicts with the context.</p><p>• There are some minor logic conflicts in the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>• The response is coherent with the context. Score Informativeness 0</p><p>• The response doesn't contain any information.</p><p>• This response just repeats the context and fails to bring any additional information. • The information is invalid, as the coherence score is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>• The information has conflicts with common sense.</p><p>• There are factual errors in the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>• The response has appropriate and correct information.</p><p>Score Engagingness 0</p><p>• I don't want to talk with this speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>• It is kind of boring, but it is still ok to talk with this speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>• I would like to talk with this speaker for a long conversation.</p><p>Score Humanness 0</p><p>• This speaker seems like a bot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>• This speaker gives unnatural responses occasionally and seems not that human-like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>• This speaker seems like a human being. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Let us find a time to go swimming together. All right. Let us have dinner together. Swimming first, then dinner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Good</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>It is time for dinner</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">要吃啥 吃代码 What are you going to eat? , I am coding too</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Good</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>let us go swimming together</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Aren&apos;t you afraid I will throw you into the river? 4: Human-bot chat examples by Microsoft XiaoIce and PLATO-2. References Daniel Adiwardana</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards a human-like open-domain chatbot</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Know more about each other: Evolving dialogue strategy via compound assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5382" to="5391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Plato: Pre-trained dialogue generation model with discrete latent variable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pushshift reddit dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating multiple diverse responses with multi-mapping and posterior mapping selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaotao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4918" to="4924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<title level="m">Training deep nets with sublinear memory cost</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The second conversational intelligence challenge (convai2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NeurIPS&apos;18</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Competition</title>
		<imprint>
			<biblScope unit="page" from="187" to="208" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Wizard of wikipedia: Knowledge-powered conversational agents. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03087</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Joint Conference on Natural Language Processing</title>
		<meeting>the 8th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training. Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards empathetic opendomain conversation models: A new benchmark and dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5370" to="5381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13637</idno>
		<title level="m">Recipes for building an open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can you put it all together: Evaluating conversational agents&apos; ability to blend skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dialogpt: Large-scale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="93" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
