<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L 2 -constrained Softmax Loss for Discriminative Face Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
							<email>rranjan1@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
							<email>carlos@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">L 2 -constrained Softmax Loss for Discriminative Face Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the performance of face verification systems has significantly improved using deep convolutional neural networks (DCNNs). A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we add an L 2 -constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. Specifically, we achieve state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept Rate 0.0001 on the face verification protocol. Additionally, we achieve state-of-the-art performance on LFW dataset with an accuracy of 99.78%, and competing performance on YTF dataset with accuracy of 96.08%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face verification in unconstrained settings is a challenging problem. Despite the excellent performance of recent face verification systems on curated datasets like Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b13">[14]</ref>, it is still difficult to achieve similar accuracy on faces with extreme variations in viewpoints, resolution, occlusion and image quality. This is evident from the performance of the traditional algorithms on the publicly available IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset. Data quality imbalance in the training set is one of the reason for this performance gap. Existing face recognition training datasets contain large amount of high quality and frontal faces, whereas the unconstrained and difficult faces occur rarely. Most of the DCNN-based methods trained with softmax loss for classification tend to over-fit to the high quality data and fail to correctly classify faces acquired in difficult conditions.</p><p>Using softmax loss function for training face verification system has its own pros and cons. On the one hand, it can be easily implemented using inbuilt functions from the publicly available deep leaning toolboxes such as Caffe <ref type="bibr" target="#b14">[15]</ref>, Torch <ref type="bibr" target="#b6">[7]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref>. Unlike triplet loss <ref type="bibr" target="#b27">[28]</ref>, it does not have any restrictions on the input batch size and converges quickly. The learned features are discriminative enough for efficient face verification without any metric learning.</p><p>On the other hand, the softmax loss is biased to the sample distribution. Unlike contrastive loss <ref type="bibr" target="#b28">[29]</ref> and triplet loss <ref type="bibr" target="#b27">[28]</ref> which specifically attend to hard samples, the softmax loss maximizes the conditional probability of all the samples in a given mini-batch. Hence, it fits well to the high quality faces, ignoring the rare difficult faces from a training mini-batch. We observe that the L 2 -norm of features learned using softmax loss is informative of the quality of the face <ref type="bibr" target="#b22">[23]</ref>. Features for good quality frontal faces have a high L 2 -norm while blurry faces with extreme pose have low L 2 -norm (see <ref type="figure" target="#fig_0">Figure 1(b)</ref>). Moreover, the softmax loss does not optimize the verification requirement of keeping positive pairs closer and negative pairs far from each other. Due to this reason, many methods either apply metric learning on top of softmax features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref> or train an auxiliary loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> along with the softmax loss to achieve better verification performance.</p><p>In this paper, we provide a symptomatic treatment to issues associated with the softmax loss. We propose an L 2softmax loss that adds a constraint on the features during training such that their L 2 -norm remain constant. In other words, we restrict the features to lie on a hypersphere of a fixed radius. The proposed L 2 -softmax loss has a dual advantage. Firstly, it provides similar attention to both good and bad quality faces since all the features have the same L 2 -norm now, which is essential for better performance in unconstrained settings. Secondly, it strengthens the verification signal by forcing the same subject features to be closer and different subject features to be far from each other in the normalized space. Thus, it maximizes the margin for the normalized L 2 distance or cosine similarity score between negative and positive pairs. Thus, it overcomes the main disadvantages of the regular softmax loss.</p><p>The L 2 -softmax loss also retains the advantages of the regular softmax loss. Similar to the softmax loss, it is a one network, one loss system. It doesn't necessarily require any joint supervision as used by many recent methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref>. It can be easily implemented using inbuilt functions from Caffe <ref type="bibr" target="#b14">[15]</ref>, Torch <ref type="bibr" target="#b6">[7]</ref> and Tensor-Flow <ref type="bibr" target="#b0">[1]</ref>, and converges very fast. It introduces just a single scaling parameter to the network. Compared to the regular softmax loss, the L 2 -softmax loss gains a significant boost in the performance. It achieves new state-of-the-art results on IJB-A dataset, and competing results on LFW and YouTube Face datasets. It surpasses the performance of several state-of-the-art systems, which use multiple networks or multiple loss functions or both. In summary, this paper contributes to the following aspects:</p><p>1. We propose a simple, novel and effective L 2 -softmax loss for face verification that restricts the L 2 -norm of the feature descriptor to a constant value α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We study the variations in the performance with respect to the scaling parameter α and provide suitable bounds on its value for achieving consistently high performance.</p><p>3. The proposed method yields a consistent and significant boost on all the three challenging face verification datasets namely LFW <ref type="bibr" target="#b13">[14]</ref>, YouTube Face <ref type="bibr" target="#b18">[19]</ref> and IJB-A <ref type="bibr" target="#b15">[16]</ref> Moreover, the gains from L 2 -softmax loss are complementary to metric learning (eg: TPE <ref type="bibr" target="#b26">[27]</ref>, joint-Bayes <ref type="bibr" target="#b2">[3]</ref>) or auxiliary loss functions (eg: center loss <ref type="bibr" target="#b32">[33]</ref>, contrastive loss <ref type="bibr" target="#b28">[29]</ref>). We show that applying these techniques on top of the L 2 -softmax loss can further improve the verification performance. Combining with TPE <ref type="bibr" target="#b26">[27]</ref>, L 2 -softmax loss achieves a record True Accept Rate (TAR) of 0.909 at False Accept Rate (FAR) of 0.0001 on the challenging IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, there has been a significant improvement in the accuracy of face verification using deep learning methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>. Most of these methods have even surpassed human performance on the LFW <ref type="bibr" target="#b13">[14]</ref> dataset. Although these methods use DCNNs, they differ by the type of loss function they use for training. For face verification, its essential for the positive subjects pair features to be closer and negative subjects pair features far apart. To solve this problem, researchers have adopted two major approaches.</p><p>In the first approach, pairs of face images are input to the training algorithm to learn a feature embedding where positive pairs are closer and negative pairs are far apart. In this direction, Chopra et al. <ref type="bibr" target="#b4">[5]</ref> proposed siamese networks with contrastive loss for training. Hu et al. <ref type="bibr" target="#b12">[13]</ref> designed a discriminative deep metric with a margin between positive and negative face pairs. FaceNet <ref type="bibr" target="#b27">[28]</ref> introduces triplet loss to learn the metric using hard triplet face samples.</p><p>In the second approach, the face images along with their subject labels are used to learn discriminative identification features in a classification framework. Most of the recent methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref> train a DCNN with softmax loss to learn these features which are later used either to directly compute the similarity score for a pair of faces or to train a discriminative metric embedding <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3]</ref>. Another strategy is to train the network for joint identification-verification task <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Xiong et al. <ref type="bibr" target="#b35">[36]</ref> proposed transferred deep feature fusion (TDFF) which infolves two-stage fusion of features trained with different networks and datasets. Template adaptation <ref type="bibr" target="#b7">[8]</ref> is applied to further boost the performance.</p><p>A recent approach <ref type="bibr" target="#b32">[33]</ref> introduced center loss to learn better discriminative face features. Our proposed method is different from the center loss in the following aspects. First, we use one loss function (i.e., L 2 -softmax loss) whereas <ref type="bibr" target="#b32">[33]</ref> uses center loss jointly with the softmax loss during training. Second, center loss introduces C × D additional parameters during training where C is the number of classes and D is the feature dimension. On the other hand, the L 2 -softmax loss introduces just a single parameter that defines the fixed L 2 -norm of the features. Moreover, the center loss can also be used in conjunction with L 2softmax loss, which performs better than center loss trained with regular softmax loss (see Section 5.1.4).</p><p>Recently, a few algorithms have used feature normalization during training to improve performance. SphereFace <ref type="bibr" target="#b19">[20]</ref> proposes angular softmax (A-softmax) loss that enables DCNNs to learn angularly discriminative features. Another method called DeepVisage <ref type="bibr" target="#b9">[10]</ref> uses a special case of batch normalization technique to normalize the feature descriptor before applying the softmax loss. Our proposed method is different as it applies an L 2 -constraint on the feature descriptors enforcing them to lie on a hypersphere of a given radius.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>We first summarize the general pipeline for training a face verification system using DCNN as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given a training dataset with face images and corresponding identity labels, a DCNN is trained as a classification task where the network learns to classify a given face image to its correct identity label. A softmax loss function is used for training the network, given by Equation 1 </p><formula xml:id="formula_0">L S = − 1 M M i=1 log e W T y i f (xi)+by i C j=1 e W T j f (xi)+bj ,<label>(1)</label></formula><p>where M is the training batch size, x i is the i th input face image in the batch, f (x i ) is the corresponding output of the penultimate layer of the DCNN, y i is the corresponding class label, and W and b are the weights and bias for the last layer of the network which acts as a classifier. At test time, feature descriptors f (x g ) and f (x p ) are extracted for the pair of test face images x g and x p respectively using the trained DCNN, and normalized to unit length. Then, a similarity score is computed on the feature vectors which provides a measure of distance or how close the features lie in the embedded space. If the similarity score is greater than a set threshold, the face pairs are decided to be of the same person. Usually, the similarity score is computed as the L 2 -distance between the normalized features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> or by using cosine similarity <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> s, as given by Equation 2. Both these similarity measures are equivalent and produce same results.</p><formula xml:id="formula_1">s = f (x g ) T f (x p ) f (x g ) 2 f (x p ) 2<label>(2)</label></formula><p>There are two major issues with this pipeline. First, the training and testing steps for face verification task are decoupled. Training with softmax loss doesn't necessarily ensure the positive pairs to be closer and the negative pairs to be far separated in the normalized or angular space.</p><p>Secondly, the softmax classifier is weak in modeling difficult or extreme samples. In a typical training batch with data quality imbalance, the softmax loss gets minimized by increasing the L 2 -norm of the features for easy samples, and ignoring the hard samples. The network thus learns to respond to the quality of the face by the L 2 -norm of its feature descriptor. To validate this theory, we perform a simple experiment on the IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset where we divide the templates (groups of images/frames of same subject) into three different sets based on the L 2 -norm of their feature descriptors. The features were computed using Face-Resnet <ref type="bibr" target="#b32">[33]</ref> trained with regular softmax loss. Templates with descriptors' L 2 -norm &lt;90 are assigned to set1. The templates with L 2 -norm &gt;90 but &lt;150 are assigned to set2, while templates with L 2 -norm &gt;150 are assigned to set3. In total they form six sets of evaluation pairs. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows the performance of the these six different sets for the IJB-A face verification protocol. It can be clearly seen that pairs having low L 2 -norm for both the templates perform very poor, while the pairs with high L 2 -norm perform the best. The difference in performance between each set is quite significant. <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows some sample templates from set1, set2 and set3 which confirms that the L 2 -norm of the feature descriptor is informative of its quality.</p><p>To solve these issues, we enforce the L 2 -norm of the features to be fixed for every face image. Specifically, we add an L 2 -constraint to the feature descriptor such that it lies on a hypersphere of a fixed radius. This approach has two advantages. Firstly, on a hypersphere, minimizing the softmax loss is equivalent to maximizing the cosine similarity for the positive pairs and minimizing it for the negative pairs, which strengthens the verification signal of the features. Secondly, the softmax loss is able to model the extreme and difficult faces better, since all the face features have same L 2 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>The proposed L 2 -softmax loss is given by Equation 3</p><formula xml:id="formula_2">minimize − 1 M M i=1 log e W T y i f (xi)+by i C j=1 e W T j f (xi)+bj subject to f (x i ) 2 = α, ∀i = 1, 2, ...M,<label>(3)</label></formula><p>where x i is the input image in a mini-batch of size M , y i is the corresponding class label, f (x i ) is the feature descriptor obtained from the penultimate layer of DCNN, C is the number of subject classes, and W and b are the weights and bias for the last layer of the network which acts as a classifier. This equation adds an additional L 2 -constraint to the regular softmax loss defined in Equation 1. We show the effectiveness of this constraint using MNIST <ref type="bibr" target="#b16">[17]</ref> data. We study the effect of L 2 -softmax loss on the MNIST dataset <ref type="bibr" target="#b16">[17]</ref>. We use a deeper and wider version of LeNet mentioned in <ref type="bibr" target="#b32">[33]</ref>, where the last hidden layer output is restricted to 2-dimensions for easy vizualization. For the first setup, we train the network end-to-end using the regular softmax loss for digits classifcation with number of classes = 10. For the second setup, we add an L 2 -normalize layer and scale layer to the 2-dimensional features which enforces the L 2 -constraint described in Equation 3 (seen Section 4.2 for details). <ref type="figure" target="#fig_2">Figure 3</ref> depicts the 2-D features for different classes for MNIST test set containing 10, 000 digit images. Each of the lobes shown in the figure represents 2-D features of unique digits classes. The features for the second setup were obtained before the L 2 -normalization layer. We find two clear differences between the features learned using the two setups discussed above. First, the intra-class angular variance is large when using the regular softmax loss, which can be estimated by the average width of the lobes for each class. On the other hand, the features obtained with L 2 -softmax loss have lower intra-class angular variability, and are represented by thinner lobes. Second, the magnitudes of the features are much higher with the softmax loss (ranging upto 150), since larger feature norms result in a higher probability for a correctly classified class. In contrast, the feature norm has minimal effect on the L 2 -softmax loss since every feature is normalized to a circle of fixed radius before computing the loss. Hence, the network focuses on bringing the features from the same class closer to each other and separating the features from different classes in the normalized or angular space. <ref type="table" target="#tab_0">Table 1</ref> lists the accuracy obtained with the two setups on MNIST test set. L 2 -softmax loss achieves a higher performance, reducing the error by more than 15%. Note that these accuracy numbers are lower compared to a typical DCNN since we are using only 2-dimensional features for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MNIST Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Here, we provide the details of implementing the L 2constraint described in <ref type="bibr">Equation 3</ref> in the framework of DC-NNs. The constraint is enforced by adding an L 2 -normalize layer followed by a scale layer as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. This module is added just after the penultimate layer of DCNN which acts as a feature descriptor. The L 2normalize layer normalizes the input feature x to a unit vector given by <ref type="bibr">Equation 4</ref>. The scale layer scales the input unit vector to a fixed radius given by the parameter α (Equation 5). In total, we just introduce one scalar parameter (α) which can be trained along with the other parameters of the network.</p><formula xml:id="formula_3">y = x x 2 (4) z = α · y<label>(5)</label></formula><p>The module is fully differentiable and can be used in the end-to-end training of the network. At test time, the proposed module is redundant, since the features are eventually normalized to unit length while computing the cosine similarity. At training time, we backpropagate the gradients through the L2-normalize and the scale layer, as well as compute the gradients with respect to the scaling parameter α using the chain rule as given below.</p><formula xml:id="formula_4">∂l ∂y i = ∂l ∂z i · α ∂l ∂α = D j=1 ∂l ∂z j · y j ∂l ∂x i = D j=1 ∂l ∂y j · ∂y j ∂x i ∂y i ∂x i = x 2 2 − x 2 i x 3 2 ∂y j ∂x i = −x i · x j x 3 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Bounds on Parameter α</head><p>The scaling parameter α plays a crucial role in deciding the performance of L 2 -softmax loss. There are two ways to enforce the L 2 -constraint: 1) by keeping α fixed throughout the training, and 2) by letting the network to learn the parameter α. The second way is elegant and always improves over the regular softmax loss. But, the α parameter learned by the network is high which results in a relaxed L 2 -constraint. The softmax classifier aimed at increasing the feature norm for minimizing the overall loss, increases the α parameter instead, allowing it more freedom to fit to the easy samples. Hence, α learned by the network forms an upper bound for the parameter. A better performance is obtained by fixing α to a lower constant value.</p><p>On the other hand, with a very low value of α, the training doesn't converge. For instance, α = 1 performs very poorly on the LFW <ref type="bibr" target="#b13">[14]</ref> dataset, achieving an accuracy of 86.37% (see <ref type="figure" target="#fig_5">Figure 7</ref>). The reason being that a hypersphere with small radius (α) has limited surface area for embedding features from the same class together and those from different classes far from each other.</p><p>Here, we formulate a theoretical lower bound on α. Assuming the number of classes C to be lower than twice the feature dimension D, we can distribute the classes on a hypersphere of dimension D such that any two class centers are at least 90 • apart. <ref type="figure" target="#fig_4">Figure 5</ref>(a) represents this case for C = 4 class centers distributed on a circle of radius α. We assume the classifier weights (W i ) to be a unit vector pointing in the direction of their respective class centers. We ignore the bias term. The average softmax probability p for correctly classifying a feature is given by Equation 7 p = e W T i Xi 4 j=1 e W T j Xi = e α e α + 2 + e −α <ref type="bibr" target="#b6">(7)</ref> Ignoring the term e −α and generalizing it for C classes, the average probability becomes:  <ref type="figure" target="#fig_4">Figure 5</ref>(b) plots the probability score as a function of the parameter α for various number of classes C. We can infer that to achieve a given classification probability (say p = 0.9), we need to have a higher α for larger C. Given the number of classes C for a dataset, we can obtain the lower bound on α to achieve a probability score of p by using Equation <ref type="bibr" target="#b8">9</ref>.</p><formula xml:id="formula_5">p = e α e α + C − 2<label>(8)</label></formula><formula xml:id="formula_6">α low = log p(C − 2) 1 − p<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We use the publicly available Face-Resnet [33] DCNN for our experiments. <ref type="figure">Figure 6</ref> shows the architecture of the network. It contains 27 convolutional layers and 2 fully-connected layers. The dimension of the feature descriptor is 512. It utilizes the widely used residual skipconnections <ref type="bibr" target="#b11">[12]</ref>. We add an L 2 -normalize layer and a scale layer after the fully-connected layer to enforce the L 2constraint on the descriptor. All our experiments are carried out in Caffe <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline experiments</head><p>In this subsection, we experimentally validate the usefulness of the L 2 -softmax loss for face verification. We form two subsets of training dataset from the MS-Celeb-1M <ref type="bibr" target="#b8">[9]</ref> dataset: 1) MS-small containing 0.5 million face images with the number of subjects being 13403, and 2) MSlarge containing 3.7 million images of 58207 subjects. The <ref type="figure">Figure 6</ref>. The Face-Resnet architecture <ref type="bibr" target="#b32">[33]</ref> used for the experiments. C denotes Convolution Layer followed by PReLU <ref type="bibr" target="#b10">[11]</ref> while P denotes Max Pooling Layer. Each pooling layer is followed by a set of residual connections, the count for which is denoted alongside. After the fully-connected layer (FC), we add an L2-Normalize layer and Scale Layer which is followed by the softmax loss.</p><p>dataset was cleaned using the clustering algorithm mentioned in <ref type="bibr" target="#b17">[18]</ref>. We train the Face-Resnet network with softmax loss as well as L 2 -softmax loss for various α. While training with MS-small, we start with a base learning rate of 0.1 and decrease it by 1/10 th after 16K and 24K iterations, upto a maximum of 28K iterations. For training on MS-large, we use the same learning rate but decrease it after 50K and 80K iterations upto a maximum of 100K iterations. A training batch size of 256 was used. Both softmax and L 2 -softmax loss functions consume the same amount of training time which is around 9 hours for MS-small and 32 hours for MS-large training set respectively, on two TITAN X GPUs. We set the learning multiplier and decay multiplier for the scale layer to 1 for trainable α, and 0 for fixed α during the network training. We evaluate our baselines on the widely used LFW dataset <ref type="bibr" target="#b13">[14]</ref> for the unrestricted setting, and the challenging IJB-A dataset <ref type="bibr" target="#b15">[16]</ref> on the 1:1 face verification protocol. The faces were cropped and aligned to the size of 128 × 128 in both training and testing phases by implementing the face detection and alignment algorithm mentioned in <ref type="bibr" target="#b24">[25]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experiment with small training set</head><p>Here, we compare the network trained on MS-small dataset using our proposed L 2 -softmax loss, against the one trained with regular softmax loss. <ref type="figure" target="#fig_5">Figure 7</ref> shows that the regular softmax loss attains an accuracy of 98.1% whereas the proposed L 2 -softmax loss achieves the best accuracy of 99.28%, thereby reducing the error by more than 62%. It also shows the variations in performance with the scale parameter α. The performance is poor when α is below a certain threshold and stable with α higher than the threshold. This behavior is consistent with the theoretical analysis presented in Section 4.3. From the figure, the performance of L 2 -Softmax is better for α &gt;12 which is close to its lower bound computed using equation 9 for C = 13403 with a probability score of 0.9. A similar trend is observed for 1:1 verification protocol on IJB-A <ref type="bibr" target="#b15">[16]</ref> as shown in <ref type="table" target="#tab_1">Table 2</ref>, where the numbers denote True Accept Rate (TAR) at False Accept Rates (FAR) of 0.0001, 0.001, 0.01 and 0.1. Our proposed approach improves the TAR@FAR=0.0001 by 19% compared to the baseline softmax loss. The performance is consistent with α ranging between 16 to 32. Another point to note is that by allowing the network to learn the scale parameter α by itself results in a slight decrease in performance, which shows that having a tighter constraint is a better choice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experiment with large training set</head><p>We train the network on the MS-large dataset for this experiment. <ref type="figure">Figure 8</ref> shows the performance on the LFW dataset. Similar to the small training set, the L 2 -softmax loss significantly improves over the baseline, reducing the error by 60% and achieving an accuracy of 99.6%. Similarly, it improves the TAR@FAR=0.0001 on IJB-A by more than 10% <ref type="table" target="#tab_2">(Table 3</ref>). The performance of L 2 -softmax is consistent with α in the range 40 and beyond. Unlike, the small set training, the self-trained α performs equally good compared to fixed α of 40 and 50. The theoretical lower bound on α is not of much use in this case since improved performance is achieved for α &gt;30. We can deduce that as the number of subjects increases, the lower bound on α is less reliable, and the self-trained α is more reliable with performance. This experiment clearly suggests that the proposed L 2 -softmax loss is consistent across the training and testing datasets. <ref type="figure">Figure 8</ref>. The red curve shows the variations in LFW accuracy with the parameter α for L2-softmax loss. The green line is the accuracy using the softmax loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Experiment with a different DCNN</head><p>To check the consistency of our proposed L 2 -softmax loss, we apply it on the All-In-One Face <ref type="bibr" target="#b24">[25]</ref> instead of the Face-Resnet. We use the recognition branch of the All-In-One Face to fine-tune on the MS-small training set. The recognition branch of All-In-One Face consists of 7 convolution layers followed by 3 fully-connected layers and a softmax loss. We add an L 2 -normalize and a scale layer after the 512 dimension feature descriptor. <ref type="figure">Figure 9</ref> shows the comparison of L 2 -softmax loss and the base softmax loss on LFW dataset. Similar to the Face-Resnet, All-In-One Face with L 2 -softmax loss improves over the base softmax performance, reducing the error by 40%, and achieving an accuracy of 98.82%. The improvement obtained by using All-In-One Face is smaller compared to the Face-Resnet. This shows that residual connections and depth of the network generate better feature embedding on a hypersphere. The performance variation with scaling parameter α is similar to that of Face-Resnet, indicating that the optimal scale parameter does not depend on the choice of the network. <ref type="figure">Figure 9</ref>. The red curve shows the variations in LFW accuracy with the parameter α for L2-softmax loss. The green line is the accuracy using the softmax loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Experiment with auxiliary loss</head><p>Similar to softmax loss, the L 2 -softmax loss can be coupled with auxiliary losses such as center loss, contrastive loss, triplet loss, etc. to further improve the performance. Here we study the performance variation of L 2 -softmax loss when coupled with the center loss. We use the MS-small dataset for training the networks. <ref type="table" target="#tab_3">Table 4</ref> lists the accuracy obtained on LFW dataset by different loss functions. The softmax loss performs the worst. The center loss improves the performance significantly when trained in conjunction with the softmax loss, and is comparable to the L 2 -softmax loss. Training center loss with the L 2 -softmax loss gives the best performance of 99.33% accuracy. This shows that L 2softmax loss is as versatile as the regular softmax loss and can be used efficiently with other auxiliary loss functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with recent methods</head><p>We compare our algorithm with recently reported face verification methods on LFW <ref type="bibr" target="#b13">[14]</ref>, YouTube Face <ref type="bibr" target="#b33">[34]</ref> and IJB-A <ref type="bibr" target="#b15">[16]</ref> datasets. We crop and align the images for all these datasets by implementing the algorithm mentioned in <ref type="bibr" target="#b24">[25]</ref>. We train the Face-Resnet (FR) with L 2 -softmax as well as regular softmax loss using the MS-large training set. Additionally, we train ResNet-101(R101) <ref type="bibr" target="#b11">[12]</ref> and ResNeXt-101(RX101) <ref type="bibr" target="#b34">[35]</ref> deep networks for face recognition using MS-large training set with L 2 -softmax loss. Both R101 and RX101 models were initialized with parameters pre-trained on ImageNet <ref type="bibr" target="#b25">[26]</ref> dataset. A fully-connected layer of dimension 512 was added before the L 2 -softmax classifier. The scaling parameter was kept fixed with a value of α = 50. Experimental results on different datasets show that L 2 -softmax works efficiently with deeper models.</p><p>The LFW dataset <ref type="bibr" target="#b13">[14]</ref> contains 13, 233 web-collected images from 5749 different identities. We evaluate our model following the standard protocol of unrestricted with labeled outside data. We test on 6,000 face pairs and report the experiment results in <ref type="table" target="#tab_4">Table 5</ref>. Along with the accuracy values, we also compare with the number of images, networks and loss functions used by the methods for their overall training. The proposed method attains the state-ofthe-art performance with the RX101 model, achieving an accuracy of 99.78%. Unlike other methods which use auxiliary loss functions such as center loss and contrastive loss along with the primary softmax loss, our method uses a single loss training paradigm which makes it easier and faster to train.</p><p>YouTube Face (YTF) <ref type="bibr" target="#b33">[34]</ref> dataset contains 3425 videos of 1595 different people, with an average length of 181.3 frames per video. It contains 10 folds of 500 video pairs. We follow the standard verification protocol and report the average accuracy on splits with cross-validation in <ref type="table" target="#tab_4">Table 5</ref>. We achieve the accuracy of 96.08% using L 2 -softmax loss with RX101 network. Our method outperforms many recent algorithms and is only behind DeepVisage <ref type="bibr" target="#b9">[10]</ref> which uses larger number of training samples, and VGG Face <ref type="bibr" target="#b23">[24]</ref> which further uses a discriminative metric learning on YTF.</p><p>The IJB-A dataset <ref type="bibr" target="#b15">[16]</ref> contains 500 subjects with a total of 25, 813 images including 5, 399 still images and 20, 414 video frames. It contains faces with extreme viewpoints, resolution and illumination which makes it more challenging than the commonly used LFW dataset. Given a template containing multiple faces of the same individual, we gener-  <ref type="bibr" target="#b26">[27]</ref>. <ref type="table" target="#tab_5">Table 6</ref> lists the performance of recent DCNN-based methods on IJB-A dataset. We achieve state-of-the-art result for both verification and the identification protocols. Since the L 2softmax loss can be coupled with any other auxiliary loss, we use the Triplet Probabilistic Embedding (TPE) <ref type="bibr" target="#b26">[27]</ref> to learn a 128-dimensional embedding using the training splits of IJB-A. It further improves the performance and achieves a record TAR of 0.909 @ FAR = 0.0001. To the best of our knowledge, we are the first ones to surpass TAR of 0.9 @ FAR of 0.0001 on IJB-A. Our method performs significantly better than existing methods in most of the other metrics as well. The results on LFW <ref type="bibr" target="#b13">[14]</ref>, YTF <ref type="bibr" target="#b33">[34]</ref> and IJB-A <ref type="bibr" target="#b15">[16]</ref> datasets clearly suggests the effectiveness of the proposed L 2 -softmax loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we added a simple, yet effective, L 2constraint to the regular softmax loss for training a face verification system. The constraint enforces the features to lie on a hypersphere of a fixed radius characterized by parameter α. We also provided bounds on the value of α for achieving a consistent performance. Experiments on LFW, YTF and IJB-A datasets show that the proposed L 2 -softmax loss provides a significant and consistent boost over the regular softmax loss and achieves the state-of-the-art result on IJB-A <ref type="bibr" target="#b15">[16]</ref> dataset. In conclusion, L 2 -softmax loss is a valuable replacement for the existing softmax loss, for the task of face verification. In the future, we would further explore the possibility of exploiting the geometric structure of the feature encoding using manifold-based metric learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Face Verification Performance on IJB-A dataset. The templates are divided into 3 sets based on their L2-norm. '1' denotes the set with low L2-norm while '3' represents high L2-norm. The legend 'x-y' denote the evaluation pairs where one template is from set 'x' and another from set 'y'. (b) Sample template images from IJB-A dataset with high, medium and low L2-norm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A general pipeline for training and testing a face verification system using DCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Vizualization of 2-dimensional features for MNIST digit classification test set using (a) Softmax Loss. (b) L2-Softmax Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>We add an L2-normalize layer and a scale layer to constrain the feature descriptor to lie on a hypersphere of radius α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) 2-D vizualization of the assumed distribution of features (b) Variation in Softmax probability with respect to α for different number of classes C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The red curve shows the variations in LFW accuracy with the parameter α for L2-softmax loss. The green line is the accuracy using the usual softmax loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>- 0. 2 (</head><label>2</label><figDesc>0.008) 0.41(0.014) 0.63(0.023) 0.047(0.02) 0.235(0.03) 0.443(0(0.06) 0.733(0.034) 0.895(0.013) 0.383(0.063) 0.613(0.032) 0.820(0.024) -VGG-Face[24] -0.604(0.06) 0.805(0.03) 0.937(0.01) 0.46(0.07) 0.67(0.03) 0.913(0.01) 0.981(0.005) DCN N manual +metric [4(0.02) 0.90(0.01) 0.964(0.005) 0.753(0.03) 0.863(0.014) 0.932(0.01) 0.977(0.005) Template Adaptation [8] -0.836(0.027) 0.939(0.013) 0.979(0.004) 0.774(0.049) 0.882(0.016) 0.928(0.01) 0.986(0.003) All-In-One Face [25] -0.823(0.02) 0.922(0.01) 0.976(0.004) 0.792(0.02) 0.887(0.014) 0.947(0.008) 0.988(0.003) NAN [37] -0.881(0.011) 0.941(0.008) 0.979(0.004) 0.817(0.041) 0.917(0.009) 0.958(0.005) 0.986(0.003) TDFF [36] 0.875(0.013) 0.919(0.006) 0.961(0.007) 0.988(0.003) 0.878(0.035) 0.941(0.010) 0.964(0.006) 0.992(0.002) TDFF [36]+TPE<ref type="bibr" target="#b26">[27]</ref> 0.877(0.018) 0.921(0.005) 0.961(0.007) 0.989(0.003) 0.881(0.039) 0.940(0.009) 0.964(0.007) 0.992(0.003) softmax (FR) 0.730(0.076) 0.851(0.021) 0.926(0.01) 0.972(0.004) 0.788(0.048) 0.892(0.015) 0.953(0.008) 0.984(0.004) L 2 -S (FR) 0.832(0.027) 0.906(0.016) 0.952(0.007) 0.981(0.003) 0.852(0.042) 0.930(0.01) 0.963(0.007) 0.986(0.002) L 2 -S (FR)+TPE [27] 0.863(0.012) 0.910(0.013) 0.951(0.006) 0.979(0.003) 0.873(0.024) 0.931(0.01) 0.961(0.007) 0.983(0.003) L 2 -S (R101) 0.879(0.028) 0.937(0.008) 0.967(0.005) 0.984(0.002) 0.895(0.055) 0.953(0.007) 0.973(0.005) 0.987(0.003) L 2 -S (R101)+TPE [27] 0.898(0.019) 0.942(0.006) 0.969(0.004) 0.983(0.003) 0.910(0.045) 0.956(0.007) 0.971(0.005) 0.986(0.003) L 2 -S (RX101) 0.883(0.032) 0.938(0.008) 0.968(0.004) 0.987(0.002) 0.903(0.046) 0.955(0.007) 0.975(0.005) 0.990(0.002) L 2 -S (RX101)+TPE [27] 0.909(0.007) 0.943(0.005) 0.970(0.004) 0.984(0.002) 0.915(0.041) 0.956(0.006) 0.973(0.005) 0.988(0.003)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy on MNIST test set in (%)</figDesc><table><row><cell></cell><cell cols="2">Softmax Loss L2-Softmax Loss</cell></row><row><cell>Accuracy</cell><cell>98.88</cell><cell>99.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">TAR on IJB-A 1:1 Verification Protocol @FAR</cell></row><row><cell></cell><cell>0.0001 0.001 0.01</cell><cell>0.1</cell></row><row><cell>softmax</cell><cell cols="2">0.553 0.730 0.881 0.957</cell></row><row><cell>L 2 -softmax (α=8)</cell><cell cols="2">0.257 0.433 0.746 0.953</cell></row><row><cell>L 2 -softmax (α=12)</cell><cell cols="2">0.620 0.721 0.875 0.970</cell></row><row><cell>L 2 -softmax (α=16)</cell><cell cols="2">0.734 0.834 0.924 0.974</cell></row><row><cell>L 2 -softmax (α=20)</cell><cell cols="2">0.740 0.820 0.922 0.973</cell></row><row><cell>L 2 -softmax (α=24)</cell><cell cols="2">0.744 0.831 0.912 0.974</cell></row><row><cell>L 2 -softmax (α=28)</cell><cell cols="2">0.740 0.834 0.922 0.975</cell></row><row><cell>L 2 -softmax (α=32)</cell><cell cols="2">0.727 0.831 0.921 0.972</cell></row><row><cell cols="3">L 2 -softmax (α trained) 0.698 0.817 0.914 0.971</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">TAR on IJB-A 1:1 Verification Protocol @FAR</cell></row><row><cell></cell><cell>0.0001 0.001 0.01</cell><cell>0.1</cell></row><row><cell>softmax</cell><cell cols="2">0.730 0.851 0.926 0.972</cell></row><row><cell>L 2 -softmax (α=30)</cell><cell cols="2">0.775 0.871 0.938 0.978</cell></row><row><cell>L 2 -softmax (α=40)</cell><cell cols="2">0.827 0.900 0.951 0.982</cell></row><row><cell>L 2 -softmax (α=50)</cell><cell cols="2">0.832 0.906 0.952 0.981</cell></row><row><cell cols="3">L 2 -softmax (α trained) 0.832 0.903 0.950 0.980</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Accuracy on LFW (%) L 2 -softmax loss 99.<ref type="bibr" target="#b32">33</ref> </figDesc><table><row><cell>softmax loss</cell><cell>98.10</cell></row><row><cell>center loss [33] + softmax loss</cell><cell>99.23</cell></row><row><cell>L 2 -softmax loss</cell><cell>99.28</cell></row><row><cell>center loss [33] +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Verification accuracy (in %) of different methods on LFW and YTF datasets.</figDesc><table><row><cell>Method</cell><cell cols="5">Images #nets One loss LFW YTF</cell></row><row><cell>Deep Face [30]</cell><cell>4M</cell><cell>3</cell><cell>No</cell><cell cols="2">97.35 91.4</cell></row><row><cell>DeepID-2+ [29]</cell><cell>-</cell><cell>25</cell><cell>No</cell><cell cols="2">99.47 93.2</cell></row><row><cell>FaceNet [28]</cell><cell>200M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.63 95.12</cell></row><row><cell cols="2">VGG Face [24] 2.6M</cell><cell>1</cell><cell>No</cell><cell cols="2">98.95 97.3</cell></row><row><cell>Baidu [19]</cell><cell>1.3M</cell><cell>1</cell><cell>No</cell><cell>99.13</cell><cell>-</cell></row><row><cell cols="2">Wen et al. [33] 0.7M</cell><cell>1</cell><cell>No</cell><cell cols="2">99.28 94.9</cell></row><row><cell>NAN [37]</cell><cell>3M</cell><cell>1</cell><cell>No</cell><cell>−</cell><cell>95.72</cell></row><row><cell cols="2">DeepVisage [10] 4.48M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.62 96.24</cell></row><row><cell cols="2">SphereFace [20] 0.5M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.42 95.0</cell></row><row><cell>softmax(FR)</cell><cell>3.7M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.0 93.82</cell></row><row><cell>L 2 -S (FR)</cell><cell>3.7M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.60 95.54</cell></row><row><cell>L 2 -S (R101)</cell><cell>3.7M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.67 96.02</cell></row><row><cell cols="2">L 2 -S (RX101) 3.7M</cell><cell>1</cell><cell>Yes</cell><cell cols="2">99.78 96.08</cell></row><row><cell cols="6">ate a common vector representation by media pooling the</cell></row><row><cell cols="4">individual face descriptors, as explained in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Face Identification and Verification Evaluation on IJB-A dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="2">IJB-A Verification (TAR@FAR)</cell><cell></cell><cell></cell><cell cols="2">IJB-A Identification</cell><cell></cell></row><row><cell>Method</cell><cell>0.0001</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell><cell>FPIR=0.01</cell><cell>FPIR=0.1</cell><cell>Rank=1</cell><cell>Rank=10</cell></row><row><cell>GOTS [16]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 1, 2</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lekust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-to-many face recognition with bilinear cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03958</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepvisage: Making face recognition simple yet with powerful generalization skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08388</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A proximityaware hierarchical clustering of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08063</idno>
		<title level="m">Sphereface: Deep hypersphere embedding for face recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Trn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="579" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Parde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Colon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01751</idno>
		<title level="m">Deep convolutional neural network features and the original image</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00851</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Triplet probabilistic embedding for face verification and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>IEEE 8th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent factor guided convolutional neural networks for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4893" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A good practice towards top performance of face recognition: Transferred deep feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00438</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05474</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
