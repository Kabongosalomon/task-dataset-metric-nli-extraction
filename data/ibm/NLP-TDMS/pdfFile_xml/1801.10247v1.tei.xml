<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 FASTGCN: FAST LEARNING WITH GRAPH CONVOLU- TIONAL NETWORKS VIA IMPORTANCE SAMPLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
							<email>chenjie@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
							<email>tengfei.ma1@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
							<email>cxiao@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 FASTGCN: FAST LEARNING WITH GRAPH CONVOLU- TIONAL NETWORKS VIA IMPORTANCE SAMPLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work-FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate. * These two authors contribute equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are universal representations of pairwise relationship. Many real world data come naturally in the form of graphs; e.g., social networks, gene expression networks, and knowledge graphs. To improve the performance of graph-based learning tasks, such as node classification and link prediction, recently much effort is made to extend well-established network architectures, including recurrent neural networks (RNN) and convolutional neural networks (CNN), to graph data; see, e.g., <ref type="bibr" target="#b2">Bruna et al. (2013)</ref>; <ref type="bibr" target="#b5">Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b14">Li et al. (2015)</ref>; <ref type="bibr" target="#b11">Jain et al. (2015)</ref>; <ref type="bibr" target="#b10">Henaff et al. (2015)</ref>; <ref type="bibr" target="#b17">Niepert et al. (2016)</ref>; <ref type="bibr" target="#b12">Kipf &amp; Welling (2016a;</ref><ref type="bibr">b)</ref>.</p><p>Whereas learning feature representations for graphs is an important subject among this effort, here, we focus on the feature representations for graph vertices. In this vein, the closest work that applies a convolution architecture is the graph convolutional network (GCN) <ref type="bibr" target="#b12">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr">b)</ref>. Borrowing the concept of a convolution filter for image pixels or a linear array of signals, GCN uses the connectivity structure of the graph as the filter to perform neighborhood mixing. The architecture may be elegantly summarized by the following expression:</p><formula xml:id="formula_0">H (l+1) = σ(ÂH (l) W (l) ),</formula><p>whereÂ is some normalization of the graph adjacency matrix, H (l) contains the embedding (rowwise) of the graph vertices in the lth layer, W (l) is a parameter matrix, and σ is nonlinearity.</p><p>As with many graph algorithms, the adjacency matrix encodes the pairwise relationship for both training and test data. The learning of the model as well as the embedding is performed for both data simultaneously, at least as the authors proposed. For many applications, however, test data may not be readily available, because the graph may be constantly expanding with new vertices (e.g. new members of a social network, new products to a recommender system, and new drugs for functionality tests). Such scenarios require an inductive scheme that learns a model from only a training set of vertices and that generalizes well to any augmentation of the graph.</p><p>A more severe challenge for GCN is that the recursive expansion of neighborhoods across layers incurs expensive computations in batched training. Particularly for dense graphs and powerlaw graphs, the expansion of the neighborhood for a single vertex quickly fills up a large portion of the graph. Then, a usual mini-batch training will involve a large amount of data for every batch, even with a small batch size. Hence, scalability is a pressing issue to resolve for GCN to be applicable to large, dense graphs.</p><p>To address both challenges, we propose to view graph convolutions from a different angle and interpret them as integral transforms of embedding functions under probability measures. Such a view provides a principled mechanism for inductive learning, starting from the formulation of the loss to the stochastic version of the gradient. Specifically, we interpret that graph vertices are iid samples of some probability distribution and write the loss and each convolution layer as integrals with respect to vertex embedding functions. Then, the integrals are evaluated through Monte Carlo approximation that defines the sample loss and the sample gradient. One may further alter the sampling distribution (as in importance sampling) to reduce the approximation variance.</p><p>The proposed approach, coined FastGCN, not only rids the reliance on the test data but also yields a controllable cost for per-batch computation. At the time of writing, we notice a newly published work GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017</ref>) that proposes also the use of sampling to reduce the computational footprint of GCN. Our sampling scheme is more economic, resulting in a substantial saving in the gradient computation, as will be analyzed in more detail in Section 3.3. Experimental results in Section 4 indicate that the per-batch computation of FastGCN is more than an order of magnitude faster than that of GraphSAGE, while classification accuracies are highly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Over the past few years, several graph-based convolution network models emerged for addressing applications of graph-structured data, such as the representation of molecules <ref type="bibr" target="#b5">(Duvenaud et al., 2015)</ref>. An important stream of work is built on spectral graph theory <ref type="bibr" target="#b2">(Bruna et al., 2013;</ref><ref type="bibr" target="#b10">Henaff et al., 2015;</ref><ref type="bibr" target="#b4">Defferrard et al., 2016)</ref>. They define parameterized filters in the spectral domain, inspired by graph Fourier transform. These approaches learn a feature representation for the whole graph and may be used for graph classification.</p><p>Another line of work learns embeddings for graph vertices, for which <ref type="bibr" target="#b7">Goyal &amp; Ferrara (2017)</ref> is a recent survey that covers comprehensively several categories of methods. A major category consists of factorization based algorithms that yield the embedding through matrix factorizations; see, e.g., <ref type="bibr" target="#b20">Roweis &amp; Saul (2000)</ref>; <ref type="bibr" target="#b1">Belkin &amp; Niyogi (2001)</ref>; <ref type="bibr" target="#b0">Ahmed et al. (2013)</ref>; <ref type="bibr" target="#b3">Cao et al. (2015)</ref>; <ref type="bibr" target="#b18">Ou et al. (2016)</ref>. These methods learn the representations of training and test data jointly. Another category is random walk based methods <ref type="bibr" target="#b19">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b8">Grover &amp; Leskovec, 2016</ref>) that compute node representations through exploration of neighborhoods. <ref type="bibr">LINE (Tang et al., 2015)</ref> is also such a technique that is motivated by the preservation of the first and second-order proximities. Meanwhile, there appear a few deep neural network architectures, which better capture the nonlinearity within graphs, such as SDNE <ref type="bibr">(Wang et al., 2016)</ref>. As motivated earlier, <ref type="bibr">GCN (Kipf &amp; Welling, 2016a)</ref> is the model on which our work is based.</p><p>The most relevant work to our approach is GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, which learns node representations through aggregation of neighborhood information. One of the proposed aggregators employs the GCN architecture. The authors also acknowledge the memory bottleneck of GCN and hence propose an ad hoc sampling scheme to restrict the neighborhood size. Our sampling approach is based on a different and more principled formulation. The major distinction is that we sample vertices rather than neighbors. The resulting computational savings are analyzed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRAINING AND INFERENCE THROUGH SAMPLING</head><p>One striking difference between GCN and many standard neural network architectures is the lack of independence in the sample loss. Training algorithms such as SGD and its batch generalization are designed based on the additive nature of the loss function with respect to independent data samples. For graphs, on the other hand, each vertex is convolved with all its neighbors and hence defining a sample gradient that is efficient to compute is beyond straightforward.</p><p>Concretely, consider the standard SGD scenario where the loss is the expectation of some function g with respect to a data distribution D:</p><formula xml:id="formula_1">L = E x∼D [g(W ; x)].</formula><p>Here, W denotes the model parameter to be optimized. Of course, the data distribution is generally unknown and one instead minimizes the empirical loss through accessing n iid samples x 1 , . . . , x n :</p><formula xml:id="formula_2">L emp = 1 n n i=1 g(W ; x i ), x i ∼ D, ∀ i.</formula><p>In each step of SGD, the gradient is approximated by ∇g(W ; x i ), an (assumed) unbiased sample of ∇L. One may interpret that each gradient step makes progress toward the sample loss g(W ; x i ).</p><p>The sample loss and the sample gradient involve only one single sample x i .</p><p>For graphs, one may no longer leverage the independence and compute the sample gradient ∇g(W ; x i ) by discarding the information of i's neighboring vertices and their neighbors, recursively. We therefore seek an alternative formulation. In order to cast the learning problem under the same sampling framework, let us assume that there is a (possibly infinite) graph G with the vertex set V associated with a probability space (V , F, P ), such that for the given graph G, it is an induced subgraph of G and its vertices are iid samples of V according to the probability measure P . For the probability space, V serves as the sample space and F may be any event space (e.g., the power set F = 2 V ). The probability measure P defines a sampling distribution.</p><p>To resolve the problem of lack of independence caused by convolution, we interpret that each layer of the network defines an embedding function of the vertices (random variable) that are tied to the same probability measure but are independent. See <ref type="figure">Figure 1</ref>. Specifically, recall the architecture of GCÑ</p><formula xml:id="formula_3">H (l+1) =ÂH (l) W (l) , H (l+1) = σ(H (l+1) ), l = 0, . . . , M − 1, L = 1 n n i=1 g(H (M ) (i, :)).</formula><p>(1) For the functional generalization, we writẽ</p><formula xml:id="formula_4">h (l+1) (v) = Â (v, u)h (l) (u)W (l) dP (u), h (l+1) (v) = σ(h (l+1) (v)), l = 0, . . . , M − 1,<label>(2)</label></formula><formula xml:id="formula_5">L = E v∼P [g(h (M ) (v))] = g(h (M ) (v)) dP (v).<label>(3)</label></formula><p>Here, u and v are independent random variables, both of which have the same probability measure P . The function h (l) is interpreted as the embedding function from the lth layer. The embedding functions from two consecutive layers are related through convolution, expressed as an integral transform, where the kernelÂ(v, u) corresponds to the (v, u) element of the matrixÂ. The loss is the expectation of g(h (M ) ) for the final embedding h <ref type="bibr">(M )</ref> . Note that the integrals are not the usual Riemann-Stieltjes integrals, because the variables u and v are graph vertices but not real numbers; however, this distinction is only a matter of formalism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph convolution view</head><p>Integral transform view batch H (2) <ref type="figure">Figure 1</ref>: Two views of GCN. On the left (graph convolution view), each circle represents a graph vertex. On two consecutive rows, a circle i is connected (in gray line) with circle j if the two corresponding vertices in the graph are connected. A convolution layer uses the graph connectivity structure to mix the vertex features/embeddings. On the right (integral transform view), the embedding function in the next layer is an integral transform (illustrated by the orange fanout shape) of the one in the previous layer. For the proposed method, all integrals (including the loss function) are evaluated by using Monte Carlo sampling. Correspondingly in the graph view, vertices are subsampled in a bootstrapping manner in each layer to approximate the convolution. The sampled portions are collectively denoted by the solid blue circles and the orange lines.</p><formula xml:id="formula_6">H (1) H (0) h (2) (v) h (1) (v) h (0) (v)</formula><p>Theorem 1. If g and σ are continuous, then lim</p><formula xml:id="formula_7">t0,t1,...,t M →∞ L t0,t1,...,t M = L with probability one.</formula><p>In practical use, we are given a graph whose vertices are already assumed to be samples. Hence, we will need bootstrapping to obtain a consistent estimate. In particular, for the network architecture (1), the output H (M ) is split into batches as usual. We will still use u</p><formula xml:id="formula_8">(M ) 1 , . . . , u (M )</formula><p>t M to denote a batch of vertices, which come from the given graph. For each batch, we sample (with replacement) uniformly each layer and obtain samples u (l) i , i = 1, . . . , t l , l = 0, . . . , M − 1. Such a procedure is equivalent to uniformly sampling the rows of H (l) for each l. Then, we obtain the batch loss</p><formula xml:id="formula_9">L batch = 1 t M t M i=1 g(H (M ) (u (M ) i , :)),<label>(4)</label></formula><p>where, recursively,</p><formula xml:id="formula_10">H (l+1) (v, :) = σ   n t l t l j=1Â (v, u (l) j )H (l) (u (l) j , :)W (l)   , l = 0, . . . , M − 1.<label>(5)</label></formula><p>Here, the n inside the activation function σ is the number of vertices in the given graph and is used to account for the normalization difference between the matrix form (1) and the integral form <ref type="formula" target="#formula_4">(2)</ref>. The corresponding batch gradient may be straightforwardly obtained through applying the chain rule on each H (l) . See Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VARIANCE REDUCTION</head><p>As for any estimator, one is interested in improving its variance. Whereas computing the full variance is highly challenging because of nonlinearity in all the layers, it is possible to consider each single layer and aim at improving the variance of the embedding function before nonlinearity. Specifically, consider for the lth layer, the functionh For each layer l, sample uniformly t l vertices u</p><formula xml:id="formula_11">(l) 1 , . . . , u (l) t l 3:</formula><p>for each layer l do Compute batch gradient ∇L batch 4:</p><p>If v is sampled in the next layer,</p><formula xml:id="formula_12">∇H (l+1) (v, :) ← n t l t l j=1Â (v, u (l) j )∇ H (l) (u (l) j , :)W (l) 5: end for 6: W ← W − η∇L batch SGD step 7: end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function</head><p>Samples Num. samples</p><formula xml:id="formula_13">Layer l + 1; random variable vh (l+1) t l+1 (v) → y(v) u (l+1) i → v i t l+1 → s Layer l; random variable u h (l) t l (u)W (l) → x(u) u (l) j → u j t l → t</formula><p>Under the joint distribution of v and u, the aforementioned sample average is</p><formula xml:id="formula_14">G := 1 s s i=1 y(v i ) = 1 s s i=1   1 t t j=1Â (v i , u j )x(u j )   .</formula><p>First, we have the following result. Proposition 2. The variance of G admits</p><formula xml:id="formula_15">Var{G} = R + 1 st Â (v, u) 2 x(u) 2 dP (u) dP (v),<label>(6)</label></formula><p>where</p><formula xml:id="formula_16">R = 1 s 1 − 1 t e(v) 2 dP (v) − 1 s e(v) dP (v) 2 and e(v) = Â (v, u)x(u) dP (u).</formula><p>The variance <ref type="formula" target="#formula_15">(6)</ref> consists of two parts. The first part R leaves little room for improvement, because the sampling in the v space is not done in this layer. The second part (the double integral), on the other hand, depends on how the u j 's in this layer are sampled. The current result <ref type="formula" target="#formula_15">(6)</ref> is the consequence of sampling u j 's by using the probability measure P . One may perform importance sampling, altering the sampling distribution to reduce variance. Specifically, let Q(u) be the new probability measure, where the u j 's are drawn from. We hence define the new sample average approximation</p><formula xml:id="formula_17">y Q (v) := 1 t t j=1Â (v, u j )x(u j ) dP (u) dQ(u) uj , u 1 , . . . , u t ∼ Q,</formula><p>and the quantity of interest</p><formula xml:id="formula_18">G Q := 1 s s i=1 y Q (v i ) = 1 s s i=1   1 t t j=1Â (v i , u j )x(u j ) dP (u) dQ(u) uj   .</formula><p>Clearly, the expectation of G Q is the same as that of G, regardless of the new measure Q. The following result gives the optimal Q. Theorem 3. If</p><formula xml:id="formula_19">dQ(u) = b(u)|x(u)| dP (u) b(u)|x(u)| dP (u) where b(u) = Â (v, u) 2 dP (v) 1 2 ,<label>(7)</label></formula><p>then the variance of G Q admits</p><formula xml:id="formula_20">Var{G Q } = R + 1 st b(u)|x(u)| dP (u) 2 ,<label>(8)</label></formula><p>where R is defined in Proposition 2. The variance is minimum among all choices of Q.</p><p>A drawback of defining the sampling distribution Q in this manner is that it involves |x(u)|, which constantly changes during training. It corresponds to the product of the embedding matrix H (l) and the parameter matrix W (l) . The parameter matrix is updated in every iteration; and the matrix product is expensive to compute. Hence, the cost of computing the optimal measure Q is quite high.</p><p>As a compromise, we consider a different choice of Q, which involves only b(u). The following proposition gives the precise definition. The resulting variance may or may not be smaller than <ref type="formula" target="#formula_15">(6)</ref>.</p><p>In practice, however, we find that it is almost always helpful. Proposition 4. If</p><formula xml:id="formula_21">dQ(u) = b(u) 2 dP (u) b(u) 2 dP (u)</formula><p>where b(u) is defined in <ref type="formula" target="#formula_19">(7)</ref>, then the variance of G Q admits</p><formula xml:id="formula_22">Var{G Q } = R + 1 st b(u) 2 dP (u) x(u) 2 dP (u),<label>(9)</label></formula><p>where R is defined in Proposition 2.</p><p>With this choice of the probability measure Q, the ratio dQ(u)/dP (u) is proportional to b(u) 2 , which is simply the integral ofÂ(v, u) 2 with respect to v. In practical use, for the network architecture (1), we define a probability mass function for all the vertices in the given graph:</p><formula xml:id="formula_23">q(u) = Â (:, u) 2 / u ∈V Â (:, u ) 2 , u ∈ V</formula><p>and sample t vertices u 1 , . . . , u t according to this distribution. From the expression of q, we see that it has no dependency on l; that is, the sampling distribution is the same for all layers. To summarize, the batch loss L batch in (4) now is recursively expanded as</p><formula xml:id="formula_24">H (l+1) (v, :) = σ   1 t l t l j=1Â (v, u (l) j )H (l) (u (l) j , :)W (l) q(u (l) j )   , u (l) j ∼ q, l = 0, . . . , M − 1. (10)</formula><p>The major difference between <ref type="formula" target="#formula_10">(5)</ref> and <ref type="formula">(10)</ref> is that the former obtains samples uniformly whereas the latter according to q. Accordingly, the scaling inside the summation changes. The corresponding batch gradient may be straightforwardly obtained through applying the chain rule on each H (l) . See Algorithm 2. for each layer l do Compute batch gradient ∇L batch 5:</p><p>If v is sampled in the next layer, W ← W − η∇L batch SGD step 8: end for</p><formula xml:id="formula_25">∇H (l+1) (v, :) ← 1 t l t l j=1Â (v, u (l) j ) q(u (l) j ) ∇ H (l) (u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INFERENCE</head><p>The sampling approach described in the preceding subsection clearly separates out test data from training. Such an approach is inductive, as opposed to transductive that is common for many graph algorithms. The essence is to cast the set of graph vertices as iid samples of a probability distribution, so that the learning algorithm may use the gradient of a consistent estimator of the loss to perform parameter update. Then, for inference, the embedding of a new vertex may be either computed by using the full GCN architecture (1), or approximated through sampling as is done in parameter learning. Generally, using the full architecture is more straightforward and easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPARISON WITH GRAPHSAGE</head><p>GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017</ref>) is a newly proposed architecture for generating vertex embeddings through aggregating neighborhood information. It shares the same memory bottleneck with GCN, caused by recursive neighborhood expansion. To reduce the computational footprint, the authors propose restricting the immediate neighborhood size for each layer. Using our notation for the sample size, if one samples t l neighbors for each vertex in the lth layer, then the size of the expanded neighborhood is, in the worst case, the product of the t l 's. On the other hand, FastGCN samples vertices rather than neighbors in each layer. Then, the total number of involved vertices is at most the sum of the t l 's, rather than the product. See experimental results in Section 4 for the order-of-magnitude saving in actual computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We follow the experiment setup in Kipf &amp; Welling (2016a) and <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref> to demonstrate the effective use of FastGCN, comparing with the original GCN model as well as Graph-SAGE, on the following benchmark tasks: (1) classifying research topics using the Cora citation data set <ref type="bibr" target="#b15">(McCallum et al., 2000)</ref>; (2) categorizing academic papers with the Pubmed database; and (3) predicting the community structure of a social network modeled with Reddit posts. These data sets are downloaded from the accompany websites of the aforementioned references. The graphs have increasingly more nodes and higher node degrees, representative of the large and dense setting under which our method is motivated. Statistics are summarized in <ref type="table" target="#tab_1">Table 1</ref>. We adjusted the training/validation/test split of Cora and Pubmed to align with the supervised learning scenario. Specifically, all labels of the training examples are used for training, as opposed to only a small portion in the semi-supervised setting <ref type="bibr" target="#b12">(Kipf &amp; Welling, 2016a)</ref>. Such a split is coherent with that of the other data set, Reddit, used in the work of GraphSAGE. Additional experiments using the original split of Cora and Pubmed are reported in the appendix. We first consider the use of sampling in FastGCN. The left part of <ref type="table" target="#tab_2">Table 2</ref> (columns under "Sampling") lists the time and classification accuracy as the number of samples increases. For illustration purpose, we equalize the sample size on both layers. Clearly, with more samples, the per-epoch training time increases, but the accuracy (as measured by using micro F1 scores) also improves generally.</p><p>An interesting observation is that given input features H (0) , the productÂH (0) in the bottom layer does not change, which means that the chained expansion of the gradient with respect to W (0) in   <ref type="table" target="#tab_1">Table 1.</ref> the last step is a constant throughout training. Hence, one may precompute the product rather than sampling this layer to gain efficiency. The compared results are listed on the right part of <ref type="table" target="#tab_2">Table 2</ref> (columns under "Precompute"). One sees that the training time substantially decreases while the accuracy is comparable. Hence, all the experiments that follow use precomputation.</p><p>Next, we compare the sampling approaches for FastGCN: uniform and importance sampling. <ref type="figure" target="#fig_0">Figure 2</ref> summarizes the prediction accuracy under both approaches. It shows that importance sampling consistently yields higher accuracy than does uniform sampling. Since the altered sampling distribution (see Proposition 4 and Algorithm 2) is a compromise alternative of the optimal distribution that is impractical to use, this result suggests that the variance of the used sampling indeed is smaller than that of uniform sampling; i.e., the term (9) stays closer to (8) than does (6). A possible reason is that b(u) correlates with |x(u)|. Hence, later experiments will apply importance sampling.</p><p>We now demonstrate that the proposed method is significantly faster than the original GCN as well as GraphSAGE, while maintaining comparable prediction performance. See <ref type="figure" target="#fig_3">Figure 3</ref>. The bar heights indicate the per-batch training time, in the log scale. One sees that GraphSAGE is a substantial improvement of GCN for large and dense graphs (e.g., Reddit), although for smaller ones (Cora and Pubmed), GCN trains faster. FastGCN is the fastest, with at least an order of magnitude improvement compared with the runner up (except for Cora), and approximately two orders of magnitude speed up compared with the slowest. Here, the training time of FastGCN is with respect to the sample size that achieves the best prediction accuracy. As seen from the table on the right, this accuracy is highly comparable with the best of the other two methods.  <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref>. The timings of using other aggregators, such as GraphSAGE-mean, are similar. GCN refers to using batched learning, as opposed to the original version that is nonbatched; for more details of the implementation, see the appendix. The nonbatched version of GCN runs out of memory on the large graph Reddit. The sample sizes for FastGCN are 400, 100, and 400, respectively for the three data sets.</p><p>In the discussion period, the authors of GraphSAGE offered an improved implementation of their codes and alerted that GraphSAGE was better suited for massive graphs. The reason is that for small graphs, the sample size (recalling that it is the product across layers) is comparable to the graph size and hence improvement is marginal; moreover, sampling overhead might then adversely affect the timing. For fair comparison, the authors of GraphSAGE kept the sampling strategy but improved the implementation of their original codes by eliminating redundant calculations of the sampled nodes. Now the per-batch training time of GraphSAGE compares more favorably on the smallest graph Cora; see <ref type="table" target="#tab_3">Table 3</ref>. Note that this implementation does not affect large graphs (e.g., Reddit) and our observation of orders of magnitude faster training remains valid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented FastGCN, a fast improvement of the GCN model recently proposed by Kipf &amp; Welling (2016a) for learning graph embeddings. It generalizes transductive training to an inductive manner and also addresses the memory bottleneck issue of GCN caused by recursive expansion of neighborhoods. The crucial ingredient is a sampling scheme in the reformulation of the loss and the gradient, well justified through an alternative view of graph convoluntions in the form of integral transforms of embedding functions. We have compared the proposed method with additionally GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, a newly published work that also proposes using sampling to restrict the neighborhood size, although the two sampling schemes substantially differ in both algorithm and cost. Experimental results indicate that our approach is orders of magnitude faster than GCN and GraphSAGE, while maintaining highly comparable prediction performance with the two.</p><p>The simplicity of the GCN architecture allows for a natural interpretation of graph convolutions in terms of integral transforms. Such a view, yet, generalizes to many graph models whose formulations are based on first-order neighborhoods, examples of which include MoNet that applies to (meshed) manifolds <ref type="bibr" target="#b16">(Monti et al., 2017)</ref>, as well as many message-passing neural networks (see e.g., <ref type="bibr" target="#b21">Scarselli et al. (2009);</ref><ref type="bibr" target="#b6">Gilmer et al. (2017)</ref>). The proposed work elucidates the basic Monte Carlo ingredients for consistently estimating the integrals. When generalizing to other networks aforementioned, an additional effort is to investigate whether and how variance reduction may improve the estimator, a possibly rewarding avenue of future research. </p><formula xml:id="formula_26">t1 (v) = 1 t 0 t0 j=1Â (v, u (0) j )h (0) (u (0) j )W (0)</formula><p>converges almost surely toh (1) (v). Then, because the activation function σ is continuous, the continuous mapping theorem implies that h</p><p>(1)</p><formula xml:id="formula_27">t1 (v) = σ(h (1) t1 (v)) converges almost surely to h (1) (v) = σ(h (1) (v)). Thus, Â (v, u)h (1) t1 (u)W (1) dP (u) converges almost surely toh (2) (v) = Â (v, u)h (1) (u)W (1) dP (u),</formula><p>where note that the probability space is with respect to the 0th layer and hence has nothing to do with that of the variable u or v in this statement. Similarly,</p><formula xml:id="formula_28">h (2) t2 (v) = 1 t 1 t1 j=1Â (v, u (1) j )h (1) t1 (u (1) j )W (1)</formula><p>converges almost surely to Â (v, u)h </p><p>and the variance is 1/t times that ofÂ(v, u)x(u), i.e.,</p><formula xml:id="formula_30">Var{y(v)|v} = 1 t Â (v, u) 2 x(u) 2 dP (u) − e(v) 2 .<label>(12)</label></formula><p>Instantiating <ref type="formula" target="#formula_29">(11)</ref> and <ref type="formula" target="#formula_4">(12)</ref> with iid samples v 1 , . . . , v s ∼ P and taking variance and expectation in the front, respectively, we obtain</p><formula xml:id="formula_31">Var E 1 s s i=1 y(v i ) v 1 , . . . , v s = Var 1 s s i=1 e(v i ) = 1 s e(v) 2 dP (v)− 1 s e(v) dP (v) 2 , and E Var 1 s s i=1 y(v i ) v 1 , . . . , v s = 1 st Â (v, u) 2 x(u) 2 dP (u) dP (v) − 1 st e(v) 2 dP (v).</formula><p>Then, applying the law of total variance</p><formula xml:id="formula_32">Var 1 s s i=1 y(v i ) = Var E 1 s s i=1 y(v i ) v 1 , . . . , v s +E Var 1 s s i=1 y(v i ) v 1 , . . . , v s ,</formula><p>we conclude the proof.</p><p>Proof of Theorem 3. Conditioned on v, the variance of y Q (v) is 1/t times that of</p><formula xml:id="formula_33">A(v, u)x(u) dP (u) dQ(u) (where u ∼ Q),</formula><p>i.e.,</p><formula xml:id="formula_34">Var{y Q (v)|v} = 1 t Â (v, u) 2 x(u) 2 dP (u) 2 dQ(u) − e(v) 2 .</formula><p>Then, following the proof of Proposition 2, the overall variance is</p><formula xml:id="formula_35">Var{G Q } = R + 1 st Â (v, u) 2 x(u) 2 dP (u) 2 dP (v) dQ(u) = R + 1 st b(u) 2 x(u) 2 dP (u) 2 dQ(u) .</formula><p>Hence, the optimal dQ(u) must be proportional to b(u)|x(u)| dP (u). Because it also must integrate to unity, we have</p><formula xml:id="formula_36">dQ(u) = b(u)|x(u)| dP (u) b(u)|x(u)| dP (u) ,</formula><p>in which case</p><formula xml:id="formula_37">Var{G Q } = R + 1 st b(u)|x(u)| dP (u) 2 . Proof of Proposition 4. Conditioned on v, the variance of y Q (v) is 1/t times that of A(v, u)x(u) dP (u) dQ(u) =Â (v, u) sgn(x(u)) b(u) b(u)|x(u)| dP (u),</formula><p>i.e.,</p><formula xml:id="formula_38">Var{y Q (v)|v} = 1 t b(u)|x(u)| dP (u) 2 Â (v, u) 2 b(u) 2 dQ(u) − e(v) 2 .</formula><p>The rest of the proof follows that of Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENT DETAILS B.1 BASELINES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN:</head><p>The original GCN cannot work on very large graphs (e.g., Reddit). So we modified it into a batched version by simply removing the sampling in our FastGCN (i.e., using all the nodes instead of sampling a few in each batch). For relatively small graphs (Cora and Pubmed), we also compared the results with the original GCN.</p><p>GraphSAGE: For training time comparison, we use GraphSAGE-GCN that employs GCN as the aggregator. It is also the fastest version among all choices of the aggregators. For accuracy comparison, we also compared with GraphSAGE-mean. We used the codes from https: //github.com/williamleif/GraphSAGE. Following the setting of <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref>, we use two layers with neighborhood sample sizes S 1 = 25 and S 2 = 10. For fair comparison with our method, the batch size is set to be the same as FastGCN, and the hidden dimension is 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 EXPERIMENT SETUP</head><p>Datasets: The Cora and Pubmed data sets are from https://github.com/tkipf/gcn. As we explained in the paper, we kept the validation index and test index unchanged but changed the training index to use all the remaining nodes in the graph. The Reddit data is from http: //snap.stanford.edu/graphsage/.</p><p>Experiment Setting: We preformed hyperparameter selection for the learning rate and model dimension. We swept learning rate in the set {0.01, 0.001, 0.0001}. The hidden dimension of Fast-GCN for Reddit is set as 128, and for the other two data sets, it is 16. The batch size is 256</p><p>for Cora and Reddit, and 1024 for Pubmed. Dropout rate is set as 0. We use Adam as the optimization method for training. In the test phase, we use the trained parameters and all the graph nodes instead of sampling. For more details please check our codes in a temporary git repository https://github.com/matenure/FastGCN.</p><p>Hardware: Running time is compared on a single machine with 4-core 2.5 GHz Intel Core i7, and 16G RAM.</p><p>C ADDITIONAL EXPERIMENTS C.1 TRAINING TIME COMPARISON <ref type="figure" target="#fig_3">Figure 3</ref> in the main text compares the per-batch training time for different methods. Here, we list the total training time for reference. It is impacted by the convergence of SGD, whose contributing factors include learning rate, batch size, and sample size. See <ref type="table" target="#tab_5">Table 4</ref>. Although the orders-ofmagnitude speedup of per-batch time is slightly weakened by the convergence speed, one still sees a substantial advantage of the proposed method in the overall training time. Note that even though the original GCN trains faster than the batched version, it does not scale because of memory limitation.</p><p>Hence, a fair comparison should be gauged with the batched version. We additionally show in <ref type="figure">Figure 4</ref> the evolution of prediction accuracy as training progresses. We suspect that the model significantly overfits the data, because perfect training accuracy (i.e., 1) is attained.</p><p>One may note a subtlety that the training of <ref type="bibr">GCN (original)</ref> is slower than what is reported in <ref type="table" target="#tab_5">Table 4</ref>, even though fewer labels are used here. The reason is that we adopt the same hyperparameters as in <ref type="bibr" target="#b12">Kipf &amp; Welling (2016a)</ref> to reproduce the F1 scores of their work, whereas for <ref type="table" target="#tab_5">Table 4</ref>, a better learning rate is found that boosts the performance on the new split of the data, in which case GCN (original) converges faster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CONVERGENCE</head><p>Strictly speaking, the training algorithms proposed in Section 3 do not precisely follow the existing theory of SGD, because the gradient estimator, though consistent, is biased. In this section, we fill the gap by deriving a convergence result. Similar to the case of standard SGD where the convergence rate depends on the properties of the objective function, here we analyze only a simple case; a comprehensive treatment is out of the scope of the present work. For convenience, we will need a separate system of notations and the same notations appearing in the main text may bear a different meaning here. We abbreviate "with probability one" to "w.p.1" for short.</p><p>We use f (x) to denote the objective function and assume that it is differentiable. Differentiability is not a restriction because for the nondifferentiable case, the analysis that follows needs simply change the gradient to the subgradient. The key assumption made on f is that it is l-strictly convex; that is, there exists a positive real number l such that</p><formula xml:id="formula_39">f (x) − f (y) ≥ ∇f (y), x − y + l 2 x − y 2 ,<label>(13)</label></formula><p>for all x and y. We use g to denote the gradient estimator. Specifically, denote by g(x; ξ N ), with ξ N being a random variable, a strongly consistent estimator of ∇f (x); that is, lim N →∞ g(x; ξ N ) = ∇f (x) w.p.1.</p><p>Moreover, we consider the SGD update rule</p><formula xml:id="formula_40">x k+1 = x k − γ k g(x k ; ξ (k) N ),<label>(14)</label></formula><p>where ξ (k)</p><p>N is an indepedent sample of ξ N for the kth update. The following result states that the update converges on the order of O(1/k).</p><p>Theorem 5. Let x * be the (global) minimum of f and assume that ∇f (x) is uniformly bounded by some constant G &gt; 0. If γ k = (lk) −1 , then there exists a sequence B k with B k ≤ max{ x 1 − x * 2 , G 2 /l 2 } k such that x k − x * 2 → B k w.p.1.</p><p>Proof. Expanding x k+1 − x * 2 by using the update rule <ref type="formula" target="#formula_9">(14)</ref>, we obtain</p><formula xml:id="formula_41">x k+1 − x * 2 = x k − x * 2 − 2γ k g k , x k − x * + γ 2 k g k 2 ,</formula><p>where g k ≡ g(x k ; ξ (k) N ). Because for a given x k , g k converges to ∇f (x k ) w.p.1, we have that conditioned on x k ,</p><formula xml:id="formula_42">x k+1 − x * 2 → x k − x * 2 − 2γ k ∇f (x k ), x k − x * + γ 2 k ∇f (x k ) 2 w.p.1.<label>(15)</label></formula><p>On the other hand, applying the strict convexity (13), by first taking x = x k , y = x * and then taking x = x * , y = x k , we obtain</p><formula xml:id="formula_43">∇f (x k ), x k − x * ≥ l x k − x * 2 .<label>(16)</label></formula><p>Substituting <ref type="formula" target="#formula_15">(16)</ref> to <ref type="formula" target="#formula_10">(15)</ref>, we have that conditioned on x k ,</p><p>x k+1 − x * 2 → C k w.p.1 for some</p><formula xml:id="formula_44">C k ≤ (1 − 2lγ k ) x k − x * 2 + γ 2 k G 2 = (1 − 2/k) x k − x * 2 + G 2 /(l 2 k 2 ).<label>(17)</label></formula><p>Now consider the randomness of x k and apply induction. For the base case k = 2, the theorem clearly holds with B 2 = C 1 . If the theorem holds for k = T , let L = max{ x 1 − x * 2 , G 2 /l 2 }. Then, taking the probabilistic limit of x T on both sides of (17), we have that C T converges w.p.1 to some limit that is less than or equal to (1 − 2/T )(L/T ) + G 2 /(l 2 T 2 ) ≤ L/(T + 1). Letting this limit be B T +1 , we complete the induction proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 2</head><label>2</label><figDesc>FastGCN batched training (one epoch), improved version 1: For each vertex u, compute sampling probability q(u) ∝ Â (:, u) 2 2: for each batch do 3:For each layer l, sample t l vertices u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Prediction accuracy: uniform versus importance sampling. The three data sets from top to bottom are ordered the same as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Per-batch training time in seconds (left) and prediction accuracy (right). For timing, GraphSAGE refers to GraphSAGE-GCN in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>)W (1) dP (u) and thus toh (2) (v). A simple induction completes the rest of the proof.Proof of Proposition 2. Conditioned on v, the expectation of y(v) is E[y(v)|v] = Â (v, u)x(u) dP (u) = e(v),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 FastGCN batched training (one epoch)</figDesc><table /><note>1: for each batch do2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset StatisticsImplementation details are as following. All networks (including those under comparison) contain two layers as usual. The codes of GraphSAGE and GCN are downloaded from the accompany websites and the latter is adapted for FastGCN. Inference with FastGCN is done with the full GCN network, as mentioned in Section 3.2. Further details are contained in the appendix.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Classes Features</cell><cell>Training/Validation/Test</cell></row><row><cell>Cora</cell><cell>2, 708</cell><cell>5, 429</cell><cell>7</cell><cell>1, 433</cell><cell>1, 208/500/1, 000</cell></row><row><cell cols="2">Pubmed 19, 717</cell><cell>44, 338</cell><cell>3</cell><cell>500</cell><cell>18, 217/500/1, 000</cell></row><row><cell>Reddit</cell><cell cols="2">232, 965 11, 606, 919</cell><cell>41</cell><cell>602</cell><cell>152, 410/23, 699/55, 334</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Benefit of precomputingÂH (0) for the input layer. Data set: Pubmed. Training time is in seconds, per-epoch (batch size 1024). Accuracy is measured by using micro F1 score.</figDesc><table><row><cell></cell><cell>F1 Score</cell><cell>0.75 0.8 0.85</cell><cell></cell><cell></cell><cell>Uniform Importance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell></cell><cell>F1 Score</cell><cell>0.8 0.85 0.9</cell><cell></cell><cell></cell><cell>Uniform Importance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Sampling Time F1 25 0.760 0.873 0.144 0.879 Precompute t 1 Time F1 5 10 0.755 0.863 0.141 0.870 0.737 0.859 0.139 0.849</cell><cell>F1 Score</cell><cell>0.9 0.92 0.94</cell><cell cols="3">Sample size 25 50 100</cell><cell>Uniform Importance</cell></row><row><cell>50 0.774 0.864 0.142 0.880</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Further comparison of per-batch training time (in seconds) with new implementation of GraphSAGE for small graphs. The new implementation is in PyTorch whereas the rest are in Ten-sorFlow.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Pubmed Reddit</cell></row><row><cell>FastGCN</cell><cell cols="2">0.0084 0.0047 0.0129</cell></row><row><cell cols="3">GraphSAGE-GCN (old impl) 1.1630 0.3579 0.4260</cell></row><row><cell cols="3">GraphSAGE-GCN (new impl) 0.0380 0.3989</cell><cell>NA</cell></row><row><cell>GCN (batched)</cell><cell cols="2">0.0166 0.0815 2.1731</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, WWW '15, pp. 1067-1077, 2015. ISBN 978-1-4503-3469-3. Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</figDesc><table><row><cell>KDD '16, pp. 1225-1234, 2016. ISBN 978-1-4503-4232-2.</cell></row><row><cell>A PROOFS</cell></row><row><cell>Proof of Theorem 1. Because the samples u (0) j are iid, by the strong law of large numbers,</cell></row><row><cell>h (1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Total training time (in seconds). Training/test accuracy versus training time. From left to right, the data sets are Cora, Pubmed, and Reddit, respectively.C.2 ORIGINAL DATA SPLIT FOR CORA AND PUBMEDAs explained in Section 4, we increased the number of labels used for training in Cora and Pubmed, to align with the supervised learning setting of Reddit. For reference, here we present results by using the original data split with substantially fewer training labels. We also fork a separate version of FastGCN, called FastGCN-transductive, that uses both training and test data for learning. SeeTable 5.The results for GCN are consistent with those reported by<ref type="bibr" target="#b12">Kipf &amp; Welling (2016a)</ref>. Because labeled data are scarce, the training of GCN is quite fast. FastGCN beats it only on Pubmed. The accuracy results of FastGCN are inferior to GCN, also because of the limited number of training labels. The transductive version FastGCN-transductive matches the accuracy of that of GCN. The results for GraphSAGE are curious.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Cora Pubmed Reddit</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">FastGCN</cell><cell>2.7</cell><cell>15.5</cell><cell></cell><cell>638.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">GraphSAGE-GCN 72.4</cell><cell>259.6</cell><cell cols="2">3318.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GCN (batched)</cell><cell>6.9</cell><cell cols="3">210.8 58346.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GCN (original)</cell><cell>1.7</cell><cell>21.4</cell><cell></cell><cell>NA</cell></row><row><cell></cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell>GCN (batched)</cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell cols="2">GCN (batched)</cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell>GCN (batched)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell></row><row><cell>Training accuracy</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell>Training accuracy</cell><cell>0.82 0.84 0.86 0.88</cell><cell></cell><cell></cell><cell>Training accuracy</cell><cell>0.75 0.8 0.85 0.9</cell></row><row><cell></cell><cell>10 -2 0</cell><cell>10 0</cell><cell>10 2</cell><cell>10 -2 0.8</cell><cell>10 0</cell><cell>10 2</cell><cell>10 4</cell><cell>10 0 0.7</cell><cell>10 5</cell></row><row><cell></cell><cell cols="3">Training time (seconds)</cell><cell cols="3">Training time (seconds)</cell><cell></cell><cell cols="2">Training time (seconds)</cell></row><row><cell></cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell>GCN (batched)</cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell cols="2">GCN (batched)</cell><cell>FastGCN</cell><cell>GraphSAGE</cell><cell>GCN (batched)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell></row><row><cell>Test accuracy</cell><cell>0.4 0.6 0.8</cell><cell></cell><cell>Test accuracy</cell><cell>0.84 0.86 0.88</cell><cell></cell><cell></cell><cell>Test accuracy</cell><cell>0.8 0.85 0.9</cell></row><row><cell></cell><cell>10 -2 0.2</cell><cell>10 0</cell><cell>10 2</cell><cell>10 -2 0.82</cell><cell>10 0</cell><cell>10 2</cell><cell>10 4</cell><cell>10 0 0.75</cell><cell>10 5</cell></row><row><cell></cell><cell cols="3">Training time (seconds)</cell><cell cols="3">Training time (seconds)</cell><cell></cell><cell cols="2">Training time (seconds)</cell></row><row><cell cols="2">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Total training time and test accuracy for Cora and Pubmed, original data split. Time is in seconds.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell cols="2">Pubmed</cell></row><row><cell></cell><cell>Time</cell><cell>F1</cell><cell>Time</cell><cell>F1</cell></row><row><cell>FastGCN</cell><cell cols="2">2.52 0.723</cell><cell cols="2">0.97 0.721</cell></row><row><cell>FastGCN-transductive</cell><cell cols="2">5.88 0.818</cell><cell cols="2">8.97 0.776</cell></row><row><cell>GraphSAGE-GCN</cell><cell cols="4">107.95 0.334 39.34 0.386</cell></row><row><cell>GCN (original)</cell><cell cols="4">2.18 0.814 32.65 0.795</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Writing GCN in the functional form allows for evaluating the integrals in the Monte Carlo manner, which leads to a batched training algorithm and also to a natural separation of training and test data, as in inductive learning. For each layer l, we use t l iid samples u(l) 1 , . . . , u (l) t l ∼ P to approximately evaluate the integral transform (2); that is, h (l+1) t l+1 (v) := 1 t l t l j=1Â (v, u (l) j )h (l) t l (u (l) j )W (l) , h (l+1) t l+1 (v) := σ(h (l+1) t l+1 (v)), l = 0, . . . , M − 1,with the convention h (0) t0 ≡ h (0) . Then, the loss L in (3) admits an estimator L t0,t1,...,t M := 1 t M t M i=1 g(h (M ) t M (u (M ) i )).The follow result establishes that the estimator is consistent. The proof is a recursive application of the law of large numbers and the continuous mapping theorem; it is given in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(l+1) t l+1 (v) as an approximation to the convolu-tion Â (v, u)h (l) t l (u)W (l) dP (u). When taking t l+1 samples v = u (l+1) 1 , . . . , u (l+1) t l+1 , the sample average ofh (l+1)t l+1 (v) admits a variance that captures the deviation from the eventual loss contributed by this layer. Hence, we seek an improvement of this variance. Now that we consider each layer separately, we will do the following change of notation to keep the expressions less cumbersome:</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno>978-1-4503-2035-1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web, WWW &apos;13</title>
		<meeting>the 22Nd International Conference on World Wide Web, WWW &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS&apos;01</title>
		<meeting>the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS&apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<idno>978-1-4503-3794-6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1606.09375</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<idno>abs/1705.02801</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>978-1-4503-4232-2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
		<idno>abs/1511.05298</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
		<idno>1386-4564</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
		<idno>abs/1605.05273</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno>978-1-4503- 4232-2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno>978-1-4503-2956-9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.290.5500.2323</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
