<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HR-Depth : High Resolution Self-Supervised Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Liu</surname></persName>
							<email>linaliu@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>yongliu@iipc.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
							<email>yuanyi@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Fuxi AI Lab</orgName>
								<address>
									<country>NetEase</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HR-Depth : High Resolution Self-Supervised Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning shows great potential in monocular depth estimation, using image sequences as the only source of supervision. Although people try to use the high-resolution image for depth estimation, the accuracy of prediction has not been significantly improved. In this work, we find the core reason comes from the inaccurate depth estimation in large gradient regions, making the bilinear interpolation error gradually disappear as the resolution increases. To obtain more accurate depth estimation in large gradient regions, it is necessary to obtain high-resolution features with spatial and semantic information. Therefore, we present an improved DepthNet, HR-Depth, with two effective strategies: (1) redesign the skip-connection in DepthNet to get better highresolution features and (2) propose feature fusion Squeezeand-Excitation(fSE) module to fuse feature more efficiently. Using Resnet-18 as the encoder, HR-Depth surpasses all previous state-of-the-art(SoTA) methods with the least parameters at both high and low resolution. Moreover, previous state-of-the-art methods are based on fairly complex and deep networks with a mass of parameters which limits their real applications. Thus we also construct a lightweight network which uses MobileNetV3 as encoder. Experiments show that the lightweight network can perform on par with many large models like Monodepth2 at high-resolution with only 20% parameters. All codes and models will be available at https: //github.com/shawLyu/HR-Depth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Accurate depth estimation from single image is an active research filed to help computer reconstruct and understand the real scenes. It also has a large range of applications in diverse fields such as autonomous vehicles, robotics, augmented reality, etc. While supervised monocular depth estimation has been successful, it is suffering from the expensive access to ground truth. Self-supervised methods use geometrical constraints on image sequences or stereo images as the sole source of supervision.</p><p>Recent works <ref type="bibr" target="#b26">(Zhou et al. 2017;</ref><ref type="bibr" target="#b5">Godard et al. 2017</ref>) in self-supervised depth estimation are limited to training in low-resolution inputs due to the large memory requirements We compared our method with Monodepth2 <ref type="bibr" target="#b6">(Godard et al. 2019)</ref>, our method can predict higher quality and sharper depth map with fewer parameters. The input resolution for all three models is 1024 × 320.</p><p>of the model. However, with the improvement of computing and storage capacity, high-resolution images are used by more and more computer vision tasks. In depth estimation, <ref type="bibr" target="#b17">(Pillai et al. 2019</ref>) introduce sub-pixel-convolutional <ref type="bibr" target="#b21">(Shi et al. 2016</ref>) layers replacing the deconvolution and resizeconvolution layers to improve the effect of up-sampling. And <ref type="bibr" target="#b6">(Godard et al. 2019</ref>) directly leverage high-resolution images for depth estimation. Although the above works explore high resolution depth estimation, but the performance has not improved significantly on KITTI. Inspired by this observation, we perform an analysis of existing method and find the core reason comes from the inaccurate depth estimation at object boundaries. In depth estimation, object boundaries are mainly determined by two parts: semantic and spatial information. Semantic information obtains clear boundaries by constraining the categories of pixels, while spatial information uses geometric constraints to describe the outline of objects. In the previous work, the depth estimation network is based on the U-Net <ref type="bibr" target="#b20">(Ronneberger et al. 2015)</ref> architecture, which mainly uses skip-connection to fuse semantic information and spatial information. However, the semantic gap between the encoder and decoder feature maps is too large, which leads to a poor integration of spatial and semantic information. So the previous work is difficult to get an accurate depth estimation at object boundaries. In order to reduce the semantic gap, we redesigned the skip connection to better fuse feature maps. Besides, we also found that the basic convolution can not integrate spatial information and semantic information well, so we propose to replace it by a feature fusion squeeze and excitation (fSE) block. This block not only improves the feature fusion effect but also reduces the number of parameters. We evaluate our results on KITTI benchmark, and the experiments demonstrate that the our well-designed network can predict sharper edges and achieve (SoTA).</p><p>As a side effect, higher resolution input brings extra computational costs. As a consequence, lightweight is one of the key principals for high-resolution model design. However, the previous SoTA has a huge amount of parameters like Packnet-SfM with 127M parameters, and the performance of lightweight networks is not appropriate for actual application like <ref type="bibr" target="#b18">(Poggi et al. 2018)</ref>. Therefore, in order to maintain high performance of lightweight network, we introduce a simple yet effective designing strategy in this paper. We successfully train a lightweight network based on this strategy, which can achieve the accuracy of Monodepth2 with only 3.1M parameters.</p><p>To summarize, the main contributions of this work are listed below in fourfold:</p><p>• We provide a deep analysis of high-resolution monocular depth estimation and prove that predicting more accurate boundaries can improve performance.</p><p>• We redesign the skip connection to get high-resolution semantic feature maps, which can help network predict sharper edges.</p><p>• We propose feature fusion squeeze and excitation block to improve the efficiency and effect of feature fusion.</p><p>• We present a simple yet effective lightweight design strategy to train a lightweight depth estimation network that can achieve the performance of complex network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Monocular Depth Estimation</head><p>Depth estimation from a single image is an ill-posed problem as the same input image can be projected to multiple plausible depths. Therefore, many works begin with supervised learning. <ref type="bibr" target="#b2">(Eigen et al. 2015)</ref> is the first to propose learning-based depth estimation structure trained on RGB-D dataset. Their work considers depth estimation as a regression problem, using a coarse-to-fine network to derive pixel-by-pixel depth value. With the rise of fully convolution neural network, <ref type="bibr" target="#b15">(Laina et al. 2016)</ref> uses convolution layer to replace fully connected layer, and uses pre-trained encoder for feature extraction. This work enables the depth estimation task to be trained with a deeper network and has the accuracy comparable to the depth sensor. In order to predict sharp and accurate occlusion boundaries, (Ramamonjisoa et al. 2020) introduces refine net predicting an additive residual depth map to refine the first estimation results. However, supervised methods fall into bottleneck on account of the poor generalization performance and the difficulty of obtaining ground truth depth value. So people began to explore self-supervised monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Monocular Depth Estimation</head><p>Using stereo images to train depth network is one intuitive of self-supervision. <ref type="bibr" target="#b3">(Garg et al. 2016)</ref> proposes one of the earliest work in self-supervised depth estimation using stereo pairs and <ref type="bibr" target="#b5">(Godard et al. 2017)</ref> produces results superior to contemporary supervised method by introducing a left-right depth consistency loss. In order to reduce the limitation of stereo camera, <ref type="bibr" target="#b26">(Zhou et al. 2017</ref>) firstly proposes to use PoseCNN to estimate relative pose between adjacent frames. This work enables the network to be trained completely depending on monocular image sequences. <ref type="bibr" target="#b6">(Godard et al. 2019</ref>) introduces auto-masking and min re-projection loss to solve the problems of moving objects and occlusion and makes Monodepth2 become the most widely used baseline. For the sake of further improving the network performance, <ref type="bibr" target="#b8">(Guizilini et al. 2020b</ref>) introduces the pre-trained semantic segmentation network and pixel-adaptive convolution to guide depth network to further utilize semantic information. But there are two disadvantages to <ref type="bibr" target="#b8">(Guizilini et al. 2020b</ref>). First of all, we hope to relieve the pressure of pixel level annotation by self-supervising, but we need expensive semantic label in this work. Secondly, semantic segmentation and depth estimation network should run simultaneously. It will increase the cost of depth estimation. In addition, <ref type="bibr" target="#b7">(Guizilini et al. 2020a</ref>) utilizes packing and unpacking block to preserve the spatial information in image and low level feature. They thought standard convolutional and pooling operation can not preserve sufficient details, so they proposed 3D packing and unpacking blocks to replace standard down-sample and up-sample operation. Packing and unpacking block are invertible, so they can recover important spatial information for depth estimation. But these two blocks depend on 3D convolution, so the number of network parameters is greatly increased and it is difficult to deploy to mobile devices. But these two works show that abundant semantic and spatial information can get sharper edges, so as to improve the accuracy of depth estimation. In our work, we show that, by simply fusing the information extracted by the encoder, we can obtain the ideal features with spatial and semantic information. Through our insightful design, without introducing much more parameters, these features significantly improve the overall performance, obtaining sharper edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lightweight Network for Depth Estimation</head><p>Depth estimation from a single image is also a very attractive technique with several implications in robotic, autonomous navigation. Therefore, it is necessary to leverage depth prediction network to quickly infer an accurate depth map on a CPU. <ref type="bibr" target="#b23">(Wofk et al. 2019</ref>) proposed a lightweight supervised depth estimation network, who use encoder-decoder architecture and include 1.34M parameters after pruning. In unsupervised filed, <ref type="bibr" target="#b18">(Poggi et al. 2018;</ref><ref type="bibr" target="#b0">Aleotti et al. 2020</ref>) proposed PyDNet with 1.9M parameters. Although the above two works have less parameters, their performance also decreases a lot. In this paper, we propose a novel yet simple strategy for network designing, which can make the performance of lightweight network comparable to or even surpass the complex network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR-Depth Network Problem Formulation</head><p>In self-supervised monocular depth estimation task, the goal is to use depth network f D to learn depth information D from RGB image I. Due to the lack of ground truth depth value, we need an additional network f P to predict relative pose p = [R|t] between source image I s and target image I t . As a common setting, the target image is I t , and the source images set are consist of adjacent images I t−1 , I t+1 . The depth network is optimized by minimizing the photometric re-projection error:</p><formula xml:id="formula_0">r(I t , I t ) = α 2 (1−SSIM (I t , I t ))+(1−α)||I t −I t || 1 (1)</formula><p>where I t is the warped result from I s to I t , SSIM is the operator of structural similarity to measure the patch similarity. We follow the form of per pixel minimum loss in <ref type="bibr" target="#b6">(Godard et al. 2019</ref>) to handle occlusion. The photometric loss is denoted as L re = min t r(I t , I t ).</p><p>(2) Furthermore, in order to regularize the disparities in textureless low-image gradient regions, edges aware smooth regularization term is used:</p><formula xml:id="formula_1">L smooth = |δ x D t |e −|δxIt| + |δ y D t |e −|δyIt| .<label>(3)</label></formula><p>where δ x and δ y symbols for partial differentiation of depth and RGB images, and the exponential operation of a matrix is an element-wise operation. So the final loss function is the summation of the re-projected losses and the smooth loss on multi scale images:</p><formula xml:id="formula_2">L f inal = 1 s s i (L i re + λL i smooth )<label>(4)</label></formula><p>where s is the number of scales, and λ is the weight for smooth term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on High Resolution Performance</head><p>As a consensus of dense prediction tasks, higher resolution brings more accurate results instinctively <ref type="bibr" target="#b25">Zhou et al. 2019)</ref>. Especially in the depth estimation task, pixel level disparity is more important since it is inversely proportional to the square of depth error <ref type="bibr">(You et al. 2019</ref>). However, we noticed that most of the previous work use low-resolution inputs and interpolate low resolution prediction to high-resolution one. The low-resolution experimental setup essentially makes these works cannot benefit from high-resolution images. Some methods also conduct highresolution experiments where they train their models with larger images <ref type="bibr" target="#b6">(Godard et al. 2019;</ref><ref type="bibr" target="#b17">Pillai et al. 2019</ref>). However, the performance improvements upon the small inputs are very limited, i.e. <ref type="bibr" target="#b17">(Pillai et al. 2019)</ref>, their models cannot take advantage of high resolution. For instance, we evaluate a recent well-known work named Monodepth2 <ref type="bibr" target="#b6">(Godard et al. 2019</ref>) with higher resolution setting. As shown in <ref type="table">Table  1</ref>, the depth errors are almost the same with high and lowresolution settings. Therefore, we argue that their method can not make full use of the information from higher input resolution. Furthermore, we make a deep analysis on the actual reason that existing methods cannot improve the depth estimation by high-resolution inputs. We find out that the most vital point is un-negligible error from the bilinear interpolation process for up-sampling the low resolution prediction to high ones. In detail, taking Monodepth2 <ref type="bibr" target="#b6">(Godard et al. 2019</ref>) as an example, when feeding a model with small inputs, we need to up-sample the outputs to obtain a required <ref type="figure">Figure 3</ref>: Illustration of our proposed framework. The network is mainly composed of three different types of nodes. X e i denotes a feature extraction node, which is mainly composed of residual blocks. X i,j denotes a feature fusion node which only has 3 × 3 convolution operation. X d i is feature fusion node which is mainly composed of our proposed fSE module. Disparity is decoded by DispConv block, which contains 3 × 3 convolution and sigmoid activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolution</head><p>high-resolution one with interpolation. As shown in <ref type="figure" target="#fig_1">Figure  2</ref>, it can be seen that local prediction error will severely harm global accuracy in regions with large depth gradient such as instance edges, while for the region with small depth gradient. Interestingly, when the prediction of low-resolution outputs is poor like the upper right part in <ref type="figure" target="#fig_1">Figure 2</ref>, it will unexpectedly compensate the local prediction error of bilinear interpolation. The results show that the performance of lowresolution can be on par of the high one. In other words, the actual reason that most methods cannot benefit from larger inputs is that the gap between these two settings is remedied by the above interesting phenomenon. Therefore, only more precise prediction of regions with large depth gradient can make high-resolution prediction more accurate. We can also summarize that the performance of high-resolution can be improved by predicting more accurate depth in large gradient regions and sharper edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Redesign Skip Connection</head><p>Based on the above analyses, in order to predict more accurate boundaries, we try to enhance it from both the spatial and semantic feature, since we believe (1) semantic information can produce boundaries between different categories, which can reduce depth estimation error caused by misclassification, (2) spatial information can help the network to know the location of the boundaries, so as to estimate them better. Here, we will first discuss DepthNet with U-Net architecture.</p><p>Skip-connection is one of core components of U-Net, whose purpose is to recover information lost in downsampling. However, we argue that it could be less effective to directly combine features from different layers since there are gaps among them in semantic levels and spatial resolution. As previous research <ref type="bibr" target="#b24">(Zhang et al. 2018)</ref> indicates, deep neural networks represent more semantic features as the layers going deeper. Thus, if low-level features could include more semantic information, like relatively clearer semantic boundaries, then the fusion becomes easier and we can obtain sharper depth estimation by decoding these features.</p><p>In order to decrease semantic and resolution gap, we propose dense skip connection inspired by <ref type="bibr" target="#b27">(Zhou et al. 2018)</ref>. As shown in <ref type="figure">Figure 3</ref>, in addition to the nodes in the original encoder and decoder, we also add a lot of intermediate nodes to aggregate features. Let x e i denotes the output of encoder node X e i , x d i denotes the output of node X d i , x i,j denotes the output of node X i,j where i indexes the downsampling layer along the encoder and j indexes the aggregation layer along the skip connection. Consider a single image I is passed through DepthNet. The stack of feature maps is computed as</p><formula xml:id="formula_3">x e i = E(I), i = 1 E(x e i−1 ), i &gt; 1 (5) x i,j =    F( x e i , [x i,k ] j−1 0 , U(x i+1,j−1 ) ), j = 1 F( x e i , [x i,k ] j−1 0 , U(x e i+1 ) ), j &gt; 1 (6) x d i = D(U(x d i+1 )]), j = 0 D( x e i , [x i,k ] j−1 0 , U(x d i+1 ) ), j &gt; 0<label>(7)</label></formula><p>where E(·) is a feature extraction block like residual block, F(·) is a feature fusion block consist of convolution operation followed by an activation function, D(·) is a feature fusion operation composed by feature fusion block, U(·) is an upsampling block with convolution and bilinear interpolation operation, and [·] denotes the concatenation layer. The details of dense skip-connection and each node are shown in <ref type="figure">Figure 3</ref>.</p><p>With dense skip connection, each node in the decoder is presented with the final aggregated feature maps, the intermediate aggregated feature maps and the original feature maps from the encoder. Then, the decoder can use high-resolution features with richer semantic information to predict more sharper depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Fusion SE Block</head><p>The U-Net based DepthNet uses a 3 × 3 convolution to fuse upsampling feature maps and original feature maps from the encoder, and the parameters of convolution is calculated as</p><formula xml:id="formula_4">C in × C out × k 2 + C out ,<label>(8)</label></formula><p>where C in is the number of input channels, C out is the number of output channels and k denotes as convolution kernel size. Just like <ref type="bibr" target="#b13">(Huang et al. 2017)</ref>, the dense skipconnections make the decoder nodes have dramatically increased input feature maps, so this operation will inevitably decrease the efficiency of the network. Inspired by <ref type="bibr" target="#b12">(Hu et al. 2019)</ref>, we propose a lightweight module, feature fusion Squeeze-Excitation(fSE), to improve feature fusion accuracy and efficiency. The fSE module squeezes the feature maps by global average pooling to represent channel information and uses two fully-connected(FC) layers followed by a sigmoid function to measure the importance of each feature and re-weight them in the meantime. Then a 1×1 convolution is used to fuse channels to obtain high-quality feature maps. The parameter amount of this module is</p><formula xml:id="formula_5">2 r × C 2 in + (C in + 1) × C out ,<label>(9)</label></formula><p>where r denotes as reduction ratio and is always set to 4 in all experiments of this paper. When using Resnet-18 as encoder, the fSE module can reduce the parameters of HR-Depth from 16.06M to 14.62M, even less than Monodepth2 with 14.84M parameters. Meanwhile, since the fSE module will focus more on feature fusion, the network performance will also be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lite-HR-Depth</head><p>Most previous SoTA self-supervised monocular depth estimation algorithms are based on fairly complex deep neural networks with a mass of parameters which limit their applications on practical platforms like embedded devices. However, for existing lightweight networks, such as PyD-Net <ref type="bibr" target="#b18">(Poggi et al. 2018)</ref>, the accuracy is greatly decreased when they reduce the amount of parameters. In order to make the depth estimation network get rid of the limitation of GPU but keep its great performance, we propose a simple yet effective lightweight network, named Lite-HR-Depth. Our Lite-HR-Depth employs MobileNetV3 as an encoder and downsizes the feature fusion and decoder nodes with only 3.1M parameters, of which the 2.82M parameters comes from the encoder. In addition, we further improve the accuracy of our Lite-HR-Depth with knowledge distillation <ref type="bibr" target="#b10">(Hinton et al. 2015)</ref>. For self-supervised learning, due to lack of ground truth, we have to use view syntheses as the supervisory signal, which increases the difficultly of training small network. Therefore, we propose to source a direct form of supervision from the learned large model. By training a large network in a self-supervised manner, we obtain a high-performance network instance T . Then we train a second instance of lightweight model, namely S, to minimize</p><formula xml:id="formula_6">L sup = ||d T − d S ||,<label>(10)</label></formula><p>where d T means disparity from network T and d S means disparity from network S. With this strategy, we obtain a network S which even more accurate than T . It is worth noting that the parameters of Lite-HR-Depth is only 20% of Monodepth2, but it can even perform better than Mon-odepth2 at high resolution. More analyses of this lite network will be shown in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we validate that (1) our redesigned skipconnection can improve the results, especially predicting sharper edges, (2) the fSE module can significantly reduce parameters and improve accuracy, and (3) the design method we propose can easily obtain high precision lightweight network. We evaluate our models on the KITTI dataset <ref type="bibr" target="#b4">(Geiger et al. 2012)</ref>, to allow comparison with previous published monocular methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>KITTI. The KITTI benchmark <ref type="bibr" target="#b4">(Geiger et al. 2012</ref>) is most widely used for depth evaluation. We adopt the data split of <ref type="bibr" target="#b2">(Eigen et al. 2015)</ref>, and removed the static frames followed by <ref type="bibr" target="#b26">(Zhou et al. 2017)</ref>. Ultimately, we used 39810 images for training, 4424 for validation and 697 for evaluation. Furthermore, we use the same intrinsic for all images, setting the principal point of the camera to the image center and the focal length to the average of all the focal lengths in KITTI. For stereo training, we set the transformation between the two stereo frames to be a pure horizontal translation of fixed length.</p><p>CityScapes. CityScape <ref type="bibr" target="#b1">(Cordts et al. 2016</ref>) is the other large automatic driving dataset. So we also experiment with pretraining our structure on CityScape and then we finetuned and evaluated it on KITTI dataset. The leftImg8bit Sequence were considered as training split for the CityScapes dataset, using the same training parameters as KITTI for 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our models on PyTorch <ref type="bibr" target="#b16">(Paszke et al. 2017)</ref> and train them on one Telsa V100 GPU. We use the Adam Optimizer <ref type="bibr" target="#b14">(Kingma et al. 2014)</ref> with β 1 = 0.9, β 2 = 0.999. The DepthNet and PoseNet are trained for 20 epochs, with a batch size of 12. The initial learning rates for both network are 1 × 10 −3 and decayed after 15 epochs by factor of 10. The training sequences are consist of three consecutive images. We set the SSIM weight to α = 0.85 and smooth loss weight to λ = 1 × 10 −3 . DepthNet. We implement our HR-Depth with ResNet-18 <ref type="bibr" target="#b9">(He et al. 2016</ref>) as encoder, and use Mo-bileNetV3 <ref type="bibr" target="#b11">(Howard et al. 2019)</ref> as encoder for Lite-HR-Depth. The details of our proposed architecture will be described at supplemental material. All four disparity maps   <ref type="figure">Figure 4</ref>: Qualitative monocular depth estimation performance comparing HR-Depth and Lite-HR-Depth with previous SOTA. on frames from the KITTI dataset. Our network is able to predict more sharper edges than Monodepth2 <ref type="bibr" target="#b6">(Godard et al. 2019)</ref>, and its performance is comparable to PackNet-SfM while with much fewer parameters.</p><p>are used in loss calculation during training. For evaluation, we only use the maximum output scale, after being resized to the ground-truth depth resolution using bilinear interpolation.</p><p>PoseNet. The architecture of PoseNet is proposed by <ref type="bibr" target="#b6">(Godard et al. 2019)</ref>. The PoseNet is built on Resnet-18 and the first-level convolution channel is changed from 3 to 6, which allows the adjacent frames to feed into the network. And the outputs of PoseNet is the relative pose which is parameterized with 6-DOF vector. The first three dimensions represent translation vectors and the last three represent Euler angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Estimation Performance</head><p>We evaluate depth prediction on KITTI using the metrics described in <ref type="bibr" target="#b2">(Eigen et al. 2015)</ref>. The evaluation results are summarized in <ref type="table" target="#tab_2">Table 2</ref> and then illustrate their performance qualitatively in <ref type="figure">Figure 4</ref>. We show that our proposed archi-tecture outperforms all existing SoTA self-supervised approaches. We also outperform recent model <ref type="bibr" target="#b7">(Guizilini et al. 2020a</ref>) with 120M parameters. Furthermore, we also introduce an additional source of unlabeled videos, CityScapes dataset(CS+K), and we can further improve the DepthNet performance. We also show that at higher resolution out model's performance significantly increases. Our best results are achieved SoTA when processing higher resolution input images with minimum parameters.  <ref type="table" target="#tab_3">Table 3</ref>: Quantitative performance of Lite-Network on KITTI dataset for distance up to 80m. Evaluation matrices and methods are the same as <ref type="table" target="#tab_2">Table 2</ref>. In supervision column, T refers to using teacher network to guide lite network to train. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>To further explore the performance improvements that our network provides, we perform an ablative analysis on the different architectural components introduced. We choose Monodepth2 as our baseline model, and we can see all our contributions can lead a significant improvement. Benefits of dense skip-connection. As shown in the <ref type="figure" target="#fig_2">Figure  5</ref>, the semantic gap in skip connection is too large to be fused well, but the dense skip-connection can leverage intermediate features to effectively reduces the semantic gap between encoder and decoder. Therefore, we can get high-resolution feature maps with more semantic information which can significantly improve the performance. fSE Block. The fSE block is designed to improve feature fusion and computation efficiency. To validate fSE, we also apply the SE <ref type="bibr" target="#b12">(Hu et al. 2019)</ref> to dense skip connection and compare it with the proposed fSE block. As shown in <ref type="table" target="#tab_5">Table  4</ref>, the SE block will introduce additional parameters but our fSE block can greatly reduce the parameters introduced by dense skip-connection and can further improve the performance of the network, even better than SE block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature map visualization</head><p>As explained before, the purpose of redesigning skipconnection is to decrease semantic and spatial gap, so as to  obtain high-resolution feature maps with rich semantic information. Therefore, in order to illustrate the effects of feature fusion, we visualize the intermediate feature maps and we also plot the outputs of fSE to explore the influence of intermediate feature maps. <ref type="figure">Figure 4</ref> shows that the low-level node like X e 1 undergoes slight transformation and obtains simple spatial information whereas the output of decoder node like X d 2 gets the rich semantic information. Hence, there is large gap between the representation capability of X e 1 and X d 2 . The dense skip connection can gradually add semantic information to the intermediate feature, thus reducing the gap between node X e 1 and X d 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we show theoretical and empirical evidence that how to improve high-resolution estimation performance. And based on analysis, we present a new convolutional network architecture, referred as HR-Depth, for high-resolution self-supervised monocular depth estimation. It leverages novel dense skip-connection and fSE block to reduce the gap between resolution and semantic. Although purely trained on image sequences, out approach outperforms other existing self and semi-supervised methods and is even competitive with supervised method. Furthermore, we propose a simple yet efficient strategy to design the lightweight network. The experiments demonstrate that Lite-HR-Depth can perform on par with large model with fewer parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Depth prediction from single image on KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Analysis of High Resolution Depth Estimation. Abs Rel is an evaluation index in depth estimation, and lower is better. HR means high resolution and LR means low resolution. All interpolation results are caculated by OpenCV library.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of feature maps at first level Left side shows feature maps in HR-Depth and right side shows feature maps in Monodepth2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Supervision Resolution Dataset Abs rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>SfMLearner</cell><cell>M</cell><cell cols="3">416 × 128 CS + K 0.198 1.836 6.565</cell><cell>0.275</cell><cell>0.718</cell><cell>0.901</cell><cell>0.960</cell></row><row><cell>Vid2Depth</cell><cell>M</cell><cell cols="3">416 × 128 CS + K 0.159 1.231 5.912</cell><cell>0.243</cell><cell>0.784</cell><cell>0.923</cell><cell>0.970</cell></row><row><cell>Struct2Depth</cell><cell>M</cell><cell>416 × 128</cell><cell>K</cell><cell>0.141 1.026 5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>640 × 192</cell><cell>K</cell><cell>0.115 0.903 4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>PackNet-SfM</cell><cell>M</cell><cell>640 × 192</cell><cell>K</cell><cell>0.111 0.785 4.601</cell><cell>0.189</cell><cell>0.878</cell><cell>0.960</cell><cell>0.982</cell></row><row><cell>HR-Depth(Ours)</cell><cell>M</cell><cell>640 × 192</cell><cell>K</cell><cell>0.109 0.792 4.632</cell><cell>0.185</cell><cell>0.884</cell><cell>0.962</cell><cell>0.983</cell></row><row><cell>HR-Depth(Ours)</cell><cell>M</cell><cell cols="3">640 × 192 CS + K 0.108 0.955 4.800</cell><cell>0.190</cell><cell>0.887</cell><cell>0.961</cell><cell>0.981</cell></row><row><cell>Monodepth2</cell><cell>MS</cell><cell>640 × 192</cell><cell>K</cell><cell>0.106 0.818 4.750</cell><cell>0.196</cell><cell>0.874</cell><cell>0.957</cell><cell>0.979</cell></row><row><cell>HR-Depth(Ours)</cell><cell>MS</cell><cell>640 × 192</cell><cell>K</cell><cell>0.107 0.785 4.612</cell><cell>0.185</cell><cell>0.887</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>HR-Depth(Ours)</cell><cell>MS</cell><cell cols="3">640 × 192 CS + K 0.104 0.786 4.544</cell><cell>0.182</cell><cell>0.893</cell><cell>0.964</cell><cell>0.983</cell></row><row><cell>Zhou et al.</cell><cell>M</cell><cell>1248 × 384</cell><cell>K</cell><cell>0.121 0.837 4.945</cell><cell>0.197</cell><cell>0.853</cell><cell>0.955</cell><cell>0.982</cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>1024 × 320</cell><cell>K</cell><cell>0.115 0.882 4.701</cell><cell>0.190</cell><cell>0.879</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell>PackNet-SfM</cell><cell>M</cell><cell>1280 × 384</cell><cell>K</cell><cell>0.107 0.802 4.538</cell><cell>0.186</cell><cell>0.889</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell>HR-Depth(Ours)</cell><cell>M</cell><cell>1024 × 320</cell><cell>K</cell><cell>0.106 0.755 4.472</cell><cell>0.181</cell><cell>0.892</cell><cell>0.966</cell><cell>0.984</cell></row><row><cell>HR-Depth(Ours)</cell><cell>M</cell><cell>1280 × 384</cell><cell>K</cell><cell>0.104 0.727 4.410</cell><cell>0.179</cell><cell>0.894</cell><cell>0.966</cell><cell>0.984</cell></row><row><cell>Monodepth2</cell><cell>MS</cell><cell>1024 × 320</cell><cell>K</cell><cell>0.106 0.818 4.750</cell><cell>0.196</cell><cell>0.874</cell><cell>0.957</cell><cell>0.979</cell></row><row><cell>HR-Depth(Ours)</cell><cell>MS</cell><cell>1024 × 320</cell><cell>K</cell><cell>0.101 0.716 4.395</cell><cell>0.179</cell><cell>0.899</cell><cell>0.966</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results of depth estimation on KITTI dataset for distance up to 80m. For error evaluating indexes, Abs Rel, Sq Rel, RMSE and RMSE l og, lower is better, and for accuracy evaluating indexes, δ &lt; 1.25, δ &lt; 1.25 2 , δ &lt; 1.25 3 , higher is better. In the dataset column, CS + K refers to pre-training on CityScapes(CS) and fine-tuning on KITTI(K). M refers to DepthNet that is supervised by monocular(M) image sequence and MS refers to DepthNet that is supervised by monocular and stereo (MS) images. At test time, we scale outputs of DepthNet with median ground-truth LiDAR information.</figDesc><table><row><cell>Input image</cell><cell>Ours(14.62M)</cell><cell>PackNet-SfM(128.29M) Monodepth2(14.84M) Ours(Lite)(3.1M)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Supervision Resolution #P ara Abs rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>, when training with low-resolution im-</cell></row><row><cell>ages, Lite-HR-Depth can perform better than Monodepth2</cell></row><row><cell>with teacher (T) network supervising. However, when train-</cell></row><row><cell>ing with high-resolution images, Lite-HR-Depth can per-</cell></row><row><cell>form better than Monodepth2 without additional supervision</cell></row><row><cell>signal (M).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation Studies. Results for different variants of our model with monocular training on KITTI at low resolution on Eigen split. The baseline model is Monodepth2. We introduce the dense skip-connection(dense SC) to original structure and compare effect of SE block and fSE block.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the Key Research and Development Program of Guangdong Province of China (2019B010120001) and the National Natural Science Foundation of China under Grant 61836015.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zaccaroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05724</idno>
		<title level="m">Real-time single image depth perception in the wild with handheld devices</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Packing for Self-Supervised Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantically-Guided Representation Learning for Self-Supervised Monocular Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5848" to="5854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<title level="m">Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth Estimation Using Displacement Fields. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wofk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6101" to="6108" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">ExFuse: Enhancing Feature Fusion for Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="273" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6872" to="6881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
