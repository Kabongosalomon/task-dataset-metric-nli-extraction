<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>vision-and-language navigation</term>
					<term>transfer learning</term>
					<term>BERT</term>
					<term>embodied AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g.'stairs') to visual content in the environment (pixels corresponding to 'stairs').</p><p>We ask the following question -can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions [24]) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Visionand-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a sequence of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -with their combination resulting in further positive synergistic effects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider the navigation instruction in <ref type="figure">Figure 1</ref>, 'Walk through the bedroom and out of the door into the hallway. Walk down the hall along the banister rail through the open door. Continue into the bedroom with a round mirror on the wall and butterfly sculpture.' In vision-and-language navigation (VLN) <ref type="bibr" target="#b3">[4]</ref>, agents must interpret such instructions to navigate through photo-realistic environments. In this instance, the agent needs to exit the bedroom, walk past something called a 'banister rail' and find the bedroom containing a 'round mirror' and 'butterfly sculpture.' But what if the agent has never seen a butterfly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Follower Model Speaker Model Compatibility Model (Ours)</head><p>Instructions: Walk through the bedroom and out of the door into the hallway. Walk down the hall along the banister rail through the open door. Continue into the bedroom with a round mirror on the wall and butterfly sculpture. <ref type="figure">Fig. 1</ref>. We propose a compatibility model (right) for path selection in vision-andlanguage navigation (VLN). In contrast to the follower (left) and speaker (center) models that have typically been used in prior work, our model takes a path and instruction pair as input and produces a score that reflects their alignment. Based on this model we describe a training curriculum that leverages internet data in the form of image-caption pairs to improve performance on VLN.</p><p>before, let alone a sculpture of one? To solve this task, an agent needs to determine if the visual evidence along a path matches the descriptions provided in the instructions. As such, the ability to ground references to objects and scene elements like 'butterfly sculpture' and 'banister rail' is central to success. Existing work has focused on learning this grounding solely from a task-specific training dataset of path-instruction pairs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> -which are expensive, laborious, and time-consuming to collect at scale and thus tend to be relatively small (e.g. the VLN dataset contains around 14k path-instruction pairs for training). As an alternative, we propose learning visual grounding from freely-available internet data, such as the web images with alt-text captured in the Conceptual Captions dataset <ref type="bibr" target="#b23">[24]</ref>, containing around 3.3M image-text pairs. Conceptually, transfer learning from large-scale web data to embodied AI tasks such as VLN is an attractive alternative to collecting more data. Empirically, however, the effectiveness of this strategy remains open to question -would such a transfer even work? Unlike web images, which are highly-curated and stick closely to aesthetic biases, embodied data contains content and viewpoints that are not widely published online. For example, as shown in <ref type="figure">Figure 2</ref>, an embodied agent may perceive doors via a close-up view of a door frame rather than as a carefully composed image of a (typically closed) door. In VLN, image framing is a consequence of the agent's position rather than an aesthetic choice made by a photographer. Consequently, in this paper we investigate this question -to what degree can webly-supervised visual grounding learned on static images be transferred to the embodied VLN task? Put more succinctly, can 'disembodied' web data be used to improve visual grounding for embodied agents?</p><p>To answer this question, we introduce VLN-BERT, a joint visiolinguistic transformer-based compatibility model for scoring the alignment between an instruction and an agent's observations along a trajectory. We structure VLN-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conceptual Captions Matterport3D</head><p>Closet Couch Door Kitchen Dining Table <ref type="figure">Fig. 2</ref>. Images from the Conceptual Captions (CC) <ref type="bibr" target="#b23">[24]</ref> (top) and Matterport3D (MP3D) <ref type="bibr" target="#b4">[5]</ref> (bottom) datasets illustrate the differences between the two domains. Images from CC are typically well-lit, well-composed and aesthetically pleasing, while for MP3D images (used in VLN) the framing depends on the position of the agent (e.g. a couch (left) in CC is typically viewed head-on, whereas in MP3D they may be hidden to the side as an agent navigates past them).</p><p>BERT to enable straight-forward transfer learning from a model from prior work on general visiolinguistic representation learning <ref type="bibr" target="#b17">[18]</ref>, and explore a training curriculum that incorporates both large-scale internet data and embodied pathinstruction pairs. VLN-BERT is sequentially trained using 1) language-only data (Wikipedia and BooksCorpus <ref type="bibr" target="#b33">[34]</ref> as in BERT <ref type="bibr" target="#b6">[7]</ref>), 2) web image-text pairs (Conceptual Captions <ref type="bibr" target="#b23">[24]</ref> as in ViLBERT <ref type="bibr" target="#b17">[18]</ref>), and 3) path-instruction pairs from the VLN dataset <ref type="bibr" target="#b3">[4]</ref>. Following this protocol the model progressively learns to represent language, then to ground visual concepts, and finally to ground visual concepts alongside action descriptions. We evaluate VLN-BERT on a path selection task in VLN, demonstrating that this training procedure leads to significant gains over prior work (4 absolute percentage points on leaderboard success rate).</p><p>Contributions. Concretely, we make the following main contributions:</p><p>-We develop VLN-BERT, a visiolinguistic transformer-based model for scoring path-instruction pairs. We show that VLN-BERT outperforms strong single-model baselines from prior work on the path selection task -increasing success rate (SR) by 4.6 absolute percentage points.</p><p>-We demonstrate that in an ensemble of diverse models VLN-BERT improves SR by 3.0 absolute percentage points on "unseen" validation, leading to a SR of 73% on the VLN leaderboard (4 absolute percentage points higher than previously published work) 4 .</p><p>-We ablate the proposed training curriculum, and find that each stage contributes significantly to the final outcome, with a cumulative benefit that is greater than the sum of the individual effects. Notably, we find that pretraining on image-text pairs from the web provides a significant boost in path selection performance -improving SR by 9.2 absolute percentage points.</p><p>-We provide qualitative evidence that our model learns to ground object references. Specifically, using gradient-based methods <ref type="bibr" target="#b24">[25]</ref> we visualize how image-region importance shifts under modifications to the instructions given to our model, demonstrating reasonable responses to these interventions. For example, if we modify the instruction 'Walk down the stairs, then stop next to the fridge.' by removing 'stop next to the fridge' we observe that image regions containing the fridge become less important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Path Selection in VLN. In VLN <ref type="bibr" target="#b3">[4]</ref>, an agent is required to follow a navigation instruction from a start location to a goal. While most existing works focus on the setting in which the test environments are previously unseen, many also consider the scenario in which the test environment is previously explored and stored in memory (i.e., fully observable). In this setting, a high-probability path is typically generated by performing beam search through the environment and ranking paths according to either: (1) their probability under a 'follower' model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, as in <ref type="figure">Figure 1</ref> (left), or (2) by how well they explain the instruction according to a 'speaker' (instruction generation) model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>, as in <ref type="figure">Figure 1</ref> (center). In contrast, we use beam search with an existing agent model <ref type="bibr" target="#b27">[28]</ref> to generate a set of candidate paths, which we then evaluate using our discriminative path-instruction compatibility model, as in <ref type="figure">Figure 1</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation and Auxiliary Tasks in VLN.</head><p>To compensate for the small size of existing VLN datasets, previous works have investigated various data augmentation strategies and auxiliary tasks. Many papers report results trained on augmented data including instructions synthesized by a speaker model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Tan et al. <ref type="bibr" target="#b27">[28]</ref> use environmental dropout to mimic additional training environments to improve generalization. Li et al. <ref type="bibr" target="#b16">[17]</ref> incorporate language-only pretraining using a BERT model. Several existing papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> and one concurrent hitherto-unpublished work <ref type="bibr" target="#b9">[10]</ref> consider path-instruction compatibility as an auxiliary loss function or reward for VLN agents. We focus on path-instruction compatibility in the context of transfer learning from large-scale internet data, which has not been previously explored.</p><p>Vision-and-Language Pretraining. There has been significant recent progress towards learning transferable joint representations of images and text <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Using BERT-like <ref type="bibr" target="#b6">[7]</ref> self-supervised objectives and Transformer <ref type="bibr" target="#b28">[29]</ref> architectures, these models have achieved state-of-the-art results on multiple vision-and-language tasks by pretraining on aligned image-and-text data collected from the web <ref type="bibr" target="#b23">[24]</ref> and transferring the base architecture to other tasks such as VQA <ref type="bibr" target="#b8">[9]</ref>, referring expressions <ref type="bibr" target="#b11">[12]</ref>, and caption-based image retrieval <ref type="bibr" target="#b5">[6]</ref>. However, these tasks are all based on single images. The extent to which these pretrained models can generalize from human-composed and curated internet images to embodied AI tasks has not been investigated. In this work we propose a training curriculum to handle potential domain-shift and augment a previ-ous model architecture to process panoramic image sequences, extending the progress in vision-and-language to vision-and-language navigation (VLN).</p><p>3 Preliminaries: Self-Supervised Learning from the Web Recent works have demonstrated that high-capacity models trained under selfsupervised objectives on large-scale web data can learn strong, generalizable representations for both language and images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. We build upon these works as a basis for transfer and describe them briefly here.</p><p>Language Modeling with BERT. The BERT <ref type="bibr" target="#b6">[7]</ref> model is a large transformerbased <ref type="bibr" target="#b28">[29]</ref> architecture for language modeling. The model takes as input sequences of tokenized words augmented with positional embeddings and outputs a representation for each. For example, a two sentence input could be written as</p><formula xml:id="formula_0">&lt;CLS&gt; w (1) 1 , . . . , w (1) L1 &lt;SEP&gt; w (2) 1 , . . . , w<label>(2)</label></formula><p>L2 &lt;SEP&gt;</p><p>where CLS, and SEP are special tokens. To train this approach, <ref type="bibr" target="#b6">[7]</ref> introduce two self-supervised objectives -masked language modelling and next sentence prediction. Given two input sentences from a text corpus, the masked language modelling objective masks out some percentage of tokens and tasks the model to predict their values given the remaining tokens as context. The next sentence prediction objective requires the model to predict whether the two sentences follow each other in the original corpus or not. BERT is then trained under these objectives on large language corpuses from the web (Wikipedia and BooksCorpus <ref type="bibr" target="#b33">[34]</ref>). This model forms the basis for both our approach and the visiolinguistic representation learning discussed next.</p><p>Visiolinguistic Representations Learning with ViLBERT. Extending BERT, ViLBERT <ref type="bibr" target="#b17">[18]</ref> (and a number of similar approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>) focuses on learning joint visiolinguistic representations from paired image-text data, specifically web images and their associated alt-text collected in the Conceptual Captions dataset <ref type="bibr" target="#b23">[24]</ref>. ViLBERT is composed of two BERT-like processing streams that operate on visual and textual inputs, respectively. The input to the visual stream is composed of image regions (generated by an object detector <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> pretrained on Visual Genome <ref type="bibr" target="#b13">[14]</ref>) that act as "words" in the visual domain. Concretely, given a single image I consisting of a set of image regions {v 1 , . . . , v k } and a text sequence (i.e. a caption) w 1 , . . . , w L , we can write the input to ViLBERT as the sequence</p><formula xml:id="formula_2">&lt;IMG&gt; v 1 , . . . , v k &lt;CLS&gt; w 1 , . . . , w L &lt;SEP&gt;<label>(2)</label></formula><p>where IMG, CLS, and SEP are special tokens marking the different modality subsequences. The two streams are connected using co-attention <ref type="bibr" target="#b18">[19]</ref> transformer layers, which attend from the visual stream over language stream and vice versa. Notably, the language stream of ViLBERT is designed to mirror BERT such that it can be initialized by a pretrained BERT model. After processing, the model produces a contextualized output representation for each input token.</p><p>cross-modal attention vision stream language stream IMG r 0 r 1 r N CLS Turn right door</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy(path, instruction)</head><p>Training Curriculum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language-Only (Wikipedia and BookCorpus)</head><p>A couch, also known as a sofa is a piece of furniture for seating two or three people...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-Instruction Pairs</head><p>(Room-to-Room)</p><p>Turn right and into the living room. Walk past the sofa and stop by the door.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Caption Pairs (Conceptual Captions)</head><p>blue sofa in the living room <ref type="figure">Fig. 3</ref>. We propose VLN-BERT (top), a visiolinguistic transformer-based model similar to the model from <ref type="bibr" target="#b17">[18]</ref>, to process image regions from a sequence of panoramas and words from an instruction. We demonstrate that with the proposed training curriculum (bottom) visual grounding learned from image-text pairs from the web (center) can be transferred to significantly improve performance in VLN.</p><p>In analogy to the training objectives in BERT, ViLBERT introduces the masked multimodal modelling and multimodal alignment tasks. In the first, a random subset of language tokens and image regions are masked and must be predicted given the remaining context. For image regions, this amounts to predicting a distribution over object classes present in the masked region. Masked text tokens are handled as in BERT. The multimodal alignment objective trains the model to determine if an image-text pair matches, i.e. if the text describes the image content. Individual token outputs are used to predict masked inputs in the masking objective, and the IMG and CLS tokens are used for the image-caption alignment objective. We build upon this model extensively in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We first describe the path selection setting in Vision-and-Language Navigation (Section 4.1), then our proposed model architecture (Section 4.2), and finally our transfer learning curriculum (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Vision-and-Language Navigation as Path Selection</head><p>In the Vision-and-Language Navigation (VLN) <ref type="bibr" target="#b3">[4]</ref> task, agents are placed in an environment specified as a navigation-graph G = {V, E}. Nodes v ∈ V represent different positions within the environment, and are represented by 360-degree panoramas taken at that viewpoint. Meanwhile, edges delineate navigable paths between panorama positions. The agent is provided with a navigation instruction x that describes the shortest-path between a starting position v s and goal position v g (as illustrated in the bottom right of <ref type="figure">Figure 3</ref>). Agents are considered to have succeeded if they traverse</p><formula xml:id="formula_3">a path τ = [v s , v 1 , v 2 , . . . , v N ] with a final position v N that is within 3m of the goal v g .</formula><p>Much of the work in VLN focuses on this problem as an exploration task in new environments; however, many practical deployments of robotic agents would be long-term in relatively fixed environments (e.g. an assistant operating in a single home). In this paper, we consider the setting in which the environment is previously explored with the navigation-graph and panoramas are stored in the agent's memory. This setting has been studied in prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> and is operationalized by providing the agent unrestricted access to the Matterport3D Simulator <ref type="bibr" target="#b3">[4]</ref> during inference such that it can consider arbitrarily many valid paths originating from the starting position v s , before selecting one to follow.</p><p>In this setting, the navigation task becomes one of identifying the path best aligned with the instructions. Concretely, given a set of valid paths T with the same starting position v s and an instruction x, the problem of navigation is to identify a trajectory τ * such that</p><formula xml:id="formula_4">τ * = argmax τ ∈T f (τ, x)<label>(3)</label></formula><p>for some compatibility function f that determines if the trajectory follows the instruction and terminates near the goal. The two major challenges are how to learn a compatibility function f and how to efficiently search through the large set of possible paths. Given that our focus is on transfer learning, we address the first challenge within a simple path selection setting. Specifically, we consider a small set of paths T = {τ 1 , τ 2 , . . . , τ M } for each instruction, which are generated using beam-search with a greedy instruction-following agent <ref type="bibr" target="#b27">[28]</ref>, and task f with selecting the path that best aligns with the instruction from this set. Future work might explore how f could be further used as a heuristic to efficiently search through the larger, exhaustive set of candidate paths T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Instruction-Path Compatibility</head><p>To formalize the task, we consider a function f that maps a trajectory τ and an instruction x to compatibility score f (·, ·). We model f (τ, x) as a visiolinguistic transformer-based model denoted as VLN-BERT. The architecture of VLN-BERT is structural similar to ViLBERT <ref type="bibr" target="#b17">[18]</ref>; this is by design because it enables straight-forward transfer of visual grounding learned from large-scale web data. Specifically, we make a number of VLN-specific adaptations to ViLBERT, but they are all structured as augmentations (adding modules) rather than ablations (removing existing network components) so that pretrained weights can be transferred to initialize large portions of the model.</p><p>Representing Trajectories and Instructions. Predicting path-instruction compatibility requires jointly reasoning over a sequence of observations and a sequence of instruction words. As in prior work <ref type="bibr" target="#b7">[8]</ref>, a trajectory is represented as a sequence of panoramic images (as in <ref type="figure">Figure 3</ref>   <ref type="figure">Fig. 4</ref>. We encode spatial information for each region to include not only the region position, but also its relation to the trajectory path (a). We form overall region encoding by summing visual features, an embedding indicating the index of the source panorama in the trajectory, and an embedding of panoramic spatial information (b).</p><p>are poses. Further, we represent each panorama I i as a set of image regions {r</p><formula xml:id="formula_5">(i) 1 , . . . , r (i) K }.</formula><p>Let an instruction x be a sequence of tokens w 1 , . . . , w L . We can thus write a path-instruction pair for VLN-BERT as the input sequence &lt;IMG&gt; r</p><formula xml:id="formula_6">(1) 1 , . . . , r (1) K , . . . , &lt;IMG&gt; r (N ) 1 , . . . , r (N ) K , &lt;CLS&gt; w 1 , . . . , w L &lt;SEP&gt; (4)</formula><p>where IMG, CLS, and SEP are special tokens as before.</p><p>The transformer models on which VLN-BERT (as well as BERT and ViL-BERT) is based are inherently invariant to sequence order -only representing interactions between inputs as a function of their values. The common practice to introduce this information is to add positional embeddings to the input token representations. For language, this is straight-forward and amounts to an indexin-sequence encoding. Panorama trajectories on the other have significantly more complex relationships. While the panoramas themselves are a sequence, there are also geometric relationships between them (e.g. two panoramas being 1.2 meters apart at 10 degrees off north). Further, each individual image region not only has a position in the image (as modelled in ViLBERT) but also an angle relative to the heading of an agent as it traverses the trajectory. These are important considerations for language-guided navigation -after all, something on your left going one way is on your right if you go in the opposite direction. Being able to reason about the order of panoramas and the relative heading of image content is integral for following instructions like 'Go down the hallway on the right then stop when you see a table on your left.'.</p><p>To address this, as visualized in <ref type="figure">Figure 4</ref>(a), we encode the spatial location of each image region r i in terms of its location in the panorama (top-left and bottom-right corners in normalized coordinates as well as area of the image covered), its elevation relative to the horizon, and its heading relative to the agents current and next viewing directions. All angles are encoded as [cos(θ), sin(θ)]. The resulting 11-dimensional vector S i is projected into 2048 dimensions using a learned projection W S . To capture the sequential order of the panoramas within a trajectory, we project the scalar panorama index to 2048 dimensions using a learned embedding W P . As shown in <ref type="figure">Figure 4</ref>  k } for each panorama, we process the panoramas by first generating 600 × 600 pixel perspective projections using an 80 degree field of view at the 36 discrete heading and elevation directions used in previous work <ref type="bibr" target="#b3">[4]</ref>. Similarly to ViLBERT, we then use the bottom-up attention model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> pretrained on Visual Genome <ref type="bibr" target="#b13">[14]</ref> to independently extract a set of image regions and features from each perspective image. Since the perspective images have substantial overlap we remove redundant regions within each panorama. First, we discard regions that are centred more than 20 degrees away from the center of the image (i.e. we discard regions along the boarders). We assume that the discarded regions will be captured in a neighboring perspective image (spaced at 30 degree heading increments), with more visual context. Next, we examine pairs of image regions within each panorama in order of decreasing feature similarity. We discard the region in the pair with the lower bottom-up attention class detection score, until a maximum of 100 regions per panorama remain. We define similarity as the cosine distance between image features to which we add the absolute difference in region heading and elevation. Including heading and elevation differences ensures that visually similar features found in different regions of the panorama are unlikely to be classified as redundant.</p><p>Training for Path Selection. To train VLN-BERT for path selection, we consider a 4-way multiple-choice task. Given an instruction x, we sample four trajectories out of which only one is successful {τ</p><formula xml:id="formula_7">+ 1 , τ − 2 , τ − 3 , τ − 4 }.</formula><p>We run VLN-BERT on each instruction-trajectory pair and extract their corresponding final representations. We denote these outputs for the CLS and the first IMG token as h CLS and h IMG respectively and compute a compatibility score s i as</p><formula xml:id="formula_8">s i = f (τ i , x) = W h (i) CLS h (i) IMG<label>(5)</label></formula><p>where denotes element-wise multiplication and W is a learned transformation matrix. Scores, normalized via a softmax, are supervised with cross-entropy loss, p = softmax(s)</p><formula xml:id="formula_9">L x, {τ + 1 , τ − 2 , τ − 3 , τ − 4 } = CrossEntropy p, 1[τ + 1 ]<label>(6)</label></formula><p>where 1[τ + 1 ] is a 1-hot vector with mass at the index of τ + . At inference, we simply sort trajectories by their compatibility scores s i .</p><p>Mining Negative Examples. We find that choosing an appropriate set of path-instruction pairs is critical to performance. Ideally, samples would span the space of all possible pairs, including hard negatives such as a hypothetical example where the agent must select between paths that end at two different 'butterfly sculptures'. The question is how to find varied path-instruction pairs with semantically meaningful differences? We find that using beam search with an instruction-following model yields a diverse set of paths that are effective for training. Furthermore, the paths are conditioned on the instructions, and we find that in practice the incorrect paths often make semantically meaningful mistakes. Specifically, we sample up to 30 beams per instruction from the follower model of Tan et al. <ref type="bibr" target="#b27">[28]</ref> and label the path as successful if it meets the VLN success criteria (i.e. it ends &lt; 3m from the goal). Finally, one positive and three negatives pairs are sampled uniformly at random for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Internet-to-Embodied Transfer Learning</head><p>While VLN-BERT can be trained from scratch, as described above we designed the model to specifically enable transfer learning from language <ref type="bibr" target="#b6">[7]</ref> and visiolinguistic <ref type="bibr" target="#b17">[18]</ref> models trained on large-scale web corpora. This transfer is especially important in the VLN task which is relatively data-sparse (containing approximately 14k path-instruction pairs for training) and has a natural bias towards describing only objects present in training environments (i.e. objects that are unique to the testing environments are never referenced in the training instructions). In this section, we describe a pretraining curriculum for transferring models learned on 'disembodied' web data to the embodied VLN task.</p><p>We summarize the pretraining process in <ref type="figure">Figure 3</ref>. In total, we consider three stages focused on learning language, visual grounding, and action grounding.</p><p>-Stage 1: Language. To capture strong language understanding capabilities, we initialize the language stream of our model with weights from a BERT <ref type="bibr" target="#b6">[7]</ref> model trained on Wikipedia and the BooksCorpus <ref type="bibr" target="#b33">[34]</ref> under the masked language modelling and next sentence prediction objectives. Directly training on the path selection task after this stage is analogous to introducing a BERT encoder to represent instructions.</p><p>-  <ref type="bibr" target="#b23">[24]</ref> under the masked multimodal language modelling and multimodal alignment objectives. In this stage, we initialize model weights with a ViLBERT model trained in this manner. Transferring directly from this stage provides an initialization that can associate descriptions with image regions.</p><p>-Stage 3: Action Grounding. In the final stage, we pair paths and instructions from VLN and train the model under the masked multimodal modelling objective from <ref type="bibr" target="#b17">[18]</ref>. While the previous stage learns to ground visual concepts, this stage additionally exposes the model to actions and their trajectory-based referents. For example, correctly predicting a masked instruction phrase like 'turn ' or 'stop at the ' requires the model to reason about the agent's path from the visual inputs and positional encodings.</p><p>After these pretraining stages, we fine-tune our VLN-BERT model for path selection as described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments primarily address following questions:</p><p>1. Does pretraining on web image-text pairs improve VLN performance? 2. How does the performance of VLN-BERT compare with strong baselines? 3. Does VLN-BERT consider relevant image regions to produce alignment scores?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We conduct experiments using the Room-to-Room (R2R) navigation task <ref type="bibr" target="#b3">[4]</ref> that was generated using the Matterport3D dataset <ref type="bibr" target="#b4">[5]</ref>. R2R contains humanannotated path-instruction pairs that are divided into training, seen and unseen validation, and unseen testing sets. To generate a dataset for path selection we run beam search on the instruction-follower model from <ref type="bibr" target="#b27">[28]</ref>, to produce a set of up to 30 candidate paths for each instruction in R2R. We find that with a beam size of 30 over 99% of the candidate sets contain one path that reaches the goal, which places an acceptable upper bound on path selection performance. In all of the experiments that follow, results are reported for selecting one path from the set of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We compare the performance of different models using standard VLN metrics. Note that for path selection we calculate metrics using only the selected path, which corresponds with the pre-explored environment setting. However, for the VLN leaderboard results we follow the required approach of prepending the exploration path to the selected path (which affects path length based metrics).</p><p>-Success rate (SR) measures the percentage of selected paths that stop within 3m of the goal. In path selection this is our primary metric of interest.</p><p>-Oracle Success rate (SR) measures the percentage of selected paths with any position that passes within 3m of the goal.</p><p>-Navigation error (NE) measures the average distance of the shortest path from the last position in the selected path to the goal position.</p><p>-Path length (PL) measures the average length of the selected path -a lower PL is preferred if the trajectory is successful.</p><p>-Success rate weighted by path length (SPL), as defined in <ref type="bibr" target="#b0">[1]</ref>, provides a measure of success normalized by the ratio between the length of the shortest path and the selected path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Baseline Models</head><p>We compare with the follower and speaker models from <ref type="bibr" target="#b27">[28]</ref>, which achieve stateof-the-art performance on the VLN test set in an ensemble model setting. The only auxiliary dataset used to train these baseline models is ImageNet <ref type="bibr" target="#b22">[23]</ref> (used to pretrain an image feature extractor). All of the other components are trained from scratch (including word embeddings). Data augmentation, via environmental dropout <ref type="bibr" target="#b27">[28]</ref>, is used to train the follower model and greatly improves performance. We report results using code and weights provided by Tan et al. <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does pretraining on web image-text pairs improve VLN performance?</head><p>To answer this question we dissect our proposed training curriculum as indicated <ref type="table">Table 1</ref>. We compare the contribution from the different stages of pretraining. We find that stage 2 and 3 both contribute significantly to task performance (improving Val Unseen Success Rate (SR) by ∼15-20 absolute percentage points over the no pretraining baseline -rows 3 and 4 vs. 1), with their full combination providing further synergistic gains (row 5). Notably, skipping the visual grounding stage (but still doing the others) results in a 9 absolute percentage point drop in Val Unseen SR (compare rows 4 and 5) -demonstrating the importance of internet-to-embodied transfer of visual grounding. in <ref type="table">Table 1</ref>, and find that in general each stage of training does contribute to performance. First, we find that our model has limited performance learning from scratch, achieving only 30.5% SR (compared with the 54.7% SR achieved by the speaker model from <ref type="bibr" target="#b27">[28]</ref>). However, language-only pretraining, which corresponds to initializing our model with BERT <ref type="bibr" target="#b6">[7]</ref> weights, improves performance substantially to 45.2% SR (an improvement of 14.7 absolute percentage points) -indicating that language understanding plays an important role in VLN.</p><p>Next, we find that both pretraining on image-text pairs from the Conceptual Captions <ref type="bibr" target="#b23">[24]</ref> (visual grounding) and pretraining on path-instruction pairs from VLN <ref type="bibr" target="#b3">[4]</ref> (action grounding) similarly improve success rate (by 4.5 and 4.9 absolute percentage points, respectively) when used independently. However, when the two pretraining stages are combined in series the improvement jumps to 14.1 absolute percentage points in success rate or 9.2 absolute percentage points over the next best setting. The substantial level of improvement that results from our full training curriculum suggests that not only does pretraining on weblysupervised image-text pairs from <ref type="bibr" target="#b23">[24]</ref> improve path selection performance, but it also constructively supports the action grounding stage (Stage 3) of pretraining.</p><p>How does VLN-BERT compare with strong baseline methods? The results in <ref type="table">Table 2</ref> compare path selection performance of VLN-BERT with the state-of-the-art speaker and follower models from <ref type="bibr" target="#b27">[28]</ref>. We evaluate path selection using the set of up to 30 candidate paths generated with beam search using the follower from <ref type="bibr" target="#b27">[28]</ref>. For the follower model results this amounts to taking the top beam from the candidate set. In the single model setting we see that VLN-BERT, trained with our full curriculum, achieves 59.3% SR, which is 4.6 absolute percentage points better than either of the other two methods.</p><p>In the pre-explored setting, the speaker and follower models are typically combined in an ensemble to further improve path selection performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>. The two models are typically combined as a linear combination using a hyperparameter α that is selected through grid search on the val unseen split of R2R <ref type="bibr" target="#b7">[8]</ref>. <ref type="table">Table 2</ref>. Results comparing VLN-BERT with the follower and speaker from <ref type="bibr" target="#b27">[28]</ref>. Notably, in the ensemble models setting, combining VLN-BERT with the speaker and follower results in a 3 absolute percentage point improvement in Val Unseen Success Rate (SR) over the next best three-model ensemble (compare rows 6 and 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Val Seen</head><p>Val Unseen  In the ensemble models section of <ref type="table">Table 2</ref>, the speaker + follower line (row 4) represents our execution of the state-of-the-art ensemble model from <ref type="bibr" target="#b27">[28]</ref>. In rows 5-7, we consider three model ensembles composed of a speaker, follower, and one additional model combined using two hyperparameters α and β (again selected through grid search on val unseen). We find that adding another (randomly seeded) speaker or follower model yields modest improvements of 1.2 and 2.7 absolute percentage points in SR (rows 5 and 6). In contrast, adding VLN-BERT results in a 5.7 absolute percentage point boost in SR (row 7), which is 3.0 absolute percentage points higher on success rate than the next best ensemble.</p><formula xml:id="formula_11"># Re-ranking Model PL NE ↓ SPL ↑ OSR ↑ SR ↑ PL NE ↓ SPL ↑ OSR ↑ SR ↑ Single</formula><p>In <ref type="table" target="#tab_4">Table 3</ref> we report results on the VLN test set via the VLN leaderboard. In the leaderboard setting we use a three-model ensemble that includes a speaker, follower, and VLN-BERT. The ensemble achieves a success rate of 73%, which is 4 absolute percentage points greater than previously published work <ref type="bibr" target="#b27">[28]</ref>, and 2 absolute percentage points greater than concurrent, unpublished work <ref type="bibr" target="#b32">[33]</ref>.</p><p>Does VLN-BERT consider relevant image regions to produce alignment scores? One motivation behind our proposed approach is to improve the grounding of object references in VLN instruction. To test whether this actually happens and to gain insight into the improved performance of VLN-BERT, we visualize which parts of the visual input affect the compatibility score. We perform this analysis using a simple gradient-based visualization technique <ref type="bibr" target="#b24">[25]</ref>. We take the gradient of our learned score f (x, τ ) with respect to the feature repre-sentation for each region from each panorama, and sum this 2048-dimensional gradient vector over the feature dimension to produce a scalar measure of region importance. To gain deeper insight we analyze how the region importance changes when the instruction is perturbed by removing parts of the description.</p><p>Three examples of this analysis are illustrated in <ref type="figure">Figure 5</ref>. The left panel provides the original and modified versions of the instruction with high-importance regions highlighted in green and purple, respectively. In each row, the original instruction is modified by removing a phrase that references a high-importance region (e.g. in the first row 'then stop next to the fridge' is removed). In the middle and right panels, the region importance histograms and top 5 regions illustrate which parts of the visual input most influence the compatibility score. For example, in the first row regions containing a 'fridge' are important for the original instruction, whereas for the modified instruction the importance shifts to the 'stairs'. Independently, the center and right panels demonstrate that VLN-BERT produces compatibility scores based on relevant image regions. Furthermore, by comparing the top 5 regions before and after the instruction is modified, we see that the grounding of object references appropriately shifts under linguistic interventions. This analysis provides qualitative evidence that VLN-BERT properly learns to associate instruction phrases with image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we demonstrated internet-to-embodied transfer of visual concept grounding -leveraging large-scale image-text data from the web to improve a discriminative path-instruction alignment model for VLN. In our path re-ranking setting, this model improves over prior work and our ablations show each stage of our transfer curriculum contributes significantly. Original Instruction: Moving towards the stairs, but without using them, take a left, walk down the stairs, then stop next to the fridge.</p><p>Modified Instruction: Moving towards the stairs, but without using them, take a left, walk down the stairs.</p><p>Original Instruction: Leave the kitchen, and walk through the pantry. In the hall take a left, and take a right at the end of the hall. Stop next to the plant on the table in the entryway.</p><p>Modified Instruction: Leave the kitchen, and walk through the pantry. In the hall take a left, and take a right at the end of the hall.  <ref type="figure">Fig. 5</ref>. We compare region importance histograms for three path-instruction pairs under perturbations of the instruction -removing a phrase or sentence. The region importance histogram is calculated by taking the gradient of the compatibility score with respect to the image region features. The images above each histogram correspond to the most influential regions (i.e. the peaks in the histogram). The underlined instruction phrases correspond with the regions outlined in green and purple to provide a qualitative assessment of the visiolinguistic grounding learning by VLN-BERT. In the first example, removing the reference to the 'fridge' (in green) shifts the importance to other regions along the path (i.e. the 'stairs' in purple), suggesting that VLN-BERT considers visually relevant image regions to score path-instruction pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>In this section we provide further implementation details for VLN-BERT (Section A.1) and additional qualitative analysis of performance (Section A.2) and the pretraining curriculum (Section A.3). Code will be provided to reproduce the results of all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Details</head><p>The experiments described in Section 5 utilize a 12-layer BERT BASE <ref type="bibr" target="#b6">[7]</ref> architecture for both the vision and language streams in the model. Following <ref type="bibr" target="#b17">[18]</ref>, we use 6 cross-modal attention layers to connect the two streams. To operationalize the language-only pretraining stage (Stage 1), VLN-BERT is initialized with BERT weights that result from pretraining on English Wikipedia and BooksCorpus <ref type="bibr" target="#b33">[34]</ref>. Similarly, for the visual grounding stage (Stage 2), VLN-BERT is initialized with ViLBERT weights, which result from pretraining on the Conceptual Captions <ref type="bibr" target="#b23">[24]</ref> dataset. For action grounding pretraining and path-selection finetuning (Stage 3) we use path-instruction pairs from the Room-to-Room <ref type="bibr" target="#b3">[4]</ref> dataset. During this stage, models are trained with the Adam optimizer with a learning rate of 4e-5 and a batch size of 64. We use a learning rate schedule with a linear warmup and cooldown. We train for 50 epochs in pretraining and 20 epochs in finetuning. During finetuning, we utilize early stopping based on the success rate on the unseen split of the validation set. Using 8 Titan X GPUs Stage 3 of training takes approximately 66 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Qualitative Examples of Success and Failure</head><p>This section provides qualitative examples of the path selection performance of VLN-BERT using the full training curriculum described in Section 4.3. To gain insight at the region-level, we estimate region importance using the gradientbased visualization technique described in Section 5.4 (i.e. importance is calculated as the sum of the gradient of the model's output score with respect to the features for each region). In all examples, the model selects one path from a set of up to 30 candidate paths for a given set of instructions. The selected path is successful if it terminates within 3m of the goal location.</p><p>Three examples of successful path selection are illustrated in <ref type="figure">Figure 6</ref>. In the first example, VLN-BERT selects a path that does not initially follow the ground truth path, but correctly stops at the goal location. In this example, the model accurately grounds the phrase 'antelope head', which does not appear in the VLN <ref type="bibr" target="#b3">[4]</ref> training dataset (the term 'antelope' appears 3 times). In the second and third examples, the selected paths closely match the ground truth, and the top 5 regions include key objects mentioned in the instructions -'freezers' and 'statue'. The term 'freezers' (with and without the 's') does not occur the VLN training dataset, and 'statue' appears 105 times. These examples suggest that VLN-BERT is able to transfer visual grounding learned on the image-text pairs in the Conceptual Captions <ref type="bibr" target="#b23">[24]</ref> dataset to the embodied task of path selection. <ref type="figure">Fig. 6</ref>. Examples of successful paths selected by VLN-BERT (middle -blue), with ground truth paths (middle -orange) and navigation errors (middle -red) provided for comparison. The right column illustrates the top 5 regions that influence the model's predictions -importance is determined by taking the gradient of the score for a pathinstruction pair with respect to the input region features (as in Section 5.4). A qualitative assessment of accurate visiolinguistic grounding (in green) highlights phrases that rarely occur in VLN training dataset: 'antelope head' (0 occurrences), 'antelope' (3 occurrences), 'freezer(s)' (0 occurrences), 'statue' (105 occurrences). These results suggest that VLN-BERT has effectively learned to transfer grounding from image-text pairs from the web to the embodied task of VLN.</p><p>Three unsuccessful examples are shown in <ref type="figure">Figure 7</ref>. In each example the characteristics of the errors are qualitatively different. In the first row, VLN-BERT selects a path that does not stop at the correct goal location. However, the top 5 regions include key visual landmarks mentioned in the instruction (e.g.'treadmill' and 'sofa'). In contrast, in the second row, VLN-BERT selects a path that does not reach the goal bedroom. In this instance, the top 5 regions include 'curtains' from a different location, which may have led to this particular error. In the last row every aspect of the selected path seems mismatched from the instructions: the path goes to the left of the table not right and objects mentioned in the instructions are missing from the top 5 regions. <ref type="figure">Fig. 7</ref>. Examples of unsuccessful paths selected by VLN-BERT (middle -blue), with ground truth paths (middle -orange) and navigation errors (middle -red) provided for comparison. The right column illustrates the top 5 regions that influence the model's predictions -importance is determined by taking the gradient of the score for a pathinstruction pair with respect to the input region features (as in Section 5.4). In the first row, the selected path goes past the goal location, but the visual grounding appears accurate. In the second row, the selected path does not enter the correct bedroom and the visual grounding appears to identify the wrong curtains. In the final example the model fails completely -selecting a path going to the left (not right) of the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Qualitative Analysis of the Pretraining Curriculum</head><p>In section Section 5.4 we demonstrated that the visual grounding pretraining stage (Stage 2) quantitatively improves performance. In <ref type="figure">Figure 8</ref> we qualitatively compare the visual grounding that is learned with and without stage 2 of pretraining. In the first example, the model trained without stage 2 of pretraining (right) fails to ground the phrase 'mini fridge', which only occurs 1 time in the VLN training dataset. Similarly, in the second example, without stage 2 pretraining, the model fails to ground the phrase 'massage table' (29 occurrences in the VLN training dataset). In both cases, the model without stage 2 pretraining selects an unsuccessful path. In contrast, when trained with the full curriculum, VLN-BERT correctly grounds these key phrases and selects successful paths for these two examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>bottom right) with positional information -i.e. τ = [(I 1 , p 1 ), . . . , (I N , p N )] where (I i ) are panoramas and (p i ) (a) Panoramic Spatial Information (b) Overall Region Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b), the complete visual input representation for the image region is the element-wise sum of the visual features, panorama index embedding, and panoramic spatial embedding. Extracting Image Regions from Panoramas. To extract the image regions {v (i) 1 , . . . , v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Now at Google. arXiv:2004.14973v2 [cs.CV] 1 May 2020</figDesc><table><row><cell>Walk through the bedroom...</cell><cell>Walk out of the room...</cell><cell>Walk through</cell><cell>score: 1.87</cell></row><row><cell></cell><cell></cell><cell>the bedroom...</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SPL ↑ OSR ↑ SR ↑ PL NE ↓ SPL ↑ OSR ↑ SR ↑</figDesc><table><row><cell cols="3">Pretraining Stage</cell><cell>Val Seen</cell><cell>Val Unseen</cell></row><row><cell>Language</cell><cell>Visual</cell><cell>Action</cell></row><row><cell cols="4"># 1 2 Grounding PL NE ↓ VLN-BERT Only Grounding (no pretraining) 10.78 6.78 0.35 54.22 37.55 10.29 6.81 0.27 50.62 30.52 10.33 4.89 0.55 69.31 58.73 9.59 5.47 0.41 57.34 45.17 3 10.42 4.48 0.58 71.57 62.16 9.70 4.96 0.45 62.79 49.64</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>10.51 4.28 0.60 72.65 63.82 9.81 5.05 0.46 62.75 50.02</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell>10.28 3.73 0.66 76.47 70.20 9.60 4.10 0.55 69.22 59.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Leaderboard results on Test Unseen for methods using beam search.</figDesc><table><row><cell>Test Unseen</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">evalai.cloudcv.org/web/challenges/challenge-page/97/leaderboard/270</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE, Amazon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 8</ref><p>. Comparison of pretraining with the proposed curriculum (middle) vs. omitting the visual grounding stage (right). Region importance histograms are estimated as in Section 5.4. The top 5 regions correspond with the successful path selected by VLN-BERT when pretrained with the full curriculum. Without the visual grounding pretraining stage (right), the model fails to ground phrases that rarely occur in the VLN training dataset (e.g.'mini fridge' (1 occurrence) and 'massage table' (29 occurrences)). Furthermore, in both cases without the visual grounding pretraining stage, VLN-BERT selects an unsuccessful path (not illustrated) for the given instructions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<title level="m">On evaluation of embodied navigation agents</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chasing ghosts: Instruction following as bayesian state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3314" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards learning a generic agent for vision-and-language navigation via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10638</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-modal discriminative model for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13358</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tactical rewind: Self-correction via backtracking in vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6741" to="6749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02244</idno>
		<title level="m">Robust navigation with language pretraining and stochastic sampling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Selfmonitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03035</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The regretful agent: Heuristicaided navigation through progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6732" to="6740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to navigate unseen environments: Back translation with environmental dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vision-language navigation with selfsupervised auxiliary reasoning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07883</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
