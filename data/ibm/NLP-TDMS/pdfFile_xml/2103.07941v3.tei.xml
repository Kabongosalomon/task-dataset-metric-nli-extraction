<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho</forename><forename type="middle">Kei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC/HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC/HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuaishou</forename><surname>Technology</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC/HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC/HKUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Modular interactive VOS (MiVOS) framework which decouples interaction-to-mask and mask propagation, allowing for higher generalizability and better performance. Trained separately, the interaction module converts user interactions to an object mask, which is then temporally propagated by our propagation module using a novel top-k filtering strategy in reading the space-time memory. To effectively take the user's intent into account, a novel difference-aware module is proposed to learn how to properly fuse the masks before and after each interaction, which are aligned with the target frames by employing the space-time memory. We evaluate our method both qualitatively and quantitatively with different forms of user interactions (e.g., scribbles, clicks) on DAVIS to show that our method outperforms current state-of-the-art algorithms while requiring fewer frame interactions, with the additional advantage in generalizing to different types of user interactions. We contribute a large-scale synthetic VOS dataset with pixel-accurate segmentation of 4.8M frames to accompany our source codes to facilitate future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) aims to produce highquality segmentation of a target object instance across an input video sequence, which has wide applications in video understanding and editing. Existing VOS methods can be categorized by the types of user input: semi-supervised methods require pixel-wise annotation of the first frame, while interactive VOS approaches take user interactions (e.g., scribbles or clicks) as input where users can iteratively refine the results until satisfaction. This paper focuses on interactive VOS (iVOS) which finds more applications in video editing, because typical user interactions such as scribbles or clicks (a few seconds per frame) are much easier than specifying full annotation Source code, pretrained models and dataset are available at: https: //hkchengrex.github.io/MiVOS. This research is supported in part by Kuaishou Technology and the Research Grant Council of the Hong Kong SAR under grant no. 16201420.  <ref type="figure">Figure 1</ref>. User annotates one of the frames (e.g., with clicks at the top-left frame) and MiVOS bidirectionally propagates the masks to the entire video sequence. Our difference-aware fusion module guides the segmentation network to correct the masks across frames based on user's intended correction on another frame (e.g., with scribbles on the bottom-right frame).</p><p>(∼79 seconds per instance), with the iterative or successive refinement scheme allowing the user more control over result accuracy versus interaction budget trade-off <ref type="bibr" target="#b0">[1]</ref>.</p><p>Conceptually, iVOS can be considered as the combination of two tasks: interaction understanding (e.g., mask generation from interactions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>) and temporal propagation (e.g., semi-supervised VOS methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>). Current methods usually perform the two tasks jointly, using interconnected encoders <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> or memory-augmented interaction features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The strong coupling limits the form of user interaction (e.g., scribbles only) and makes training difficult. Attempts to decouple the two tasks fail to reach state-of-the-art accuracy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> as user's intent cannot be adequately taken into account in the propagation process.</p><p>One advantage of unified methods over decoupled methods is that the former can efficiently pick up small corrective interactions across many frames, which is suited to the DAVIS evaluation robot <ref type="bibr" target="#b0">[1]</ref>. However, we believe that human users tend to interactively correct a single frame to high accuracy before checking other frames, as the visual examination itself takes time and human labor while free for an evaluation robot. Our method requires less interacted frames by letting the user focus on a single frame multiple times while attaining the same or even better accuracy. Our method is efficient as single-frame interaction can be done almost instantly <ref type="bibr" target="#b3">[4]</ref>, with the more time-consuming propagation performed only sparsely.</p><p>In this paper we present a decoupled modular framework to address the iVOS problem. Note that naïve decoupling may lead to loss of user's intent as the original interaction is no longer available in the propagation stage. This problem is circumvented by our new difference-aware fusion module which models the difference in the mask before and after each interaction to inject the user's intent in propagation. Thus the user's intent is preserved and propagated to the rest of the video sequence. We argue that mask difference is a better representation than raw interactions which is unambiguous and does not depend on interaction types. With our decoupling approach, our method can accept different types of user interactions and achieve better performance on various qualitative and quantitative evaluations. Our main contributions can be summarized as follows:</p><p>• We innovate on the decoupled interaction-propagation framework and show that this approach is simple, effective, and generalizable. • We propose a novel lightweight top-k filtering scheme for the attention-based memory read operation in mask generation during propagation. • We propose a novel difference-aware fusion module to faithfully capture the user's intent which improves iVOS accuracy and reduces the amount of user interaction. We will show how to efficiently align the masks before and after an interaction at the target frames by using the space-time memory in propagation. • We contribute a large-scale synthetic VOS dataset with 4.8M frames to accompany our source codes to facilitate future research. Semi-Supervised Video Object Segmentation. This task aims to segment a specific object throughout a video given only a fully-annotated mask in the first frame. Early methods often employ test-time finetuning on the given frame <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> to improve the model's discriminatory power, but such finetuning is often too slow. Recently, diverse approaches have been explored including pixel-wise embedding <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, mask propagation and tracking <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, building a target model <ref type="bibr" target="#b31">[32]</ref>, and memory features matching <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b35">37]</ref>. In particular, STM <ref type="bibr" target="#b6">[7]</ref> constructs a memory bank from past frames and predicts the mask using a querykey-value attention mechanism. While simple and effective, this method can achieve state-of-the-art results. In this work, we propose to transfer the technical progress of semisupervised VOS methods to the interactive domain. Our space-time memory network, which is inspired by STM <ref type="bibr" target="#b6">[7]</ref>, is used in our propagation backbone. Interactive Video Object Segmentation (iVOS). Usersupplied hints are provided in iVOS. The interactions can  <ref type="figure" target="#fig_1">Figure 2</ref>. Progress in iVOS <ref type="bibr" target="#b39">[41]</ref> has significantly reduced the amount of human labor required to segment objects in videos compared with traditional rotoscoping methods. By leveraging more spatially dense yet temporally sparse interactions, our method further reduces the human effort required to examine the output video in a more tedious, back-and-forth manner (see Section 6.3 for user study) while reaching the same or even better accuracy. Our method can be regarded as lifting 2D image segmentation to 3D. be used to either segment an object or a correct previously misclassified region <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b0">1]</ref>. Most recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> have focused on scribble interaction which is used and provided by the DAVIS challenge <ref type="bibr" target="#b39">[41]</ref>. A recent method <ref type="bibr" target="#b21">[22]</ref> has extended their embedding network in the interactive setting with clicks as user input. Our method can generalize to a wide range of user interactions due to the modular design by simply replacing the interaction-tomask component. The majority of current deep learning based iVOS methods is based on deep feature fusion to incorporate user interactions into the segmentation task, where two interconnected encoder networks are designed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, or scribble features are stored as memory which are referenced later in the segmentation process <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. These approaches inevitably tie the particular form of user inputs with the mask propagation process. This property makes training difficult as the model needs to adapt to both understanding the interactions and accurately propagating masks at the same time. Alternatively, some methods have attempted to decouple the interaction and propagation network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> by first generating a mask given an interaction in any types, followed by propagating this mask bidirectionally. But these methods fail to achieve state-of-the-art performance. We believe that this is due to the dismissal of user intent as the propagation network no longer has access to the original user interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This paper proposes to overcome the above problem by considering the difference in the mask domain before and after an interaction round in order to directly and faithfully represent the user intent in the propagation process. Interactive Image Segmentation. The problem of interactive image segmentation or cutout has a long history with a wide range of applications <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b1">2]</ref>. The recent adoption of deep convolutional neural network has greatly improved state-of-the-art performance with different types of user interactions such as bounding boxes <ref type="bibr" target="#b2">[3]</ref>, clicks <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4]</ref>, or extreme points <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">46]</ref>. Our modular approach can adapt to any of these types of interactions by adopting the corresponding interaction-to-mask algorithm in our framework.  <ref type="figure">Figure 3</ref>. MiNet overview. In interaction round r, the user picks a frame t and interactively correct the object mask until satisfaction using the Scribble-to-Mask (S2M) module (Section 3.2) running in real time. The corrected mask will then be bidirectionally propagated through the video sequence with the propagation module (Section 3.3). To incorporate information from previous rounds, a difference-aware fusion module is used to fuse previous and current masks. The difference in the interacted mask before and after the interaction (which conveys user's intention) is used in the fusion module via an attention mechanism (Section 3.4). In the first round, all masks are initialized to zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Initially, the user selects and interactively annotates one frame (e.g., using scribbles or clicks) to produce a mask. Our method then generates segmentation for every frame in the video sequence. After that, the user examines the output quality, and if needed, starts a new "round" by correcting an erroneous frame with further interactions. We denote r as the current interaction round. Using superscript, the userinteracted frame index in the r-th round is t r , and the mask results of the r-th round is M r ; using subscript, the mask of individual j-th frame is denoted as M r j . Refer to supplementary material for a quick index of the paper's notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MiNet Overview</head><p>As illustrated in <ref type="figure">Figure 3</ref>, our method consists of three core components: interaction-to-mask, mask propagation, and difference-aware fusion. The interaction module operates in an instant feedback loop, allowing the user to obtain real-time feedback and achieve a satisfactory result on a single frame before the more time-consuming propagation process 1 . In the propagation module, the corrected mask is bidirectionally propagated independently of M r−1 . Finally, the propagated masks are fused with M r−1 with the fusion module which aims to fuse the two sequences while avoiding possible decay or loss of user's intent. The user intent is captured using the difference in the selected mask before and after user interaction. This difference is fed into the fusion module as guidance. <ref type="bibr" target="#b0">1</ref> To the best of our knowledge, most related state-of-the-art works take &gt; 100ms per frame, with current "fast" methods taking &gt; 15ms per frame for propagation. This justifies our single-frame interaction and propagation where the latter runs at ∼ 100ms per frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interaction-to-Mask</head><p>Various interactive image segmentation methods can be used here as long as they can compute an object mask from user interactions. Users are free to use their favorite segmentation tool or even tailored pipeline for specific tasks (e.g., human segmentation for movie editing). Methods that use information from an existing mask (M r−1 t r ) might be more labor-efficient but such property is optional.</p><p>We design a Scribble-to-Mask (S2M) network to evaluate our method on the DAVIS <ref type="bibr" target="#b39">[41]</ref> benchmark. Our pipeline has high versatility not restricted by any one type of such interaction network -we additionally employ click-based interaction <ref type="bibr" target="#b3">[4]</ref>, freehand drawing, and a local control module that allows fine adjustment which are experimented in the user study Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2M</head><p>The goal of the S2M network is to produce a single-image segmentation in real time given input scribbles. Our design is intentionally straightforward with a standard DeepLabV3+ <ref type="bibr" target="#b45">[47]</ref> semantic segmentation network as the backbone. The network takes a six-channel input: RGB image, existing mask, and positive/negative scribble maps, and deals with two cases: initial interaction (where the existing mask is empty) and corrective interaction (where the existing mask contains error). Unlike previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>, we train with a simpler single-round approach on a large collection of static images <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51]</ref>. We are able to leverage these non-video large datasets by the virtue of our decoupled paradigm.</p><p>For each input image, we randomly pick one of the two cases (with an empirically set probability of 0.5) and syn-thesize the corresponding input mask which is either set to zeros or perturbed from the ground-truth with random dilation/erosion <ref type="bibr" target="#b50">[52]</ref>. We do not reuse the output mask to form a second training stage <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> to reduce training cost and complications. Input scribbles are then generated correspondingly in the error regions using strategies <ref type="bibr" target="#b39">[41]</ref> such as thinning or random Bézier curves.</p><p>Local Control While state-of-the-art interactive segmentation methods such as f-BRS <ref type="bibr" target="#b3">[4]</ref> often use a large receptive field to enable fast segmentation with few clicks, it may harm the global result when only local fine adjustment is needed toward the end of the segmentation process. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates one such case where the global shape is correct except for the ears. With our decoupled approach, it is straightforward to assert local control by limiting the interactive algorithm to apply in a user-specified region as shown in the figure. The region's result can be effortlessly stitched back to the main segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Propagation</head><p>Given an object mask, the propagation module tracks the object and produces corresponding masks in subsequent frames. Following STM <ref type="bibr" target="#b6">[7]</ref>, we consider the past frames with object masks as memory frames which are used to predict the object mask for the current (query) frame using an attention-based memory read operation. Notably, we propose a novel and lightweight top-k operation that integrates with STM and show that it improves both performance and speed without complicated training tricks.</p><p>Memory Read with Top-k Filtering We build two encoder networks: the memory encoder and the query encoder. Their network backbones are extracted from ResNet50 <ref type="bibr" target="#b51">[53]</ref> up to stage-4 (res4) with a stride of 16. Extra input channels are appended to the first convolution of the memory encoder which accepts object masks as input. At the end of each encoder, two separate convolutions are used to produce two features maps: key k ∈ R C k ×HW and value v ∈ R C v ×HW where H and W are the image dimensions after stride, and C k and C v are set to 128 and 512 respectively. With top-k filtering, the propagation is more stable and performs better especially for temporally far away frames when the noise-to-k ratio is large. <ref type="figure">Figure 5</ref> illustrates our space-time memory read operation. For each of the T memory frames, we compute keyvalue features and concatenate the output as memory key k M ∈ R C k ×T HW and memory value v M ∈ R C v ×T HW . The key k Q computed from the query is matched with k M via a dot product:</p><formula xml:id="formula_0">F = k M T k Q ,<label>(1)</label></formula><p>where each entry in F ∈ R T HW ×HW represents the affinity between a query position and a memory position. Previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">54]</ref> would then apply softmax along the memory dimension and use the resultant probability distribution as a weighted-sum for v M . We have two observations on this softmax strategy: 1) For each query position, most of the weights will fall into a small set of memory positions and the rest are noises, and 2) these noises grow with the size of the memory and are performance-degrading when the sequence is long. Based on these observations, we propose to filter the affinities such that only the top-k entries are kept. This effectively removes noises regardless of the sequence length. Since softmax preserves order, we can apply top-k filtering beforehand to reduce the number of expensive exp calls. In practice, our new top-k strategy not only increases robust-ness but also overcomes the overhead of top-k (see <ref type="table" target="#tab_5">Table 3</ref>). <ref type="figure">Figure 6</ref> reports the performance increase and robustness brought by top-k filtering. Note that KMN <ref type="bibr" target="#b52">[54]</ref> (a recent modification of STM) imposes a Gaussian locality prior on the query using the memory, while our top-k operation filters the memory using the query. Refer to the supplementary material for a detailed comparison.</p><p>In summary, the affinity of memory position i with query position j can be computed by:</p><formula xml:id="formula_1">W ij = exp (F ij ) p∈Top k j (F) (exp (F pj )) , if i ∈ Top k j (F) (2)</formula><p>and 0 otherwise. Top k j (F) denotes the set of indices that are top-k in the j-th column of F. These attentional weights are used to compute a weighted-sum of v M . For query position j, the feature m j is read from memory by:</p><formula xml:id="formula_2">m j = T HW p v M p W pj<label>(3)</label></formula><p>The read features will be concatenated with v Q and passed to the decoder to generate the object mask. Skipconnections (not shown for clarity) from the query encoder to the decoder help to create a more accurate mask. The output of the decoder is a stride 4 mask which is bilinearly upsampled to the original resolution. When there are multiple objects, we process each object one by one and combine the masks using soft aggregation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Propagation strategy <ref type="figure" target="#fig_6">Figure 7</ref> illustrates our bidirectional propagation strategy, similar to <ref type="bibr" target="#b8">[9]</ref>. Given a userinteracted reference frame M r t r , we bidirectionally propagate the segmentation to other frames with two (forward and backward) independent passes. Given that each interacted frame is sufficiently well-annotated (which is more easily satisfied under our decoupled framework), the propagation stops once hitting a previously interacted frame or the end of the sequence. Following STM <ref type="bibr" target="#b6">[7]</ref>, every 5th frame will be included and cached in the memory bank. The frame immediately before the query frame will also be included as temporary memory. In interactive settings, all user-interacted frames are trusted and added to the memory bank.  Evaluation The propagation module can be isolated for evaluation in a semi-supervised VOS setting (where the first-frame ground-truth segmentation is propagated to the entire video). <ref type="table">Table 1</ref> tabulates our validation of the effectiveness of top-k filtering (our new dataset BL30K to be detailed in Section 4). The algorithm is not particularly sensitive to the choice of k with similar performance for k = 20 through 100. k = 50 in all our experiments. In principle, the value of k should be linear to the image resolution such that the effective area after filtering is approximately the same. With top-k filtering, our multi-object propagation runs at <ref type="bibr" target="#b10">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-pass bidirectional propagation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Difference-Aware Fusion</head><p>If the propagation ends with hitting a previously interacted frame t c , there may exist conflicts in frames within t c and t r . Fusion is thus required between the current propagated mask M r and the previous mask results M r−1 . Previous approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> often employ a linear weighting scheme which is agnostic to the correction made and thus fails to capture the user's intent. Oftentimes, the user correction will disappear mid-way between t r and t c .</p><p>As illustrated in <ref type="figure">Figure 8</ref>, we propose a novel learnable fusion module that can keep the user correction in mind during fusion. Specifically, the user correction is captured as the differences in the mask before and after the user interaction at frame t r :</p><formula xml:id="formula_3">D + = M r t r − M r−1 t r + D − = M r−1 t r − M r t r + (4)</formula><p>where (·) + is the max(·, 0) operator. We compute the positive and negative changes separately as two masks D + and D − . To fuse t i , which is between t r and t c , these masks cannot be used directly as they are not aligned with the target frame t i . The key insight is that we can leverage the affinity matrix W in Eq. (2) computed by our spacetime memory reader ( <ref type="figure">Figure 5</ref>) for correspondence matching. The interacted frame t r and target frame t i are used as memory and query respectively. The aligned masks are For the interacted frame For every frame to be fused <ref type="figure">Figure 8</ref>. Mechanism of the difference-aware fusion module. The current propagated mask M r t i at frame It i is fused with the previous mask M r−1 t i , guided by the mask difference from interaction at frame tr. Only the negative terms D − , A − are shown here for clarity. Note that although a correct mask is captured in M r t i , it is non-trivial to pick it up in the fusion step as shown in <ref type="figure">Figure 9</ref>.</p><p>computed by two matrix products:</p><formula xml:id="formula_4">A + = WD + A − = WD −<label>(5)</label></formula><p>Where D + and D − are downsampled using area averaging to match the image stride of W, and the results are upsampled bilinearly to the original resolution. Additionally, traditional linear coefficients are also used to model possible decay during propagation:</p><formula xml:id="formula_5">n r = |t i − t r | |t c − t r | n c = |t i − t c | |t c − t r |<label>(6)</label></formula><p>Note that n r + n c = 1. Finally, the set of features (I ti , M r ti , M r−1 ti , A + , A − , n r , n c ) are fed into a simple five-layer residual network which is terminated by a sigmoid to output a final fused mask.</p><p>As illustrated in <ref type="figure">Figure 9</ref>, our fusion method can capture the user's intention as an aligned attention map, which allows our algorithm to propagate corrections beyond the mid-point. Such fusion cannot be achieved in previous linear-blending methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> (non-symmetric blending <ref type="bibr" target="#b10">[11]</ref> will fail if we swap the order of interaction). Evaluation of the fusion module is presented in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset: BL30K</head><p>High-quality VOS datasets are expensive to collect at a large scale -DAVIS <ref type="bibr" target="#b39">[41]</ref> is high-quality yet lacks quantity; YouTubeVOS <ref type="bibr" target="#b54">[56]</ref> is large but has moderate quality annotations. In this paper we contribute a new synthetic VOS dataset BL30K that not only is large-scale but also provides pixel-accurate segmentations. Using an open-source rendering engine Blender <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref>, we animate 51,300 three-dimensional models from <ref type="figure">Figure 9</ref>. Continuing <ref type="figure">Figure 8</ref>, showing popularly used linear blending is insufficient. Suppose the user first annotates tc = 25, then corrects the mask at tr = 89. For the query frame with ti = 51 which is closer to 25 than to 89, linear blending (or any symmetric function that only uses the temporal distance) fails in (d). With our difference aware fusion, we use the mask difference (e) to form an aligned attention (f) that captures the correction. Our result is shown in (g).</p><formula xml:id="formula_6">(a) It i (b) M r−1 t i (c) M r t i (d) Linear (e) D − (f) A − (g) Fused (h) GT</formula><p>ShapeNet <ref type="bibr" target="#b57">[59]</ref> and produce the corresponding RGB images and segmentations with a two-pass rendering scheme. Background images and object textures are collected using Google image search to enrich the dataset. Each video consists of 160 frames with a resolution of 768 × 512. Compared with FlythingThings3D <ref type="bibr" target="#b58">[60]</ref>, our videos have a higher frame rate and a much longer sequence length, making ours suitable for the VOS task while <ref type="bibr" target="#b58">[60]</ref> is not applicable. <ref type="figure">Figure 10</ref> shows one sample in our dataset. To the best of our knowledge, BL30K is the largest publicly available VOS dataset to date. Despite that the dataset being synthetic, it does significantly help in improving real-world performance as shown in our ablation study (Section 6.2). Note that this gain is not simply caused by more training iterations as extended training on YouTubeVOS <ref type="bibr" target="#b54">[56]</ref> and DAVIS <ref type="bibr" target="#b0">[1]</ref> leads to severe overfitting in our experiments. <ref type="figure">Figure 10</ref>. Sample data from the BL30K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>All three modules can be efficiently trained using just two 11GB GPU with the Adam optimizer <ref type="bibr" target="#b59">[61]</ref>. The propagation module is first trained on synthetic video sequences from static images following <ref type="bibr" target="#b6">[7]</ref>, which is then transferred to BL30K, YouTubeVOS <ref type="bibr" target="#b54">[56]</ref> and DAVIS <ref type="bibr" target="#b0">[1]</ref>. In each training iteration, we pick three random frames in a video sequence, with the maximum distance between frames increased from 5 to 25 gradually (curriculum learning) and annealed back to 5 toward the end of training <ref type="bibr" target="#b60">[62]</ref>. The S2M module is independently trained on static images only. The fusion module is trained with the output of a pretrained propagation module, first on BL30K, and then transferred to DAVIS <ref type="bibr" target="#b0">[1]</ref>. YouTubeVOS <ref type="bibr" target="#b54">[56]</ref> is not used here due to its less accurate annotation.  <ref type="table" target="#tab_5">Table 3</ref>. Running time analysis of each component in our model. Time is measured on the 480p DAVIS 2017 validation set; time for propagation is amortized. For an average of two objects in DAVIS 2017, our baseline performance matches the one reported in STM <ref type="bibr" target="#b13">[14]</ref>. Run time of f-BRS depends on the input as adaptive optimization is involved. Note that propagation is performed sparsely which keep our algorithm the fastest among competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">DAVIS Interactive Track</head><p>In the DAVIS 2020 Challenge <ref type="bibr" target="#b39">[41]</ref> interactive track, the robot first provides scribbles for a selected frame, waits for the algorithm's output, and then provides corrective scribbles for the worst frame of all the candidate frames listed by the algorithm. The above is repeated up to 8 rounds.</p><p>To demonstrate the effectiveness of our proposed decoupled method which requires less temporally dense interactions, we limit ourselves to interact with only three frames. Specifically, we force the robot to only pick a new frame in the 1 st , 4 th , and 7 th interactions. Our algorithm stays in an instant feedback loop for the same frame and performs propagation only when the robot has finished annotating one frame. Note that this behavior can be implemented without altering the official API. <ref type="table" target="#tab_6">Table 4</ref> tabulates the comparison results. <ref type="figure">Figure 11</ref> plots the performance measured on J &amp;F versus time. Note that, even with the above additional constraint, our method outperforms current state-of-the-art methods. We use the same GPU (RTX 2080Ti) as our closest competitor <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure">Figure</ref>   <ref type="table">Table 5</ref> tabulates the quantitative evaluation on the effectiveness of BL30K and the fusion module. We show that 1)  <ref type="table" target="#tab_5">100   0  1  2  3  4  5  6  7  8  9  10  11  12</ref> J&amp;F Time (seconds) <ref type="figure">Figure 11</ref>. J &amp;F performance on the DAVIS validation set. Clustered points represent real-time corrections in the instant feedback loop; each cluster represents a frame switch and propagation. Our method is highly efficient, achieving better performance in ∼12 seconds on average compared with 55+ seconds in <ref type="bibr" target="#b10">[11]</ref> or 37 seconds in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study</head><p>the proposed top-k memory read transfers well to the interactive setting, 2) BL30K helps in real-world tasks despite being synthetic, and 3) Difference-aware fusion module outperforms naïve linear blending and difference-agnostic (learnable) fusion with the same network architecture. Additionally, we show the upper bound performance of our method given perfect interaction masks.  <ref type="table">Table 5</ref>. Ablation study on the DAVIS interactive validation set. Our decoupled baseline already outperforms SOTA by a large margin. Despite the high baseline, we show that top-k memory filtering, pretraining in the BL30K dataset, and the difference-aware fusion module can further improve its performance. In the last row, we replace the interaction module with an oracle that provides ground-truth masks to evaluate the upper-bound of our method given perfect interactions in 3 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">User Study</head><p>We conduct a user study to quantitatively evaluate user's preferences and human effort required to label a video using iVOS algorithms. Specifically, we quantify the required human effort by the total user time which includes the time for interaction, searching, or pausing to think while excluding all computational time. We linearly interpolate the IoU versus user-time graph and compute the area under curve (AUC) for evaluation. We compare with ATNet <ref type="bibr" target="#b10">[11]</ref> which is the best performing method with available source code to the best of our knowledge. We use two variants of our method -one with S2M as the only interaction option (Ours-S2M), and the other allows users to use a combination of S2M, f-BRS <ref type="bibr" target="#b3">[4]</ref> and free-hand drawing, with the local control option (Ours-Free).</p><p>We recruited 10 volunteers who were given sufficient time to familiarize themselves with different algorithms and the GUI. They were asked to label 5 videos in the DAVIS 2017 multi-object validation set with satisfactory ATNet Ours ATNet Ours <ref type="figure" target="#fig_1">Figure 12</ref>. Top four rows: Qualitative comparison of our method with ATNet <ref type="bibr" target="#b10">[11]</ref> on the DAVIS interactive track (top two) and on previously unseen Internet video (middle two) with real user interactions (as detailed as possible on two frames). Bottom two rows: More results from our method on real-world videos from the Internet. Additional video results can be found on the project website. accuracy as fast as possible, within a 2-minute wall clock time limit. To avoid familiarity bias, they studied the images and ground truths of each video before each session. <ref type="figure" target="#fig_9">Figure 13</ref> shows the IoU versus user-time plot and <ref type="table">Table 6</ref> tabulates the average performance gain after each interaction. Our method achieves better results with less interaction time, while including more interaction options (f-BRS, free-hand drawing, and local control) which allows our method to converge faster and to a higher final accuracy for experienced users.  showing the interquartile range. Our methods achieve higher final accuracy and AUC than ATNet <ref type="bibr" target="#b10">[11]</ref>. In Ours-Free, users make use of f-BRS <ref type="bibr" target="#b3">[4]</ref> to obtain a faster initial segmentation. Experienced users can use free hand drawing and local control to achieve higher final accuracy given more time.  <ref type="table">Table 6</ref>. Mean incremental IoU improvement after each interaction round. ∆i denotes the IoU gain after the ith frame interaction and propagation. ATNet <ref type="bibr" target="#b10">[11]</ref> requires more interactions to achieve stable performance while ours achieves higher accuracy with less interactions. Enabling other interaction modes such as f-BRS or local control (Ours-Free) is beneficial to both the speed and the final accuracy. Note that sum does not equal to the final mean IoU in the left plot because not all users interacted for five rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose MiVOS, a novel decoupled approach consisting of three modules: Interaction-to-Mask, Propagation and Difference-Aware Fusion. By decoupling interaction from propagation, MiVOS is versatile and not limited by the type of interactions. On the other hand, the proposed fusion module reconciles interaction and propagation by faithfully capturing the user's intent and mitigates the information lost in the decoupling process, thus enabling MiVOS to be both accurate and efficient. We hope our MiVOS can inspire and spark future research in iVOS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>positions our MiVOS with other related works in interactive image/video object segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The local control pathway (red) uses an ROI to prevent deterioration spread by the global interaction path (blue) when only a small local refinement (around ears) is needed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Implementation of our space-time memory reader as described in Section 3.3. Tensor reshaping is performed when needed. Skip-connections from the query encoder to the decoder are omitted for clarity. Mean IoU drop along propagation with or without top-k filtering with the color bands showing the interquartile range (the higher the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of our propagation scheme. The frames between the current reference frame and previously interacted frame require fusion which is described in Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(AUC -74.93) Ours -Free (AUC -77.34)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Mean IoU versus user time plot with shaded regions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 Table 2 .</head><label>22</label><figDesc>Comparison between different VOS datasets. Only frames with publicly available ground truths are counted.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>compares the three</cell></row><row><cell>datasets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell># Videos</cell><cell># Frames</cell><cell>Label Quality</cell></row><row><cell>DAVIS [41]</cell><cell>90</cell><cell>6,208</cell><cell>High</cell></row><row><cell>YV [56]</cell><cell>3,471</cell><cell>94,588</cell><cell>Moderate</cell></row><row><cell>BL30K</cell><cell>29,989</cell><cell>4,783,680</cell><cell>High</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>tabulates the running time of different components in our model. Refer to our opensourced code for detailed hyperparameters settings. It takes about two weeks to train all the modules with two GPUs.</figDesc><table><row><cell></cell><cell>Time (ms) / frame / instance</cell></row><row><cell>Scribble-to-Mask (S2M)</cell><cell>29</cell></row><row><cell>f-BRS [4]</cell><cell>∼60</cell></row><row><cell>Propagation w/o top-k</cell><cell>51</cell></row><row><cell>Propagation w/ top-k</cell><cell>44</cell></row><row><cell>Fusion</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>12 provides qualitative comparisons and visual results. Performance on the DAVIS interactive validation set. Our method outperforms all competitors while receiving only interactions in 3 frames instead of 8. †Interpolated value @60s.</figDesc><table><row><cell>Methods</cell><cell>AUC-J</cell><cell>J  †</cell><cell cols="2">AUC-J &amp;F J &amp;F  †</cell></row><row><cell>Oh et el. [9]</cell><cell>69.1</cell><cell>73.4</cell><cell>-</cell><cell>-</cell></row><row><cell>MANet [12]</cell><cell>74.9</cell><cell>76.1</cell><cell>-</cell><cell>-</cell></row><row><cell>ATNet [11]</cell><cell>77.1</cell><cell>79.0</cell><cell>80.9</cell><cell>82.7</cell></row><row><cell>STM [14]</cell><cell>-</cell><cell>-</cell><cell>80.3</cell><cell>84.8</cell></row><row><cell>Ours</cell><cell>84.9</cell><cell>85.4</cell><cell>87.9</cell><cell>88.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The 2018 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ToG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep grabcut for object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">f-brs: Rethinking backpropagating refinement for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using sparse-to-dense networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Yeong Jun Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using global and local transfer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Yeong Jun Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory aggregated cfbi+ for interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Space-time memory networks for video object segmentation with user guidance. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00269</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation with multiple reference views, self refinement, and guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Cuong</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The-Anh</forename><surname>Vu-Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Triet</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatiotemporal cnn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglian</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning fast and robust target models for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">Jaremo</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Capsulevos: Semi-supervised video object segmentation using capsule routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravin</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ToG</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Livecut: Learning-based interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lazy snapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ToG</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Intelligent scissors for image composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACMCGIT</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interactive full image segmentation by considering all regions jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for fewshot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cascadepsp: Toward class-agnostic and very highresolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongje</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Euntai</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danelljan</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender Online Community</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Denninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Winkelbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Zidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Olefir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01911</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Blenderproc</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Angel Xuan Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bang Zhang, and Pan Pan. Spatial consistent memory network for semi-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
