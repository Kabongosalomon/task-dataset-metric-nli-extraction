<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cyclic Differentiable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">MSRA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">UCAS</orgName>
								<address>
									<region>CRIPAC, NLPR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">MSRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">MSRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">MSRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">UCAS</orgName>
								<address>
									<region>CRIPAC, NLPR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cyclic Differentiable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, differentiable architecture search has draw great attention due to its high efficiency and competitive performance. It searches the optimal architecture in a shallow network, and then measures its performance in a deep evaluation network. This leads to the optimization of architecture search is independent of the target evaluation network, and the discovered architecture is sub-optimal. To address this issue, we propose a novel cyclic differentiable architecture search framework (CDARTS). Considering the structure difference, CDARTS builds a cyclic feedback mechanism between the search and evaluation networks. First, the search network generates an initial topology for evaluation, so that the weights of the evaluation network can be optimized. Second, the architecture topology in the search network is further optimized by the label supervision in classification, as well as the regularization from the evaluation network through feature distillation. Repeating the above cycle results in a joint optimization of the search and evaluation networks, and thus enables the evolution of the topology to fit the final evaluation network. The experiments and analysis on CIFAR, ImageNet and NAS-Bench-201 demonstrate the efficacy of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has enabled remarkable progress in a variety of vision tasks over the past years. One crucial factor for this progress is the design of novel neural network architectures. Most of current employed architectures are designed by human experts, which is time-consuming and error-prone. Because of this, there is growing interest in automated Neural Architecture Search (NAS) for vision tasks, such as image recognition <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b3">4]</ref>, object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b64">65]</ref> and semantic segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Differentiable architecture search, i.e., DARTS <ref type="bibr" target="#b38">[39]</ref>, recently became popular in network architecture search due to its relatively low computation cost and competitive performance. Different from previous methods <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b67">68]</ref>    <ref type="figure">Figure 1</ref>: Comparisons of prior DARTS and our CDARTS. In prior DARTS <ref type="bibr" target="#b38">[39]</ref> and PDARTS <ref type="bibr" target="#b5">[6]</ref>, the target evaluation network does not involve into the progress of architecture search. In contrast, our CDARTS combines the search and evaluation networks into a joint optimization framework. The blue and gold boxes indicate the search and evaluation networks, respectively. relaxes the search space to be continuous, so that the architecture can be optimized by the gradient descent. The efficiency of gradient-based optimization allows DARTS to search a robust architecture within a few GPU days.</p><p>However, limited by the high GPU memory consumption, existing DARTS approaches have to divide the search process into two steps: search and evaluation, as shown in <ref type="figure">Fig. 1</ref>. The search step employs a small network to discover the optimal cell 1 structures, while the evaluation step stacks the discovered cells to construct a large network for final evaluations. This dividing results in the separation of the search and evaluation processes, thus leading to the search optimization of architectures is independent from the target evaluation network. As a consequence, the performance of discovered architectures in the search network has limited correlation with the actual performance of the evaluation network. The learned architecture hyperparameters is also insufficient to reflect the relative ranking of different architectures during evaluation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>To alleviate these issues, we propose a cyclic differentiable architecture search algorithm, dubbed CDARTS. It integrates the search and evaluation networks into a unified architecture, and jointly train the two networks in a cyclical way, as visualized in <ref type="figure">Fig. 1(c)</ref>. The shallow search network provides the best intermediate architectures to the evaluation network. In turn, the evaluation network (with higher model capacity due to more layers) distills the feature knowledge into the search network to enhance the search of architectures. Repeating this procedure leads to a cyclic optimization between the search and evaluation networks, and thus allows the evolution of cell structures to fit the final evaluation network.</p><p>Moreover, instead of training the search and evaluation networks separately, we propose an joint learning algorithm to optimize the integrated architecture in a cooperative manner. It consists of a pre-training stage and a joint learning stage. The pre-training stage aims to optimize the search and evaluation networks to have good initializations. The joint learning stage is to update the architecture topologies and network weights alternatively. Specifically, it first optimizes the architecture topologies and the evaluation network weights by jointly training the search and evaluation networks. Then, it optimizes the search network weights according to the updated architecture topologies. These two learning stages are performed alternatively, leading to a cyclic optimization between the search and evaluation networks. Eventually, the target evaluation network obtain a shaped topology tailored by the search network.</p><p>We evaluate our CDARTS algorithm on image classification task and conduct experiments on CIFAR <ref type="bibr" target="#b29">[30]</ref>, Ima-geNet <ref type="bibr" target="#b11">[12]</ref>, as well as the recent proposed NAS-Bench-201 benchmark <ref type="bibr" target="#b15">[16]</ref>. The experiments demonstrate that, on CI-FAR, CDARTS achieves superior performance to existing state-of-the-art DARTS approaches, such as PDARTS <ref type="bibr" target="#b5">[6]</ref>, PCDARTS <ref type="bibr" target="#b62">[63]</ref> and FairDARTS <ref type="bibr" target="#b9">[10]</ref>. Meanwhile, on the large-scale ImageNet, the proposed CDARTS also shows its superiority in DARTS families, while achieving comparable performance to MobileNet-V3 <ref type="bibr" target="#b24">[25]</ref>, which blends automatic search techniques with human intuition. Moreover, for a fair comparison with other NAS algorithms, such as one-shot <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and reinforcement learning-based <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b46">47]</ref> approaches, we conduct an experiment on NAS-Bench-201 benchmark, which provides a fixed search space with a unified training setting. The experiment shows that CDARTS achieves competitive performance compared with 10 recent prevailing NAS approaches. Last but not the least, the analysis on NAS-Bench-201 demonstrates the efficacy of our approach is due to the improved search stability. We observe that the performance of discovered architectures is improved gradually along with the increase of search epochs, and eventually reaches stable and converged. The correlation between the learned architecture hyperparameters and the true actual performance of discovered architectures is enhanced as well. We provide all information (training and test code, documentations) needed to reproduce the proposed approach and the results at https://github.com/ researchmm/CDARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early NAS approaches focus on searching a network motif by using either reinforcement learning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b67">68]</ref> or evolution algorithms <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60]</ref> and build a complete network for evaluation by stacking the motif. Unfortunately, these approaches require to train hundreds or thousand of candidate architectures from scratch, resulting in barely affordable computational overhead. Thus, several follow-up works are proposed to speed-up the training by reducing the search space <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>More recent works resort to the weight-sharing one-shot model to amortize the searching cost <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>. The key idea of one-shot approach is to train a single overparameterized model, and then share the weights between sampled child models. This weight sharing mechanism allows the searching of a high-quality architecture within a few GPU days <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">59]</ref>. Single path one-shot model families <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref> propose to train a single sampled path of the one-shot model in each iteration, rather than the entire over-parameterized model. Once the training process is finished, the child models can be ranked by the shared weights. Differentiable architecture search, i.e. DARTS <ref type="bibr" target="#b38">[39]</ref>, is another representative one-shot model. Instead of searching over a discrete set of candidate architectures, DARTS relaxes the search space to be continuous, so that the architecture can be optimized by the efficient gradient descent.</p><p>Despite DARTS has achieved promising performance by only using orders of magnitude less computation resources, it still has some drawbacks. ProxylessNAS <ref type="bibr" target="#b4">[5]</ref> argues that the optimization objectives of search and evaluation networks are inconsistent in DARTS <ref type="bibr" target="#b38">[39]</ref>. Thus it proposes to employ a binary connection to tackle the issue. PDARTS <ref type="bibr" target="#b5">[6]</ref> points out the depth gap problem of DARTS, and thereby presents a progressive learning approach, which gradually increases the number of layers in search network. Moreover, PCDARTS <ref type="bibr" target="#b62">[63]</ref> addresses the problem of high GPU memory cost through introducing a partially-connected strategy in network optimization.</p><p>Our proposed approach differs from existing DARTS algorithms <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10]</ref> on two fundermental points. One point is that our approach integrate the search and evaluation networks into a unified architecture. Also, we build up connections between the two networks, allowing the information exchange during the search of architectures. The other difference is the searching algorithm. In contrast to previous methods that the optimization of architecture hyperparameter is independent from the evaluation network, our approach enables the evaluation network to guide the search of architectures by joint training.</p><p>There are few recent one-shot methods trying to improve the search process by using the supervision of other highcapacity networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref>. Different from these methods uses either a human-designed teacher network <ref type="bibr" target="#b3">[4]</ref> or a high-capacity third-party model <ref type="bibr" target="#b32">[33]</ref>, the teacher network of our method is generated by the model itself. Hence, our model does not need to spend extra time to train a teach network alone. This is crucial for practical applications because there may be no available teacher model in new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cyclic Differentiable Architecture Search</head><p>In this section, we propose the cyclic differentiable architecture search approach. We first briefly review the individual learning algorithm of prior DARTS works, which train the search and evaluation network seperately <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10]</ref>. Then, we propose the joint learning algorithm to optimize the search and evaluation in a cyclical manner. Finally, we present the network architecture of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Previous Individual Learning</head><p>The goal of DARTS <ref type="bibr" target="#b38">[39]</ref> is to search a cell motif, which can be stacked repeatedly to construct a convolutional network. A cell is a directed acyclic graph consisting of an ordered sequence of N nodes {x i } N −1 i=0 . Each node x i is a latent representation (e.g., a feature map), while each directed edge (i, j) is associated with some operation o (i,j) which transforms the information from x i to x j . Let O denote the operation space, consisting of a set of candidate operations, such as convolution, max-pool, skip-connect, etc. Each operation represents a function o(·) to be performed on x i . To make the search space continuous, DARTS relaxes the choice of a particular operation to a softmax over all possible operations <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_0">o (i,j) (x i ) = o∈O exp(α (i,j) o ) o ∈O exp(α (i,j) o ) · o(x i ),<label>(1)</label></formula><p>where the operation weights for a pair of nodes (i, j) are parameterized by a vector α(i, j) of dimension |O|. Here, the parameter α is the encoding of architectures to be optimized. An intermediate node of the cell is computed based on all of its predecessors as x j = i&lt;jō (i,j) (x i ), and the output node x N −1 is obtained by concatenating all the intermediate nodes in the channel dimension.</p><p>With the definition of a cell, the search of the optimal architecture becomes to solve the following bilevel optimization problem:</p><formula xml:id="formula_1">min α L val (w * S , α), s.t. w * S = arg min w S L train (w S , α),<label>(2)</label></formula><p>where α is the architecture hyperparameters optimized on the validation data (val), and w S is the parameters of the search network learnt on the training data (train). Eq.(2) amounts to optimize the network and architecture hyperparameter, i.e., w S and α, in an alternative way. After getting the optimal architecture, DARTS constructs a new evaluation network by stacking the discovered neural cells and retrains from scratch over the training of the target task. From the above formulations, we can observe that the evaluation network does not involve into the search process of architectures, i.e., Eq.(2). As a consequence, the discovered architecture may be sub-optimal for the final evaluation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Joint Learning</head><p>Different from previous methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63]</ref> where the search network is separated from the evaluation network, our Cyclic DARTS integrates the two networks into a unified architecture and models the architecture search as a joint optimization problem of the two networks: min</p><formula xml:id="formula_2">α L val (w * E , w * S , α), s.t. w * S = arg min w S L train (w S , α), w * E = arg min w E L val (w E , α),<label>(3)</label></formula><p>where w E is the weight of evaluation network. To optimize this objective function, we propose an alternating learning algorithm, consisting of two iterative stages: a pretraining stage and a joint learning stage. The former is to pre-train the search and evaluation networks on the train and val datasets respectively, and enable them to have good initilization weights w S and w E . The latter is to learn the architecture hyperparameter α and the network weights jointly. These two stages are performed alternatively until convergence, leading to a combined cycle optimization between the network search and evaluation, as presented in Algorithm 1. This cyclic process allows the evolution of search architectures to fit the final target evaluation network.</p><p>Pre-training. The goal of this stage is to pre-train the search and evaluation networks individually and make them adaptive to the data. Specifically, for the search network, the architecture hyperparameter α is initialized randomly before training. Then, the weight w S is optimized on the train data as follows:</p><formula xml:id="formula_3">w * S = arg min w S L S train (w S , α),<label>(4)</label></formula><p>where L S train defines the loss function, and w * S denotes the learned weight. For image classification problem, we specify L S train as a cross-entropy loss. For the evaluation network, its internal cell structures are generated by discretizing the learned hyperparameter α. Following the previous work <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b38">39]</ref>, we retain the top-k (k = 2) operations for each node in the cells by thresholding the learned values in α. The pre-training of evaluation network is performed on val set through optimizing the following objective function: Update w S 16: end for where α indicates the top-k discretization of the continuous α, and w * S represents the learned weight. The pre-training of w S and w E enables the search and evaluation networks to obtain a good initialization.</p><formula xml:id="formula_4">w * E = arg min w E L E val (w E , α),<label>(5)</label></formula><p>Joint Learning. In this optimization stage, the search algorithm updates the architecture hyperparameter α with the feature feedback from the evaluation network through knowledge distillation. More concretely, the joint optimization of the two networks is formulated as:</p><formula xml:id="formula_5">α * , w * E = arg min α,w E L S val (w * S , α) + L E val (w E , α) +λ L S,E val (w * S , α, w E , α),<label>(6)</label></formula><p>where minimizing L S val (w * S , α) is to optimize the architecture hyperpaparameter α with a fixed weight w * S in the search network, L E val (w E , α) is to optimize the weight w E with a fixed architecture α in the evaluation network, and L S,E val (w * S , α, w E , α) allows the knowledge transfer from evaluation network to search network. L S,E val (·) employs the features derived from the evaluation network as a supervision signal to guide the updates of the architecture hyperparameter α in the search network. It is formulated by a soft target cross entropy function as</p><formula xml:id="formula_6">L S,E val (w * S , α, w E , α) = T 2 N N i=1 (p(w E , α)log( p(w E , α) q(w * S , α)</formula><p>)), <ref type="bibr" target="#b6">(7)</ref> where N is the number of training samples, T is a temperature coefficient (set to 2). Here, p(·) and q(·) represent the output feature logits of the evaluation and search network respectively, each of which is calculated as the soft target distribution <ref type="bibr" target="#b22">[23]</ref> over the feature logits, i.e.,</p><formula xml:id="formula_7">p(w E , α) = exp(f E i /T ) j exp(f E j /T ) , q(w * S , α) = exp(f S i /T ) j exp(f S j /T ) ,<label>(8)</label></formula><p>where f E i and f S i denote the features generated by the search and evaluation networks respectively (see <ref type="figure" target="#fig_2">Fig. 2</ref> for example). The optimization of Eq. (7) distills the feature knowledge of the evaluation network to guide the updates of architecture hyperparameter in the search network, while the joint training in Eq. (6) allows the knowledge transfer between the two networks.</p><p>In addition, it has been observed that DARTS algorithms tend to search the architectures with plenty of skipconnect operations rather than convolutions or poolings, because that the skip-connect allows rapid gradient descent <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>. This is essentially a kind of overfitting of architecture search. To address this issue, we propose to impose an 1 -norm regularization on the weight of the skipconnect operation in the architecture hyperparameter α as</p><formula xml:id="formula_8">L Reg = λ α 1 ,<label>(9)</label></formula><p>where · 1 represents the 1 norm, and λ is a positive tradeoff coefficient. Eq. (9) is finally optimized with Eq. (6) jointly as an auxiliary item to avoid overfitting.</p><p>It is worth noting that during the pre-training of the evaluation network, i.e., Eq. (5), we use a weight-sharing strategy for the update of weight w E to alleviate the insufficient training. More specifically, when the discretized architecture hyperparameter α is updated, the architecture of evaluation network will be changed accordingly. The weights of the new evaluation network are initialized with the parameters inheriting from previous training. In other words, the evaluation network has a one-shot model which shares weights between architectures that have common edges. This speeds up the mature convergence of the new evaluation network, thus elevating its capacity on feature representation. This weight-sharing strategy is different from that in single-path one-shot approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>, which perform random sampling for architecture selection. In contrast, our method selects architectures to be optimized by the search network, which alleviates the issue of unbalanced training in prior methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>The network architecture of the proposed Cyclic DARTS is presented in <ref type="figure" target="#fig_2">Fig. 2</ref>. It consists of two branches: a search network with a few stacked cells and an evaluation network with dozens of cells. Note that the search and evaluation  For information transfer, we build up connections between the two branches. More concretely, there is an topology transfer path delivering the discovered cell motifs from the search branch to the evaluation branch, as the top bold arrow line presented in <ref type="figure" target="#fig_2">Fig. 2</ref>. Note that the cell structure discovered by the search network is a fully connected graph due to the continuous relaxation of search space. In other words, all the candidate operations are applied to calculate the feature of each node in the graph, i.e. Eq (1). When using this continuous cell structure to construct a new evaluation network, we need to conduct discretization first. Following previous works <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b38">39]</ref>, we retain only the top-k (k = 2) strongest operations among all the candidate operations collected from all the previous nodes. This derived discrete cell structure serves as the basic building block for the evaluation branch.</p><p>On the other hand, there is another feature distillation path transferring the feature feedback of evaluation branch to the search branch, as the bottom solid arrow shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The feedback serves as the supervision signal for the search network to find better cell structures. In details, we use multi-level features of evaluation network as the feedback signals, since they are representative on capturing image semantics. As the lateral embedding connections presented in <ref type="figure" target="#fig_2">Fig. 2</ref>, the multi-level features combine low-resolution, semantically strong features with highresolution, semantically weak features. The features are derived from the outputs of each stage, and then passed through an embedding module to generate the corresponding feature logits. The function of the embedding module is to project the dense feature maps into a low dimensional subspace. The obtained logits of evaluation network is used as the supervision signals for search network through a soft cross-entropy layer, as the formulation in Eq. (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We evaluate the proposed CDARTS algorithm on image classification task and conduct a series of experiments on CIFAR <ref type="bibr" target="#b29">[30]</ref>, ImageNet <ref type="bibr" target="#b11">[12]</ref>, as well as the recent proposed NAS-Bench-201 benchmark <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>In Tab. 2, we elaborate the setting of the hyperparameters used in CDARTS on different benchmarks. The other settings are the same as DARTS families <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63]</ref> if not specified. During the search, the λ decays from 5 to 0 and remains to 0 in the last 10 epochs. In line with prior works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63]</ref>, the architectures found on CIFAR10 and ImageNet need to train from scratch. The retrain settings are the same as the ones in PCDARTS <ref type="bibr" target="#b62">[63]</ref> if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on NAS Benchmark</head><p>NAS-Bench-201 <ref type="bibr" target="#b15">[16]</ref> contains 15,625 neural cell candidates, covering all possible architectures generated by the fixed search space of 4 nodes and 5 associated operation options. It evaluates all the architectures on CIFAR10 <ref type="bibr" target="#b29">[30]</ref>, CIFAR100 <ref type="bibr" target="#b29">[30]</ref> and ImageNet-16-120 <ref type="bibr" target="#b6">[7]</ref>, and provides the corresponding performance. It also benchmarks 10 recent NAS algorithms on the search space using the same setup for fair a comparison.</p><p>According to the evaluation rules of NAS-Bench-201 <ref type="bibr" target="#b15">[16]</ref>, i.e., no regularization for a specific operation, report results of multiple searching runs, we remove the 1norm regularization imposed on skip-connect, i.e., Eq.(9), and report the results of three independent runs. All the DARTS-based methods perform 50 searching epochs, following the same setting as <ref type="bibr" target="#b15">[16]</ref>.</p><p>We first compare our method with the 10 NAS methods that have been benchmarked on NAS-Bench-201 <ref type="bibr" target="#b15">[16]</ref>, such as the evolution algorithm based REA <ref type="bibr" target="#b47">[48]</ref> and the one-shot based ENAS <ref type="bibr" target="#b46">[47]</ref>. As the results presented in Tab. 1, our CDARTS outperforms 9/10 methods on the three datasets. It achieves comparable performance to REA method <ref type="bibr" target="#b47">[48]</ref>,  while searching much faster. Both of REA and CDARTS perform superior to the human-design ResNet <ref type="bibr" target="#b21">[22]</ref>, being close to theoretical optimum on the benchmark, i.e., the "Optimal" in Tab. 1. The original DARTS is underperforming due to overfitting <ref type="bibr" target="#b15">[16]</ref>, and its standard deviation of performance is 0 because the final found architecture has plenty of skip-connections. In contrast, our CDARTS can perform well-balanced and achieves superior performance. The underlying reason is due to the proposed unified architecture, which allows the search network to discover architectures tailored for the target evaluation network. Except for our CDARTS, the performance of all the NAS approaches in Tab. 1 are provided by the official NAS-Bench-201 benchmark <ref type="bibr" target="#b15">[16]</ref>. We simply reuse the provided results for a fair comparison.</p><p>We conduct another experiment to evaluate the searching stability. With the NAS-Bench-201 <ref type="bibr" target="#b15">[16]</ref> benchmark, it is easy to track the validation and test accuracy of every discovered architecture after every training epochs, as visualized in <ref type="figure" target="#fig_3">Fig. 3</ref>. We can observe that DARTS approach <ref type="bibr" target="#b38">[39]</ref> achieves a relatively high accuracy at early stage, however, as the search process continues, its accuracy drops significantly and the stability becomes poor. Finally, DARTS falls into the overfitting status, in which the final architectures contains many skip-connections. In contrast, the performance of our CDARTS is improved gradually along with the increase of search epochs, and eventually reaches stable and converged. This may attribute to the knowledge transfer between the search and evalution networks. CDARTS continually leverages the supervision from the evaluation network to guide the search process, thus preventing the searched architecture from collapsing. Moreover, it is observed that the one-shot NAS method SETN <ref type="bibr" target="#b13">[14]</ref> performs inferiorly to our method in term of both accuracy and searching stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on CIFAR</head><p>The CIFAR image classification dataset contains two subsets, CIFAR10 and CIFAR100. CIFAR10 has 10 classes. Each class consists of 6,000 images, in which 5,000 images are used for training and 1,000 for testing. Similarly, CIFAR100 consists of 100 classes. Each class containing 600 images, where 500 images are used for training and the rest for testing.</p><p>We search the architecture on CIFAR10 test set, and evaluate it on both CIFAR10 and CIFAR100. The cell topologies discovered on CIFAR10 are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. We observe that the network prefers to choose separable convolutions <ref type="bibr" target="#b25">[26]</ref>, which is capable of improving model capacity and serves as a key component for network construction. The performance of discovered cells are reported in Tab. 3. It is observed that our CDARTS achieves superior performance to some ConvNets designed manually or automatically. For instance, CDARTS surpasses the humandesigned DenseNet-BC [27] by 1.18 and 3.93 points on CIFAR10 and CIFAR100, respectively. Compared with the original DARTS method <ref type="bibr" target="#b38">[39]</ref>, CDARTS also achieves better performance (97.52 ± 0.04% v.s. 97.00 ± 0.14%). CDARTS is also slightly superior to the recently proposed  <ref type="bibr" target="#b15">[16]</ref>. DARTSV1 and DARTSV2 indicate the first-order and second-order DARTS methods <ref type="bibr" target="#b38">[39]</ref>, whose results are provided by the benchmark <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR10</head><p>CIFAR100 ImageNet-16-120 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on ImageNet</head><p>ImageNet <ref type="bibr" target="#b30">[31]</ref> is a large-scale image classification dataset. It consists of 1,000 categories with 1.2 million training images and 50K validation images. Note that, for a fair comparison, the images in ImageNet are resized to 224×224 pixels in our experiments in line with prior DARTS works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="figure" target="#fig_4">Fig. 4</ref> shows a diagram of the discovered normal cell and reduction cell. We observe that the cells discovered on ImageNet is deeper than the ones on CIFAR10, because the classification on ImageNet is more complex. This is aligned with the evidence that increasing network depth is beneficial for elevating model capability <ref type="bibr" target="#b21">[22]</ref>. Moreover, the discovered cells on ImageNet contain larger convolution kernels (i.e., 5x5 sep_conv), which is helpful to improve model capacity.</p><p>The evaluation results of the searched architectures are reported in Tab. 4. It shows that the architectures discoverd by our CDARTS is slightly better than the manually designed models, such as MobileNet-V2 <ref type="bibr" target="#b49">[50]</ref> and Shuf-fleNet <ref type="bibr" target="#b66">[67]</ref>. Compared with automated methods, e.g., RLbased MobileNet-V3 <ref type="bibr" target="#b24">[25]</ref>, our gradient-based CDARTS achieves comparable performance. MobileNet-V3 blends automatic search techniques with the interaction of human design, while CDARTS is purely algorithmic. Moreover, CDARTS outperforms the prior DARTS <ref type="bibr" target="#b38">[39]</ref> by 3.0 points in term of top-1 accuracy. This improvements is induced by the proposed cyclic search algorithm. It is observed that  the flops of CDARTS is a little higher than other DARTS methods. However, this is solely caused by the search algorithm itself, because the search space is the same among these DARTS methods. To follow the mobile setting <ref type="bibr" target="#b38">[39]</ref> comparison, i.e., the number of multiply-add operations in the model is restricted to be less than 600M , we reduce the number of model channels from 48 to 44, while keeping the other settings unchanged. We get 75.9% top-1 accuracy with 571M flops, which is still slightly superior than PDARTS <ref type="bibr" target="#b5">[6]</ref> and PCDARTS <ref type="bibr" target="#b62">[63]</ref>. In addition, when training the discovered CIFAR10 cells on ImageNet, CDARTS obtains a top-1 accuracy is 75.6% which is on par with Im-ageNet cell 76.3%. This demonstrates the generalization potentials of CDARTS approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Component-wise Analysis. To further understand the effects of the components in the proposed cyclic search al-  gorithm, we test four variations of CDARTS on ImageNet dataset. In particular, each variation corresponds to an optimization objective listed in Tab. 5, and parameters for each model are tuned separately to obtain the optimal results. It is worth noting that the alternating optimization of L S train + L S val failed to search on ImageNet, whose cells are full of skip-connections. So we use the 1 -norm regularization to stabilize the searching process, and achieve a top-1 accuracy of 72.8%. By adding the proposed joint learning L S,E val into optimization, the performance is improved to 75.7%. This indicates the effectiveness of the multi-level feature semantics guidance of the evaluation network, and the integration of the search and evaluation is beneficial to discover more robust architecture. Moreover, during joint training, updating the weights of evaluation network can further improve the performance by 0.9%.</p><p>Correlation Analysis. In differentiable architecture search methods, the operations and edges with weak attention (i.e., small weights) are considered as redundant and are pruned to obtain a compact architecture (i.e., top-k discretization). However, it is not clear whether the operations and edges with weak attention are redundant, while strong attention indicates the high importance. Therefore, we conduct another experiment to evaluate the correlation between the architecture hyperparameter and the true performance of architectures. Specifically, we sample a variety of cell architectures and rank them according to the learned weights of architecture hyperparameter. The quantitative results shown in <ref type="figure">Fig. 5</ref> demonstrate that our method obtain better correla-tion than DARTS approach <ref type="bibr" target="#b38">[39]</ref>. To some extent, the architecture hyperparameter in CDARTS is able to reflect the relative ranking of architectures. But it is worth noting that CDARTS still cannot well distinguish the architectures with close performance.</p><p>Depth of Evaluation Network. Due to the limitation of GPU memory, the search network of DARTS can only stack 8 cells, while the evaluation network contains 20 cells. This brings the so-called depth gap issue <ref type="bibr" target="#b5">[6]</ref> studied in PDARTS <ref type="bibr" target="#b5">[6]</ref>. We show that such problem does not exist in our method, because we integrate the search and evaluation into a unified architecture. As shown in <ref type="figure">Fig. 6</ref>, we compare the performances of different number of cells in the evaluation network. It clearly shows that the 20-cell evaluation network (the red line) performs better than the 8-cell network (the green line). This the proposed jointly training of the two networks mitigates the impacts of the depth gap.</p><p>Impact of Search Epoch. We further study the impact of the number of search epochs. From <ref type="figure">Fig. 6</ref>, we can see that when the number of search epoch approaches to 30, the performance becomes saturated, also the structure of the evaluation network tends to be stable. So the search epoch is set to 30 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, motivated by the separation problem of the search and evaluation networks in DARTS, we proposed a cyclic differentiable search algorithm which integrates the two networks into a unified architecture. The alternating joint learning enables the search of architectures to fit the final evaluation network. Experiments demonstrate the efficacy of the proposed algorithm and searched architectures, which achieve competitive performance on CIFAR, Ima-geNet and NAS-Bench-201. In the future work, we plan to augment the architecture search with more operations and migrate the algorithm to other vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. CIFAR10</head><p>To train the networks, we divide the 50K training images of CIFAR10 <ref type="bibr" target="#b29">[30]</ref> into two equal parts. One part serves as the train set to learn the weight w S of the search network, while the other part works as the val set to optimize architecture hyperparameter α and the evaluation network weights w E . During training, the total number of search step S S is set to 30 epochs, the evaluation step S E and the update step S U are both set to 1 epoch. When training the weight w S and w E individually, the batch size, learning rate, momentum and weight decay are set to 64, 0.1, 0.9 and 3 × 10 −4 , respectively. When jointly updating the α and w E , we adopt Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with a fixed learning rate of 3 × 10 −4 . The momentum and weight decay are set to {0.5, 0.99} and 3 × 10 −4 , respectively. The coefficient λ is fixed to 4, while λ decays from 5 to 0 in the first 20 epochs and remains at 0 in the last 10 epochs.</p><p>After discovering the evaluation network architecture, we retrain it by following the same setting as PDARTS <ref type="bibr" target="#b5">[6]</ref>. Specifically, the number of channels is set to 36 and the structure is the same with the search stage. We use the entire CIFAR training images to train the network from scratch with 600 epochs. To speed up the training, the batch size is set to 128 and the learning rate decays from 0.025 to 0 with a cosine annealing <ref type="bibr" target="#b40">[41]</ref>. We choose SGD <ref type="bibr" target="#b48">[49]</ref> as the optimizer with a momentum of 0.9 and weight decay of 5 × 10 −4 . In line with PDARTS, the drop-path <ref type="bibr" target="#b31">[32]</ref> rate is set to 0.3, the auxiliary towers <ref type="bibr" target="#b52">[53]</ref> is set to 0.4, and the cutout regularization <ref type="bibr" target="#b12">[13]</ref> length is set to 16. The experiments are executed on one NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. ImageNet</head><p>ImageNet <ref type="bibr" target="#b30">[31]</ref> is much larger than CIFAR <ref type="bibr" target="#b29">[30]</ref> in both scale and complexity. In line with prior works, i.e., PC-DARTS <ref type="bibr" target="#b62">[63]</ref> and DARTS+ <ref type="bibr" target="#b34">[35]</ref>, we randomly sample 10% images from ImageNet to compose the train data, while sampling another 10% images to form the val data. The construction of networks tested on ImageNet is similar to the one on CIFAR, but has two difference. First, to align with previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b62">63]</ref> on ImageNet, we set the number of cells to 8 and 14 for search and evaluation networks respectively. Second, the stem layer contains 3 convolution operations reducing the feature size from 224×244 to 28×28, which shares the same setting as DARTS <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63]</ref>. During search, we first pre-train the search network with 10 epochs when the architecture hyperparameter is fixed. Then, the number of search step S S is set to 30 epochs, while the evaluation step S E is set to 3 epochs.</p><p>When training the weight w S and w E individually, the batch size, learning rate, momentum and weight decay are set to 1024, 0.8, 0.9 and 3 × 10 −5 , respectively. When jointly updating the α and w E , we adopt Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with a fixed learning rate of 3 × 10 −4 . The momentum and weight decay are set to {0.5, 0.99} and 3 × 10 −5 , respectively. The coefficient λ is fixed to 4, and λ decays from 5 to 0 in the first 20 epochs and remains at 0 in the rest epochs. We use 8 NVIDIA Tesla V100 GPUs with a batch size of 1024 to search on ImageNet. It takes about 5 hours with our Py-Torch <ref type="bibr" target="#b45">[46]</ref> implementation.</p><p>Once the search process is completed, we retrain the discovered architecture following the same setting as PDARTS <ref type="bibr" target="#b5">[6]</ref>. The final evaluation network contains 14 cells with a channel number of 48. We retrain it for 250 epochs from scratch with full ImageNet training data. A standard SGD <ref type="bibr" target="#b48">[49]</ref> optimizer is adopted with a momentum of 0.9 and weight decay of 3×10 −5 . The learning rate is set to 0.5 with cosine scheduler, meanwhile, a learning rate warmup <ref type="bibr" target="#b19">[20]</ref> is applied in the first 5 epochs. The same as DARTS <ref type="bibr" target="#b38">[39]</ref>, the label smoothing <ref type="bibr" target="#b54">[55]</ref> ratio is set to 0.1 and the auxiliary tower is set to 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Independent Searches</head><p>In this section, we provide more comparisons and visualization results to verify the stability of the proposed CDARTS approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. CIFAR10</head><p>We report the performance of five individual runs of different search algorithms in Tab. 6. In the five independent runs, CDARTS consistently outperforms prior DARTS <ref type="bibr" target="#b38">[39]</ref> and PCDARTS <ref type="bibr" target="#b62">[63]</ref>. Moreover, in terms of performance deviations, CDARTS also performs better than prior methods, i.e., CDARTS: 97.52 ± 0.04% v.s. DARTSV1 <ref type="bibr" target="#b38">[39]</ref>: 96.93 ± 0.13%, DARTSV2: 96.85 ± 0.29%, PC-DARTS: 97.33 ± 0.07%. The cell motifs discovered on CIFAR-10 are presented in <ref type="figure">Fig. 7</ref>. We observe that the network prefers to choose separable convolutions <ref type="bibr" target="#b25">[26]</ref>, which is capable of improving model capacity and serves as a key component for network construction. Besides, these cells usually contain either one or two skip-connects, and the depth of these cell are usually two (the longest path from the input nodes to the output node).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. ImageNet</head><p>In Tab. 7, we present the results of five independent searches on ImageNet. We observe that the five runs of our approach consistently exceed that of PDARTS (75.6%) and PCDARTS (75.8%). The discovered cells on ImageNet are presented in <ref type="figure">Fig. 8</ref>. We observe that the cells discovered  on ImageNet is much deeper than those on CIFAR10, because the classification on ImageNet is more complex. This is aligned with the evidence that increasing network depth is beneficial for elevating model capability <ref type="bibr" target="#b21">[22]</ref>. Moreover, the discovered cells on ImageNet also contain larger convolution kernels (i.e., 5x5 sep_conv), which is helpful to improve model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of Big Model</head><p>To further unleash the power of the searched architectures, we enlarge the evaluation network channels and train from scratch with more data augmentations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, which is denoted as big model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Big Model on CIFAR10</head><p>We enlarge the numbers of feature channels from 36 to 50. After 2000 epochs training, CDARTS-BIG model obtains impressive performance improvements. On CIFAR10, the test accuracy increase from 97.52% to 98.32%. This improvement further verifies the effectiveness of the proposed cyclic search method. In addition, to have a fair comparison, we retrain the evaluation networks discovered by other DARTS methods, such as DARTS <ref type="bibr" target="#b38">[39]</ref>, PDARTS <ref type="bibr" target="#b5">[6]</ref> and PCDARTS <ref type="bibr" target="#b62">[63]</ref> with the same big setting. As the results presented in Tab. 8, our CDARTS performs the best among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Big Model on ImageNet</head><p>We increase the channel number from 48 to 96 and the input image size from 224×224 to 320×320 to construct a big model with 5.6B flops. This big model is trained for 250 epochs, and obtains a 4.86 points absolute improvement over the original CDARTS, achieving 81.12 top-1 accuracy on ImageNet, which is comparable to EfficientNet-B2 <ref type="bibr" target="#b56">[57]</ref>. This verifies the effectiveness and generalizability of the proposed cyclic differentiable architecture search algorithm. Following the same settings, we train the big models of PDARTS and PCDARTS. Their performances are inferior to our CDARTS, as presented in Tab. 8. This demonstrates the generalization potentials of CDARTS approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>Algorithm complexity. The complexity of CDARTS is comparable to DARTSV1. Both of them adopt the firstorder optimization method <ref type="bibr" target="#b38">[39]</ref> to train the networks. Compared with the original DARTS <ref type="bibr" target="#b38">[39]</ref>, we have an additional evaluation network and update it along with the search network in the search process. We denote the complexity of updating the search network as O(|W S |), which is the same as DARTSV1. In the search network cell, the number of edges between nodes is ED S and the number operations in each edge is OP S . The number of stacked cells in the search network is N S . Correspondingly, these factors of the eval-uation network are denoted as ED E , OP E and N E . Then the complexity of the evaluation network O(|W E |) is:</p><formula xml:id="formula_9">O(|W E |) = N E · ED E · OP E N S · ED S · OP S · O(|W S |)<label>(10)</label></formula><p>In the experiments on CIFAR10, the ED E , OP E and N E are set to 8, 1, and 20, respectively, while the ED S , OP S and N S are set t o14, 8, and 8. Hence, the complexity of the evaluation network O(|W E |) is about <ref type="bibr">1 6</ref> of O(|W S |). Compare to these two networks, the complexity of the embedding modules is much smaller and can be ignored in complexity estimation. Considering the initialization stage of the evaluation network (i.e., one training epoch for CI-FAR10), the final complexity of the evaluation network is about <ref type="bibr">1 3</ref> of O(|W S |). In ImageNet, the final complexity of the evaluation network is about <ref type="bibr">1 2</ref> of O(|W S |). Therefore, compared to DARTSV1 <ref type="bibr" target="#b38">[39]</ref>, the complexity of CDARTS is only increased by ∼0.3 times.</p><p>Fast search speed on ImageNet. It is worth noting that our method takes about five hours to complete the search with eight GPUs on ImageNet. Such a quick search speed mainly attributes to the following three reasons. First, we use the fast first-order optimization algorithm <ref type="bibr" target="#b38">[39]</ref> when updating the hyperparameter of architecture. Second, following PC-DARTS <ref type="bibr" target="#b62">[63]</ref>, only 10% data in each category are used. Besides, the evaluation network is relatively lightweight and adopts the weight sharing strategy to speed up network training, so the extra computational cost of updating its parameters is small.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>which search over a discrete set of candidate architectures, DARTS * Work performed when Hongyuan was an intern of Microsoft Research Asia. † Corresponding author: houwen.peng@microsoft.com.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed cyclic DARTS. Our model contains two networks, the search network(left) and the evaluation network(right). The Embedding module maps each stage feature map to a one-dimensional vector.networks share the same architectures with prior DARTS approaches<ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b9">10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Test and validation accuracy(mean ± std) v.s. search epochs on NAS-Bench-201</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Discovered cells. ImageNet cells are more deeper than CIFAR10. The normal cell discovered on ImageNet (b) The reduction cell discovered on CIFAR10 (d) The reduction cell discovered on ImageNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Operation weights and retrain accuracy correlation analysis. Weights Rank 1-7 indicates the weight sorting of the learned architecture hyperparameter from small to large. Ablation of search epochs and the depth of evaluation network. Evaluation-8/20 represent the evaluation networks with 8/20 cells, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Discovered normal and reduction cells on CIFAR10. Discovered normal and reduction cells on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Cyclic DARTSInput: The train and val data, search and evaluation iterations S S and S E , update iterations S U , architecture hyperparameter α, and weights w S and w E for search S-Net and evaluation E-Net.</figDesc><table><row><cell cols="2">Output: Evaluation network.</cell></row><row><cell cols="2">1: Initialize α with randoms</cell></row><row><cell cols="2">2: Initialize w S</cell></row><row><cell cols="2">3: for each search step i ∈ [0, S S ] do</cell></row><row><cell>4:</cell><cell>if i M od S U = 0 then</cell></row><row><cell>5:</cell><cell>Discretize α to α by selecting the top-k</cell></row><row><cell>6:</cell><cell>Generate E-Net with α</cell></row><row><cell>7:</cell><cell>for each evaluation step j ∈ [0, S E ] do</cell></row><row><cell>8:</cell><cell>Calculate L E val according to Eq.(5)</cell></row><row><cell>9:</cell><cell>Update w E</cell></row><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell>12:</cell><cell>Calculate L S val , L E val and L S,E val according to Eq.(6)</cell></row><row><cell>13:</cell><cell>Jointly update α and w E</cell></row><row><cell>14:</cell><cell>Calculate L S train according to Eq.(4)</cell></row><row><cell>15:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the 10 NAS methods provided by NAS-Bench-201 benchmark<ref type="bibr" target="#b15">[16]</ref>.Optimal indicates the best performing architecture in the search space. 19±0.31 93.92±0.30 71.81±1.12 71.84±0.99 45.15±0.89 45.54±1.03 RS [2] 90.93±0.36 93.70±0.36 70.93±1.09 71.04±1.07 44.45±1.10 44.57±1.25 RL [58] 91.09±0.37 93.85±0.37 71.61±1.12 71.71±1.09 45.05±1.02 45.24±1.18 BOHB [17] 90.82±0.53 93.61±0.52 70.74±1.29 70.85±1.28 44.26±1.36 44.42±1.49 ENAS [47] 39.77±0.00 54.30±0.00 15.03±0.00 15.61±0.00 16.43±0.00 16.32±0.00 RSPS [34] 84.16±1.69 87.66±1.69 59.00±4.60 58.33±4.34 31.56±3.28 31.14±3.88 GDAS [15] 90.00±0.21 93.51±0.13 71.14±0.27 70.61±0.26 41.70±1.26 41.84±0.90 SETN [14] 82.25±5.17 86.19±4.63 56.86±7.59 56.87±7.77 32.54±3.63 31.90±4.07 DARTSV1 [39] 39.77±0.00 54.30±0.00 15.03±0.00 15.61±0.00 16.43±0.00 16.32±0.00 DARTSV2 [39] 39.77±0.00 54.30±0.00 15.03±0.00 15.61±0.00 16.43±0.00 16.32±0.00 CDARTS 91.13±0.44 94.02±0.31 72.12±1.23 71.92±1.30 45.09±0.61 45.51±0.72</figDesc><table><row><cell>Method</cell><cell>CIFAR10 validation</cell><cell>test</cell><cell cols="2">CIFAR100 validation</cell><cell>test</cell><cell cols="2">ImageNet-16-120 validation test</cell></row><row><cell>ResNet [22]</cell><cell>90.83</cell><cell>93.97</cell><cell>70.42</cell><cell cols="2">70.86</cell><cell>44.53</cell><cell>43.63</cell></row><row><cell>Optimal</cell><cell>91.61</cell><cell>94.37</cell><cell>73.49</cell><cell cols="2">73.51</cell><cell>46.77</cell><cell>47.31</cell></row><row><cell>REA [48]</cell><cell>91.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The hyperparameters used in search. BS, LR, OPT, MOME and WD are the batch size, learning rate, optimizer, momentum and weight decay used in both the search and the evaluation network, respectively. λ and λ are the coefficients in Eq. 6 and Eq. 9. Runs means the number of independent runs. The unit s of SS, SE and SU in Algorithm 1 represents the number of steps in one epoch.</figDesc><table><row><cell>Benchmark</cell><cell>BS</cell><cell>LR</cell><cell>OPT</cell><cell>MOME</cell><cell>WD</cell><cell>SS</cell><cell>SE</cell><cell>SU</cell><cell>λ</cell><cell>λ</cell><cell>Runs</cell></row><row><cell>NAS-Bench-201</cell><cell>64</cell><cell>0.1</cell><cell>SGD</cell><cell>0.9</cell><cell>3e-4</cell><cell>50s</cell><cell>1s</cell><cell>1s</cell><cell>4</cell><cell>5</cell><cell>3</cell></row><row><cell>CIFAR10</cell><cell>64</cell><cell>0.1</cell><cell>SGD</cell><cell>0.9</cell><cell>3e-4</cell><cell>30s</cell><cell>1s</cell><cell>1s</cell><cell>4</cell><cell>5</cell><cell>5</cell></row><row><cell>ImageNet</cell><cell>1024</cell><cell>0.8</cell><cell>SGD</cell><cell>0.9</cell><cell>3e-5</cell><cell>30s</cell><cell>3s</cell><cell>1s</cell><cell>4</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art architectures on CIFAR10 and CIFAR100. † We use the same search space as DARTS<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Test Top-1 Acc. (%) CIFAR10 CIFAR100</cell><cell cols="2">Param Search Cost (M) (GPU days)</cell><cell>Search Method</cell></row><row><cell>Wide ResNet [66]</cell><cell>96.20</cell><cell>82.70</cell><cell>36.5</cell><cell>-</cell><cell>manual</cell></row><row><cell>ResNeXt-29, 16x64d [61]</cell><cell>96.42</cell><cell>82.69</cell><cell>68.1</cell><cell>-</cell><cell>manual</cell></row><row><cell>DenseNet-BC [27]</cell><cell>96.52</cell><cell>82.82</cell><cell>25.6</cell><cell>-</cell><cell>manual</cell></row><row><cell>NASNet-A [69]</cell><cell>-</cell><cell>97.35</cell><cell>3.3</cell><cell>1800</cell><cell>RL</cell></row><row><cell>AmoebaNet-B [48]</cell><cell>97.45</cell><cell>-</cell><cell>2.8</cell><cell>3150</cell><cell>evolution</cell></row><row><cell>PNAS [37]</cell><cell>96.59</cell><cell>-</cell><cell>3.2</cell><cell>225</cell><cell>SMBO</cell></row><row><cell>ENAS [47]</cell><cell>97.11</cell><cell>-</cell><cell>4.6</cell><cell>0.5</cell><cell>RL</cell></row><row><cell>NAONet [42]</cell><cell>96.82 1</cell><cell>84.33</cell><cell>10.6</cell><cell>200</cell><cell>NAO</cell></row><row><cell>SNAS (moderate) [62]</cell><cell>97.15 ± 0.02</cell><cell>-</cell><cell>2.8</cell><cell>1.5</cell><cell>gradient</cell></row><row><cell>ProxylessNAS [5]</cell><cell>97.92</cell><cell>-</cell><cell>5.7</cell><cell>4</cell><cell>gradient</cell></row><row><cell>Random  †</cell><cell>96.75 ± 0.18</cell><cell>-</cell><cell>3.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DARTSV1 [39]  †</cell><cell>97.00 ± 0.14</cell><cell>82.24</cell><cell>3.3</cell><cell>1.5</cell><cell>gradient</cell></row><row><cell>DARTSV2 [39]  †</cell><cell>97.24 ± 0.09</cell><cell>82.46</cell><cell>3.3</cell><cell>4.0</cell><cell>gradient</cell></row><row><cell>PDARTS [6]  †</cell><cell>97.50</cell><cell>83.45</cell><cell>3.4</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PCDARTS [63]  †</cell><cell>97.43 ± 0.06</cell><cell>-</cell><cell>3.6</cell><cell>0.1</cell><cell>gradient</cell></row><row><cell>FairDARTS [10]  †</cell><cell>97.41 ± 0.14</cell><cell>-</cell><cell>3.8</cell><cell>0.1</cell><cell>gradient</cell></row><row><cell>CDARTS(Ours)  †</cell><cell>97.52 ± 0.04</cell><cell>84.31</cell><cell>3.8</cell><cell>0.3</cell><cell>gradient</cell></row></table><note>PDARTS [6] and PCDARTS [63] on both CIFAR10 and CI- FAR100. It is worthy noting that the performance of Prox- ylessNAS [5] is slightly better than ours, but its model pa- rameter size is larger and the search time is longer as well. Moreover, compared with other non-gradient based meth- ods, such as reinforcement learning (RL) or evolutionary al- gorithms, our method is also competitive.For example, the RL-based ENAS [47] method achieves 97.11% test accu- racy on CIFAR10, which is slightly inferior to 97.52% of CDARTS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on ImageNet.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Test Acc. (%) Top-1 Top-5</cell><cell>Params (M)</cell><cell>×+ (M)</cell><cell cols="2">Search Cost Search Method (GPU days)</cell></row><row><cell>Inception-V1 [54]</cell><cell>69.8</cell><cell>89.9</cell><cell>6.6</cell><cell>1448</cell><cell>-</cell><cell>manual</cell></row><row><cell>SqueezeNext [19]</cell><cell>67.5</cell><cell>88.2</cell><cell>3.23</cell><cell>708</cell><cell>-</cell><cell>manual</cell></row><row><cell>MobileNet-V2 (1.4×) [50]</cell><cell>74.7</cell><cell>-</cell><cell>6.9</cell><cell>585</cell><cell>-</cell><cell>manual</cell></row><row><cell>ShuffleNet-V2 (2×) [43]</cell><cell>74.9</cell><cell>-</cell><cell>7.4</cell><cell>591</cell><cell>-</cell><cell>manual</cell></row><row><cell>NASNet-A [69]</cell><cell>74.0</cell><cell>91.6</cell><cell>5.3</cell><cell>564</cell><cell>1800</cell><cell>RL</cell></row><row><cell>AmoebaNet-C [48]</cell><cell>75.7</cell><cell>92.4</cell><cell>6.4</cell><cell>570</cell><cell>3150</cell><cell>evolution</cell></row><row><cell>PNAS [37]</cell><cell>74.2</cell><cell>91.9</cell><cell>5.1</cell><cell>588</cell><cell>225</cell><cell>SMBO</cell></row><row><cell>MnasNet-92 [56]</cell><cell>74.8</cell><cell>92.0</cell><cell>4.4</cell><cell>388</cell><cell>-</cell><cell>RL</cell></row><row><cell>EfficientNet-B0 [57]</cell><cell>76.3</cell><cell>93.2</cell><cell>5.3</cell><cell>390</cell><cell>-</cell><cell>RL</cell></row><row><cell>SPOS [21]</cell><cell>74.3</cell><cell>-</cell><cell>-</cell><cell>319</cell><cell>-</cell><cell>evolution</cell></row><row><cell>FairNAS-A [9]</cell><cell>75.3</cell><cell>92.4</cell><cell>4.6</cell><cell>388</cell><cell>-</cell><cell>evolution</cell></row><row><cell>MobileNet-V3 [25]</cell><cell>76.6</cell><cell>-</cell><cell>7.5</cell><cell>356</cell><cell>-</cell><cell>RL</cell></row><row><cell>MoGA-A [8]</cell><cell>75.9</cell><cell>92.8</cell><cell>5.1</cell><cell>304</cell><cell>12</cell><cell>evolution</cell></row><row><cell>SNAS (mild) [62]</cell><cell>72.7</cell><cell>90.8</cell><cell>4.3</cell><cell>522</cell><cell>1.5</cell><cell>gradient</cell></row><row><cell>ProxylessNAS [5]</cell><cell>75.1</cell><cell>92.5</cell><cell>7.1</cell><cell>465</cell><cell>8.3</cell><cell>gradient</cell></row><row><cell>ASAP [45]</cell><cell>73.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2</cell><cell>gradient</cell></row><row><cell>DARTS [39]  †</cell><cell>73.3</cell><cell>91.3</cell><cell>4.7</cell><cell>574</cell><cell>4.0</cell><cell>gradient</cell></row><row><cell>FairDARTS [10]  †</cell><cell>75.6</cell><cell>92.6</cell><cell>4.3</cell><cell>440</cell><cell>3.0</cell><cell>gradient</cell></row><row><cell>PDARTS [6]  †</cell><cell>75.6</cell><cell>92.6</cell><cell>4.9</cell><cell>557</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PCDARTS [63]  †</cell><cell>75.8</cell><cell>92.7</cell><cell>5.3</cell><cell>597</cell><cell>3.8</cell><cell>gradient</cell></row><row><cell>Ours(MS)  †</cell><cell>75.9</cell><cell>92.6</cell><cell>5.4</cell><cell>571</cell><cell>1.7</cell><cell>gradient</cell></row><row><cell>Ours  †</cell><cell cols="2">76.3 ± 0.3 92.9 ± 0.2</cell><cell>6.1</cell><cell>732</cell><cell>1.7</cell><cell>gradient</cell></row></table><note>† We use the same search space as DARTS [39]. MS denotes mobile setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on ImageNet.</figDesc><table><row><cell>L S train + L S val</cell><cell></cell><cell></cell></row><row><cell>+ LReg</cell><cell></cell><cell></cell></row><row><cell>+ L S,E val + L E val</cell><cell></cell><cell></cell></row><row><cell>Top-1 Acc. (%)</cell><cell>-</cell><cell>72.8 75.7 76.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Top-1 accuracy of searched architectures in five independent search runs on CIFAR10.</figDesc><table><row><cell>Methods</cell><cell>#1</cell><cell>#2</cell><cell>Runs #3</cell><cell>#4</cell><cell>#5</cell><cell>Mean±std</cell></row><row><cell cols="7">DARTSV1 [39](%) 97.11 96.85 97.01 96.93 96.73 96.93±0.13</cell></row><row><cell cols="7">DARTSV2 [39](%) 96.89 96.32 97.23 96.86 96.94 96.85±0.29</cell></row><row><cell cols="7">PCDARTS [63](%) 97.28 97.33 97.43 97.25 97.36 97.33±0.07</cell></row><row><cell>Ours(%)</cell><cell cols="6">97.53 97.45 97.60 97.52 97.54 97.52±0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Evaluations of searched architectures in five independent search runs on ImageNet.</figDesc><table><row><cell>Accuracy</cell><cell>#1</cell><cell>#2</cell><cell>Runs #3</cell><cell>#4</cell><cell>#5</cell><cell>Mean±std</cell></row><row><cell>Top1(%)</cell><cell cols="6">76.45 75.90 76.61 76.40 75.94 76.26±0.29</cell></row><row><cell>Top5(%)</cell><cell cols="6">92.83 92.80 93.20 93.03 92.75 92.92±0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Top-1 accuracy of big models.</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR10 ImageNet</cell></row><row><cell>DARTS</cell><cell>97.95</cell><cell>-</cell></row><row><cell>PDARTS</cell><cell>98.00</cell><cell>80.04</cell></row><row><cell>PCDARTS</cell><cell>97.92</cell><cell>79.81</cell></row><row><cell>Ours</cell><cell>98.32</cell><cell>81.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Cell is a basic building block for network construction. It consists of convolution, pooling, nonlinearity and a prudent selection of connections.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01314</idno>
		<title level="m">Searching beyond mobilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12126</idno>
		<title level="m">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeezenext: Hardware-aware neural network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. Dauphin David Lopez-Paz Hongyi</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cisse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards oracle knowledge distillation with neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13019</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Blockwisely supervised neural architecture search with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuchun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13053</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darts+</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Search to distill: Pearls are everywhere but not the eyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09074</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast neural architecture search of compact semantic segmentation models via auxiliary cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04123</idno>
		<title level="m">Asap: Architecture search, anneal and prune</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nas evaluation is frustratingly hard. ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detnas: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu Zhang Gaofeng Meng Xinyu Xiao Jian Sun Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
