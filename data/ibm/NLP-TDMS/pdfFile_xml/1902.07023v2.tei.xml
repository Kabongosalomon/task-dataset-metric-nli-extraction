<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Walk-based Model on Entity Graphs for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Centre for Text Mining</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Artificial Intelligence Research Center (AIRC)</orgName>
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<postCode>468-8511</postCode>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Artificial Intelligence Research Center (AIRC)</orgName>
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
							<email>sophia.ananiadou@manchester.ac.ukmakoto-miwa@toyota-ti.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Centre for Text Mining</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Artificial Intelligence Research Center (AIRC)</orgName>
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Walk-based Model on Entity Graphs for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to l-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-ofthe-art systems on the ACE 2005 dataset without using any external tools 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) is a task of identifying typed relations between known entity mentions in a sentence. Most existing RE models treat each relation in a sentence individually <ref type="bibr" target="#b13">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b16">Nguyen and Grishman, 2015)</ref>. However, a sentence typically contains multiple relations between entity mentions. RE models need to consider these pairs simultaneously to model the dependencies among them. The relation between a pair of interest (namely "target" pair) can be influenced by other pairs in the same sentence. The example illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>  in the sentence. The person entity (PER) Toefting is directly related with teammates through the preposition with. Similarly, teammates is directly related with the geopolitical entity (GPE) capital through the preposition in. Toefting and capital can be directly related through in or indirectly related through teammates. Substantially, the path from Toefting to teammates to capital can additionally support the relation between Toefting and capital.</p><p>Multiple relations in a sentence between entity mentions can be represented as a graph. Neural graph-based models have shown significant improvement in modelling graphs over traditional feature-based approaches in several tasks. They are most commonly applied on knowledge graphs (KG) for knowledge graph completion <ref type="bibr" target="#b7">(Jiang et al., 2017)</ref> and the creation of knowledge graph embeddings <ref type="bibr" target="#b19">Shi and Weninger, 2017)</ref>. These models rely on paths between existing relations in order to infer new associations between entities in KGs. However, for relation extraction from a sentence, related pairs are not predefined and consequently all entity pairs need to be considered to extract relations. In addition, state-of-the-art RE models sometimes depend on external syntactic tools to build the shortest dependency path (SDP) between two entities in a sentence <ref type="bibr" target="#b23">(Xu et al., 2015;</ref><ref type="bibr" target="#b13">Miwa and Bansal, 2016)</ref>. This dependence on external tools leads to domain dependent models.</p><p>In this study, we propose a neural relation extraction model based on an entity graph, where entity mentions constitute the nodes and directed edges correspond to ordered pairs of entity mentions. The overview of the model is shown in <ref type="figure">Figure 2</ref>. We initialize the representation of an edge (an ordered pair of entity mentions) from the representations of the entity mentions and their context. The context representation is achieved by employing an attention mechanism on context words. We then use an iterative process to aggregate up-to l-length walk representations between two entities into a single representation, which corresponds to the final representation of the edge.</p><p>The contributions of our model can be summarized as follows:</p><p>• We propose a graph walk based neural model that considers multiple entity pairs in relation extraction from a sentence. • We propose an iterative algorithm to form a single representation for up-to l-length walks between the entities of a pair. • We show that our model performs comparably to the state-of-the-art without the use of external syntactic tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Walk-based Model</head><p>The goal of the RE task is given a sentence, entity mentions and their semantic types, to extract and classify all related entity pairs (target pairs) in the sentence. The proposed model consists of five stacked layers: embedding layer, BLSTM Layer, edge representation layer, walk aggregation layer and finally a classification layer. As shown in <ref type="figure">Figure 2</ref>, the model receives word representations and produces simultaneously a representation for each pair in the sentence. These representations combine the target pair, its context words, their relative positions to the pair entities and walks between them. During classification they are used to predict the relation type of each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding Layer</head><p>The embedding layer involves the creation of n w , n t , n p -dimensional vectors which are assigned to words, semantic entity types and relative positions to the target pairs. We map all words and semantic types into real-valued vectors w and t respec-  <ref type="figure">Figure 2</ref>: Overview of the walk-based model.</p><p>tively. Relative positions to target entities are created based on the position of words in the sentence. In the example of <ref type="figure" target="#fig_0">Figure 1</ref>, the relative position of teammates to capital is −3 and the relative position of teammates to Toefting is +16. We embed real-valued vectors p to these positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bidirectional LSTM Layer</head><p>The word representations of each sentence are fed into a Bidirectional Long-short Term Memory (BLSTM) layer, which encodes the context representation for every word. The BLSTM outputs new word-level representations h (Hochreiter and Schmidhuber, 1997) that consider the sequence of words.</p><p>We avoid encoding target pair-dependent information in this BLSTM layer. This has two advantages: (i) the computational cost is reduced as this computation is repeated based on the number of sentences instead of the number of pairs, (ii) we can share the sequence layer among the pairs of a sentence. The second advantage is particularly important as it enables the model to indirectly learn hidden dependencies between the related pairs in the same sentence.</p><p>For each word t in the sentence, we concatenate the two representations from left-to-right and right-to-left pass of the LSTM into a n e -dimensional vector,</p><formula xml:id="formula_0">e t = [ − → h t ; ← − h t ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Edge Representation Layer</head><p>The output word representations of the BLSTM are further divided into two parts: (i) target pair representations and (ii) target pair-specific context representations. The context of a target pair can be expressed as all words in the sentence that are not part of the entity mentions. We represent a related pair as described below. A target pair contains two entities e i and e j . If an entity consists of N words, we create its BLSTM representation as the average of the BLSTM representations of the corresponding words, e = 1 |I| i∈I e i , where I is a set with the word indices inside entity e.</p><p>We first create a representation for each pair entity and then we construct the representation for the context of the pair. The representation of an entity e i is the concatenation of its BLSTM representation e i , the representation of its entity type t i and the representation of its relative position to entity e j , p ij . Similarly, for entity e j we use its relative position to entity e i , p ji . Finally, the representations of the pair entities are as follows:</p><formula xml:id="formula_1">v i = [e i ; t i ; p ij ] and v j = [e j ; t j ; p ji ].</formula><p>The next step involves the construction of the representation of the context for this pair. For each context word w z of the target pair e i , e j , we concatenate its BLSTM representation e z , its semantic type representation t z and two relative position representations: to target entity e i , p zi and to target entity e j , p zj . The final representation for a context word w z of a target pair is, v ijz = [e z ; t z ; p zi ; p zj ]. For a sentence, the context representations for all entity pairs can be expressed as a three-dimensional matrix C, where rows and columns correspond to entities and the depth corresponds to the context words.</p><p>The context words representations of each target pair are then compiled into a single representation with an attention mechanism. Following the method proposed in <ref type="bibr" target="#b26">Zhou et al. (2016)</ref>, we calculate weights for the context words of the targetpair and compute their weighted average,</p><formula xml:id="formula_2">u = q tanh(C ij ), α = softmax(u), c ij = C ij α ,<label>(1)</label></formula><p>where q ∈ R n d , n d = n e + n t + 2n p denotes a trainable attention vector, α is the attended weights vector and c ij ∈ R n d is the context representation of the pair as resulted by the weighted average. This attention mechanism is independent of the relation type. We leave relation-dependent attention as future work.</p><p>Finally, we concatenate the representations of the target entities and their context (∈ R nm ). We use a fully connected linear layer, W s ∈ R nm×ns with n s &lt; n m to reduce the dimensionality of the resulting vector. This corresponds to the representation of an edge or a one-length walk between nodes i and j: v</p><formula xml:id="formula_3">(1) ij = W s [v i ; v j ; c ij ] ∈ R ns .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Walk Aggregation Layer</head><p>Our main aim is to support the relation between an entity pair by using chains of intermediate relations between the pair entities. Thus, the goal of this layer is to generate a single representation for a finite number of different lengths walks between two target entities. To achieve this, we represent a sentence as a directed graph, where the entities constitute the graph nodes and edges correspond to the representation of the relation between the two nodes. The representation of one-length walk between a target pair v (1) ij , serves as a building block in order to create and aggregate representations for one-to-l-length walks between the pair. The walkbased algorithm can be seen as a two-step process: walk construction and walk aggregation. During the first step, two consecutive edges in the graph are combined using a modified bilinear transformation,</p><formula xml:id="formula_4">f (v (λ) ik , v (λ) kj ) = σ v (λ) ik (W b v (λ) kj ) ,<label>(2)</label></formula><p>where v (λ) ij ∈ R n b corresponds to walks representation of lengths one-to-λ between entities e i and e j , represents element-wise multiplication, σ is the sigmoid non-linear function and W b ∈ R n b ×n b is a trainable weight matrix. This equation results in walks of lengths two-to-2λ.</p><p>In the walk aggregation step, we linearly combine the initial walks (length one-to-λ) and the extended walks (length two-to-2λ),</p><formula xml:id="formula_5">v (2λ) ij = βv (λ) ij + (1 − β) k =i,j f (v (λ) ik , v (λ) kj ), (3)</formula><p>where β is a weight that indicates the importance of the shorter walks. Overall, we create a representation for walks of length one-to-two using Equation (3) and λ = 1. We then create a representation for walks of length one-to-four by re-applying the equation with λ = 2. We repeat this process until the desired maximum walk length is reached, which is equivalent to 2λ = l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classification Layer</head><p>For the final layer of the network, we pass the resulted pair representation into a fully connected layer with a softmax function,</p><formula xml:id="formula_6">y = softmax(W r v (l) ij + b r ),<label>(4)</label></formula><p>where W r ∈ R n b ×nr is the weight matrix, n r is the total number of relation types and b r is the bias vector.</p><p>We use in total 2r+1 classes in order to consider both directions for every pair, i.e., left-to-right and right-to-left. The first argument appears first in a sentence in a left-to-right relation while the second argument appears first in a right-to-left relation. The additional class corresponds to non-related pairs, namely "no relation" class. We choose the most confident prediction for each direction and choose the positive and most confident prediction when the predictions contradict each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate the performance of our model on ACE 2005 2 for the task of relation extraction. ACE 2005 includes 7 entity types and 6 relation types between named entities. We follow the preprocessing described in <ref type="bibr" target="#b13">Miwa and Bansal (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>We implemented our model using the Chainer library <ref type="bibr" target="#b21">(Tokui et al., 2015)</ref>. <ref type="bibr">3</ref> The model was trained with Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2015)</ref>. We initialized the word representations with existing pre-trained embeddings with dimensionality of 200. 4 Our model did not use any external tools except these embeddings.</p><p>The forget bias of the LSTM layer was initialized with a value equal to one following the work of <ref type="bibr" target="#b8">Jozefowicz et al. (2015)</ref>. We use a batchsize of 10 sentences and fix the pair representation dimensionality to 100. We use gradient clipping, dropout on the embedding and output layers and L2 regularization without regularizing the biases, to avoid 2 https://catalog.ldc.upenn.edu/ldc2006t06 3 https://chainer.org/ 4 https://github.com/tticoin/LSTM-ER overfitting. We also incorporate early stopping with patience equal to five, to chose the number of training epochs and parameter averaging. We tune the model hyper-parameters on the respective development set using the RoBO Toolkit <ref type="bibr" target="#b10">(Klein et al., 2017)</ref>. Please refer to the supplementary material for the values.</p><p>We extract all possible pairs in a sentence based on the number of entities it contains. If a pair is not found in the corpus, it is assigned the "no relation" class. We report the micro precision, recall and F1 score following <ref type="bibr" target="#b13">Miwa and Bansal (2016)</ref> and <ref type="bibr" target="#b16">Nguyen and Grishman (2015)</ref>. <ref type="table">Table 1</ref> illustrates the performance of our proposed model in comparison with SPTree system <ref type="bibr" target="#b13">Miwa and Bansal (2016)</ref> on ACE 2005. We use the same data split with SPTree to compare with their model. We retrained their model with gold entities in order to compare the performances on the relation extraction task. The Baseline corresponds to a model that classifies relations by using only the representations of entities in a target pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>As it can be observed from the table, the Baseline model achieves the lowest F1 score between the proposed models. By incorporating attention we can further improve the performance by 1.3 percent point (pp). The addition of 2-length walks further improves performance (0.9 pp). The best results among the proposed models are achieved for maximum 4-length walks. By using up-to 8-length walks the performance drops almost by 2 pp. We also compared our performance with Nguyen and Grishman (2015) (CNN) using their data split. 5 For the comparison, we applied our 5 The authors kindly provided us with the data split. # Entities l = 1 l = 2 l = 4 l = 8 Finally, we show the performance of the proposed model as a function of the number of entities in a sentence. Results in <ref type="table">Table 2</ref> reveal that for multi-pair sentences the model performs significantly better compared to the no-walks models, proving the effectiveness of the method. Additionally, it is observed that for more entity pairs, longer walks seem to be required. However, very long walks result to reduced performance (l = 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Traditionally, relation extraction approaches have incorporated a large variety of hand-crafted features to represent related entity pairs <ref type="bibr">(Hermann and Blunsom, 2013;</ref><ref type="bibr" target="#b14">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b15">Nguyen and Grishman, 2014;</ref><ref type="bibr" target="#b2">Gormley et al., 2015)</ref>. Recent models instead employ neural network architectures and achieve state-of-the-art results without heavy feature engineering. Neural network techniques can be categorized into recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The former is able to encode linguistic and syntactic properties of long word sequences, making them preferable for sequence-related tasks, e.g. natural language generation <ref type="bibr" target="#b3">(Goyal et al., 2016)</ref>, machine translation <ref type="bibr" target="#b20">(Sutskever et al., 2014)</ref>.</p><p>State-of-the-art systems have proved to achieve good performance on relation extraction using RNNs <ref type="bibr" target="#b0">(Cai et al., 2016;</ref><ref type="bibr" target="#b13">Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b12">Liu et al., 2015)</ref>. Nevertheless, most approaches do not take into consideration the dependencies between relations in a single sentence <ref type="bibr" target="#b18">(dos Santos et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen and Grishman, 2015)</ref> and treat each pair separately. Current graph-based models are applied on knowledge graphs for distantly supervised relation extraction <ref type="bibr" target="#b25">(Zeng et al., 2017)</ref>. Graphs are defined on semantic types in their method, whereas we built entity-based graphs in sentences. Other approaches also treat multiple relations in a sentence <ref type="bibr" target="#b4">(Gupta et al., 2016;</ref><ref type="bibr" target="#b14">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b11">Li and Ji, 2014)</ref>, but they fail to model long walks between entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a novel neural network model for simultaneous sentence-level extraction of related pairs. Our model exploits target and context pair-specific representations and creates pair representations that encode up-to l-length walks between the entities of the pair. We compared our model with the state-of-the-art models and observed comparable performance on the ACE2005 dataset without any external syntactic tools. The characteristics of the proposed approach are summarized in three factors: the encoding of dependencies between relations, the ability to represent multiple walks in the form of vectors and the independence from external tools. Future work will aim at the construction of an end-to-end relation extraction system as well as application to different types of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyper-parameter Settings</head><p>We tuned our proposed model using the RoBO toolkit (https://github.com/automl/ RoBO). <ref type="table" target="#tab_3">Table 3</ref> provides the selected options we used for tuning the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Options</head><p>Optimization method Bohamiann Maximizer scipy Acquisition function log ei Number of iterations 50 Initial points 3  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>explains this phenomenon. The relation between the pair of interest Toefting and capital, can be extracted directly from the target entities or indirectly by incorporating information from other related pairs 1 Source code available at https://github.com/ fenchri/walk-based-re Toefting was convicted of assaulting a pair of workers during PER a night out with national squad teammates in the capital ... Relation examples from ACE (Automatic Content Extraction) 2005 dataset (Doddington et al., 2004).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameters optimization options.The parameters that gave the best performance for the different models can be found inTables 4a-4e.</figDesc><table><row><cell>Parameter</cell><cell>Baseline</cell></row><row><cell>Position dimension n p</cell><cell>25</cell></row><row><cell>Type dimension n t</cell><cell>15</cell></row><row><cell>LSTM dimension n e</cell><cell>100</cell></row><row><cell>Input layer dropout</cell><cell>0.3</cell></row><row><cell>Output layer dropout</cell><cell>0.03</cell></row><row><cell>Learning rate</cell><cell>0.0018</cell></row><row><cell>Regularization</cell><cell>3.2 · 10 −5</cell></row><row><cell>Gradient clipping</cell><cell>25.63</cell></row><row><cell>(a)</cell><cell></cell></row><row><cell>Parameter</cell><cell>l = 1</cell></row><row><cell>Position dimension n p</cell><cell>25</cell></row><row><cell>Type dimension n t</cell><cell>25</cell></row><row><cell>LSTM dimension n e</cell><cell>100</cell></row><row><cell>Input layer dropout</cell><cell>0.13</cell></row><row><cell>Output layer dropout</cell><cell>0.38</cell></row><row><cell>Learning rate</cell><cell>0.0017</cell></row><row><cell>Regularization</cell><cell>6.1 · 10 −5</cell></row><row><cell>Gradient clipping</cell><cell>30</cell></row><row><cell>(b)</cell><cell></cell></row><row><cell>Parameter</cell><cell>l = 2</cell></row><row><cell>Position dimension n p</cell><cell>25</cell></row><row><cell>Type dimension n t</cell><cell>20</cell></row><row><cell>LSTM dimension n e</cell><cell>100</cell></row><row><cell>β</cell><cell>0.72</cell></row><row><cell>Input layer dropout</cell><cell>0.25</cell></row><row><cell>Output layer dropout</cell><cell>0.37</cell></row><row><cell>Learning rate</cell><cell>0.003</cell></row><row><cell>Regularization</cell><cell>0.0001</cell></row><row><cell>Gradient clipping</cell><cell>8.6</cell></row><row><cell>(c)</cell><cell></cell></row><row><cell>Parameter</cell><cell>l = 4</cell></row><row><cell>Position dimension n p</cell><cell>25</cell></row><row><cell>Type dimension n t</cell><cell>20</cell></row><row><cell>LSTM dimension n e</cell><cell>100</cell></row><row><cell>β</cell><cell>0.77</cell></row><row><cell>Input layer dropout</cell><cell>0.11</cell></row><row><cell>Output layer dropout</cell><cell>0.32</cell></row><row><cell>Learning rate</cell><cell>0.002</cell></row><row><cell>Regularization</cell><cell>5.7 · 10 −5</cell></row><row><cell>Gradient clipping</cell><cell>24.4</cell></row><row><cell>(d)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Best hyper-parameters settings for proposed models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We kept the same parameters when we apply our model to the this data split. We did not remove any negative examples unlike the CNN model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been carried out with funding from AIRC/AIST, the James Elson Studentship Award, BBSRC grant BB/P025684/1 and MRC MR/N00583X/1. Results were obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO). We would finally like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language generation through character-based rnns with finite-state prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentive path combination for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="590" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robo: A flexible and robust bayesian optimization framework in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Bayesian Optimization in the Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Bayesian Optimization in the Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Proje: Embedding projection for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (Learn-ingSys) in the twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (Learn-ingSys) in the twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved relation classification by deep recurrent neural networks with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1461" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incorporating relation paths in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1768" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
