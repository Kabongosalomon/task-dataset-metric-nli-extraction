<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
							<email>luhaonan@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Hu</surname></persName>
							<email>huhailin2@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Capturing the composition patterns of relations is a vital task in knowledge graph completion. It also serves as a fundamental step towards multi-hop reasoning over learned knowledge. Previously, rotation-based translational methods, e.g., RotatE, have been developed to model composite relations using the product of a series of complex-valued diagonal matrices. However, RotatE makes several oversimplified assumptions on the composition patterns, forcing the relations to be commutative, independent from entities and fixed in scale. To tackle this problem, we have developed a novel knowledge graph embedding method, named DensE, to provide sufficient modeling capacity for complex composition patterns. In particular, our method decomposes each relation into an SO(3) group-based rotation operator and a scaling operator in the three dimensional (3-D) Euclidean space. The advantages of our method are twofold: (1) For composite relations, the corresponding diagonal relation matrices can be non-commutative and related with entity embeddings; (2) It extends the concept of RotatE to a more expressive setting with lower model complexity and preserves the direct geometrical interpretations, which reveals how relations with distinct patterns (i.e., symmetry/anti-symmetry, inversion and composition) are modeled. Experimental results on multiple benchmark knowledge graphs show that DensE outperforms the current state-of-the-art models for missing link prediction, especially on composite relations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are a key component of a wide range of down-stream applications, such as machine reasoning, information retrieval and knowledge-guided natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>. Especially, learning how to hop over a variety of concepts or instances stored in knowledge graph represents a value path towards artificial general intelligence.</p><p>Mathematically, knowledge graphs are defined as a collection of triplets. Each triplet, denoted by (h, r, t), indicates a relation r pointing from the head entity h to tail entity t. Currently, numerous research efforts have been devoted to developing knowledge graph embedding (KGE) methods. These methods aim to learn a set of low-dimensional representations of entities and relations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, which is usually coupled with a score function to enable the knowledge graph completion process, i.e., predicting missing links between entities, for real-world KGs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. Sometimes, neural networks can be inserted into the process <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>, though this requires additional computation costs.</p><p>Recently, modeling relation as rotational operators has emerged as a promising approach for KGE. For example, RotatE <ref type="bibr" target="#b12">[13]</ref> first introduced relational rotation in a translational KGE method, which views relations as rotation operations of entities in a 2-D complex space. While this method has shown prominent modeling power for various relation patterns (e.g., symmetry, anti-symmetry and inversion), the modeling of composition patterns is actually over-simplified. In particular, the noncommutative composite relations are barely attended to <ref type="bibr" target="#b2">[3]</ref>. In addition, the rotation axis is forced to be orthogonal to the entity embeddings, ruling out the interaction with relation and entity embeddings. A more recent model, called QuatE <ref type="bibr" target="#b17">[18]</ref>, attempts to increase the expressiveness by conducting relational rotation as quaternion multiplication in hypercomplex space. However, its dependency on unit quaternion-based rotation still limits the modeling capacity for general composition patterns and the rotation in quaternion space fails to provide an understandable geometric interpretation to be presented. In this work, we focus on the modeling of composite relations, accounting for their possible noncommutative nature and interactions with entity embeddings. More specifically, we propose DensE (Distance-based Embedding with Non-abelian Rotation and Scaling in 3-D Euclidean Space), which decomposes the relation into an SO(3) group-based rotation operator and a scaling operator and in the 3-D Euclidean space. Intuitively, non-Abelian group (here we use SO(3) rotation group) is applied to introduce non-commutative nature to our model, and the scaling operation offers another important dimension to accommodate each triplet in the Euclidean space, which is barely explored in previous research. Our main contributions are summarized as the following: (1) For the first time, relational rotation and scaling are modeled within a unified framework, which achieves a much higher expressiveness with lower model complexity; <ref type="bibr" target="#b1">(2)</ref> Our method provides an up-to-date most comprehensive while straightforward geometric interpretation of each relation type in 3-D Euclidean space; <ref type="bibr" target="#b2">(3)</ref> Extensive experiments show that DensE outperforms the current state-of-the-art models in link prediction, offering a useful tool for knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>From the perspective of modeling composite relations (i.e., a relation path composed of a series of relations), our work is related to several previous methods, i.e., TransE <ref type="bibr" target="#b0">[1]</ref>, RotatE <ref type="bibr" target="#b12">[13]</ref> and QuatE <ref type="bibr" target="#b17">[18]</ref>. TransE and RotatE are both translational models that use Eucleadian distance as score function. While they are motivated by clear geometric interpretations, their modeling capacity for composite relations is insufficient because both of them assume a commutative pattern on relation path and do not consider entity information in inferring composition patterns. Specifically, TransE models each relation as a pure translational transformation, so it assumes a fixed addition composition pattern between relations, i.e., r 3 = r 1 + r 2 , which is commutative and irrelevant to entity embeddings. RotatE made significant progress by modeling relations as rotational operator (rotation matrix) in 2-D Euclidean space. When modeling a relation path composed of multiple relations, RotatE uses Hadamard product to combine the rotation matrices of the relations on the path, i.e., r 3 = r 1 • r 2 . In this model, all relations in the composite relation have the same rotation axis <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). Thus, the compositions in RotatE are also mandatorily commutative. Also, interactions between relation and entity embeddings are precluded as the rotation axis is always perpendicular to entity embeddings.</p><p>To offer a more expressive model for composite relations, QuatE is designed to represent each relation type as a rotational operator in quaternion space. While this method does not assume any fixed composition pattern, QuatE still has two major limitations. Firstly, as discovered in the original paper, QuatE requires normalization of relation to unit quaternion, indicating it is incapable of integrating scale information (also the case in RotatE). Secondly, as both entities and relations are embedded in quaternion hyperplanes, QuatE cannot provide a straightforward geometric interpretation in the space, which hinders the understanding of the learned embeddings. Concurrent with our work, <ref type="bibr" target="#b16">[17]</ref> proposes a group theoretic analysis for KGE methods. Their method, named NagE, represents a preliminary attempt in applying non-Abelian group in modeling relational rotations. However, their empirical results show limited performance advance over previous methods (e.g., RotatE). Also, their method does not provide sufficient geometric insights in the modeling process.</p><p>In contrast to the previous works, our model leverages both rotation and scaling operations for relation modeling. The key idea is that we can transform any non-zero vector in the Euclidean space to another arbitrary vector through decoupled rotation and scaling transformations. In addition, our model provides a clear geometric picture to demonstrate the transformation of entity representation in various relation composition patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We denote a directed knowledge graph as G(E, R, F), where E, R and F are sets of entities, relations and facts, respectively. A fact stored in a KG can be expressed as a triplet (h, r, t) ∈ F, where h, t ∈ E and r ∈ R. Herein, we focus on the knowledge completion task, which aims to predict missing links based on the observed facts. To fulfill this goal, a score function is used to measure the plausibility of proposed fact candidates, and the goal of model optimization is to give higher scores to true triplets (h, r, t) than the false triplets (h, r,t) or (h, r, t), wheret andh are randomly sampled tail and head entities, respectively. Mathematically, the entity and relation embeddings are usually represented by tensors, and the score function can thus be written into the form of f r (h, t).</p><p>In principle, KGE models should be designed to accommodate various relation patterns existing in real world KGs, such as symmetry(r(x, y) ⇒ r(y, x)), anti-symmetry (r(x, y) ⇒ ¬r(y, x))) and inversion (r 1 (x, y) ⇒ r 2 (y, x)). Formal definitions and examples of these patterns can be found in the Appendix. In contrast to the above atomic relation patterns (inferable within one hop), the complex composition patterns pose a particular challenge to modeling, as discussed below. Definition 1. Relation r 3 is composed of relation r 1 and relation r 2 if ∀x, y, z</p><formula xml:id="formula_0">r 1 (x, y)Λr 2 (y, z) ⇒ r 3 (x, z)<label>(1)</label></formula><p>Here r 3 is also referred to as a composite relation and possesses certain composition pattern. In particular, our model design takes the following properties into account.</p><p>Property 1. The two relations in the composition are not always commutative. For example, given r 1 = is_f ather_of , r 2 = is_mother_of , based on the Definition 1, we will get r 3 = is_grandf ather_of . However, when we change the order, i.e., r 1 = is_mother_of , r 2 = is_f ather_of , we will get r 3 = is_grandmother_of .</p><p>Property 2. The composition patterns are not always inferable by the relations alone. For example, given that y is x s younger sister and z is y s elder brother, we can not answer whether z is elder or younger than x from the given information. Actually, to answer this question, we need to know more about x/y/z from their own attributes and their other relationships.</p><p>Property 3. In a composition, the relations involved are not necessarily different. Given the twohop example above, besides the situation that r 1 , r 2 and r 3 are mutually different, there are also four different cases that satisfy the definition of composition, i.e., r 1 = r 2 = r 3 , r 1 = r 2 = r 3 , r 1 = r 3 = r 2 and r 1 = r 2 = r 3 <ref type="figure" target="#fig_0">(Figure 1</ref>  . More specifically, we can use a unit quaternion to encode the rotation using three degrees of freedom (i.e., θ, φ and ψ). Actually, it can be viewed as a group structure on a 3-sphere (i.e., S3) which gives the group Spin <ref type="bibr" target="#b2">(3)</ref>. Note that this group structure is isomorphic to SU(2) group and also to the universal cover of SO(3) group. Formally, the unit quaternion q to model a rotation through an angle of ψ around the aforementioned axis − → v can be derived using an extension of Euler's formula:</p><formula xml:id="formula_1">q = e ψ 2 (vxi+vyj+vzk) = cos ψ 2 + sin ψ 2 * (v x i + v y j + v z k),<label>(2)</label></formula><p>where i, j, k are imaginary units of the quaternion representation, which satisfies the condition i 2 = j 2 = k 2 = ijk = −1. Unlike real/complex numbers, the multiplication of quaternions (Hamilton product) is sensitive to the orders as we have: ij = k, ji = −k, jk = i, kj = −i, ki = j, ik = −j. For</p><formula xml:id="formula_2">Q 1 = a 1 + b 1 i + c 1 j + d 1 k and Q 2 = a 2 + b 2 i + c 2 j + d 2 k,</formula><p>their Hamilton product is:</p><formula xml:id="formula_3">Q 1 ⊗ Q 2 = a 1 a 2 − b 1 b 2 − c 1 c 2 − d 1 d 2 + (a 1 b 2 + b 1 a 2 + c 1 d 2 − d 1 c 2 )i +(a 1 c 2 − b 1 d 2 + c 1 a 2 + d 1 b 2 )j + (a 1 d 2 + b 1 c 2 − c 1 b 2 + d 1 a 2 )k<label>(3)</label></formula><p>A 3-D Euclidean vector − → w with the coordinate (x, y, z) can be expressed as a pure quaternion (meaning the real part of quaternion is zero), i.e., W = xi + yj + zk, giving the following theorem:</p><p>Theorem 1 Given a 3-D Euclidean vector − → w and its counterpart in the quaternion space W, the desired rotation axis − → v , the magnitude of the rotation ψ, the destination coordinate of the vector after the rotation, i.e.,W = x i + y j + z k, can be calculated by the Hamilton product of quaternions:</p><formula xml:id="formula_4">W = qWq −1 (4) where q −1 is the inverse of q, i.e., q −1 = e − ψ 2 (vxi+vyj+vzk) = cos ψ 2 − sin ψ 2 * (v x i + v y j + v z k)</formula><p>. The form of Eq.4 and a factor of 1 2 for the angle ψ in Eq.2 indicate that there is a 2 : 1 homomorphism from quaternions of unit norm to SO(3). Considering each 3-D Euclidean vector can also be expressed as a pure quaternion, we can now represent the rotation using a matrix R(q) by expanding Eq.4 and letting C = cos ψ and S = sin ψ:</p><formula xml:id="formula_5">− → w = R(q) − → w =   C + v 2 x (1 − C) v x v y (1 − C) + v z S v x v z (1 − C) − v y S v x v y (1 − C) − v z S C + v 2 y (1 − C) v y v z (1 − C) + v x S v x v z (1 − C) + v y S v y v z (1 − C) − v x S C + v 2 z (1 − C)   x y z<label>(5)</label></formula><p>In our framework, two rotations can be combined into one equivalent rotation operation (this is also consistent with the closure property of group theory). In other words, we can define q = q 2 q 1 , where q corresponds to the rotation q 1 followed by the rotation q 2 . Therefore, a series of rotations can be composed together and then applied as a single rotation. Note that quaternion multiplication is not commutative unless q 1 and q 2 share the same rotation axes (i.e., −</p><formula xml:id="formula_6">→ v 1 = − → v 2 )</formula><p>, which can be seen from Eq.3. This makes it possible to model both commutative and non-commutative relation patterns. The geometrical interpretation of non-commutative modeling can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Integrating scaling operations</head><p>Following Eq.2 and letting q to be the unit quaternion, an arbitrary quaternion with non-unit norm can be written as: Q = a + bi + cj + dk = |Q|q, with the norm given by |Q| = √</p><formula xml:id="formula_7">a 2 + b 2 + c 2 + d 2 where a = |Q| cos ψ 2 , b = |Q| sin ψ 2 sin θ cos φ, c = |Q| sin ψ 2 sin θ sin φ and d = |Q| sin ψ 2 cos θ.</formula><p>By multiplying a scalar |Q| in the Eq.5, we can further introduce length as another degree of freedom to better match the ground-truth tail embedding vector ( <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_8">(b),</formula><p>Step 2). Formally, we have:</p><formula xml:id="formula_9">− → w = |Q|R(q) − → w = O(Q) − → w, where Q ∈ H, − → w, − → w ∈ R 3 ,<label>(6)</label></formula><p>where O(Q) = |Q|R(q) is the combined operator of rotation and scaling transformations, H denotes quaternion algebra, and R 3 represents 3-D Euclidean algebra. Here we call |Q| the scaling f actor.</p><p>Therefore, we now have a uniform framework with interpretable geometric meaning, i.e., (|Q|, θ, φ, ψ) to describe the transformation corresponding to a specific relation type. We can also define the reverse operation O(Q −1 ) = |Q| −1 R(q −1 ), which describes the reverse process: rotate a vector about the axis − → v with angle −ψ (from another direction), and then scale the vector with a factor of |Q| −1 . Combining the Eq.5 and Eq.6, we can always find a operator O(</p><formula xml:id="formula_10">Q 3 ) = O(Q 2 )O(Q 1 )</formula><p>, which corresponds to the application of O(Q 1 ) followed by the application of O(Q 2 ), where we have</p><formula xml:id="formula_11">|Q 3 | = |Q 1 | * |Q 2 | and R(q 3 ) = R(q 2 )R(q 1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Score function and optimization</head><p>A score function aims to correctly measure the plausibility of a triple of interest. Formally, as a distance-based model, our scoring function is defined as</p><formula xml:id="formula_12">f r (h, t) = − 1 2 (|O(r)h − t| + |O(r −1 )t − h|)</formula><p>. Here, | · | denotes the Euclidean distance and O(·) stands for the transformation conducted on each element of the entity embeddings, i.e.,</p><formula xml:id="formula_13">t i = O(r i )h i , and h i = O(r −1 i )t i , where r i ∈ H, h i , t i ∈ R 3</formula><p>, and i indicates i-th embedding unit of h. The arrow of h i and t j are omitted for clarity. To properly train the model parameters, here we use a loss function similar to the self-adversarial negative sampling loss proposed in <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_14">L = − log σ(γ + f r (h, t)) − n j=1 p(h j , r,t j ) log σ(−(γ + f r (h (j) ,t (j) ))),<label>(7)</label></formula><p>where γ is a fixed margin, n is the number of negative sampling size, (h j , r,t j ) is the j-th negative triplet of the fact (h, r, t), and σ is the sigmoid function.h (j) andt (j) are the embeddings corresponding to the negative triplet (h j , r,t j ). p(h j , r,t j ) is the weight of the negative sample, which gives the higher scored negative samples with larger weight during training. The details about self-adversarial negative sampling technique can be found in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Capability of DensE in modeling symmetry, antisymmetry, inversion and composition</head><p>Enabled by the geometric interpretation provided by each degree of freedom of our model, here we discuss how DensE models the aforementioned relation patterns. Clearly, a relation r is symmetric in DensE if and only if each dimension of its embedding r i satisfies |r i | = 1 and the rotation angle satistifies ψ ri = 0 or π. For anti-symmetry relation pattern, the embedding r i satisfies |r i | = 1 , but the rotation angle ψ ri should be neither 0 nor π. Also, two relations r 1 and r 2 are in inverse pattern, if and only if they satisfy: |r 1i | * |r 2i | = 1, θ r1i = θ r2i , φ r1i = φ r2i and ψ r1i = −ψ r2i , meaning the embeddings of these two relations share the same rotation axes, but rotate in two opposite directions.</p><p>As discussed in Section 4.1, the commutative and non-commutative composition patterns can be naturally modeled by the guarantee of the property of group theory, which covers Property 1 of composite relations. Also, following the intuition of Property 2, our model does not enforce a uniform mode of each element in relation representations. Instead, it learns to model the interaction between relations and entities as well as the ambiguity in composition pattern inference, leading to a disperse distribution in the relation embedding space. Last but not least, our model can smoothly deal with constraints posed by relation types in inferring composition patterns, as stated in Property 3. For instance, when modeling the pattern r 1 (x, y)Λr 2 (y, z) ⇒ r 2 (x, z), the representation from RotatE tends to degenerate to a trivial case where the rotation angle of r 1 and r 2 both set to be 0 or 2π or r 1 = 2π, r 2 = π. In DensE, since the entity embeddings are not required to be perpendicular to the rotation axis, it can also place the embedding of entity x to be collinear with the rotation axis of r 1 . In this way, the rotation axis of r 1 and r 2 are not required to be the same, making the model to be more expressive. In another example, as for the pattern r 1 (x, y)Λr 1 (y, z) ⇒ r 2 (x, z), besides capturing the relationship of the two rotation angles (i.e., ψ r2i = 2ψ r1i ), the scaling transformation offers an additional degree of freedom, where our model tends to give |r 2i | = |r 1i | 2 . Again, we point out that these "rules" are not constant solutions, as the information entities will further guide the model to deviate from the statistical mode for a better accommodation of each triplet. Other composition patterns presented in the Property 3 can be analyzed in the similar way (see Appendix).  <ref type="bibr" target="#b13">[14]</ref> is extracted from the original Freebase dataset FB15K <ref type="bibr" target="#b0">[1]</ref> by removing inverse relations. In addition, we also use the YAGO3-10 <ref type="bibr" target="#b6">[7]</ref> dataset, which consists of a large collection of triplets from multilingual Wikipedia. The latter three datasets aim to assess the model performance on composition patterns. Here, we report mean reciprocal rank (MRR) and Hits at 1 (H@1) for evaluation (the higher, the better). Statistics of these datasets and experiments in other evaluation metrics (mean rank, H@3 and H@10) are provided in the Appendix.</p><p>Implementation details We use the Adam optimizer and tune the hyperparameters on the validation dataset. During training, we adopt a similar reciprocal learning approach as used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>. Early stopping is applied based on the performance on validation dataset every 1000 epochs. The ranges for hyperparameter grid search and the best hyperparameter settings are listed in Appendix. All parameters are randomly initialized from the interval</p><formula xml:id="formula_15">[− 1 √ 2k , 1 √ 2k ],</formula><p>where k is the embedding size.</p><p>Baselines We mainly compare DensE with top-performing baseline models for KG link prediction, including TransE <ref type="bibr" target="#b0">[1]</ref>, ComplEx <ref type="bibr" target="#b14">[15]</ref>, RotatE <ref type="bibr" target="#b12">[13]</ref>, NagE <ref type="bibr" target="#b16">[17]</ref> and QuatE <ref type="bibr" target="#b17">[18]</ref>. These baselines have included both translational model (TransE, NagE and RotatE) and semantic matching model (ComplEx and QuatE). Note that in the original QuatE paper, the authors reported the results in three settings. One of them involves a "type constraint" operation which altered standard evaluation protocol. To ensure fairness, we do not include this setting for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>Prediction performance We report the link prediction results on the four benchmark datasets in Model complexity As listed in <ref type="table" target="#tab_3">Table 3</ref>, we show that compared with high-performance models such as RotatE and QuatE + , DensE has much lower model complexity in terms of parameter number and training epochs. We reason that this is mainly achieved by introducing a decoupled scaling operation, thus lowering the embedding dimension required in rotation-only modeling.  Geometric interpretation Here we show several examples to illustrate the geometric insight given by DensE, which basically reflects the geometric intuition discussed in Section 4.4. In <ref type="figure" target="#fig_3">Figure 2</ref>(a), we first show the distribution of element-wise addition of embeddings from two inverse relations (has_part and part_of ) of ψ, one representative degree of freedom in modeling relational rotation (Other degrees of freedom can be found in the Appendix). In this way, we can visualize how the two embeddings agree with each other. As the two relations are fully inferable by each other, we do observe a clear conjugation as expected in Section 4.4. This is also reflected in the representation of scaling from these two relations, where the element-wise products tend to be one <ref type="figure" target="#fig_3">(Figure 2(b)</ref>). Interestingly, we do observe two different embeddings are learned for each relation <ref type="figure" target="#fig_3">(Figure 2(c)-(d)</ref>), demonstrating the scaling operation do capture relation-specific features. For composition patterns, we slightly change the experiment protocol, with each histogram showing the element-wise difference between the embeddings of a composite relation and the embeddings calculated by multiplying each relation in the relation path. In a case from WN18RR, we demonstrate how DensE models a composition pattern for r 1 (h, h )Λr 2 (h , t) ⇒ r 1 (h, t), where r 1 = derivationally_related_f orm and r 2 = hypernym. This is a typical case where the composite relation actually equals to the first relation in the relation path. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>(e), while most embedding dimensions still agree well between the actual composite relation and the calculated relation path, the distribution tends to disperse to a large range, indicating the existence of ambiguity and interaction between entities and relations. To further explore the ambiguity issue in the above case, we perform the same analysis on another small portion of triplets that actually give r 1 (h, h )Λr 2 (h , t) ⇒ r 3 (h, t), where r 3 = synset_domain_topic_of <ref type="figure" target="#fig_3">(Figure 2(f)</ref>). Interestingly, we also observe that part of embedding dimensions of O(r2)O(r1) are aligned with O(r3) (embedding difference close to zero), demonstrating the flexibility of our model to capture potentially ambiguous relation compositions. On the other hand, we also observe that the model can learn to put the rotation axis of r 1 collinear with the embedding of head entities h in the composition mode: r 1 (h, h )Λr 2 (h , t) ⇒ r 1 (h, t) as shown in <ref type="figure" target="#fig_3">Figure 2(g)</ref>, reflecting the interaction of entities and relations. This example clearly demonstrates the interpretability of our method in modeling complex composition patterns. The geometric patterns for other relation patterns can be found in the Appendix. Ablation study To examine the effectiveness of each module in our model, we perform a series of ablation experiments <ref type="table" target="#tab_4">(Table 4</ref>). On WN18 dataset, we observe no significant performance decrease except for eliminating reciprocal training, which is beneficial in modeling atomic relations. On WN18RR dataset, the most significant performance decrease occurs when we cancel the scaling operation, i.e., only model the relation as rotations. This confirms the contribution from scaling to the whole model. On FB15K-237 and YAGO3-10 datasets, we also observe a large drop of performance when removing the scaling operation. Also, self-adversarial negative sampling (adv) shows significant contribution, indicating the necessity to incorporate proper training techniques. </p><formula xml:id="formula_16">(a) ψ has_part + ψ part_of (b) |Q has_part | * |Q part_of | (c) |Q has_part | (d) |Q part_of | (e) ψ(O(r2)O(r1)) − ψ(O(r1)) (f) ψ(O(r2)O(r1))−ψ(O(r3)) (g) θ(O(r1)) − θ(h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this work, we present significant progress in reasoning capability relating to KG embedding. Harvesting knowledge is the ultimate goal that human beings seek along the advancement of technology and humanity. This research, along with many KG embedding representation studies, is attempting to design systems that have better use of human knowledge. Technically speaking, our method possesses both higher modeling performance and lower computation cost. This especially fits the current trend in applying knowledge graph on a larger and larger scale and the initiative on energy saving in the AI application.</p><p>When it comes to who may be put at disadvantage from this research, we do not think it is applicable since our study of addressing the reasoning capabilities of AI is still at an early stage. Having said so, we are fully aware that we need to develop AI ethically and responsibly. The potential bias in the data that may lead to wrong knowledge needs to be cleaned or corrected with validation and interpretation mechanisms. Therefore, in this study, we are devoted to bringing interpretability to the reasoning process. In the future, we will continue to put transparency of modeling knowledge composition as a focus in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Source code</head><p>We have published DensE as an open source software and the source code can be found online at https://github.com/anonymous-dense-submission/DensE B The geometrical interpretation of non-commutative nompositions Left: A rotation about axis-z followed by a rotation about axis-x, the initial vector h is placed along axis-x. It can be seen that the final state is along axis-z, and two rotation operations are equivalent to one operation with rotation axis to be about axis-y.</p><p>Right: The rotation operation sequence is reversed from the left <ref type="figure">figure.</ref> A rotation about axis-x is followed by a rotation about axis-z. The final state is then changed to be along axis-y. Since the initial vector is collinear with the first rotation axis, the two rotation operations are equal to the last rotation (rotation about axis-z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Formal definitions and examples of relation patterns</head><p>Here, we formally define the relation patterns discussed in our work, each of which is followed by an example for intuitive understanding. Let x, y, z be the entities in a given KG, and r(·, ·) maps the relation between the two entities, we have:</p><formula xml:id="formula_17">Definition 1.</formula><p>Relation r 3 is composed of relation r 1 and relation r 2 if ∀x, y, z</p><formula xml:id="formula_18">r 1 (x, y)Λr 2 (y, z) ⇒ r 3 (x, z)<label>(8)</label></formula><p>Here r 3 is also referred to as a composite relation and possesses certain composition pattern. A typical example is: given y is x s father and z is y s wife, we can infer the relationship between x and z. </p><p>F riend is a typical example of symmetric relation, which means if we know x is friend of y, we can infer y is also friend of x. F iliation is an example of anti-symmetric relation. </p><p>For instance, has_part and part_of fit into the scope of inverse relations, which means if we know x is a part of y, we can infer that y has part x. Note that both symmetric/antisymmetric and inverse relation patterns can be inferred in one hop, so they are also called atomic relation.  <ref type="table" target="#tab_0">WN18  40943  18  141442  5000  5000  WN18RR  40943  11  86835  3034  3134  FB15k-237  14541  237  272115  17535  20466  YAGO3-10  123182  37  1079040</ref> 5000 5000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The basic statistics of the benchmark datasets</head><p>The experiments are conducted on four commonly used benchmark datasets, including WN18, WN18RR, Fb15k-237 and YAGO3-10. WN18 comes from WordNet <ref type="bibr" target="#b7">[8]</ref>, which consists of many lexical relations of English words. As WN18 has a significant part of inverse relations, it is mainly used here to reflect inversion pattern of relations. A new dataset, named WN18RR <ref type="bibr" target="#b1">[2]</ref>, has been proposed where inverse relations are removed. Similarly, the FB15k-237 dataset <ref type="bibr" target="#b13">[14]</ref> is extracted from the original Freebase dataset FB15K <ref type="bibr" target="#b0">[1]</ref> by removing inverse relations. In addition, we also use the YAGO3-10 <ref type="bibr" target="#b6">[7]</ref> data, which consists of a large collection of triplets from multilingual Wikipedia. The latter three datasets are involved to assess the model performance on composition patterns. The basic statistics of these datasets are provided in <ref type="table" target="#tab_5">Supplementary Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Link prediction results with more evaluation metrics</head><p>In the main text, we report Mean Rank (MR), Mean Reciprocal Rank (MRR) and Hits at 1 (H@1) for evaluation for each dataset. Here, we provide more metrics including Mean Rank (MR), H@3 and H@10. The results are shown in <ref type="table" target="#tab_6">Supplementary Table 6</ref> and <ref type="table" target="#tab_7">Supplementary Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Hyperparameters setting</head><p>The ranges of the hyperparameters for the grid search are set as follows: Embedding size k ∈ {100, 200, 500, 1000} (In our model, each entity is represented with a matrix with a size of 3 × k, and each relation with a matrix with a size of 4 × k), batch size b ∈ {256, 512, 1024}, fixed margin γ ∈ {3.0, 6.0, 9.0, 12.0, 15.0, 24.0, 30.0}, negative sampling size n ∈ {256, 512, 1024}, self-adversarial sampling temperature α ∈ {0.3, 0.5, 1.0}. The initial learning rate η is set to be 0.1, and it decays with a factor of 1/2 if the training loss does not decrease in 1000 epochs. We list the best hyperparameters setting of DensE on the benchmark datasets in <ref type="table" target="#tab_8">Supplementary Table 8</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Capability of DensE in modeling relation patterns</head><p>In this Section, we provide a detailed analysis on how our method tend to model each relation pattern in an interpretable way. In our experiment, we calculate the statistical rule of each degree of freedom to reflect the effect of specific relation patterns. In addition, we also sometimes compare the embeddings of two relation types (or a relation type and a entity) per element, i.e., we perform element-wise addition, subtraction, multiplication on each embedding dimension. Then, we use the distribution of these results to demonstrate how the two compared embeddings agree with each other. Note that below we use an addition subscript i to denote each dimension in the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Symmetry/anti-symmetry pattern</head><p>As pointed out in main text Section 4.4, for the symmetry relation pattern, the scaling factor |Q| of symmetric relation tend to be one, and the rotation angle ψ should be 0 or π in [0, 2π). For anti-symmetry relation pattern, one can easily check that the scaling factor |Q| should also be one, but the rotation angle ψ should be neither 0 nor π in the range of [0, 2π). Here we show the distributions of rotation angle ψ and scaling factor |Q| of four relations with symmetry pattern in WN18RR ( <ref type="figure">Supplementary Figure 4)</ref>. It should be noted that since the embedding size k in our model for WN18RR is set to be 200, the sum of frequency in these distributions also equals to 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Inversion pattern</head><p>In main text Section 4.4, we assert that if two relations r 1 and r 2 satisfy the inverse pattern, if and only if they satisfy: |r 1i | * |r 2i | = 1, θ r1i = θ r2i , φ r1i = φ r2i and ψ r1i = −ψ r2i . In <ref type="figure">Supplementary  Figure 5</ref>, we show a case of paired relations with inversion pattern from WN18 dataset, namely has_part, part_of . We plot the scaling factor |Q|, magnitude of the rotation ψ, and (θ, φ) that describe the rotation axis for each relation (first two columns), as well as their element-wise alignment results (the last column). <ref type="figure">Figure 4</ref>: Geometric interpretation of how DensE models symmetry patterns (four examples from WN18RR). Each row shows the distribution of |Q| and ψ for a given relation, respectively. <ref type="figure">Figure 5</ref>: Geometric interpretation of how DensE models inverse pattern, using an example of (has_part, part_of ) from WN18. At each row, we show the embedding from one degree of freedom of our model. The first two columns show the embeddings of each relation type, and the last column shows the alignment of the two embeddings regarding a specific degree of freedom.</p><formula xml:id="formula_21">(a) |Q derivationally_related_f orm | (b) ψ derivationally_related_f orm (c) |Q also_see | (d) ψ also_see (e) |Q similar_to | (f) ψ similar_to (g) |Q verb_group | (h) ψ verb_group</formula><formula xml:id="formula_22">(a) |Q has_part | (b) |Q part_of | (c) |Q has_part | * |Q part_of | (d) ψ has_part (e) ψ part_of (f) ψ has_part + ψ part_of (g) θ has_part (h) θ part_of (i) θ has_part − θ part_of (j) φ has_part (k) φ part_of (l) φ has_part − φ part_of</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Composition pattern G.3.1 Ambiguity in composition pattern</head><p>In a real-world KG (here we take a sub-graph from WN18RR as an example), due to the ambiguity issue mentioned in the main text (composition pattern Property 2), there exist plenty of examples where a third relation (the composite relation) cannot be inferred given the two participating relations alone ( <ref type="figure" target="#fig_8">Supplementary Figure 6(a)</ref>). For example, given r 1 = derivationally_related_f orm and r 2 = hypernym, we have the composition pattern as shown with the blue lines: (Trade(VB), deriva-tionally_related_form, Trade(NN)), (Trade(NN), hypernym, transaction) and (Trade(VB), derivation-ally_related_form , transaction). From these cases, it seems that one can summarize the composition pattern as: r 3 = r 1 = derivationally_related_f orm, i.e., r 1 (h, h )Λr 2 (h , t) ⇒ r 1 (h, t). However, we also have the triangle with red lines, i.e., (Trade(VB), derivationally_related_form, Selling), (Selling, hypernym, mercantilism) and (Trade(VB), synset_domain_topic_of, mercantilism). In these cases, it looks like the composition pattern has the form that r 1 (h, h )Λr 2 (h , t) ⇒ r 3 (h, t), where r 3 = synset_domain_topic_of . This ambiguity means that the composition mode is not uniform but depends on specific entities and their other neighborhoods. Therefore, in order to give the model sufficient flexibility to learn this, our model does not require all the dimensions in a relation embedding to fit in one single composition mode (e.g., r 1 (h, h )Λr 2 (h , t) ⇒ r 1 (h, t) or r 1 (h, h )Λr 2 (h , t) ⇒ r 3 (h, t)). In consequence, the learned relation embedding for a com-posite relation are actually distributed in a disperse manner, with the majority of embedding dimensions following mode r 1 (h, h )Λr 2 (h , t) ⇒ r 1 (h, t), and some minor portions following r 1 (h, h )Λr 2 (h , t) ⇒ r 3 (h, t), which is consistent with the abundance of each mode in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.2 Case study: a two-hop composition pattern</head><p>As we mentioned in the main text, in a composition pattern, the relations involved are not necessarily different (composition pattern Property 3). Here we discuss the simplest case of a two-hop relation composition. Even under the assumption that the composite relation can be inferred from its composition alone (i.e., an unambiguous composition), there are still five possible situations, as shown in <ref type="figure" target="#fig_8">Supplementary Figure 6</ref> Here we provide a detailed analysis on these five situations, using real cases in WN18RR as examples ( <ref type="figure" target="#fig_8">Supplementary Figure 6(a)</ref>).</p><p>Situation 1: r 1 , r 2 and r 3 are all the same, e.g., the triangle in yellow color with three triplets: (man, has_part, arm), (arm, has_part, paw) and (man, has_part, paw). To satisfy the composition relation constraint, in principle the model can adopt several approaches to fit this situation. From the perspective of relation embedding, it can learn to fit |Q has_part | = 1 or ψ has_part ∈ {0, 2π}. Also, in terms of interaction between entities and relations, it can also learn to align the rotation axis of relation with the entity embedding (see <ref type="figure">Supplementary Figure 7(c)</ref>, showing the differences between the θ component of r 1 in the mode r 1 (h, h )Λr 1 (h , t) ⇒ r 1 (h, t) and the corresponding head entities h in spherical coordinate system). Note that when the rotation axis aligns with head entity embedding in spherical coordinate system, the composition pattern can be modeled by the scaling factor alone, i.e., |Q has_part | 2 = |Q has_part |, regardless of the rotation magnitude ψ.</p><p>In this case, we see the model mainly adopts the first approach ( <ref type="figure">Supplementary Figure 7(a)</ref>), as |Q has_part | is roughly around 1. In addition, the partial alignment between relation rotation axis and entities is also observed ( <ref type="figure">Supplementary Figure 7(c)</ref>). In contrast, for ψ has_part , we see very few dimensions satisfy the condition (0 or 2π), which is also consistent with the dominate role of scaling factor in this case. <ref type="figure">Figure 7</ref>: Geometric interpretation of composition patterns in Situation 1.</p><formula xml:id="formula_23">(a) |Q has_part | (b) ψ has_part (c) θ(O(r1)) − θ(h)</formula><p>Situation 2: r 1 and r 2 are the same, but not equal to r 3 . An example is given by the triangle in green with three triplets: (T rade(V B), derivationally_related_f orm, Selling), (Selling, derivationally_related_f orm, Sell) and (T rade(V B), verb_group, Sell). As mentioned in main text Section 4.4, we expect the model to learn ψ verb_group = 2ψ derivationally_related_f orm and |Q verb_group | = |Q derivationally_related_f orm | 2 , both of which are confirmed in this case. Besides this, we also observe that the rotation axes of the two relations are aligned on some dimensions, i.e, θ verb_group = θ derivationally_related_f orm and φ verb_group = φ derivationally_related_f orm . The corresponding distributions are shown in <ref type="figure">Supplementary Figure 8</ref>, where we have r 1 = r 2 = derivationally_related_f orm and r 3 = verb_group. Situation 3: r 1 and r 3 are the same, but not equal to r 2 , e.g., the triangle in blue color with three triplets set: (T rade(V B),derivationally_related_f orm , T rade(N N )), (T rade(N N ), hypernym, transaction) and (T rade(V B), derivationally_related_f orm, transaction). Here we compare the difference between the embedding of a composite relation and the embedding calculated by multiplying each relation in the relation path. We have already shown the distribution of ψ in the main context. Other distributions are shown in <ref type="figure" target="#fig_10">Supplementary Figure 9</ref>, where we have r 1 = r 3 = derivationally_related_f orm and r 2 = hypernym. Situation 4: r 2 and r 3 are the same, but not equal to r 1 , e.g., the triangle in purple color with three triplets set: (T rade(V B), hypernym, transact), (transact, derivationally_related_f orm, transaction) and (T rade(V B), derivationally_related_f orm, transaction). Here we compare the difference between the embedding of a composite relation and the embedding calculated by  <ref type="figure" target="#fig_0">Figure 10</ref>, where we have r 1 = hypernym and r 2 = r 3 = derivationally_related_f orm. In this case, we see that the composition pattern given by O(r2)O(r1) is learned to have similar scaling factor |Q| and rotation magnitude ψ with r 2 , and the rotation axes (θ, φ) are also partially aligned. Situation 5: r 1 , r 2 and r 3 are mutually different relations, e.g., the triangle in red color with three triplets set: (T rade(V B), derivationally_related_f orm, Selling), (Selling, hypernym, mercantilism) and (T rade(V B), synset_domain_topic_of , mercantilism). Here we compare the difference between the embedding of a composite relation and the embedding calculated by multiplying each relation in the relation path. Related distributions are shown in <ref type="figure" target="#fig_0">Supplementary Figure 11</ref>, where we have r 1 = derivationally_related_f orm, r 2 = hypernym and r 3 = synset_domain_topic_of .</p><p>The reason for relatively large dispersion here is discussed in the main text, i.e., the majority of triplets exemplify derivationally_related_f orm Λ hypernym ⇒ derivationally_related_f orm (See also the discussion above of Situation 3), while only a small portion have the pattern derivationally_related_f orm Λ hypernym ⇒ synset_domain_topic_of . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Variance of the prediction performance</head><p>The mean values and corresponding variance of MRR on WN18, WN18RR, FB15k-237 and YAGO3-10 datasets are shown in <ref type="table" target="#tab_9">Supplementary Table 9</ref>. The results are obtained by training DensE with five different random seeds, showing that the prediction performance of DensE is quite stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Effect of self-adversarial negative sampling on DensE and RotatE</head><p>In the ablation study, we observe a significant contribution of the self-adversarial negative sampling technique on the prediction performance of FB15k-237 and YAGO3-10. Therefore, we compare our model with RotatE in the setting where both models are trained without self-adversarial negative sampling ( <ref type="table" target="#tab_0">Supplementary Table 10</ref>). These results further confirm the superiority of our model without self-adversarial negative sampling. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) In RotatE model, composite relations are assumed to be commutative. Changing the order of relational rotation of r1 and r2 gives the same composition r3. Also, the rotation axis (perpendicular to the paper) is orthogonal to entity embeddings. (b) DensE decomposes a relation into a rotation operator and a scaling operator on the head entity h in 3-D Euclidean space. (c) Examples of composition patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 Method 4 . 1</head><label>441</label><figDesc>Modeling relational rotation using SO(3) rotation group One of the ways to model a rotation operation in the 3-D space is called axis-angle representation, which parameterizes a rotation by two quantities: 1) A unit vector − → v indicating the direction of the axis of rotation, i.e., − → v = (v x , v y , v z ) = (sin θ cos φ, sin θ sin φ, cos θ), where θ ∈ [0, π] and φ ∈ [0, 2π); and 2) An angle ψ describing the magnitude of the rotation about the rotation axis, where ψ ∈ [0, 2π). Given an entity vector − → w in the 3-D space with the coordinate (x, y, z), its rotation about axis − → v with an angle of ψ can be modeled using the SO(3) group theory (Figure 1(b), Step 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Geometric interpretation provided by DensE. Each histogram shows a distribution of each dimension of the learned embeddings. Angular parameters are in radian units. (a)-(d) A case study for inversion patterns (Other degrees of freedom can be found in the Appendix). (e)-(f) A case study for composition patterns, reflecting the scenario of r1(h, h )Λr2(h , t) ⇒ r1(h, t) and r1(h, h )Λr2(h , t) ⇒ r3(h, t), respectively. ψ(·) denotes the rotation angle about the rotation axis of a relational operator. (g) Collinearity of entity and relation embedding. θ(O(r1)) is the θ component of relation r1. θ(h) is the θ value of the head entity h's embedding in the spherical coordinate system of the 3-D Euclidean space. All the head entities satisfying r1(h, h )Λr2(h , t) ⇒ r1(h, t) are included for the analysis. The entities and relations involved can be found in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>A simple example for how rotation in the 3-D Euclidean space can model non-commutative relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 2 .</head><label>2</label><figDesc>A relation r is symmetric/anti-symmetric if ∀x, y r(x, y) ⇒ r(y, x)(r(x, y) ⇒ ¬r(y, x))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition 3 .</head><label>3</label><figDesc>Relation r 1 is inverse to relation r 2 if ∀x, y r 1 (x, y) ⇒ r 2 (y, x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>349 0.256 0.384 0.535 1450 0.541 0.465 0.585 0.678</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>(a) A representative subgraph from WN18RR to show ambiguity. (b) The relations involved in a composition pattern are not necessarily to be mutually different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Q(O(r3) − Q 2 (O(r1)) (b) ψ(O(r3)) − 2 * ψ(O(r1)) (c) θ(O(r3)) − θ(O(r1)) (d) φ(O(r3)) − φ(O(r1))Figure 8: Geometric interpretation of composition patterns in Situation 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>θ(O(r2)O(r1)) − θ(O(r1)) (d) φ(O(r2)O(r1))−φ(O(r1))Geometric interpretation of composition patterns in Situation 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(r2)O(r1)) − Q(O(r2)) (b) ψ(O(r2)O(r1)) − ψ(O(r2)) (c) θ(O(r2)O(r1)) − θ(O(r2)) (d) φ(O(r2)O(r1))−φ(O(r2))Figure 10: Geometric interpretation of composition patterns in Situation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(r2)O(r1)) − ψ(O(r3)) (c) θ(O(r2)O(r1)) − θ(O(r3)) (d) φ(O(r2)O(r1))−φ(O(r3))Figure 11: Geometric interpretation of composition patterns in Situation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on benchmark datasets. Best results are labeled in bold and the second best are underlied. Performance of TransE is from<ref type="bibr" target="#b10">[11]</ref> and<ref type="bibr" target="#b9">[10]</ref>; Performance of ComplEx is from<ref type="bibr" target="#b1">[2]</ref>; Performance of RotatE, QuatE and NagE are taken from the corresponding original papers. QuatE and QuatE + denote the plain QuatE model and QuatE model with N3 regularization and reciprocal learning, respectively. Model performance in other evaluation metrics can be found in the Appendix.</figDesc><table><row><cell></cell><cell>WN18</cell><cell cols="2">WN18RR</cell><cell cols="2">FB15K-237</cell><cell cols="2">YAGO3-10</cell></row><row><cell>Model</cell><cell cols="7">MRR H@1 MRR H@1 MRR H@1 MRR H@1</cell></row><row><cell>TransE</cell><cell cols="2">0.495 0.113 0.226</cell><cell>-</cell><cell>0.294</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">ComplEx 0.941 0.936 0.44</cell><cell cols="4">0.41 0.247 0.158 0.36</cell><cell>0.26</cell></row><row><cell>RotatE</cell><cell cols="7">0.949 0.944 0.476 0.428 0.338 0.241 0.495 0.402</cell></row><row><cell>NagE</cell><cell cols="5">0.950 0.944 0.477 0.432 0.340 0.244</cell><cell>-</cell><cell>-</cell></row><row><cell>QuatE</cell><cell cols="5">0.949 0.941 0.481 0.436 0.311 0.221</cell><cell>-</cell><cell>-</cell></row><row><cell>QuatE +</cell><cell cols="5">0.950 0.944 0.482 0.436 0.366 0.271</cell><cell>-</cell><cell>-</cell></row><row><cell>DensE</cell><cell cols="7">0.950 0.945 0.491 0.443 0.349 0.256 0.541 0.465</cell></row><row><cell>5 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Datasets and evaluation metrics The experiments are conducted on four commonly used bench- mark datasets, including WN18, WN18RR, Fb15k-237 and YAGO3-10. WN18 comes from Word- Net [8], which consists of many lexical relations of English words. As WN18 has a significant part of inverse relations, it mainly reflects inversion pattern of relations. Later, WN18RR [2] is proposed where inverse relations are removed. Similarly, the FB15k-237 dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Expectedly, on WN18 dataset, due to the widely existing atomic relations, all the models show comparably high performance. In contrast, when the inverse relations are removed, we observe a significant margin between most of the compared methods, except TransE, which cannot model symmetric relations. In particular, we show that DensE outperforms all the other models on all metrics. To confirm the source of performance gains, we conduct a further analysis that compares the of the baseline models, including RotatE and plain version of QuatE. On this specific dataset, we also notice QuatE using N3 regularization and reciprocal learning (QuatE + ) performs especially well. However, this improvement increases the parameter size of QuatE by a factor of ten (5.8M to 58.2M), which greatly hinders its application on large scale dataset. On YAGO3-10, DensE also shows a large margin over RotatE and ComplEx, which further demonstrates the superiority of DensE on various types of datasets. Results by other evaluation metrics can be found in the Appendix.</figDesc><table /><note>MRR performance of DensE to RotatE on each relation type, as listed in Table 2. Intriguingly, we notice a large performance increase on composite relations, as exemplified by hypernym, the most abundant composite relation in test data. We show that DensE improves MRR on this relation by as much as 3.3%. These results indicate a particular advantage of DensE in modeling composition relation patterns. On FB15k-237, we show that the performance of DensE is superior to most</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MRR comparison on each relation type of WN18-RR. Performance increases are in parentheses.</figDesc><table><row><cell>Relation type</cell><cell>Relation Name</cell><cell cols="2">% in test data RotatE</cell><cell>DensE</cell></row><row><cell></cell><cell>derivationally_related_form</cell><cell>34%</cell><cell cols="2">0.947 0.955 (+0.008)</cell></row><row><cell>Atomic</cell><cell>also_see verb_group</cell><cell>1.8% 1.3%</cell><cell cols="2">0.585 0.647 (+0.062) 0.943 0.955 (+0.012)</cell></row><row><cell></cell><cell>similar_to</cell><cell>0.2%</cell><cell>1</cell><cell>1 (+0)</cell></row><row><cell></cell><cell>hypernym</cell><cell>39.5%</cell><cell cols="2">0.148 0.181 (+0.033)</cell></row><row><cell></cell><cell>instance_hypernym</cell><cell>4%</cell><cell cols="2">0.318 0.349 (+0.031)</cell></row><row><cell></cell><cell>member_meronym</cell><cell>8.1%</cell><cell cols="2">0.232 0.249 (+0.017)</cell></row><row><cell>Composite</cell><cell>synset_domain_topic_of</cell><cell>3.8%</cell><cell cols="2">0.341 0.412 (+0.071)</cell></row><row><cell></cell><cell>has_part</cell><cell>5.5%</cell><cell cols="2">0.184 0.205 (+0.021)</cell></row><row><cell></cell><cell>member_of_domain_usage</cell><cell>0.8%</cell><cell cols="2">0.318 0.326 (+0.008)</cell></row><row><cell></cell><cell>member_of_domain_region</cell><cell>1%</cell><cell>0.2</cell><cell>0.407 (+0.207)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>RotatE</cell><cell>QuatE +</cell><cell>DensE</cell></row><row><cell>Space</cell><cell>C k</cell><cell>H k</cell><cell>h, t ∈ R 3×k , r ∈ H k</cell></row><row><cell>WN18</cell><cell cols="2">41.0M/48K 163.8M/-</cell><cell>24.6M/18K</cell></row><row><cell>WN18RR</cell><cell cols="2">41.0M/45K 163.8M/-</cell><cell>24.6M/15K</cell></row><row><cell cols="3">FB15k-237 29.3M/100K 58.2M/-</cell><cell>22.0M/27K</cell></row><row><cell cols="2">YAGO3-10 123.2M/90K</cell><cell>-</cell><cell>73.9M/33K</cell></row></table><note>Model complexity (#parameters/#training epochs). Values obtained from the best configurations reported in the original paper. Training epochs of RotatE are obtained by re-implementation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on WN18, WN18-RR, FB15K-237 and YAGO3-10 datasets. performance in knowledge completion with much lower model complexity. Also, DensE provides a straightforward geometric interpretation for the relations, leading to meaningful insights for complex relation patterns.</figDesc><table><row><cell></cell><cell>WN18</cell><cell>WN18RR</cell><cell>FB15K-237</cell><cell>YAGO3-10</cell></row><row><cell>Model</cell><cell cols="4">MRR H@1 MRR H@1 MRR H@1 MRR H@1</cell></row><row><cell>DensE</cell><cell cols="4">0.950 0.945 0.491 0.443 0.349 0.256 0.541 0.465</cell></row><row><cell>-scaling</cell><cell cols="4">0.950 0.945 0.475 0.432 0.335 0.240 0.486 0.392</cell></row><row><cell cols="5">-reciprocal 0.944 0.939 0.487 0.445 0.343 0.251 0.530 0.453</cell></row><row><cell>-adv</cell><cell cols="4">0.950 0.945 0.486 0.443 0.306 0.219 0.429 0.342</cell></row><row><cell>7 Conclusion</cell><cell></cell><cell></cell><cell></cell></row></table><note>In this work, we propose an effective method, named DensE, for knowledge graph embedding. DensE decomposes a relation operator into an SO(3) group-based rotation as well as a scaling transformation. Extensive experiments show that DensE possesses overall state-of-the-art</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of datasets used in this study</figDesc><table><row><cell>Dataset</cell><cell># Entities # Relations #Training #Validation #Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on WN18 and WN18-RR datasets. Best results are labeled in bold and the second best are underlied. Performance of TransE is from [11] and [10]; Performance of ComplEx is from [2]; Performance of RotatE, QuatE and NagE are taken from the corresponding original papers. QuatE and QuatE + denote the plain QuatE model and QuatE model with N3 regularization and reciprocal learning, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell>WN18RR</cell></row><row><cell>Model</cell><cell cols="7">MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10</cell></row><row><cell>TransE</cell><cell>-</cell><cell cols="6">0.495 0.113 0.888 0.943 3384 0.226</cell><cell>-</cell><cell>-</cell><cell>0.501</cell></row><row><cell cols="3">DistMult 655 0.797</cell><cell>-</cell><cell>-</cell><cell cols="3">0.946 5110 0.43</cell><cell>0.39</cell><cell>0.44</cell><cell>0.49</cell></row><row><cell>ComplEx</cell><cell>-</cell><cell cols="6">0.941 0.936 0.945 0.947 5261 0.44</cell><cell>0.41</cell><cell>0.46</cell><cell>0.51</cell></row><row><cell>RotatE</cell><cell cols="7">309 0.949 0.944 0.952 0.959 3340 0.476 0.428 0.492 0.571</cell></row><row><cell>NagE</cell><cell>-</cell><cell cols="4">0.950 0.944 0.953 0.960</cell><cell>-</cell><cell>0.477 0.432 0.493 0.574</cell></row><row><cell>QuatE</cell><cell cols="7">388 0.949 0.941 0.954 0.960 3472 0.481 0.436 0.500 0.564</cell></row><row><cell>QuatE +</cell><cell>-</cell><cell cols="4">0.950 0.944 0.954 0.962</cell><cell>-</cell><cell>0.482 0.436 0.499 0.572</cell></row><row><cell>DensE</cell><cell cols="7">285 0.950 0.945 0.954 0.959 3052 0.491 0.443 0.508 0.579</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on FB15k-237 and YAGO3-10 datasets. Best results are labeled in bold and the second best are underlied. Performance of TransE is from [10]; Performance of ComplEx is from [2]; Performance of RotatE, QuatE and NagE are taken from the corresponding original papers. QuatE and QuatE + denote the plain QuatE model and QuatE model with N3 regularization and reciprocal learning, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters settings of DensE in this study.</figDesc><table><row><cell>Dataset</cell><cell>k Embedding size</cell><cell cols="3">b Batch size Margin γ sample size n Negative</cell><cell>adv temperature α</cell></row><row><cell>WN18</cell><cell>200</cell><cell>512</cell><cell>12.0</cell><cell>1024</cell><cell>0.3</cell></row><row><cell>WN18RR</cell><cell>200</cell><cell>512</cell><cell>6.0</cell><cell>512</cell><cell>0.5</cell></row><row><cell>FB15k-237</cell><cell>500</cell><cell>1024</cell><cell>9.0</cell><cell>256</cell><cell>1.0</cell></row><row><cell>YAGO3-10</cell><cell>200</cell><cell>1024</cell><cell>24.0</cell><cell>512</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>The mean values and variance of MRR on WN18, WN18RR, FB15k-237 and YAGO3-10 datasets. 950±0.001 0.491±0.001 0.349±0.001 0.541±0.001 multiplying each relation in the relation path. Related distributions are shown in Supplementary</figDesc><table><row><cell>Dataset</cell><cell>WN18</cell><cell>WN18RR</cell><cell>FB15k-237</cell><cell>YAGO3-10</cell></row><row><cell>MRR</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Results of DensE and RotatE without self-adversarial negative sampling training technique, where "adv" represents "self-adversarial".</figDesc><table><row><cell>Model</cell><cell cols="4">WN18 WN18RR FB15k-237 YAGO3-10</cell></row><row><cell>DensE (w/o adv)</cell><cell>0.950</cell><cell>0.486</cell><cell>0.306</cell><cell>0.429</cell></row><row><cell cols="2">RotatE (w/o adv) 0.947</cell><cell>0.470</cell><cell>0.297</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A non-commutative bilinear model for answering path queries in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<title level="m">Dat Quoc Nguyen, and Dinh Phung. A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing pre-trained language representations with rich knowledge for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2346" to="2357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A group-theoretic framework for knowledge graph embedding. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1905.07129</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
