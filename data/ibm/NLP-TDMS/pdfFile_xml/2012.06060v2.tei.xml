<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatially Conditioned Graphs for Detecting Human-Object Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatially Conditioned Graphs for Detecting Human-Object Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of detecting human-object interactions in images using graphical neural networks. Unlike conventional methods, where nodes send scaled but otherwise identical messages to each of their neighbours, we propose to condition messages between pairs of nodes on their spatial relationships, resulting in different messages going to neighbours of the same node. To this end, we explore various ways of applying spatial conditioning under a multi-branch structure. Through extensive experimentation we demonstrate the advantages of spatial conditioning for the computation of the adjacency structure, messages and the refined graph features. In particular, we empirically show that as the quality of the bounding boxes increases, their coarse appearance features contribute relatively less to the disambiguation of interactions compared to the spatial information. Our method achieves an mAP of 31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming state-of-the-art on fine-tuned detections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of detecting human-object interactions (HOIs) requires localising and describing pairs of interacting humans and objects. In particular, an HOI is defined as a (subject, predicate, object) triplet, following the definition of visual relations from Lu et al. <ref type="bibr" target="#b22">[23]</ref>, where the subject and object are typically represented as labelled bounding boxes. For HOI triplets, the subject is always a human, so the interactions of interest simplify to pairs of predicates and objects, e.g., riding a horse or sitting on a bench.</p><p>Since the output representations are inherently similar, HOI detection is most often approached as a downstream task of object detection. Given a set of object detections from an image, one may construct candidate human-object pairs by matching up the detected human and object instances exhaustively. Indeed, the vast majority of previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref> use an off-the-shelf  <ref type="figure">Figure 1</ref>. Many images contain far more non-interactive humanobject pairs than interactive ones (a). Correct inference of the interaction type and the correspondences requires a combination of appearance and spatial information. When using appearance features only, the adjacency matrix for a graphical neural network tends to be dominated by a few salient objects (b). Since messages from each node to its neighbours are identical apart from an adjacency scaling, this leads to the node features being dominated by those of the most salient objects, confusing the classifier.</p><p>object detector <ref type="bibr" target="#b25">[26]</ref> as a preprocessing stage. We take the same approach, leveraging the success of modern object detectors. While this converts the HOI detection task into the simpler HOI recognition task on a set of candidate humanobject pairs, it is still far from being solved. Recognising HOIs is extremely challenging. While image recognition discriminates between scene types <ref type="bibr" target="#b31">[32]</ref> or prominent object types <ref type="bibr" target="#b26">[27]</ref>, focusing on the holistic understanding of an image, HOI recognition requires an understanding of the interactions between specific humans and objects at a much finer level. This requires reasoning about the subtle relationships between the instances as well as <ref type="table">Table 1</ref>. The use of appearance (A) and spatial (S) modalities at different stages of the graphical model, in recent HOI works. Refinement refers to late-stage fusion that takes place after message passing and fuses the graph features with other modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjacency</head><p>Message Refinement Methods (early fusion) (mid fusion) (late fusion)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPNN [25]</head><p>A A -Wang et al. <ref type="bibr" target="#b10">[11]</ref> A, S A A, S DRG <ref type="bibr" target="#b4">[5]</ref> S S -VSGNet <ref type="bibr" target="#b27">[28]</ref> A A A, S Ours A, S A, S A, S their contexts. This is particularly necessary when there are multiple human-object pairs with the same interaction type, where the model needs to correctly infer the interaction type and the correspondences between the individual instances. In addition, many interactions do not have strong visual cues and can be quite abstract, such as buying an apple or inspecting a boat. This poses a big challenge for standard CNNs, which excel at recognising physical qualities such as texture and shape. HOI detection demands a more sophisticated architecture capable of performing logical reasoning, not merely recognising the visual cues of the humans and objects of interest. The complexity and ambiguity of the problem is such that even humans can fail to correctly recognise HOIs in images, despite our ability to reason about visual cues and spatial relationships. Following prior work, we make use of graphical models to model these interrelationships and perform structured prediction. Since humans and objects in an image play different roles in the interactions, we build a bipartite graph to characterise these interrelationships, wherein each human node is connected to each object node. As is intuitive, we use the appearance features for a detected instance as the node encoding, be it a person or an object. Edge encodings, however, have been under-explored in the HOI detection problem. Previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref> take the appearance feature extracted from the minimum covering rectangle of the human and object boxes as the edge encoding. This representation does not necessarily encode the spatial relationships between a human-object pair, and there could be additional objects in the tight box other than the intended pair. Instead, we use explicitly learned spatial representations as the edge encodings. To shed some light on their significance, let us consider the example shown in <ref type="figure">Figure 1</ref>. Graphical models allow the propagation of contextual information between nodes. In this instance, each human node will receive information suggesting the presence of bikes. If the messages sent from a single bike node to all human nodes are identical, with the sole variable being a learnable weight that characterises the connectivity, this could result in a high confidence on bike-related interactions for most prominent human instances in the image. Therefore, it cannot easily distinguish between nine putative human-bike pairs for the three most prominent humans. As such, we contend that it is crucial to incorporate spatial information to regulate the message passing procedure. Our intuition is that, with spatial conditioning, each human node receives information of the presence of a bike and its relative location. Therefore, the interaction riding a bike could potentially be suppressed for a human instance if all bikes in the image are, say, to its left, as opposed to being directly under it.</p><p>Our primary contribution is a spatially conditioned message passing algorithm that renders outgoing messages which are dependent on the receiving nodes. For our bipartite graph, the algorithm also passes anisotropic messages across the bipartition. Furthermore, we extend the spatial conditioning mechanism to other parts of the graph-the computation of the adjacency structure and the refinement of the graph features-through a proposed multi-branch fusion module. While previous works have also combined appearance and spatial modalities at these two stages of the network as shown in <ref type="table">Table 1</ref>, our approach is consistent at each fusion stage and, in particular, gains significant performance improvements from using both modalities during message passing. Our secondary contribution is an analysis of the relative significance of the different modalities. We empirically show that as detection quality improves, the importance of the coarse appearance features decreases compared to that of the spatial information. We obtain state-of-the-art performance on the HICO-DET <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b8">[9]</ref> datasets, establishing a new benchmark for detecting human-object interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The HOI detection pipeline has significant overlap with that of object detection. Analogous to two-stage object detectors, a common approach is to first generate humanobject pair proposals and then classify their interactions. Specifically, Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> has been used in many preceding works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> to generate objects, each of which is associated with a predicted class and a confidence score. Afterwards, with appropriate filtering, human-object pairs are constructed exhaustively from the remaining detections. That is, each human instance will be paired up with each object instance. The rest of the pipeline varies, but typically employs a network with multiple streams to exploit different modalities of information. For instance, Chao et al. <ref type="bibr" target="#b2">[3]</ref> proposed a three-branch architecture to process the human and object appearance features and their pairwise spatial relationships. Different to many previous works, Liao et al. <ref type="bibr" target="#b17">[18]</ref> presented a proposalfree HOI detection pipeline, where interactions are directly detected as keypoints. Such a keypoint represents the centre of the minimum covering rectangle for a human-object pair engaged in the predicted interaction. Positions of the <ref type="figure">Figure 2</ref>. Diagram of proposed bipartite graph structure and message passing algorithm. The graph structure and its connectivity is shown on the left. On the right, we zoom in on a particular pair of nodes and illustrate the message passing process. Messages are computed using the proposed multi-branch fusion mechanism, which takes as input the node encodings of the sender and the pairwise spatial features between the sending and the receiving nodes. To compute the adjacency between a node pair, we employ another multi-branch fusion module, which takes as input the concatenated node encodings and their pairwise spatial features. The output is fed into an activation function (ReLU) and a linear layer to generate the adjacency value (pre-softmax). When message passing is finished, the predicted scores for K target classes are computed using the same fusion module.</p><p>human and object instances are obtained by regressing the displacements with respect to the detected interaction keypoint, similar to CornerNet <ref type="bibr" target="#b14">[15]</ref>, a keypoint-based object detector. Instead, we adopt the ubiquitous approach of using an off-the-shelf detector, due to their high performance and stability, and focus on improving the classification performance given a set of detections.</p><p>The choice of features has undergone significant development in recent research on HOI detection. Chao et al. <ref type="bibr" target="#b2">[3]</ref> used RoIPool <ref type="bibr" target="#b6">[7]</ref> to extract human and object appearance features and handcrafted a two-channel binary mask to encode the pairwise spatial relationships. While RoIAlign <ref type="bibr" target="#b11">[12]</ref> is now used in preference to RoIPool for appearance feature extraction, the binary mask is still widely used <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref>. However, Gupta et al. <ref type="bibr" target="#b9">[10]</ref> argued that a handcrafted spatial feature is a more effective way to encode the spatial relationships, explicitly exposing the coordinates of the bounding box pairs, the intersection over union, the aspect ratios, etc. They and others <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref> also proposed the use of human pose as additional information, which leads to some success in a few previous methods. We observe similar benefits to using handcrafted spatial encodings, but do not make use of human pose information in this work. Instead, we focus on showing how structured architectures can best exploit appearance and spatial information to disambiguate human-object interactions.</p><p>Graphical models were introduced to HOI detection by Qi et al. <ref type="bibr" target="#b24">[25]</ref>. They proposed a fully-connected graph with detected human and object instances as nodes. The node features are initialised with box appearance features and iteratively updated with a message passing algorithm. Wang et al. <ref type="bibr" target="#b10">[11]</ref> argued that the graph should take into consideration the fact that there are two sets of heterogeneous nodes, that is, the human nodes and object nodes. Thus, mes-sage passing between homogeneous nodes (intra-class messages) should be modelled differently from that between heterogeneous nodes (inter-class messages). Gao et al. <ref type="bibr" target="#b4">[5]</ref> also took advantage of the heterogeneity in nodes by constructing separate human-centric and object-centric graphs. They modelled human-object pairs as nodes, and employed the pairwise spatial relationships as node encodings. Lastly, Ulutan et al. <ref type="bibr" target="#b27">[28]</ref> proposed a bipartite graph in addition to a visual branch, which makes use of the appearance features of human-object pairs and the global scene. Most of the previous methods use both appearance and spatial modalities in graphical models as shown in <ref type="table">Table 1</ref>. However, the messages in all of their graphical models contain only one of the two modalities. Furthermore, the messages sent from a node to its neighbours are identical except weighted by adjacency values, which is what makes this work distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatially Conditioned Graphs</head><p>To reason jointly about the appearance and spatial information of an image, we propose a graph neural network for detecting human-object interactions. The structure of the graph is shown in <ref type="figure">Figure 2</ref>. To obtain an initial set of detections {d i } n i=1 for each image, we run an off-the-shelf object detector and apply appropriate filtering. We use Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>, although our model is detector agnostic. The detections are given by the tuple</p><formula xml:id="formula_0">d i = (b i , s i , c i ), with bound- ing box coordinates b i ∈ R 4 , confidence score s i ∈ [0, 1]</formula><p>and predicted object class c i ∈ K, where K is the set of object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Bipartite Graph Structure</head><p>We denote the bipartite graph as G = (H, O, E), where H = {d i | c i = "person"}, O = {d i | c i = "person"}, and E is the set of edges, such that all vertices on one side of the bipartition are densely connected to those on the other. The node encodings are initialised with appearance features extracted using RoIAlign <ref type="bibr" target="#b11">[12]</ref>, and the edge encodings are computed as handcrafted feature vectors. We start by encoding the rudimentary spatial information: centre coordinates of the bounding boxes, widths, heights, aspect ratios and areas, all normalised by the corresponding dimension of the image. To characterise the pairwise relationships, we also include the intersection over union, the area of the human box normalised by that of the object box, and a directional encoding given by</p><formula xml:id="formula_1">[ReLU(d x ) ReLU(−d x ) ReLU(d y ) ReLU(−d y )], (1)</formula><p>where d x and d y are the differences between centre coordinates of the human and object boxes normalised by the dimensions of the human box. This gives us the pairwise spatial encoding p ∈ R 18 + . Following the practice of Gupta et al. <ref type="bibr" target="#b9">[10]</ref>, we concatenate the spatial encoding with its logarithm, allowing the network to learn second and higher order combinations of different terms. For numerical stability, a small constant &gt; 0 is added before taking the log, i.e., p = p ⊕ log( p + ).</p><p>(</p><p>To initialise the human and object nodes, the respective appearance features are mapped to a lower dimension with a multilayer perceptron (MLP) to get the node encodings x 0 i , y 0 j ∈ R n for indices i ∈ {1, ..., |H|}, j ∈ {1, ..., |O|} and time step t = 0. Similarly, the edge encoding z ij ∈ R n is obtained by mapping the pairwise spatial encoding to the same dimension using another MLP. The edge encodings are constant during message passing. We define our bidirectional message passing updates as</p><formula xml:id="formula_3">x t+1 i = LN   x t i + σ   |O| j=1 α ij M O→H (y t j , z ij )     (3) y t+1 j = LN   y t j + σ   |H| i=1 α ji M H→O (x t i , z ij )     ,<label>(4)</label></formula><p>where LN denotes the LayerNorm operation <ref type="bibr" target="#b0">[1]</ref>, σ is the activation function (ReLU) and α is an adjacency weight between nodes. Notably, the message function M is anisotropic, i.e., different in each direction. This design allows nodes on different sides of the bipartition to send different message types, which we found helpful in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Conditioning</head><p>Appearance and spatial features constitute the two most important sources of information in the disambiguation of complex interactions. However, in all previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref>, messages between nodes contain only one of the two modalities, and each node sends identical messages to its neighbours, modulo an adjacency scaling. We believe that this limits the representation power of the graphical model significantly. To this end, we propose to condition the messages between nodes on their spatial relationships, which allows messages to express the relative location of the human or object, not just their presence. To do so, we take the elementwise product of the edge encoding and the node encoding (of the sender). We justify this design choice in the ablation analysis in Section 4.5.</p><p>We extend this strategy to two other parts of the graph. First, we use spatial conditioning to compute the adjacency matrix. This allows the learned graph connectivity to take into account the visual similarity and spatial relationship of the nodes. Second, we apply spatial conditioning to obtain the final representations for the human-object pairs. That is, after message passing is finished, we concatenate the graph features of each human-object pair, conditioned on their edge encoding. Our final model therefore consistently applies spatial conditioning to compute the adjacency matrix, the messages, and the final refined graph features, which corresponds to early, mid and late fusion between the modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Branch Fusion</head><p>To increase the expressive power of the spatial conditioning, we use a multi-branch structure for modality fusion. We map the modalities to c subspaces with reduced dimension, fuse the projections in each subspace, and then aggregate the outputs, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. We refer to the proposed module as multi-branch fusion (MBF). Following the nomenclature of Xie et al. <ref type="bibr" target="#b30">[31]</ref>, we refer to the number of homogeneous branches as the cardinality. Importantly, the number of parameters is independent of the cardinal-ity by design, due to the subspace dimensionality reduction. We define the message functions as</p><formula xml:id="formula_4">M O→H (y t j , z ij ) = MBF o (y t j , z ij ) (5) M H→O (x t i , z ij ) = MBF h (x t i , z ij ).<label>(6)</label></formula><p>The two fusion modules do not share weights, allowing for anisotropic messages.</p><p>MBFs are also used to compute the adjacency with spatial conditioning, applying an additional linear layer to map the output to a scalar. The pre-normalised adjacency is</p><formula xml:id="formula_5">α k = w T k σ MBF α (x t i ⊕ y t j , z ij ) + b k<label>(7)</label></formula><p>where w k ∈ R n , b k ∈ R and k is a linear index corresponding to a pair of (i, j), that is, k ∈ {1, ..., |H × O|}. During message passing, the adjacency value α ij is obtained by applying softmax to the entries sharing the same index i. Similarly, α ji is normalised by fixing j.</p><p>After all iterations of message passing, we fuse the spatial features and the graph features prior to binary classification for each target class. The computation of classification scores has the same form as that of the adjacency matrix in (Eq. 7), except with an additional sigmoid layer and that the output dimension is equal to the number of target classes. In fact, the adjacency can be interpreted as general interactiveness while the class probabilities are further conditioned on action types. For this reason the two multi-branch fusion modules share weights, which reduces the number of parameters and speeds up the training process. The full model is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Contextual Cues</head><p>As with most RoIPool-based feature extraction methods, the pooled information is local to a region. While this is reasonable for object detection, longer-range information about the context or even the global scene can be crucial for understanding human-object interactions. While Qi et al. <ref type="bibr" target="#b24">[25]</ref> used appearance features extracted from the minimum covering rectangle of the human and object boxes as edge features, our model uses spatial information as edge features. To compensate for the loss of contextual information, we employ another MBF module and fuse the global features extracted from the backbone with the spatial features for each pair of nodes. The resultant features are concatenated with the graph features for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Inference</head><p>For each image during training, we append the groundtruth boxes to the set of detections and assign them a score of one. We then remove detected boxes below a threshold score and apply non-maximum suppression. The m highest scoring human and object boxes are then selected to initialise the bipartite graph. After message passing, we generate a set of human-object pairs from the graph, denoted</p><formula xml:id="formula_6">by {q k } |H×O| k=1 , where q k = (b h i , s h i , b o j , s o j , s k ).</formula><p>The bounding boxes b and object detection scores s are obtained from the corresponding human and object nodes connected by edge (i, j). The classification scores for all actions s k are multiplied by the object detections scores. In practice, however, because the object detection scores do not consider the interactiveness of object instances, they tend to be overconfident. As a result, we raise the object detection scores to the power of λ during inference to counter this effect. The purpose of such operation is the same as the LIS <ref type="bibr" target="#b16">[17]</ref> function. But we found raising the power works better for our model. The final scores are computed as</p><formula xml:id="formula_7">s k = s h i λ · s o j λ s k .<label>(8)</label></formula><p>To associate the detected human-object pairs with the ground truth, the intersection-over-union is computed between each detected pair and ground-truth pair. Following previous practice <ref type="bibr" target="#b2">[3]</ref>, the IoU is computed for human and object boxes separately and taken as the minimum of the two. Detected pairs are considered to be positive when the IoU is above a designated threshold.</p><p>Due to the nature of proposal generation, there are overwhelmingly more negative examples than positive ones. In particular, the majority of examples are easy negatives. This inhibits the model from further improving on examples that are not well classified. To alleviate this issue, we adopt the focal loss <ref type="bibr" target="#b18">[19]</ref> as a binary classification loss, given by</p><formula xml:id="formula_8">FL(ŷ, y) = −β(1 −ŷ) γ log(ŷ), y = 1 −(1 − β)ŷ γ log(1 −ŷ), y = 0<label>(9)</label></formula><p>whereŷ ∈ [0, 1] is the final score of an example for a certain class, y ∈ {0, 1} is the binary label, and β ∈ [0, 1] and γ ∈ R + are hyper-parameters. In particular, β is a balancing factor between positive and negative examples. With β &gt; 0.5, positive examples are assigned higher weights and vice versa. The parameter γ attenuates the loss incurred on well-classified examples. This prevents the large number of easy negatives from dominating the gradient. However, suppressing easy negatives reduces the focal loss' magnitude <ref type="bibr" target="#b18">[19]</ref>, and so normalisation is required. We extend Lin et al.'s <ref type="bibr" target="#b18">[19]</ref> proposal to binary classification by normalising the loss by the number of positive logits. It is also important to restrict the output space to meaningful interactions. Denote the set of actions by A and the subset of valid actions for a specific object type o ∈ K by A o . Then the interactions of interest are in the set I = ∪ o∈K A o × {o}, with I ⊆ A × K. Following the practice of Gupta et al. <ref type="bibr" target="#b9">[10]</ref>, we only compute the loss on the subset A o for each human-object pair, given the object type o. This removes predictions for non-existent interaction types, such as eating a car, allowing the network to dedicate its parameters to learning meaningful interactions.</p><p>In the HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>, interactions of interest include those between two humans (i.e., a human may be an object and a subject in an HOI triplet). To capture such interactions, we construct bipartite graphs such that object nodes subsume human nodes, that is, object nodes are identical to the set of all detections. Human nodes representing the same instance across the bipartition are initialised to be the same, yet will diverge as message passing proceeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metric</head><p>We evaluated our model on the HICO-DET <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b8">[9]</ref> datasets. HICO-DET contains 37 633 training and 9 546 test images with bounding box annotations, 80 object classes (identical to those in the MS COCO dataset <ref type="bibr" target="#b20">[21]</ref>), 117 action types, and 600 interaction types. There are 117 871 annotated human-object pairs in the training set and 33 405 in the test set. The distribution of pairs per interaction class is highly uneven, following a long tail distribution. In particular, there are 47 interaction categories with only one training example.</p><p>The evaluation metric is mean average precision (mAP). Detected human-object pairs are considered as positive when the IoU with any ground-truth pair is higher than 0.5. For multiple detected pairs associated with the same ground-truth instance, only the highest scoring pair is considered as positive. The computation of mAP follows the 11-point interpolation algorithm used in the Pascal VOC challenge <ref type="bibr" target="#b3">[4]</ref>. To capture the effectiveness of our model across interactions with different numbers of annotations, we follow previous practice <ref type="bibr" target="#b2">[3]</ref> and report results in three categories: full (all 600 interactions), rare (138 interactions with fewer than 10 training examples), and non-rare (462 interactions with 10 or more training examples).</p><p>V-COCO is a much smaller dataset with 2 533 images in the training set, 2 867 in the validation set and 4 946 in the test set. The dataset contains 26 different actions. We report our performance on this dataset for legacy reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> with ResNet50-FPN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> pretrained on MS COCO <ref type="bibr" target="#b20">[21]</ref> to generate detections. For each image, we first filter out detections with scores lower than 0.2 and perform non-maximum suppression (NMS) with a threshold of 0.5. Afterwards, we extract the m = 15 highest scoring human boxes, and the m = 15 highest scoring object boxes. This gives us at most 15(30 − 1) = 435 box pairs, after removing pairs involving the same person twice. Inference follows the same setup, except that groundtruth detections are not used.</p><p>We use ResNet50-FPN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> as the backbone for feature extraction. To utilise the feature pyramid, boxes are as-signed to different pyramid levels based on their sizes <ref type="bibr" target="#b19">[20]</ref>. The pooled box features are mapped to 1024-dimensional vectors with a two-layer MLP. Similarly, the spatial features are mapped to the same dimension (1024) with a three-layer MLP. For the MBF module, we use c = 16 and n = 1024. We use T = 2 iterations of message passing for all models unless otherwise specified. To counter the over-confidence in object scores, we set λ = 2.8 during inference while keeping λ = 1.0 during training. Lastly, for the focal loss, we set β = 0.5 and γ = 0.2. All hyper-parameters are selected using cross-validation.</p><p>We adopt an image-centric training strategy <ref type="bibr" target="#b6">[7]</ref> with slight modifications. Input images are normalised and resized such that the shorter edge is 800 pixels. Bounding boxes are then resized accordingly. Afterwards, images are batched with zero padding. To train the model, we use AdamW <ref type="bibr" target="#b21">[22]</ref> as the optimiser, with a momentum of 0.9 and weight decay of 10 −4 . We use an initial learning rate of 10 −5 for the backbone and 10 −4 for the rest of the network. The learning rates are dropped by a magnitude at the sixth epoch. All models are trained for 10 epochs on 8 GeForce GTX TITAN X devices, with an effective batch size of 32. Our code is developed under PyTorch and will be made publicly available to facilitate reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>Quantitative results on the HICO-DET <ref type="bibr" target="#b2">[3]</ref> test set are shown in <ref type="table" target="#tab_0">Table 2</ref>. We report the performance of our model with three different detectors: one pre-trained on the MS COCO dataset <ref type="bibr" target="#b20">[21]</ref>, one fine-tuned on the HICO-DET dataset as provided by Gao et al. <ref type="bibr" target="#b4">[5]</ref>, and an oracle supplying the ground truth detections. We achieve competitive performance when using the COCO pre-trained detector, but significantly outperform state-of-the-art when using the higher-quality fine-tuned detections, a 20% relative improvement. In particular, we outperform the next best method IDN <ref type="bibr" target="#b15">[16]</ref> by 5 mAP, despite slightly underperforming that method when using the pre-trained detections. This suggests that our graph neural network can better exploit the high-quality detections. This is supported by the results for the oracle detector, where we outperform the next best method by 7.5 mAP. We show an example of the different detector outputs in <ref type="figure" target="#fig_2">Figure 4</ref>. Less salient people and objects are suppressed in the fine-tuned detector, making the spatial information more discriminative. We also report the performance of our model on V-COCO <ref type="bibr" target="#b8">[9]</ref> test set, as shown in <ref type="table">Table 3</ref>. Our model achieves competitive performance using a pre-trained detector and receives consistent gains from a fine-tuned detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Contribution of Different Modalities</head><p>Notably, our model is able to gain nearly 9 mAP by using a fine-tuned detector and a further 20 mAP by using an or- acle detector that supplies ground-truth detections, which is much higher than what previous methods gain. Due to the use of spatial conditioning in our model, we hypothesise that as detection quality improves, spatial information plays   a more significant role in the disambiguation of interactions, while coarse appearance features contribute relatively less. This is supported by evidence in <ref type="table" target="#tab_1">Table 4</ref>, where we show that the performance difference between the baseline model and our full model increases as detection quality improves.</p><p>To investigate this hypothesis, we add Gaussian noise with zero mean and variable standard deviation to the appearance and spatial features separately, and observe how corruption in different modalities damages the performance. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, when using the pre-trained detector, noise in the appearance and spatial features has an approximately equal effect on performance. However, with the fine-tuned detector, noisy spatial features have a much larger impact. We conclude that spatial information contributes relatively more to performance as the detection quality improves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We conducted a series of ablation studies to validate our design choices. Our baseline is a bipartite graph with appearance features only. Specifically, the message sent from a node is computed from its appearance encodings using a linear layer. The adjacency and class probabilities are computed from the concatenated node encodings of a human- object pair using an MLP, and the computation of classification scores shares weights with that of adjacency until the logistic layer. We first investigate the importance of spatial conditioning at different stages in our model: for computing the adjacencies, messages, global features, and refined graph features. As shown in <ref type="table" target="#tab_2">Table 5</ref>, every stage improves over the baseline, and they combine together to achieve the best performance. We next demonstrate the impact of different design choices for multi-branch fusion, including the choice of fusion methods and the number of branches (cardinality). As shown in <ref type="table">Table 6</ref>, the performance improves with higher cardinality. We also show that the performance is insensitive to the choice of binary fusion operation, with our choice (elementwise product) being comparable to the elementwise sum and concatenation operations. Lastly, we show how the number of message passing iterations at test time affects the results. As shown in <ref type="table">Table 7</ref>, message passing is clearly helpful for this problem, while an additional iteration further improves the results significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We show qualitative results of our model in <ref type="figure" target="#fig_5">Figure 6</ref>. In <ref type="figure" target="#fig_5">Figure 6a</ref>, the ground-truth interaction is riding a bike. As shown in <ref type="table">Table 8</ref>, positive human-bike pairs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> and <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref> have the highest scores. However, the network   <ref type="table">Table 8</ref>, and the scores for (b) are in <ref type="table">Table 9</ref>.</p><p>also assigns the negative pair (2, 3) a relatively high score. This is due to the spatial proximity and visual similarity between bike (3) and the correct bike instance <ref type="bibr" target="#b0">(1)</ref>, which can confuse our model. Another example is given in <ref type="figure" target="#fig_5">Figure 6b</ref>, where the true interaction is sitting on a bench. As shown in <ref type="table">Table 9</ref>, our model assigns high scores to all correct humanbench pairs and suppresses the non-interactive pair <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5)</ref>. <ref type="table">Table 8</ref>. Scores for the interaction riding a bike in <ref type="figure" target="#fig_5">Figure 6a</ref>.</p><p>Instance index 2 4 6 1 0.5742 0.0027 0.0000 3 0.4617 0.4735 0.0002 5 0.0006 0.0008 0.7899 <ref type="table">Table 9</ref>. Scores for the interaction sitting on a bench in <ref type="figure" target="#fig_5">Figure 6b</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a spatially conditioned graph neural network for detecting human-object interactions. To perform spatial conditioning, we applied a multibranch fusion mechanism that modulates the appearance features with the spatial configuration of the human-object pairs. We use this mechanism consistently for computing adjacency, messages and refined graph features, and show that our model outperforms the state-of-the-art by a considerable margin with fine-tuned detections. We also show that the margin of improvement increases with the detection quality, allowing our model to most effectively exploit advances in object detector research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Known Object Setting for HICO-DET</head><p>While the default setting for HICO-DET <ref type="bibr" target="#b2">[3]</ref> has been the more popular evaluation protocol, there is an additional less frequently reported known object setting, where the object types of ground truth interactions in images are considered known, thus automatically removing predicted interactive pairs with other object types. For interested readers, we provide the performance of our model in comparison with other methods under the known object setting in <ref type="table" target="#tab_3">Table 10</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatially-Conditioned Adjacency Map</head><p>We have shown that, with appearance features only, the adjacency matrix tends to be dominated by a few of the most salient human and object instances <ref type="figure" target="#fig_7">(Figure 7b</ref>). In this section, we show that with spatial conditioning, the adjacency matrix is able to learn the inherent binary interactiveness between human-object pairs without explicit supervision. Take the same example image as shown in <ref type="figure" target="#fig_7">Figure 7a</ref>, the adjacency maps computed with spatial conditioning are shown in <ref type="figure" target="#fig_7">Figure 7c</ref>. In the adjacency map on the left, which is normalised by rows, person (2) and person (3) have the highest adjacency values with bike (6) and bike (5) respectively. Such correspondence reflects the exact interactive pairs in the image. Similarly, person (4) has the highest adjacency value with bike (7), albeit only slightly higher than the adjacency with itself. In the adjacency map on the right, which is normalised by columns, we can observe the same pattern. Bikes <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7)</ref> have the highest adjacency values with persons (3, 2, 4) respectively, reflecting the correct correspondence amongst all human and bike instances. Notably, due to the fact that HICO-DET <ref type="bibr" target="#b2">[3]</ref> also includes human-human interactions, when constructing our graphical model, the set of object nodes subsumes that of the human nodes. This is reflected in the adjacency maps, where we include both human and object instances on the x axis. In particular, when the human instances are considered as objects in an interaction <ref type="figure" target="#fig_7">(Figure 7c</ref> right), these nodes have high adjacency values with their counterparts across the bipartition that represent the same instances. Considering the fact that there are no human-human interactions (at least not labelled in the dataset) present in the current image, we hypothesise that this is a mechanism the graphical model uses to express non-interactiveness, because pairs consisting of the same human instance twice will be discarded automatically without going into the final classification stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Results</head><p>We show more qualitative results to demonstrate the strength of our model in <ref type="figure" target="#fig_9">Figure 8</ref>. We intentionally select images that have many human instances and multiple human-object pairs of the same interaction. In <ref type="figure" target="#fig_5">Figure 6a</ref>   <ref type="table">Table 11</ref> and the scores corresponding to (b) are shown in <ref type="table" target="#tab_0">Table 12</ref>. <ref type="table">Table 11</ref>. Scores for the interaction racing a horse in <ref type="figure" target="#fig_9">Figure 8a</ref>.</p><p>Each column corresponds to pairs with the same human instance. Each row corresponds to pairs with the same horse instance.</p><p>Instance index 2 4 5 7 9</p><p>1 0.2031 0.0000 0.0000 0.0000 0.0000 3 0.0000 0.0000 0.5913 0.0002 0.0000 6 0.0000 0.0000 0.0013 0.0178 0.0030 8 0.0000 0.0000 0.0001 0.0034 0.1412 0.0000 0.0000 0.0391 0.0021 0.0000 0.0000 5 0.0000 0.0000 0.0000 0.1278 0.0000 0.0004 8 0.0000 0.0000 0.0000 0.0178 0.2791 0.1098 11 0.0000 0.0000 0.0000 0.0003 0.0000 0.3858 there are 20 combinatorial human-horse pairs, with 4 of them being interactive. As shown in <ref type="table">Table 11</ref>, our model is able to assign highest scores to all four interactive pairs and suppress all non-interactive pairs. However, we do notice that small and clustered boxes can reduce the confidence of our model, e.g. person <ref type="bibr" target="#b6">(7)</ref> and horse <ref type="bibr" target="#b5">(6)</ref>. This issue can also be seen in <ref type="figure" target="#fig_9">Figure 8b</ref> and <ref type="table" target="#tab_0">Table 12</ref>. Our model is able to find the correct human-suitcase pairs <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b10">11)</ref>, <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b4">5)</ref> and predict high scores for them. Yet the positive pair (3, 4) receives a very low score due to the size of the bounding boxes and less confident object detection scores. We also notice that person (10) and suitcase (8) receive a fairly high score for carrying a suitcase. This is due to the close relative location between the pair and a plausible gesture from the person. In such scenarios, access to the depth information could be helpful. We also show some qualitative results where our model does not improve upon previous methods in <ref type="figure" target="#fig_10">Figure 9</ref>. For examples such as in <ref type="figure" target="#fig_10">Figure 9a</ref>, where there is only one human-object pair, our graphical model is not particularly superior as there are only one human and object node each passing messages between each other. And in <ref type="figure" target="#fig_10">Figure 9b</ref>, when both human-zebra pairs are in fact interactive under the interaction petting a zebra, we found that the baseline model with appearance only is also able to correctly assign high scores to both pairs, as shown in <ref type="table">Table 13</ref>. <ref type="table">Table 13</ref>. Scores for the interaction petting a zebra in <ref type="figure" target="#fig_10">Figure 9b</ref> Human-zebra pairs Scores (baseline) Scores (ours)</p><p>(1, 2) 0.6782 0.7019 <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref> 0.6945 0.6799</p><p>To sum up, we found that our graphical model with spatial conditioning is more competitive on images with large number of human and object instances, particularly when there are multiple ground truth pairs of the same interaction, but does not improve upon previous methods on clean images with very few distractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Ablations</head><p>Apart from the main contribution of the paper, we found a few other training techniques beneficial to our model. First, a larger batch size helps to stablise the focal loss. We normalise the focal loss by the number of positive logits, which in itself is a very unstable statistic. Increasing the batch size from 4 to 32 results in roughly 0.8 mAP improvement. Second, using AdamW <ref type="bibr" target="#b21">[22]</ref> instead of SGD contributes about 1 mAP to our model's performance. We attribute this improvement to the similarity between graphical models and transformers <ref type="bibr" target="#b28">[29]</ref>, for which AdamW is the de facto choice of optimiser. Last, we observe a further 1 mAP improvement from fine-tuning the backbone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>An image with detected human and object instances (b) Adjacency matrix normalised by rows (L) and columns (R)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FusionFigure 3 .</head><label>3</label><figDesc>Structure of the multi-branch fusion module. The appearance and spatial features are mapped to c subspaces, fused and mapped to an intermediate representation size. The outputs of different branches are aggregated by taking the sum. The input and output dimensions of each FC layer are marked in the diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Object detections from the pre-trained MS COCO model (left) compared to the fine-tuned HICO-DET model (right). Boxes with scores higher than 0.5 are displayed. The fine-tuned detector suppresses objects that are less likely to be engaged in interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Model performance under different levels of corruption in appearance and spatial modalities, using the pre-trained detector (left) and the fine-tuned detector (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Interaction: sitting on a bench</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results with success and failure cases for our model. The scores corresponding to (a) are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>An image with detected human and object instances (b) Adjacency matrix computed with appearance features only, normalised by rows (left) and columns (right) (c) Adjacency matrix computed with spatial conditioning, normalised by rows (left) and columns (right) With apperance features only in the computation of adjacency matrix, the matrix is dominated by salient object (b). With spatial conditioning, the matrix is able to reflect the inherent binary interactiveness between human-object pairs (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results. The scores corresponding to (a) are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative results where images contain a small number of clean human and object instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>HOI detection performance (mAP×100) on the HICO-DET<ref type="bibr" target="#b2">[3]</ref> test set under the default setting. See appendix for the known object setting. The most competitive method in each category is in bold, while the second best is underlined.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Full</cell><cell cols="2">Rare Non-rare</cell></row><row><cell cols="5">DETECTOR PRE-TRAINED ON MS COCO</cell></row><row><cell cols="2">HO-RCNN [3] CaffeNet</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell></row><row><cell cols="3">InteractNet [8] ResNet-50-FPN 9.94</cell><cell>7.16</cell><cell>10.77</cell></row><row><cell>GPNN [25]</cell><cell>ResNet-101</cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell></row><row><cell>iCAN [6]</cell><cell>ResNet-50</cell><cell>14.84</cell><cell>10.45</cell><cell>16.15</cell></row><row><cell cols="2">Bansal et al. [2] ResNet-101</cell><cell>16.96</cell><cell>11.73</cell><cell>18.52</cell></row><row><cell>TIN [17]</cell><cell>ResNet-50</cell><cell>17.03</cell><cell>13.42</cell><cell>18.11</cell></row><row><cell cols="2">Gupta et al. [10] ResNet-152</cell><cell>17.18</cell><cell>12.17</cell><cell>18.68</cell></row><row><cell>RPNN [33]</cell><cell>ResNet-50</cell><cell>17.35</cell><cell>12.78</cell><cell>18.71</cell></row><row><cell cols="3">Wang et al. [11] ResNet-50-FPN 17.57</cell><cell>16.85</cell><cell>17.78</cell></row><row><cell>DRG [5]</cell><cell cols="2">ResNet-50-FPN 19.26</cell><cell>17.74</cell><cell>19.71</cell></row><row><cell cols="3">Peyre et al. [24] ResNet-50-FPN 19.40</cell><cell>14.63</cell><cell>20.87</cell></row><row><cell>VCL [14]</cell><cell>ResNet50</cell><cell>19.43</cell><cell>16.55</cell><cell>20.29</cell></row><row><cell>VSGNet [28]</cell><cell>ResNet-152</cell><cell>19.80</cell><cell>16.05</cell><cell>20.91</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>23.36</cell><cell>22.47</cell><cell>23.63</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 21.85</cell><cell>18.11</cell><cell>22.97</cell></row><row><cell cols="5">DETECTOR FINE-TUNED ON HICO-DET</cell></row><row><cell>PPDM [18]</cell><cell>Hourglass-104</cell><cell>21.73</cell><cell>13.78</cell><cell>24.10</cell></row><row><cell cols="2">Bansal et al. [2] ResNet-101</cell><cell>21.96</cell><cell>16.43</cell><cell>23.63</cell></row><row><cell>VCL [14]</cell><cell>ResNet50</cell><cell>23.63</cell><cell>17.21</cell><cell>25.55</cell></row><row><cell>DRG [5]</cell><cell cols="2">ResNet-50-FPN 24.53</cell><cell>19.47</cell><cell>26.04</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>26.29</cell><cell>22.61</cell><cell>27.39</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 31.33</cell><cell>24.72</cell><cell>33.31</cell></row><row><cell></cell><cell cols="2">ORACLE DETECTOR</cell><cell></cell><cell></cell></row><row><cell>iCAN [6]</cell><cell>ResNet-50</cell><cell>33.38</cell><cell>21.43</cell><cell>36.95</cell></row><row><cell>TIN [17]</cell><cell>ResNet50</cell><cell>34.26</cell><cell>22.90</cell><cell>37.65</cell></row><row><cell cols="3">Peyre et al. [24] ResNet-50-FPN 34.35</cell><cell>27.57</cell><cell>36.38</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>43.98</cell><cell>40.27</cell><cell>45.09</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 51.53</cell><cell>41.01</cell><cell>54.67</cell></row><row><cell cols="5">Table 3. Performance (mAP×100) on the V-COCO [9] test set.</cell></row><row><cell cols="5">The most competitive method in each category is in bold, while</cell></row><row><cell cols="5">the second best is underlined. Using a fine-tuned detector.</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Scenario 1</cell><cell>Scenario 2</cell></row><row><cell cols="2">InteractNet [8] ResNet-50-FPN</cell><cell>40.0</cell><cell></cell><cell>-</cell></row><row><cell>GPNN [25]</cell><cell>ResNet-101</cell><cell>44.0</cell><cell></cell><cell>-</cell></row><row><cell>iCAN [6]</cell><cell>ResNet-50</cell><cell>45.3</cell><cell></cell><cell>52.4</cell></row><row><cell>TIN [17]</cell><cell>ResNet-50</cell><cell>47.8</cell><cell></cell><cell>54.2</cell></row><row><cell>DRG [5]</cell><cell>ResNet-50-FPN</cell><cell>51.0</cell><cell></cell><cell>-</cell></row><row><cell>VSGNet [28]</cell><cell>ResNet-152</cell><cell>51.8</cell><cell></cell><cell>57.0</cell></row><row><cell cols="2">Wang et al. [11] ResNet-50-FPN</cell><cell>52.7</cell><cell></cell><cell>-</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>53.3</cell><cell></cell><cell>60.3</cell></row><row><cell>Ours</cell><cell>ResNet-50-FPN</cell><cell>53.0</cell><cell></cell><cell>58.2</cell></row><row><cell>Ours</cell><cell>ResNet-50-FPN</cell><cell>54.2</cell><cell></cell><cell>60.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Difference in performance between models with appear-</cell></row><row><cell cols="4">ance and spatial features (Ours) and with only appearance features</cell></row><row><cell cols="3">(baseline), as detection quality increases to the right.</cell><cell></cell></row><row><cell>Detector</cell><cell>COCO</cell><cell>HICO-DET</cell><cell>Oracle</cell></row><row><cell>Performance ∆</cell><cell>+1.93</cell><cell>+2.90</cell><cell>+4.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablating the addition of spatial conditioning at different stages of the model on the HICO-DET dataset (mAP×100).</figDesc><table><row><cell>Stage</cell><cell>COCO Detector</cell><cell>HICO-DET Detector</cell></row><row><cell>None</cell><cell>19.92</cell><cell>28.43</cell></row><row><cell>Adjacency</cell><cell>20.56</cell><cell>29.48</cell></row><row><cell>Messages</cell><cell>20.79</cell><cell>30.06</cell></row><row><cell>Global features</cell><cell>20.44</cell><cell>29.51</cell></row><row><cell>Refinement</cell><cell>21.03</cell><cell>30.11</cell></row><row><cell>All (Ours)</cell><cell>21.85</cell><cell>31.33</cell></row><row><cell cols="3">Table 6. Ablating the multi-branch fusion design choices, includ-</cell></row><row><cell cols="3">ing the binary operation and the cardinality (c).</cell></row><row><cell>Design Choice</cell><cell cols="2">COCO Detector HICO-DET Detector</cell></row><row><cell>Product (c = 1)</cell><cell>21.18</cell><cell>30.75</cell></row><row><cell>Sum (c = 1)</cell><cell>21.35</cell><cell>30.87</cell></row><row><cell>Concat. (c = 1)</cell><cell>21.02</cell><cell>30.66</cell></row><row><cell>Product (c = 16)</cell><cell>21.85</cell><cell>31.33</cell></row><row><cell>Sum (c = 16)</cell><cell>21.81</cell><cell>31.07</cell></row><row><cell>Concat. (c = 16)</cell><cell>21.67</cell><cell>31.65</cell></row><row><cell cols="3">Table 7. Varying the number of message passing iterations (T ).</cell></row><row><cell>Model</cell><cell>COCO Detector</cell><cell>HICO-DET Detector</cell></row><row><cell>Ours (T = 0)</cell><cell>20.05</cell><cell>28.86</cell></row><row><cell>Ours (T = 1)</cell><cell>20.70</cell><cell>30.99</cell></row><row><cell>Ours (T = 2)</cell><cell>21.85</cell><cell>31.33</cell></row><row><cell>Ours (T = 3)</cell><cell>21.72</cell><cell>31.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 10 .</head><label>10</label><figDesc>HOI detection performance (mAP×100) on the HICO-DET<ref type="bibr" target="#b2">[3]</ref> test set under the known object setting. The most competitive method in each category is in bold, while the second best is underlined.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Full</cell><cell cols="2">Rare Non-rare</cell></row><row><cell cols="4">DETECTOR PRE-TRAINED ON MS COCO</cell><cell></cell></row><row><cell cols="2">HO-RCNN [3] CaffeNet</cell><cell>10.41</cell><cell>8.94</cell><cell>10.85</cell></row><row><cell>iCAN [6]</cell><cell>ResNet-50</cell><cell>16.26</cell><cell>11.33</cell><cell>17.73</cell></row><row><cell>TIN [17]</cell><cell>ResNet-50</cell><cell>19.17</cell><cell>15.51</cell><cell>20.26</cell></row><row><cell>DRG [5]</cell><cell cols="2">ResNet-50-FPN 23.40</cell><cell>21.75</cell><cell>23.89</cell></row><row><cell>VCL [14]</cell><cell>ResNet50</cell><cell>22.00</cell><cell>19.09</cell><cell>22.87</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>26.43</cell><cell>25.01</cell><cell>26.85</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 25.53</cell><cell>21.79</cell><cell>26.64</cell></row><row><cell cols="4">DETECTOR FINE-TUNED ON HICO-DET</cell><cell></cell></row><row><cell>PPDM [18]</cell><cell>Hourglass-104</cell><cell>24.58</cell><cell>16.65</cell><cell>26.84</cell></row><row><cell>VCL [14]</cell><cell>ResNet50</cell><cell>25.98</cell><cell>19.12</cell><cell>28.03</cell></row><row><cell>DRG [5]</cell><cell cols="2">ResNet-50-FPN 27.98</cell><cell>23.11</cell><cell>29.43</cell></row><row><cell>IDN [16]</cell><cell>ResNet50</cell><cell>28.24</cell><cell>24.47</cell><cell>29.37</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 34.37</cell><cell>27.18</cell><cell>36.52</cell></row><row><cell></cell><cell cols="2">ORACLE DETECTOR</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50-FPN 51.75</cell><cell>41.40</cell><cell>54.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 12 .</head><label>12</label><figDesc>Scores for the interaction carrying a suitcase in Figure 8b. Each column corresponds to pairs with the same human instance. Each row corresponds to pairs with the same suitcase instance. Missing indices correspond to detections other than suitcases.</figDesc><table><row><cell>Instance index</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>6</cell><cell>9</cell><cell>10</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions. Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DRG: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">iCAN: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Mach. Vis. Conf</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fast R-CNN. Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Weishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbiao</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PPDM: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VS-GNet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
