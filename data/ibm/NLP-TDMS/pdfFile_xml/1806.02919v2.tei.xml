<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Local Recurrent Network for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change Loy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.educcloy@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Local Recurrent Network for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. <ref type="formula">(2)</ref> We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images.</p><p>(3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice [43] that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters. The code is available at https://github.com/Ding-Liu/NLRN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image restoration is an ill-posed inverse problem that aims at estimating the underlying image from its degraded measurements. Depending on the type of degradation, image restoration can be categorized into different sub-problems, e.g., image denoising and image super-resolution (SR). The key to successful restoration typically relies on the design of an effective regularizer based on image priors. Both local and non-local image priors have been extensively exploited in the past. Considering image denoising as an example, local image properties such as Gaussian filtering and total variation based methods <ref type="bibr" target="#b31">[32]</ref> are widely used in early studies. Later on, the notion of self-similarity in natural images draws more attention and it has been exploited by non-local-based methods, e.g., non-local means <ref type="bibr" target="#b1">[2]</ref>, collaborative filtering <ref type="bibr" target="#b7">[8]</ref>, joint sparsity <ref type="bibr" target="#b27">[28]</ref>, and low-rank modeling <ref type="bibr" target="#b15">[16]</ref>. These non-local methods are shown to be effective in capturing the correlation among non-local patches to improve the restoration quality.</p><p>While non-local self-similarity has been extensively studied in the literature, approaches for capturing this intrinsic property with deep networks are little explored. Recent convolutional neural networks (CNNs) for image restoration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref> achieve impressive performance over conventional approaches but do not explicitly use self-similarity properties in images. To rectify this weakness, a few studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> apply block matching to patches before feeding them into CNNs. Nevertheless, the block matching step is isolated and thus not jointly trained with image restoration networks.</p><p>In this paper, we present the first attempt to incorporate non-local operations in CNN for image restoration, and propose a non-local recurrent network (NLRN) as an efficient yet effective network 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr√©al, Canada. arXiv:1806.02919v2 [cs.CV] 11 Dec 2018 with non-local module. First, we design a non-local module to produce reliable feature correlation for self-similarity measurement given severely degraded images, which can be flexibly integrated into existing deep networks while embracing the benefit of end-to-end learning. For high parameter efficiency without compromising restoration quality, we deploy a recurrent neural network (RNN) framework similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> such that operations with shared weights are applied recursively. Second, we carefully study the behavior of non-local operation in deep feature space and find that limiting the neighborhood of correlation computation improves its robustness to degraded images. The confined neighborhood helps concentrate the computation on relevant features in the spatial vicinity and disregard noisy features, which is in line with conventional image restoration approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. In addition, we allow message passing of non-local operations between adjacent recurrent states of RNN. Such inter-state flow of feature correlation facilitates more robust correlation estimation. By combining the non-local operation with typical convolutions, our NLRN can effectively capture and employ both local and non-local image properties for image restoration.</p><p>It is noteworthy that recent work has adopted similar ideas on video classification <ref type="bibr" target="#b42">[43]</ref>. However, our method significantly differs from it in the following aspects. For each location, we measure the feature correlation of each location only in its neighborhood, rather than throughout the whole image as in <ref type="bibr" target="#b42">[43]</ref>. In our experiments, we show that deep features useful for computing non-local priors are more likely to reside in neighboring regions. A larger neighborhood (the whole image as one extreme) can lead to inaccurate correlation estimation over degraded measurements. In addition, our method fully exploits the advantage of RNN architecture -the correlation information is propagated among adjacent recurrent states to increase the robustness of correlation estimation to degradations of various degrees. Moreover, our non-local module is flexible to handle inputs of various sizes, while the module in <ref type="bibr" target="#b42">[43]</ref> handles inputs of fixed sizes only.</p><p>We introduce NLRN by first relating our proposed model to other classic and existing non-local image restoration approaches in a unified framework. We thoroughly analyze the non-local module and recurrent architecture in our NLRN via extensive ablation studies. We provide a comprehensive comparison with recent competitors, in which our NLRN achieves state-of-the-art performance in image denoising and SR over several benchmark datasets, demonstrating the superiority of the non-local operation with recurrent architecture for image restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image self-similarity as an important image characteristic has been used in a number of non-localbased image restoration approaches. The early works include bilateral filtering <ref type="bibr" target="#b39">[40]</ref> and non-local means <ref type="bibr" target="#b1">[2]</ref> for image denoising. Recent approaches exploit image self-similarity by imposing sparsity <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>. Alternatively, similar image patches are modeled with low-rankness <ref type="bibr" target="#b15">[16]</ref>, or by collaborative Wiener filtering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref>. Neighborhood embedding is a common approach for image SR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>, in which each image patch is approximated by multiple similar patches in a manifold. Self-example based image SR approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref> exploit the local self-similarity assumption, and extract LR-HR exemplar pairs merely from the low-resolution image across different scales to predict the high-resolution image. Similar ideas are adopted for image deblurring <ref type="bibr" target="#b8">[9]</ref>.</p><p>Deep neural networks have been prevalent for image restoration. The pioneering works include a multilayer perceptron for image denoising <ref type="bibr" target="#b2">[3]</ref> and a three-layer CNN for image SR <ref type="bibr" target="#b9">[10]</ref>. Deconvolution is adopted to save computation cost and accelerate inference speed <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b10">11]</ref>. Very deep CNNs are designed to boost SR accuracy in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. Dense connections among various residual blocks are included in <ref type="bibr" target="#b40">[41]</ref>. Similarly CNN based methods are developed for image denoising in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b26">27]</ref>. Block matching as a preprocessing step is cascaded with CNNs for image denoising <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref>. Besides CNNs, RNNs have also been applied for image restoration while enjoying the high parameter efficiency <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In addition to image restoration, feature correlations are widely exploited along with neural networks in many other areas, including graphical models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>, relational reasoning <ref type="bibr" target="#b32">[33]</ref>, machine translation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref> and so on. We do not elaborate on them here due to the limitation of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-Local Operations for Image Restoration</head><p>In this section, we first present a unified framework of non-local operations used for image restoration methods, e.g., collaborative filtering <ref type="bibr" target="#b7">[8]</ref>, non-local means <ref type="bibr" target="#b1">[2]</ref>, and low-rank modeling <ref type="bibr" target="#b15">[16]</ref>, and we discuss the relations between them. We then present the proposed non-local operation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A General Framework</head><p>In general, a non-local operation takes a multi-channel input X ‚àà R N √óm as the image feature, and generates output feature Z ‚àà R N √ók . Here N and m denote the number of image pixels and data channels, respectively. We propose a general framework with the following formulation:</p><formula xml:id="formula_0">Z = diag{Œ¥(X)} ‚àí1 Œ¶(X) G(X) .<label>(1)</label></formula><p>Here, Œ¶(X) ‚àà R N √óN is the non-local correlation matrix, and G(X) ‚àà R N √ók is the multi-channel non-local transform. Each row vector X i denotes the local features in location i. Œ¶(X) j i represents the relationship between the X i and X j , and each row vector G(X) j is the embedding of X j . <ref type="bibr" target="#b0">1</ref> The diagonal matrix diag{Œ¥(X)} ‚àà R N √óN normalizes the output at each i-th pixel with normalization factor Œ¥ i (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classic Methods</head><p>The proposed framework works with various classic non-local methods for image restoration, including methods based on low-rankness <ref type="bibr" target="#b15">[16]</ref>, collaborative filtering <ref type="bibr" target="#b7">[8]</ref>, joint sparsity <ref type="bibr" target="#b27">[28]</ref>, as well as non-local mean filtering <ref type="bibr" target="#b1">[2]</ref>.</p><p>Block matching (BM) is a commonly used approach for exploiting non-local image structures in conventional methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>. A q √ó q spatial neighborhood is set to be centered at each location i, and X i reduces to the image patch centered at i. BM selects the K i most similar patches (K i q 2 ) from this neighborhood, which are used jointly to restore X i . Under the proposed non-local framework, these methods can be represented as</p><formula xml:id="formula_1">Z i = 1 Œ¥ i (X) j‚ààCi Œ¶(X) j i G(X) j , ‚àÄi .<label>(2)</label></formula><p>Here Œ¥ i (X) = j‚ààCi Œ¶(X) j i and C i denotes the set of indices of the K i selected patches. Thus, each row Œ¶(X) i has only K i non-zero entries. The embedding G(X) and the non-zero elements vary for non-local methods based on different models. For example, in WNNM <ref type="bibr" target="#b15">[16]</ref>, j‚ààCi Œ¶(X) j i G(X) j corresponds to the projection of X i onto the group-specific subspace as a function of the selected patches. Specifically, the subspace for calculating Z i is spanned by the eigenvectors U i of X T Ci X Ci . Thus Z i = X Ci U i diag{œÉ}U T i , where diag{œÉ} is obtained by applying the shrinkage function associated with the weighted nuclear norm <ref type="bibr" target="#b15">[16]</ref> to the eigenvalues of X T Ci X Ci . We show the generalization about more classic non-local image restoration methods in Section 7.</p><p>Except for the hard block matching, other methods, e.g., the non-local means algorithm <ref type="bibr" target="#b1">[2]</ref>, apply soft block matching by calculating the correlation between the reference patch and each patch in the neighborhood. Each element Œ¶(X) j i is determined only by each</p><formula xml:id="formula_2">{X i , X j } pair, so Œ¶(X) j i = œÜ(X i , X j ), where œÜ( ¬∑ )</formula><p>is determined by the distance metric. In <ref type="bibr" target="#b1">[2]</ref>, weighted Euclidean distance with Gaussian kernel is applied as the metric, such that œÜ(</p><formula xml:id="formula_3">X i , X j ) = exp{‚àí X i ‚àí X j 2 2,a /h 2 }.</formula><p>Besides, identity mapping is directly used as the embedding in <ref type="bibr" target="#b1">[2]</ref>, i.e., G(X) j = X j . In this case, the non-local framework in (1) reduces to</p><formula xml:id="formula_4">Z i = 1 Œ¥ i (X) j‚ààSi exp{‚àí X i ‚àí X j 2 2,a h 2 }X j , ‚àÄi,<label>(3)</label></formula><p>where Œ¥ i (X) = j‚ààSi exp{‚àí X i ‚àí X j 2 2,a /h 2 } and S i is the set of indices in the neighborhood of X i . Note that both a and h are constants, denoting the standard deviation of Gaussian kernel, and the degree of filtering, respectively <ref type="bibr" target="#b1">[2]</ref>. It is noteworthy that the cardinality of S i for soft BM is much larger than that of C i for hard BM, which gives more flexibility of using feature correlations between neighboring locations.</p><p>The conventional non-local methods suffer from the drawback that parameters are either fixed <ref type="bibr" target="#b1">[2]</ref>, or obtained by suboptimal approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, e.g., the parameters of WNNM are learned based on the low-rankness assumption, which is suboptimal as the ultimate objective is to minimize the image reconstruction error. <ref type="figure">Figure 1</ref>: An illustration of our non-local module working on a single location. The white tensor denotes the deep feature representation of an entire image. The red fiber is the features of this location and the blue tensor denotes the features in its neighborhood. Œ∏, œà and g are implemented by 1 √ó 1 convolution followed by reshaping operations.</p><formula xml:id="formula_5">softmax 1 √ó √ó 2 1 √ó 1 √ó 2 2 √ó 1 1 √ó √ó 1 √ó 1 √ó 1 √ó 1 √ó 1 1 : 1 √ó 1 √ó : 1 √ó 1 √ó : 1 √ó 1 √ó</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Proposed Non-Local Module</head><p>Based on the general non-local framework in (1), we propose another soft block matching approach and apply the Euclidean distance with linearly embedded Gaussian kernel <ref type="bibr" target="#b42">[43]</ref> as the distance metric. The linear embeddings are defined as follows:</p><formula xml:id="formula_6">Œ¶(X) j i = œÜ(X i , X j ) = exp{Œ∏(X i )œà(X j ) T } , ‚àÄi, j , (4) Œ∏(X i ) = X i W Œ∏ , œà(X i ) = X i W œà , G(X) i = X i W g , ‚àÄi .<label>(5)</label></formula><p>The embedding transforms W Œ∏ , W œÜ , and W g are all learnable and have the shape of m √ó l, m √ó l, m √ó m, respectively. Thus, the proposed non-local operation can be written as</p><formula xml:id="formula_7">Z i = 1 Œ¥ i (X) j‚ààSi exp {X i W Œ∏ W T œà X T j } X i W g , ‚àÄi ,<label>(6)</label></formula><p>where Œ¥ i (X) = j‚ààSi œÜ(X i , X j ). Similar to <ref type="bibr" target="#b1">[2]</ref>, to obtain Z i , we evaluate the correlation between X i and each X j in the neighborhood S i . More choices of œÜ(X i , X j ) are discussed in Section 5.</p><p>The proposed non-local operation can be implemented by common differentiable operations, and thus can be jointly learned when incorporated into a neural network. We wrap it as a non-local module by adding a skip connection, as shown in <ref type="figure">Figure 1</ref>, since the skip connection enables us to insert a non-local module into any pre-trained model, while maintaining its initial behavior by initializing W g as zero. Such a module introduces only a limited number of parameters since Œ∏, œà and g are 1 √ó 1 convolutions and m = 128, l = 64 in practice. The output of this module on each location only depends on its q √ó q neighborhood, so this operation can work on inputs of various sizes.</p><p>Relation to Other Methods: Recent works have combined non-local BM and neural networks for image restoration <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43]</ref>. Lefkimmiatis <ref type="bibr" target="#b23">[24]</ref> proposed to first apply BM to noisy image patches. The hard BM results are used to group patch features, and a CNN conducts a trainable collaborative filtering over the matched patches. Qiao et al. <ref type="bibr" target="#b30">[31]</ref> combined similar non-local BM with TNRD networks <ref type="bibr" target="#b6">[7]</ref> for image denoising. However, as conventional methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, these works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> conduct hard BM directly over degraded input patches, which may be inaccurate over severely degraded images. In contrast, our proposed non-local operation as soft BM is applied on learned deep feature representations that are more robust to degradation. Furthermore, the matching results in <ref type="bibr" target="#b23">[24]</ref> are isolated from the neural network, similar to the conventional approaches, whereas the proposed non-local module is trained jointly with the entire network in an end-to-end manner.</p><p>Wang et al. <ref type="bibr" target="#b42">[43]</ref> used similar approaches to add non-local operations into neural networks for highlevel vision tasks. However, unlike our approach, Wang et al. <ref type="bibr" target="#b42">[43]</ref> calculated feature correlations throughout the whole image. which is equivalent to enlarging the neighborhood to the entire image in our approach. We empirically show that increasing the neighborhood size does not always improve image restoration performance, due to the inaccuracy of correlation estimation over degraded input images. Hence it is imperative to choose a neighborhood of a proper size to achieve best performance for image restoration. In addition, the non-local operation in <ref type="bibr" target="#b42">[43]</ref> can only handle input images of fixed size, while our module in <ref type="formula" target="#formula_7">(6)</ref> is flexible to various image sizes. Finally, our non-local module, when incorporated into an RNN framework, allows the flow of correlation information between adjacent states to enhance robustness against inaccurate correlation estimation. This is a new unique formulation to deal with degraded images. More details are provided next.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Non-Local Recurrent Network</head><p>In this section, we describe the RNN architecture that incorporates the non-local module to form our NLRN. We adopt the common formulation of an RNN, which consists of a set of states, namely, input state, output state and recurrent state, as well as transition functions among the states. The input, output, and recurrent states are represented as x, y and s respectively. At each time step t, an RNN receives an input x t , and the recurrent state and the output state of the RNN are updated recursively as follows:</p><formula xml:id="formula_8">s t = f input (x t ) + f recurrent (s t‚àí1 ), y t = f output (s t ),<label>(7)</label></formula><p>where f input , f output , and f recurrent are reused at every time step. In our NLRN, we set the following:</p><p>‚Ä¢ s 0 is a function of the input image I. ‚Ä¢ x t = 0, ‚àÄt ‚àà {1, . . . , T }, and f input (0) = 0.</p><p>‚Ä¢ The output state y t is calculated only at the time T as the final output. We add an identity path from the very first state which helps gradient backpropagation during training <ref type="bibr" target="#b36">[37]</ref>, and a residual path of the deep feature correlation between each location and its neighborhood from the previous state. Hence, s t = {s t feat , s t corr }, and s t = f recurrent (s t‚àí1 , s 0 ), ‚àÄt ‚àà {1, . . . , T }, where s t feat denotes the feature map in time t and s t corr is the collection of deep feature correlation. For the transition function f recurrent , a non-local module is first adopted and is followed by two convolutional layers, before the feature s 0 is added from the identity path. The weights in the non-local module are shared across recurrent states just as convolutional layers, so our NLRN still keeps high parameter efficiency as a whole. An illustration is displayed in <ref type="figure">Figure 2</ref>.</p><p>It is noteworthy that inside the non-local module, the feature correlation for location i from the previous state, s t‚àí1 corr,i , is added to the estimated feature correlation in the current state before the softmax normalization, which enables the propagation of correlation information between adjacent states for more robust correlation estimation. The details can be found in <ref type="figure" target="#fig_0">Figure 3</ref>. The initial state s 0 is set as the feature after a convolutional layer on the input image. f output is represented by another single convolutional layer. All layers have 128 filters with 3 √ó 3 kernel size except for the non-local module. Batch normalization and ReLU activation function are performed ahead of each convolutional layer following <ref type="bibr" target="#b18">[19]</ref>. We adopt residual learning and the output of NLRN is the residual image√é = f output (s T ) when NLRN is unfolded T times. During training, the objective is to minimize the mean square error L(√é,ƒ®) = 1 2 ||√é + I ‚àíƒ®|| 2 , whereƒ® denotes the ground truth image. Relation to Other RNN Methods: Although RNNs have been adopted for image restoration before, our NLRN is the first to incorporate non-local operations into an RNN framework with correlation propagation. DRCN <ref type="bibr" target="#b21">[22]</ref> recursively applies a single convolutional layer to the input feature map multiple times without the identity path from the first state. DRRN <ref type="bibr" target="#b36">[37]</ref> applies both the identity path and the residual path in each state, but without non-local operations, and thus there is no correlation information flow across adjacent states. MemNet <ref type="bibr" target="#b37">[38]</ref> builds dense connections among several types of memory blocks, and weights are shared in the same type of memory blocks but are different across various types. Compared with MemNet, our NLRN has an efficient yet effective RNN structure with shallower effective depth and fewer parameters, but obtains better restoration performance, which is shown in Section 5 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset: For image denoising, we adopt two different settings to fairly and comprehensively compare with recent deep learning based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b37">38]</ref>: (1) As in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b23">24]</ref>, we choose as the training set the combination of 200 images from the train set and 200 images from the test set in the Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b29">[30]</ref>, and test on two popular benchmarks: Set12 and Set68 with œÉ = 15, 25, 50 following <ref type="bibr" target="#b50">[51]</ref>. <ref type="bibr" target="#b1">(2)</ref> As in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>, we use as the training set the combination of 200 images from the train set and 100 images from the val set in BSD, and test on Set14 and the BSD test set of 200 images with œÉ = 30, 50, 70 following <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>. In addition, we evaluate our NLRN on the Urban100 dataset <ref type="bibr" target="#b19">[20]</ref>, which contains abundant structural patterns and textures, to further demonstrate the capability of using image self-similarity of our NLRN. The training set and test set are strictly disjoint and all the images are converted to gray-scale in each experiment setup. For image SR, we follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> and use a training set of 291 images where 91 images are proposed in <ref type="bibr" target="#b47">[48]</ref> and other 200 are from the BSD train set. We adopt four benchmark sets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b49">[50]</ref>, BSD100 <ref type="bibr" target="#b29">[30]</ref> and Urban100 <ref type="bibr" target="#b19">[20]</ref> for testing with three upscaling factors: √ó2, √ó3 and √ó4. The low-resolution images are synthesized by bicubic downsampling.</p><p>Training Settings: We randomly sample patches whose size equals the neighborhood of non-local operation from images during training. We use flipping, rotation and scaling for augmenting training data. For image denoising, we add independent and identically distributed Gaussian noise with zero mean to the original image as the noisy input during training. We train a different model for each noise level. For image SR, only the luminance channel of images is super-resolved, and the other two color channels are upscaled by bicubic interpolation, following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. Moreover, the training images for all three upscaling factors: √ó2, √ó3 and √ó4 are upscaled by bicubic interpolation into the desired spatial size and are combined into one training set. We use this set to train one single model for all these three upscaling factors as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>We use Adam optimizer to minimize the loss function. We set the initial learning rate as 1e-3 and reduce it by half five times during training. We use Xavier initialization for the weights. We clip the gradient at the norm of 0.5 to prevent the gradient explosion which is shown to empirically accelerate training convergence, and we adopt 16 as the minibatch size during training. Training a model takes about 3 days with a Titan Xp GPU. For non-local module, we use circular padding for the neighborhood outside input patches. For convolution, we pad the boundaries of feature maps with zeros to preserve the spatial size of feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Analysis</head><p>In this section, we analyze our model in the following aspects. First, we conduct the ablation study of using different distance metrics in the non-local module. <ref type="table" target="#tab_1">Table 1</ref> compares instantiations including Euclidean distance, dot product, embedded dot product, Gaussian, symmetric embedded Gaussian and embedded Gaussian when used in NLRN of 12 unfolded steps. Embedded Gaussian achieves the best performance and is adopted in the following experiments.</p><p>We compare the NLRN with its variants in terms of PSNR in <ref type="table" target="#tab_2">Table 2</ref>. We have a few observations. First, the same model with untied weights performs worse than its weight-sharing counter-part. We speculate that the model with untied weights is prone to model over-fitting and suffers much slower training convergence, both of which undermine its performance. To investigate the function of nonlocal modules, we implement a baseline RNN with the same parameter number of NLRN, and find it is worse than NLRN by about 0.2 dB, showing the advantage of using non-local image properties for image restoration. Besides, we implement NLRNs where non-local module is used in every other state or every three states, and observe that if the frequency of using non-local modules in NLRN is reduced, the performance decreases accordingly. We show the benefit of propagating correlation information among adjacent states by comparing with the counter-part in terms of restoration accuracy. To further analyze the non-local module, we visualize the feature correlation maps for non-local operations in <ref type="figure">Figure 4</ref>. It can be seen that as the number of recurrent states increases, the locations   with similar features progressively show higher correlations in the map, which demonstrates the effectiveness of the non-local module for exploiting image self-similarity. <ref type="figure">Figure 5</ref> investigates the influence of the neighborhood size in the non-local module on image denoising results. The performance peaks at q = 45. This shows that limiting the neighborhood helps concentrate the correlation calculation on relevant features in the spatial vicinity and enhance correlation estimation. Therefore, it is necessary to choose a proper neighborhood size (rather than the whole image) for image restoration. We select q = 45 for the rest of this paper unless stated otherwise.</p><p>The unrolling length T determines the maximum effective depth (i.e., maximum number of convolutional layers) of NLRN. The influence of the unrolling length on image denoising results is shown in <ref type="figure">Figure 6</ref>. The performance increases as the unrolling length rises, but gets saturated after T = 12.</p><p>Given the tradeoff between restoration accuracy and inference time, we adopt T = 12 for NLRN in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with State-of-the-Art Methods</head><p>We compare our proposed model with a number of recent competitors for image denoising and image SR, respectively. PSNR and SSIM <ref type="bibr" target="#b43">[44]</ref> are adopted for measuring quantitative restoration performance.</p><p>Image Denoising: For a fair comparison with other methods based on deep networks, we train our model under two settings: (1) We use the training data as in TNRD <ref type="bibr" target="#b6">[7]</ref>, DnCNN <ref type="bibr" target="#b50">[51]</ref> and NLNet <ref type="bibr" target="#b23">[24]</ref>, and the result is shown in <ref type="table" target="#tab_4">Table 4</ref>. We cite the result of NLNet in the original paper <ref type="bibr" target="#b23">[24]</ref>, since no public code or model is available. (2) We use the training data as in RED <ref type="bibr" target="#b28">[29]</ref> and MemNet <ref type="bibr" target="#b37">[38]</ref>, and the result is shown in <ref type="table" target="#tab_5">Table 5</ref>. We note that RED uses multi-view testing <ref type="bibr" target="#b44">[45]</ref> to boost the restoration accuracy, i.e., RED processes each test image as well as its rotated and flipped versions, and all the outputs are then averaged to form the final denoised image. Accordingly, we perform the same procedure for NLRN and find its performance, termed as NLRN-MV, is consistently improved. In addition, we include recent non-deep-learning based methods: BM3D <ref type="bibr" target="#b7">[8]</ref> and WNNM <ref type="bibr" target="#b15">[16]</ref> in our comparison. We do not list other methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52</ref>] whose average performances are worse than DnCNN or MemNet. Our NLRN significantly outperforms all the competitors on Urban100 and yields the best results across almost all the noise levels and datasets.</p><p>To further show the advantage of the network design of NLRN, we compare different versions of NLRN with several state-of-the-art network models, i.e., DnCNN, RED and MemNet in <ref type="table" target="#tab_3">Table 3</ref>. NLRN uses the fewest parameters but outperforms all the competitors. Specifically, NLRN benefits   from inherent parameter sharing and uses only less than 1/10 parameters of RED. Compared with the RNN competitor, MemNet, NLRN uses only half of parameters and much shallower depth to obtain better performance, which shows the superiority of our non-local recurrent architecture.</p><p>Image Super-Resolution: We compare our model with several recent SISR approaches, including SRCNN <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref type="bibr" target="#b20">[21]</ref>, DRCN <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b22">[23]</ref>, DRRN <ref type="bibr" target="#b36">[37]</ref> and MemNet <ref type="bibr" target="#b37">[38]</ref> in <ref type="table" target="#tab_6">Table 6</ref>. We crop pixels near image borders before calculating PSNR and SSIM as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. We do not list other methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref> since their performances are worse than that of DRRN or MemNet. Besides, we do not include SRDenseNet <ref type="bibr" target="#b40">[41]</ref> and EDSR <ref type="bibr" target="#b24">[25]</ref> in the comparison because the number of parameters in these two network models is over two orders of magnitude larger than that of our NLRN and their training datasets are significantly larger than ours. It can be seen that NLRN yields the best result across all the upscaling factors and datasets. Visual results are provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a new and effective recurrent network that incorporates non-local operations for image restoration. The proposed non-local module can be trained end-to-end with the recurrent network. We have studied the importance of computing reliable feature correlations within a confined neighorhood against the whole image, and have shown the benefits of passing feature correlation messages between adjacent recurrent stages. Comprehensive evaluations over benchmarks for image denoising and super-resolution demonstrate the superiority of NLRN over existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Extension of the General Framework to Other Classic Non-Local Methods</head><p>Besides the extension to WNNM and non-local means, which are discussed in Section 3.1, we show the proposed non-local framework (1) can be extended to collaborative filtering methods, e.g., BM3D algorithm <ref type="bibr" target="#b7">[8]</ref>, as well as joint sparsity based methods, e.g., LSSC algorithm <ref type="bibr" target="#b27">[28]</ref>. We follow the same notations in Section 3.1. Both BM3D and LSSC apply block matching (BM) first before processing, and form N groups of similar patches into data matrices. The index set of the matched patches for the i-th reference patch is denoted as C i . The group of matched patches for the i-th reference patch is denoted as X Ci .</p><p>Similar to WNNM <ref type="bibr" target="#b15">[16]</ref>, BM3D <ref type="bibr" target="#b7">[8]</ref> also applies BM first to group similar patches based on their Euclidean distances. The matched patches are then processed via Wiener filtering <ref type="bibr" target="#b7">[8]</ref>, and the denoised results of the i-th group of patches are Z Ci = œÑ ‚àí1 (diag(œâ)œÑ (X Ci )).</p><p>Here œÑ (¬∑) and œÑ ‚àí1 (¬∑) denote the forward and backward Wiener filtering applied to the groups of matched patches, respectively. The diagonal matrix diag(œâ) is formed by the empirical Wiener coefficients œâ. BM3D applies data pre-cleaning, using discrete cosine transform (DCT), to estimate the original patch, and calculate the estimate of œâ <ref type="bibr" target="#b7">[8]</ref>. Since calculating Z Ci in (8) involves only linear filtering, it can also be generalized using the proposed non-local framework as <ref type="bibr" target="#b1">(2)</ref>. Unlike the extension to WNNM, here j‚ààCi Œ¶(X) j i G(X) j corresponds to the denoised results via Wiener filtering as shown in <ref type="formula" target="#formula_9">(8)</ref>, of the i-th group of matched patches.</p><p>Different from BM3D and WNNM, LSSC learns a common dictionary D for all image patches, and imposes joint sparsity <ref type="bibr" target="#b27">[28]</ref> on each data matrix of matched patches X Ci , so that the correlation of the matched patches are exploited by enforcing the same support of their sparse codes. Thus, the joint sparse coding in LSSC <ref type="bibr" target="#b27">[28]</ref> become≈ù</p><formula xml:id="formula_10">A i = argmin Ai A i 0,‚àû s.t. X T Ci ‚àí DA i 2 F ‚â§ |C i | , ‚àÄi ,<label>(9)</label></formula><p>where the (0, ‚àû) "norm" ¬∑ 0,‚àû counts the number of non-zero columns of each sparse code matrix A i <ref type="bibr" target="#b27">[28]</ref>, and |C i | is the cardinality of C i . The coefficient is a constant, which is used to upper bound the sparse modeling errors. In general, the solution to (9) is NP-hard. To simplify the discussion, we assume the dictionary to be unitary (which reduces the sparse coding problem to the transform-model sparse coding <ref type="bibr" target="#b45">[46]</ref>), i.e., D T D = I and D ‚àà R k√ók . Thus there exists a corresponding shrinkage function Œ∑(¬∑) for imposing joint sparsity on the sparse codes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>, such that the denoised estimates of the i-th patch group can be obtained as Z Ci =√Ç T i D T = Œ∑( X Ci D ) D T . Though joint sparse coding projects all data onto a union of subspaces <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46]</ref> which is a non-linear operation in general, each data matrix X Ci is projected onto one particular subspace spanned by the selected atoms corresponding to the non-zero columns in√Ç i , which is locally linear. For the i-th group of patches, such a subspace projection corresponds to j‚ààCi Œ¶(X) j i G(X) j in the proposed general framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Visual Results</head><p>We show the visual comparison of our NLRN and several competing methods: BM3D <ref type="bibr" target="#b7">[8]</ref>, WNNM <ref type="bibr" target="#b15">[16]</ref>, and MemNet <ref type="bibr" target="#b37">[38]</ref> for image denoising in <ref type="figure" target="#fig_1">Figure 7</ref>. Our method can recover more details from the noisy measurement. The visual comparison of our NLRN and several recent methods: DRCN <ref type="bibr" target="#b21">[22]</ref>, LapSRN <ref type="bibr" target="#b22">[23]</ref>, DRRN <ref type="bibr" target="#b36">[37]</ref>, and MemNet <ref type="bibr" target="#b37">[38]</ref> for image super-resolution is displayed in <ref type="figure">Figure 8</ref>. Our method is able to reconstruct sharper edges and produce fewer artifacts especially in the regions of repetitive patterns. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The operations for a single location i in the non-local module used in NLRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of image denoising results with noise level of 30. The zoom-in region in the red bounding box is shown on the right. From top to bottom: 1) the image barbara. 2) image 004 in Urban100. 3) image 019 in Urban100. 4) image 033 in Urban100. 5) image 046 in Urban100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An illustration of the transition function frecurrent in the proposed NLRN.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>corr, ‚àí1</cell><cell>1 √ó 2</cell><cell></cell></row><row><cell>feat 0</cell><cell></cell><cell></cell><cell></cell><cell>feat</cell><cell>feat, ‚àí1</cell><cell>1 √ó 1 √ó</cell><cell>1 √ó</cell></row><row><cell>feat ‚àí1</cell><cell>Non-local</cell><cell>nl</cell><cell>conv</cell><cell>conv</cell><cell></cell><cell></cell><cell>1 √ó 2</cell><cell>1 √ó 2</cell><cell>corr,</cell></row><row><cell cols="8">module Figure 2: softmax corr ‚àí1 corr 2 √ó √ó 2 1 √ó √ó √ó feat, ‚àí1</cell><cell>1 √ó 1 √ó</cell><cell>nl,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Image denoising comparison of our proposed model with various distance metrics on Set12 with noise level of 25.</figDesc><table><row><cell>Distance metric</cell><cell>œÜ(X i , X j )</cell><cell></cell><cell>PSNR</cell></row><row><cell>Euclidean distance Dot product</cell><cell>exp{‚àí X i ‚àí X j X i X T j</cell><cell cols="2">2 2 /h 2 } 30.74 30.68</cell></row><row><cell>Embedded dot product</cell><cell cols="2">Œ∏(X i )œà(X j ) T</cell><cell>30.75</cell></row><row><cell>Gaussian</cell><cell cols="2">exp{X i X T j }</cell><cell>30.69</cell></row><row><cell>Symmetric embedded Gaussian</cell><cell cols="2">exp{Œ∏(X i )Œ∏(X j ) T }</cell><cell>30.76</cell></row><row><cell>Embedded Gaussian</cell><cell>exp{Œ∏(X</cell><cell></cell></row></table><note>i )œà(X j ) T } 30.80</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Image denoising comparison of our NLRN with its variants on Set12 with noise level of 25.</figDesc><table><row><cell>Model</cell><cell>PSNR</cell></row><row><cell>NLRN w/o parameter sharing</cell><cell>30.65</cell></row><row><cell>RNN with same parameter no.</cell><cell>30.61</cell></row><row><cell cols="2">Non-local module in every other state 30.76</cell></row><row><cell>Non-local module in every 3 states</cell><cell>30.72</cell></row><row><cell>NLRN w/o propagating correlations</cell><cell>30.78</cell></row><row><cell>NLRN</cell><cell>30.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Image denoising comparison of our proposed model with stateof-the-art network models on Set12 with noise level of 50. Model complexities are also compared.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Benchmark image denoising results. Training and testing protocols are followed as in<ref type="bibr" target="#b50">[51]</ref>. Average PSNR/SSIM for various noise levels on Set12, BSD68 and Urban100. The best performance is in bold.</figDesc><table><row><cell>Dataset</cell><cell>Noise</cell><cell>BM3D</cell><cell>WNNM</cell><cell>TNRD</cell><cell>NLNet</cell><cell>DnCNN</cell><cell>NLRN</cell></row><row><cell></cell><cell>15</cell><cell cols="3">32.37/0.8952 32.70/0.8982 32.50/0.8958</cell><cell>-/-</cell><cell cols="2">32.86/0.9031 33.16/0.9070</cell></row><row><cell>Set12</cell><cell>25</cell><cell cols="3">29.97/0.8504 30.28/0.8557 30.06/0.8512</cell><cell>-/-</cell><cell cols="2">30.44/0.8622 30.80/0.8689</cell></row><row><cell></cell><cell>50</cell><cell cols="3">26.72/0.7676 27.05/0.7775 26.81/0.7680</cell><cell>-/-</cell><cell cols="2">27.18/0.7829 27.64/0.7980</cell></row><row><cell></cell><cell>15</cell><cell cols="6">31.07/0.8717 31.37/0.8766 31.42/0.8769 31.52/-31.73/0.8907 31.88/0.8932</cell></row><row><cell>BSD68</cell><cell>25</cell><cell cols="6">28.57/0.8013 28.83/0.8087 28.92/0.8093 29.03/-29.23/0.8278 29.41/0.8331</cell></row><row><cell></cell><cell>50</cell><cell cols="6">25.62/0.6864 25.87/0.6982 25.97/0.6994 26.07/-26.23/0.7189 26.47/0.7298</cell></row><row><cell></cell><cell>15</cell><cell cols="3">32.35/0.9220 32.97/0.9271 31.86/0.9031</cell><cell>-/-</cell><cell cols="2">32.68/0.9255 33.45/0.9354</cell></row><row><cell>Urban100</cell><cell>25</cell><cell cols="3">29.70/0.8777 30.39/0.8885 29.25/0.8473</cell><cell>-/-</cell><cell cols="2">29.97/0.8797 30.94/0.9018</cell></row><row><cell></cell><cell>50</cell><cell cols="3">25.95/0.7791 26.83/0.8047 25.88/0.7563</cell><cell>-/-</cell><cell cols="2">26.28/0.7874 27.49/0.8279</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Benchmark image denoising results. Training and testing protocols are followed as in<ref type="bibr" target="#b37">[38]</ref>. Average PSNR/SSIM for various noise levels on 14 images, BSD200 and Urban100. Red is the best and blue is the second best performance.</figDesc><table><row><cell>Dataset</cell><cell>Noise</cell><cell>BM3D</cell><cell>WNNM</cell><cell>RED</cell><cell>MemNet</cell><cell>NLRN</cell><cell>NLRN-MV</cell></row><row><cell></cell><cell>30</cell><cell cols="6">28.49/0.8204 28.74/0.8273 29.17/0.8423 29.22/0.8444 29.37/0.8460 29.41/0.8472</cell></row><row><cell>14 images</cell><cell>50</cell><cell cols="6">26.08/0.7427 26.32/0.7517 26.81/0.7733 26.91/0.7775 27.00/0.7777 27.05/0.7791</cell></row><row><cell></cell><cell>70</cell><cell cols="6">24.65/0.6882 24.80/0.6975 25.31/0.7206 25.43/0.7260 25.49/0.7255 25.54/0.7273</cell></row><row><cell></cell><cell>30</cell><cell cols="6">27.31/0.7755 27.48/0.7807 27.95/0.8056 28.04/0.8053 28.15/0.8423 28.20/0.8436</cell></row><row><cell>BSD200</cell><cell>50</cell><cell cols="6">25.06/0.6831 25.26/0.6928 25.75/0.7167 25.86/0.7202 25.93/0.7214 25.97/0.8429</cell></row><row><cell></cell><cell>70</cell><cell cols="6">23.82/0.6240 23.95/0.6346 24.37/0.6551 24.53/0.6608 24.58/0.6614 24.62/0.6634</cell></row><row><cell></cell><cell>30</cell><cell cols="6">28.75/0.8567 29.47/0.8697 29.12/0.8674 29.10/0.8631 29.94/0.8830 29.99/0.8842</cell></row><row><cell>Urban100</cell><cell>50</cell><cell cols="6">25.95/0.7791 26.83/0.8047 26.44/0.7977 26.65/0.8030 27.38/0.8241 27.43/0.8256</cell></row><row><cell></cell><cell>70</cell><cell cols="6">24.27/0.7163 25.11/0.7501 24.75/0.7415 25.01/0.7496 25.66/0.7707 25.71/0.7724</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Benchmark SISR results. Average PSNR/SSIM for scale factor √ó2, √ó3 and √ó4 on datasets Set5, Set14, BSD100 and Urban100. The best performance is in bold. .8879 31.90/0.8960 31.85/0.8942 31.80/0.895 32.05/0.8973 32.08/0.8978 32.19/0.8992 √ó3 28.41/0.7863 28.82/0.7976 28.80/0.7963 28.82/0.797 28.95/0.8004 28.96/0.8001 29.06/0.8026 √ó4 26.90/0.7101 27.29/0.7251 27.23/0.7233 27.32/0.728 27.38/0.7284 27.40/0.7281 27.48/0.7306 Urban100 √ó2 29.50/0.8946 30.76/0.9140 30.75/0.9133 30.41/0.910 31.23/0.9188 31.31/0.9195 31.81/0.9249 √ó3 26.24/0.7989 27.14/0.8279 27.15/0.8276 27.07/0.827 27.53/0.8378 27.56/0.8376 27.93/0.8453 √ó4 24.52/0.7221 25.18/0.7524 25.14/0.7510 25.21/0.756 25.44/0.7638 25.50/0.7630 25.79/0.7729</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>SRCNN</cell><cell>VDSR</cell><cell>DRCN</cell><cell>LapSRN</cell><cell>DRRN</cell><cell>MemNet</cell><cell>NLRN</cell></row><row><cell></cell><cell>√ó2</cell><cell cols="7">36.66/0.9542 37.53/0.9587 37.63/0.9588 37.52/0.959 37.74/0.9591 37.78/0.9597 38.00/0.9603</cell></row><row><cell>Set5</cell><cell>√ó3</cell><cell cols="7">32.75/0.9090 33.66/0.9213 33.82/0.9226 33.82/0.923 34.03/0.9244 34.09/0.9248 34.27/0.9266</cell></row><row><cell></cell><cell>√ó4</cell><cell cols="7">30.48/0.8628 31.35/0.8838 31.53/0.8854 31.54/0.885 31.68/0.8888 31.74/0.8893 31.92/0.8916</cell></row><row><cell></cell><cell>√ó2</cell><cell cols="7">32.45/0.9067 33.03/0.9124 33.04/0.9118 33.08/0.913 33.23/0.9136 33.28/0.9142 33.46/0.9159</cell></row><row><cell>Set14</cell><cell>√ó3</cell><cell cols="7">29.30/0.8215 29.77/0.8314 29.76/0.8311 29.79/0.832 29.96/0.8349 30.00/0.8350 30.16/0.8374</cell></row><row><cell></cell><cell>√ó4</cell><cell cols="7">27.50/0.7513 28.01/0.7674 28.02/0.7670 28.19/0.772 28.21/0.7721 28.26/0.7723 28.36/0.7745</cell></row><row><cell></cell><cell>√ó2</cell><cell>31.36/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BSD100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our analysis, if A is a matrix, Ai, A j , and A j i denote its i-th row, j-th column, and the element at the i-th row and j-th column, respectively.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Low-complexity single-image superresolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">External patch prior guided internal clustering for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bm3d frames and variational image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Danielyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1715" to="1728" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust single image super-resolution via deep networks with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3207" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When image denoising meets high-level vision tasks: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning non-local image diffusion for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Total variation based image restoration with free local constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Structured overcomplete sparsifying transform learning with convergence guarantees and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A tale of two bases: Local-nonlocal regularization on image patches with convolution framelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="711" to="750" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
