<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Person 3D Human Pose Estimation from Monocular Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
							<email>rdabral@cse.iitb.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axogyan</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
							<email>ganesh@cse.iitb.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
							<email>arjunjain@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axogyan</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IIT</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Person 3D Human Pose Estimation from Monocular Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Rahul Mitra IIT Bombay</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person 3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose HG-RCNN, a Mask-RCNN based network that also leverages the benefits of the Hourglass architecture for multiperson 3D Human Pose Estimation. A two-staged approach is presented that first estimates the 2D keypoints in every Region of Interest (RoI) and then lifts the estimated keypoints to 3D. Finally, the estimated 3D poses are placed in camera-coordinates using weak-perspective projection assumption and joint optimization of focal length and root translations. The result is a simple and modular network for multi-person 3D human pose estimation that does not require any multi-person 3D pose dataset. Despite its simple formulation, HG-RCNN achieves the state-of-the-art results on MuPoTS-3D while also approximating the 3D pose in the camera-coordinate system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose-estimation consists of inferring the 3D joint-locations from an image or a sequence of images. It is the key to unlocking a large number of applications in AR/VR, Human-Computer-Interaction (HCI), Gaming, Activity Recognition, Surveillance, etc.. Although, there is a vast literature on single-person 3D pose estimation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>, the space of multi-person 3D pose estimation is mostly unexplored with only a handful of prior work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>. Ironically, real-life human pose-estimation applications, most often, require multi-person pose estimation. For example, surveillance systems require real-time capturing of the poses for every person in the scene. Similarly, sports-analytics demands that all the players are simultaneously analyzed to capture inter-player interactions. Consequently, there ex-ists a gap between existing research and real-world requirements.</p><p>A simple extension of the single-person pose estimation systems to the multi-person setting involves separate detection of every person followed by single-person pose estimation on person crop.</p><p>Unfortunately, the run-time of this approach is likely to increase linearly with the number of people in the scene, making it inefficient for analysis in crowded scenes. Additionally, most existing multi-person pose estimation methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, with the exception of <ref type="bibr" target="#b36">[37]</ref> estimate 3D pose configuration only relative to the root joint. However, relative spatial ordering of different people in the scene is also needed to facilitate reasoning about human interactions and provide a better understanding of the scene. Relative spatial estimation has the potential to unlock accurate tracking of multiple persons in a scene video.</p><p>Moreover, most prior work on multi-person pose estimation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref> relies on creating or simulating a multiperson 3D human pose dataset as a necessity for training. The pre-requisite is due to the end-to-end integrated person detection and pose estimation pipeline. This limits the variability presented to the system while training because obtaining real-world in-the-wild 3D annotations in multi-person setting is challenging, expensive and a research problem in itself.</p><p>In light of the aforementioned discussion of multi-person 3D pose estimation, we propose a quasi top-down architecture that decouples the 2D key-point detection and 2D-to-3D lifting tasks. The proposed architecture, HG-RCNN, brings together the goodness of Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> and the Hourglass <ref type="bibr" target="#b22">[23]</ref> network for heatmap regression. The regressed heatmaps are then fed to an independently trained lifting module to regress the root-relative 3D poses. Consequently, we completely avoid using any multi-person 3D pose dataset in the pipeline since it leverages the existing multi-person 2D pose datasets and single-person 3D <ref type="figure">Figure 1</ref>. Some results of our proposed 3D pose estimation pipeline on some challenging samples from MS COCO. Our approach is resilient against occlusions and clutter. We also approximate the spatial ordering of people in the scene with respect to the camera. Further in-the-wild results and a 3D rendered view of the above images can be found in the supplementary material. pose datasets. Owing to its modular architecture, the first step of obtaining 2D poses can be trained with publicly available large-scale in-the-wild multi-person datasets, such as COCO <ref type="bibr" target="#b14">[15]</ref>, LIP <ref type="bibr" target="#b6">[7]</ref> and MPII 2D dataset <ref type="bibr" target="#b1">[2]</ref>. This allows HG-RCNN to cope with challenging variations in view-point, lighting, apparel, occlusion and extreme poses without the need of costly 3D annotations in-the-wild setting. The keypoint heatmaps from the HG-RCNN are passed through a soft-argmax module and fed to a 2D-3D lifting module. Finally, our pipeline approximates poseconfigurations in camera coordinates without the need of costly geometric optimization. The resulting system outperforms all previous approaches on the challenging MuPoTS-3D <ref type="bibr" target="#b19">[20]</ref> test-set that contains a majority of in-the-wild test scenarios. The method generalizes well to in-the-wild images, even without exploiting any structural priors, while running at 12-15fps on images of size 400 × 600 on a single Nvidia 1080Ti graphics card.</p><p>In summary, we contribute a state-of-the-art model for performing in-the-wild multi-person 3D pose estimation. The model can be trained without using any multi-person 3D dataset and the system also estimates the relative ordering of the persons in the 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Human Pose Estimation has been a widely studied problem. Here, we describe prior art relevant to this work from three broad viewpoints: (a) 2D Pose estimation, (b) Singleperson 3D Pose estimation and (c) Multi-Person 3D Pose estimation. A detailed survey of the area can be found in <ref type="bibr" target="#b28">[29]</ref>.</p><p>2D Human Pose Estimation: Most 2D human pose estimation methods represent their joint outputs as heatmaps, wherein a heatmap's value at a point represents the possibility of the corresponding joint's existence in that position. <ref type="bibr" target="#b33">[34]</ref> proposed Convolutional Pose Machines that iteratively refined the heatmap predictions at every stage. The Stacked Hourglass network <ref type="bibr" target="#b22">[23]</ref> was an encoder-decoder architecture with skip connections to facilitate joint reasoning of high level structural and low level textural features of human pose. Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> proposed an extension of Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> for simultaneously predicting the pose and 2D keypoints and/or instance segmentation masks. In a similar line of work, <ref type="bibr" target="#b7">[8]</ref> predicted the u-v maps of the persons which can then be used for dense reconstruction. <ref type="bibr" target="#b29">[30]</ref> proposed a variant to Mask-RCNN by defining joints as regions instead of persons. In similar spirits, our proposed pipeline attempts to synergise Mask-RCNN and Hourglass networks for multi-person 3D pose estimation task.</p><p>Single-person 3D Pose Estimation: Single person 3D pose estimation works can be broadly divided based on whether they directly regress 3D joints <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> or use a pipelined approach of inferring 3D pose from 2D pose <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref>. VNect <ref type="bibr" target="#b20">[21]</ref> proposed the first realtime approach and parameterized a 3D joint by a heatmap and 3 location maps. Using a 2D-to-3D pipeline enables the use of rich 2D pose datasets which, in turn, improves in-the-wild generalizability. Many approaches perform a direct 2D-to-3D lifting of poses <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> by either learning the transformation or by a nearest-neighbour lookup in a pose library. Furthermore, many pipelined approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24]</ref> have reported significant improvements in in-the-wild performances by using the more diverse 2D pose datasets to pre-train or jointly train their 2D prediction modules.</p><p>Several methods in the past have also reported significant improvements by using temporal cues <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref> by either learning a motion/refinement model or by using temporal constraints in a constrained optimization framework.</p><p>Multi-Person 3D Pose Estimation: Broadly, multiperson pose estimation approaches, 2D and 3D alike, can be classified into top-down and bottom-up approaches. Bottom-up approaches simultaneously predict all the keypoints followed by assembling them into full poses for all persons. On the other hand, top-down approaches first detect the human candidates and subsequently perform pose estimation for each of them. While bottom-up methods are lucrative in terms of efficiency, they tend to be less accurate. For example, the top 5 entries in MS COCO key-points challenge employ top-down approaches <ref type="bibr" target="#b14">[15]</ref>. Intuitively, it makes sense to solve for pose estimation on a person's crop, instead of solving a much more challenging problem of grouping detected key-points into a full person. In recent years, however, a middle ground has been found in the form of quasi top-down architectures based on Mask-RCNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> that have been successful in simultaneously detecting the object RoIs and performing downstream tasks on the corresponding RoI feature-maps, without having to crop the image back.</p><p>LCRNet <ref type="bibr" target="#b26">[27]</ref> was the first method to perform Multi-Person 3D Pose Estimation. They propose an integrated network based on Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> which first proposes Regions of Interest (RoIs) that are fed to a classifier and a regressor. The classifier estimates the most probable anchor pose out of the K pre-defined anchor poses obtained from a MoCap dataset. The regressor then refines the anchor poses towards an accurate pose prediction. Alternately, <ref type="bibr" target="#b19">[20]</ref> propose a bottom-up approach wherein they regress the heatmaps along with X, Y, and Z location maps for every image. The location maps provide the corresponding 3D positions of joints in metric space. The estimated 3D joints are then associated using Part Affinity fields <ref type="bibr" target="#b3">[4]</ref> based on the heatmaps. Both the approaches depend on the explicit creation or simulation of multi-person 3D pose datasets for training. Our method, on the other hand, avoids the use of such datasets and relies on 3D data only for the single person case. Further, Zanfir et al. <ref type="bibr" target="#b36">[37]</ref> proposed a largescale human sensing system for multiple people that estimates pose and shape using the top-down approach of person detection followed by pose estimation for each person. Recently, Zanfir et al. <ref type="bibr" target="#b37">[38]</ref> proposed MubyNet, a bottom-up approach that performs joint association by formulating it as a binary integer programming problem. In contrast, Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> based quasi top-down methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref> have proven to be effective for simultaneously locating objects at a coarse level and detecting finer spatial layouts like segmentation masks, key-point heatmaps, u-v maps, etc.. Our proposed HG-RCNN exploits this setting and also regresses for 3D key-points. However, unlike LCRNet <ref type="bibr" target="#b26">[27]</ref> and LCR-Net++ <ref type="bibr" target="#b27">[28]</ref>, our method does not require anchor-poses and is relatively simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>Given an image I containing N people, we estimate the poses P = (P 1 , P 2 , ..., P N ), wherein P i ∈ R n×3 and n is the number of joints. Every pose P i is a set of n joints in 3D Euclidean space with the origin set to a root joint, pelvis in this case. As an intermediate step, our method first estimates the 2D key-points K = (K 1 , K 2 , ..., K N ) with K i ∈ R n×2 in the image coordinate space. Finally, we approximate the global poses P G = (P G 1 , P G 2 , ..., P G N ) in camera coordinate space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Person 3D Pose Estimation</head><p>We follow a generic, two-step pipeline for root-relative 3D pose estimation. First, we estimate per-frame 2D keypoints of all the people in an image and lift them to 3D pose using a simple residual network. We use a Mask-RCNN based architecture to estimate 2D key-points. However, vanilla keypoint head of Mask-RCNN is not the most conducive architecture for reasoning with structured/articulated objects like human pose. Fortunately, the Hourglass <ref type="bibr" target="#b22">[23]</ref> family of networks have been found to be extremely effective in reasoning about a human pose in a structure-aware way. Therefore, we propose to employ a tiny Hourglass head as a surrogate to the key-point head. This simple patch alone leads to noticeable improvements in the results and will be discussed further in Section 5.2.</p><p>In the second step, the obtained keypoint heatmaps are lifted to 3D joints using a network with two residual modules of size 2048. When deployed in wild settings, it is trained with the heatmaps regressed on the MPI-INF-3DHP training dataset <ref type="bibr" target="#b17">[18]</ref> which provides a wide variety of viewpoints and poses activities, thereby adding to the general- <ref type="figure">Figure 2</ref>. Schematic of our Multi-Person 3D Pose Estimation approach. We augment the Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> architecture with a shallow HourGlass Network <ref type="bibr" target="#b22">[23]</ref>. The heatmaps generated by the hourglass are then input to a 3D Pose Module which regresses the root-relative 3D joint coordinates. The estimated 3D poses of all the Regions of Interests (RoI's) are then collected and their global root positions are approximated to ensure that relative spatial ordering is preserved.</p><p>ization capability of the network. It is worth noting that it is this modular structure of the pipeline that allows us to train the network without any multi-person 3D dataset. The in-the-wild performance is guaranteed by two aspects: a) The heatmaps are learnt on completely wild multi-person 2D keypoint datasets, and b) the lifting module is agnostic to the image features and trained on a dataset consisting of a wide variety of 2D-3D paired annotations.</p><p>Further details on the architecture are discussed in Section 4. At this stage, all the outputs (3D keypoints) are in their individual root relative space. For placing the detected poses in camera-relative space, we estimate the common focal length of the camera and the translation vectors from the individuals' roots to the camera center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Pose Approximation</head><p>Our approach for camera-relative pose approximation is based on jointly optimizing the root joints' global positions and the camera's focal length for the projection error. We initialize the root joint positions using a weak-perspective projection assumption, thus, requiring us to estimate the shrinking parameter α i for every pose P i in the scene. To this end, we compute the sum of bone lengths of the 2D keypoints, S 2D , followed by computing the sum of bone lengths, S 3D , of the 3D pose's orthographic projection.</p><p>The ratio S 2D /S 3D acts as a surrogate to the shrinking factor α i . This finally leads to the following formulation for estimating the global X (horizontal) and Z (depth) coordinates of a joint:</p><formula xml:id="formula_0">Z = f * S 3D S 2D ,<label>(1)</label></formula><formula xml:id="formula_1">X = (x − o x ) * S 3D S 2D<label>(2)</label></formula><p>where, x corresponds to the 2D keypoint and o x is the x co-ordinate of the image center. The focal length, f , is initialized by assuming a field-of-view of 60 • . The same formulation holds for the Y (vertical) coordinate as well.</p><p>Once the root translations are initialized and the full 3D poses are placed in the respective root positions, we iteratively optimize the translation and focal length. The global rotations are assumed to be identity. Thus, the objective function can be written as:</p><formula xml:id="formula_2">f * , t * = arg min f,t N i=1 ||K i − Π f,ti P i || 2<label>(3)</label></formula><p>where t = {t 1 , t 2 , . . . t N } with t i being the translation vector of i th subject's root joint and Π being the projection operator. This, finally, leads to the global pose, P G i = P i + t * i . It is worth noting that the proposed global pose approximation method is just an approximation that can be quickly implemented and run in real-time. The approximation is not expected to work when the person is aligned with the optical axis. We discuss further limitations in section 6. It is not intended to be highly accurate, but only expected to make spatial ordering apparent to systems that need it, eg. action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network and Training Details</head><p>HG-RCNN: The HG-RCNN is constructed by appending an hourglass on the keypoint head of Mask RCNN as shown in <ref type="figure">Figure (2)</ref>. Instead of upsampling once while deconvolving and once at the final layer, we upsample (with <ref type="table">Table 1</ref>. Comparison of our method with prior work on MuPoTS-3D on Setting 1. The top half shows results on all annotated poses in the test set. The bottom half shows results when only the detected poses are considered. The evaluation metric is 3D PCK and higher is better. *Note, that the average PCK provided in LCRNet++ <ref type="bibr" target="#b27">[28]</ref> is not weighed by the number of persons in each test sequence unlike <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref> and ours. <ref type="table" target="#tab_0">TS1 TS2 TS3 TS4 TS5 TS6 TS7 TS8 TS9 TS10 TS11 TS12 TS13 TS14 TS15 TS16 TS17 TS18 TS19 TS20</ref>   <ref type="table" target="#tab_0">Method TS1 TS2 TS3 TS4 TS5 TS6 TS7 TS8 TS9 TS10 TS11 TS12 TS13 TS14 TS15 TS16 TS17 TS18 TS19</ref>  4×) the feature maps all at once before passing the feature values on to the hourglass. The number of feature-maps is brought down from 512 to 128 using a 1 × 1 convolution layer. The original hourglass is modified to have three nested residuals (instead of 4) and has a feature-map of size 7 × 7 at the bottle-neck layer. The hourglass output is then fed to a final classification layer which predicts the heatmaps for every joint. We train the network described above with the Cross-Entropy Loss. While finetuning, we train on top 500 RoIs and use a batch size of 16. The network is trained with a base learning rate of 0.02 on a single Nvidia P6000 Quadro graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Module:</head><p>Our 2D-to-3D pose module converts the heatmap activations to 3D pose using a residual architecture and is in line with the 2D-3D lifting pipelines proposed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22]</ref>. We input the 2D poses in heatmap space after passing the heatmaps through a softargmax layer. This has two benefits: a) it makes learning possible from images of any given size and scale, and b) it facilitates end-to-end training of the network architecture. The network is trained using RMSProp optimizer and a learning rate of 2.5 exp −4 which is reduced by 10 times after 40 epochs.</p><p>While testing on MuPoTS (multi-person) dataset, we use the 3D pose module trained only on MPI-INF-3DHP dataset because both the training and the test sets had the same motion capture system. Human3.6 was captured by a different mocap system which leads to the same joint name pointing to different physical locations on the body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section describes our experiments on MuPoTS-3D <ref type="bibr" target="#b19">[20]</ref>, MS COCO <ref type="bibr" target="#b14">[15]</ref> and Human3.6M <ref type="bibr" target="#b10">[11]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Datasets</head><p>MuPoTS-3D Test Set: Multi-Person Test Set 3D <ref type="bibr" target="#b19">[20]</ref> is a recently released multi-person 3D human pose test dataset. It consists of 20 test sequences shot with a markerless mocap system -5 indoor and 15 outdoor. Every sequence contains 2-3 persons in a variety of activities. The evaluation metric used is 3D PCK -percentage of correct keypoints within a radius of 15cm -on all the annotated persons. In case of a missed detection, all the joints of the missed person are considered erroneous. An alternative evaluation mode is the one in which the evaluations are performed only on the detected joints.</p><p>The official evaluation code performs a greedy matching of detections and ground truth based on the number of 2D keypoints within a proximity of 40px. We call this method Setting 1 for MuPoTS.</p><p>We also evaluate our model in the setting wherein the greedy matching is done based on 3D distances instead of 2D distances. We call this Setting 2. This joint matching strategy is, arguably, less sensitive to cases of heavy oc- clusion which would, otherwise, confuse a keypoint based matching detector. This, as discussed in Section 5.2, leads to missed detections even when the model actually detects the appropriate person. Note, that the two settings differ only in the way the predicted poses are matched with the ground truth poses. All the other details of evaluation, like 3D PCK threshold, joints used for matching, etc remains the same.</p><p>Human3.6: Human 3.6M <ref type="bibr" target="#b10">[11]</ref> is a single-person 3D human pose dataset captured with marker-based motion capture system. It consists of 11 subjects performing 15 actions. We evaluate our model on the commonly followed protocol <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref> that uses subjects 1, 5, 6, 7 and 8 for training, The evaluations are done on subjects 9 and 11. All the videos are downsampled from 50f ps to 10f ps. The evaluation metric used is Mean Per Joint Position Error (MPJPE) which is calculated after aligning only the roots of the predicted and ground truth 3D poses.</p><p>MSCOCO Keypoints: MSCOCO Keypoints is a large scale dataset for 2D multi-person keypoint detection task with roughly 110k training images. It also provides the person bounding boxes and segmentation masks. The 2D keypoint detection task is evaluated on the commonly used Average Precision (AP) metric at different threshold levels. Similarly, the quality of bounding box detections are evaluated using AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>We now discuss the numerical results achieved on the datasets mentioned above.</p><p>MuPoTS-3D Test Set: <ref type="table">Table 1</ref> compares the performance of our simple yet effective method with the existing multi-person 3D pose results. On Setting 1, we improve the state-of-the art significantly with a 3DPCK of 71.25% as against 65% in <ref type="bibr" target="#b19">[20]</ref> and 53.8% in <ref type="bibr" target="#b26">[27]</ref>. For LCRNet <ref type="bibr" target="#b26">[27]</ref>, the reported results are evaluated by <ref type="bibr" target="#b19">[20]</ref>. We report an improved performance on several test sequences. We also significantly improve the performance of occluded joints (61% vs 48.7%) as well as the non-occluded joints (75.6% vs 70%) when compared with <ref type="bibr" target="#b19">[20]</ref>. Our method also performs significantly well when only detected persons are compared. In this setting, we observe 75% 3DPCK while the state-of-the-art being 69.8%. We also compare our performance with the recently released XNect <ref type="bibr" target="#b18">[19]</ref> and demonstrate competitive results on all annotated poses (71.3% vs 70.4%) as well as the detected poses (74.2% vs 75.8%).</p><p>We also evaluate our method on the proposed Setting 2. We observed an improved 3DPCK of 72.6% when compared with Setting 1. This improvement is facilitated by a simple tweak in the greedy matching algorithm of groundtruth and predicted persons. On deeper inspection, we see sharp improvements in sequences with heavy occlusions, like TS18 and TS19. Further, the overall improvement is significant when comparing the performance of occluded joints (64% vs 61% of <ref type="bibr" target="#b19">[20]</ref>). This observation can be attributed to the fact that matching predictions with ground- <ref type="figure">Figure 4</ref>. MPJPE based Percentile Analysis on MuPoTS-3D test set. The lower percentile is better. An important inference from the analysis is that the method is sensitive to lighting and low contrast setting. <ref type="table">Table 3</ref>. Comparative evaluation of our model on Human 3.6 using Absolute MPJPE. The evaluations were performed on subjects 9 and 11. The papers above the horizontal line are single-person pose estimation papers and the ones below the line are multi-person pose estimation papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Direction Discuss Eat Greet Phone Pose Purchase Sit Martinez <ref type="bibr" target="#b16">[17]</ref> 51.8 56.  <ref type="bibr" target="#b19">[20]</ref> under Setting 1.</p><p>The above mentioned results reveal a significant increment in the state-of-the-art. It is worth noting that all the results are comparable to performance of single-person pose estimation methods.  <ref type="table">Table 3</ref>. We achieve an MPJPE of 65.2mm after fine-tuning HG-RCNN on Human3.6M and 74.3mm without fine-tuning. It may be noted that Zanfir et al. <ref type="bibr" target="#b37">[38]</ref> report their results on the official Human3.6 test set and achieve 60mm MPJPE. Since the test circumstances are different, the comparison may not be fair. The combined results on MuPoTS-3D and Human3.6M also corroborate the claims in <ref type="bibr" target="#b11">[12]</ref> that a good performance in Human3.6M does not necessarily indicate better generalization in wild settings. We also evaluate our method under various test-train settings in <ref type="table">Table.</ref> 4 and observe that MPI-INF-3DHP <ref type="bibr" target="#b17">[18]</ref> offers a wider range of poses to train from, thus leading to better results with ground-truth detections. Our results are comparable to Mask-RCNN's reported results. We observe a slightly reduced mAP which can be attributed to the fact that Hourglass architecture is better suited for cases when the larger structure is to be considered. MS-COCO keypoints validation dataset contains multiple cases of isolated/truncated body parts. While evaluating on MuPoTS-3D, we observe improved 3DPCK using HG-RCNN on all annotated (70.1% vs 72.4%) and all occluded (61% vs 64%) joints alike. We also achieve comparable results on the person bounding box detections over Mask-RCNN as shown in <ref type="table">Table 7</ref>. <ref type="table">Table 6</ref>. HG-RCNN results on MS-COCO 2017 val-set for keypoints using a ResNeXt-101 backbone. AP AP50 AP75 AP M AP L HG-RCNN 0.6348 0.8620 0.6905 0.5840 0.7204 <ref type="table">Table 7</ref>. Results on MS-COCO 2017 val-set for person boxes. AP AP50 AP75 AP S AP M AP L HG-RCNN 0.5536 0.8381 0.6076 0.3743 0.6320 0.7235 <ref type="figure">Figure 5</ref>. Two images summing up the sources of failure in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>While our method attempts to account for structural information during inter-personal occlusions, we believe it can be explicitly taken care of with better structural constraints and bounding box consistencies.</p><p>Sources of Error: <ref type="figure">Figure 5</ref> shows interesting examples of failure cases and exposes three sources of error in our pipeline. The first source is poor 2D keypoint estimation, which is apparent in the occluding persons of <ref type="figure">Figure 5 (b)</ref>. The second source of error is an unseen activity/pose which leads to erroneous prediction. This can be seen in squatting players of both the figures, wherein the data-induced model bias leads to incorrectly predicting a person sitting on a chair instead.</p><p>Finally, our camera-coordinate 3D pose prediction is sensitive to 2D keypoint detections and can wrongly reason about the person depth. This effect is observable in <ref type="figure">Figure 5</ref>(a) in which the sitting people have been pushed back, in addition to the two outliers standing behind the player. It may also be noted that this approximation also assumes the individuals to be of roughly the same size. We observe incorrect relative positioning when the height difference is high. Finally, while we compute the sums of bone lengths only on the torso joints to avoid the adverse effects of foreshortening, the effects can not be completely alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper presents a simple extension of Faster-RCNN framework to yield a near-real-time multi-person 3D human pose estimation network HG-RCNN that can be trained without a multi-person 3D pose dataset. Our proposed framework is extremely simple to implement and outperforms previous state-of-the-art results by convincing margins. We also show that we can approximate the spatial layout of the scene. These claims are substantiated both quantitatively through experimental evaluation as well as through qualitative assessments on COCO and MuPoTS-3D datasets. The paper also proposes an improvement to the greedy-matching strategy for multi-person 3D pose estimation evaluation and show results on it. In the future, we plan to deploy this pipeline to a broader human-parsing pipeline while also seeking real-life applications such as activity detection and construct a better scene understanding system related to humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>TS20 Avg Ours (occ) 82.0 61.1 62.3 70.2 62.7 53.3 67.8 63.1 59.8 39.1 73.1 69.3 67.4 33.7 59.0 79.1 79.0 82.5 79.7 36.2 64.0 Ours (all) 85.2 69.2 73.1 75.6 77.7 52.1 65.1 66.0 57.7 77.6 76.6 69.0 71.6 53.4 70.3 86.0 84.3 84.7 83.7 72.8 72.6 Ours (occ) 82.0 61.1 62.3 71.0 62.7 53.8 69.0 63.2 59.8 39.1 78.0 69.8 67.4 47.4 59.0 79.2 79.5 82.5 79.7 76.6 67.1 Ours (all) 85.2 69.2 73.1 76.1 77.7 52.7 65.9 66.0 57.7 77.6 80.5 69.1 71.6 60.6 70.3 87.1 85.1 84.7 83.7 92.1 74.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of our results on MuPoTS-3D Test Set from different viewpoints. Notice that the model is fairly robust to occlusions. The spatial alignment is not derived from ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>* 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5 69.2 73.8 66.2 56.0 74.1 82.1 78.1 72.6 73.1 61.0 70.6 [19] 88.4 65.1 68.2 72.5 76.2 46.2 65.8 64.1 75.1 82.4 74.1 72.4 64.4 58.8 73.7 80.4 84.3 67.2 74.3 67.8 70.4 Ours 85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8 76.4 70.1 65.3 51.7 69.5 87.0 82.1 80.3 78.5 70.7 71.3 Ours 85.8 73.6 61.1 55.7 77.9 53.3 75.1 65.5 54.2 81.3 82.2 71.0 70.1 67.7 69.9 90.5 85.7 86.3 85.0 91.4 74.2 Performance of our method on MuPoTS using the Setting 2. The top half shows results on all annotated poses in the test set.The bottom half shows results when only the detected poses are considered. 'all' corresponds to evaluation on all eligible persons and 'occ' corresponds to the results on occluded persons. The evaluation metric is 3D PCK and higher is better. Notice that compared to 1, the improvement is mostly observed on sequences with significant occlusion, eg. TS18 and TS19.</figDesc><table><row><cell>Avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Set is the Area Under Curve (AUC) of PCK values. We report an AUC of 35.5 which is better than 30.1 reported by [20] and 27.6 in [21] using groud truth detections. Our detection rate is 93.5% which is comparable to the 93% detection rate of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell></row><row><cell>Zhou [39]</cell><cell></cell><cell>54.8</cell><cell>60.7</cell><cell>58.2</cell><cell>71.4</cell><cell>62.0</cell><cell>53.8</cell><cell>55.6</cell><cell>75.2</cell></row><row><cell>Sun [31]</cell><cell></cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell></row><row><cell>Dabral [6]</cell><cell></cell><cell>44.8</cell><cell>50.4</cell><cell>44.7</cell><cell>49.0</cell><cell>52.9</cell><cell>43.5</cell><cell>45.5</cell><cell>63.1</cell></row><row><cell>Hossain [25]</cell><cell></cell><cell>44.2</cell><cell>46.7</cell><cell>52.3</cell><cell>49.3</cell><cell>59.9</cell><cell>47.5</cell><cell>46.2</cell><cell>59.9</cell></row><row><cell>Sun [32]</cell><cell></cell><cell>47.5</cell><cell>47.7</cell><cell>49.5</cell><cell>50.2</cell><cell>51.4</cell><cell>43.8</cell><cell>46.4</cell><cell>58.9</cell></row><row><cell>Rogez [27]</cell><cell></cell><cell>76.2</cell><cell>80.2</cell><cell>75.8</cell><cell>83.3</cell><cell>92.2</cell><cell>79.0</cell><cell>71.7</cell><cell>105.9</cell></row><row><cell>Mehta [20]</cell><cell></cell><cell>58.2</cell><cell>67.3</cell><cell>61.2</cell><cell>65.7</cell><cell>75.8</cell><cell>62.2</cell><cell>64.6</cell><cell>82.0</cell></row><row><cell>Rogez [28]</cell><cell></cell><cell>50.9</cell><cell>55.9</cell><cell>63.3</cell><cell>56.0</cell><cell>65.1</cell><cell>52.1</cell><cell>51.9</cell><cell>81.1</cell></row><row><cell cols="2">Ours (Baseline)</cell><cell>60.2</cell><cell>64.5</cell><cell>66.2</cell><cell>70.1</cell><cell>75.6</cell><cell>65.4</cell><cell>69.4</cell><cell>83.7</cell></row><row><cell cols="2">Ours (Fine-Tuned)</cell><cell>52.6</cell><cell>61.0</cell><cell>58.8</cell><cell>61.0</cell><cell>69.5</cell><cell>58.8</cell><cell>57.2</cell><cell>76.0</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">SitDown Smoke Photo Wait</cell><cell cols="3">Walk WalkDog WalkPair</cell><cell>Avg</cell></row><row><cell cols="2">Martinez [17]</cell><cell>94.6</cell><cell>62.3</cell><cell>78.4</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Zhou [39]</cell><cell></cell><cell>111.6</cell><cell>64.1</cell><cell>65.5</cell><cell>66.0</cell><cell>51.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Sun [31]</cell><cell></cell><cell>86.7</cell><cell>61.5</cell><cell>67.2</cell><cell>53.4</cell><cell>47.1</cell><cell>61.6</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Dabral [6]</cell><cell></cell><cell>87.3</cell><cell>51.7</cell><cell>61.4</cell><cell>48.5</cell><cell>37.6</cell><cell>52.2</cell><cell>41.9</cell><cell>52.1</cell></row><row><cell>Hossain [25]</cell><cell></cell><cell>65.6</cell><cell>55.8</cell><cell>59.4</cell><cell>50.4</cell><cell>52.3</cell><cell>43.5</cell><cell>45.1</cell><cell>51.9</cell></row><row><cell>Sun [32]</cell><cell></cell><cell>65.7</cell><cell>49.4</cell><cell>55.8</cell><cell>47.8</cell><cell>38.9</cell><cell>49.0</cell><cell>43.8</cell><cell>49.6</cell></row><row><cell>Rogez [27]</cell><cell></cell><cell>127.1</cell><cell>88.0</cell><cell cols="2">105.7 83.7</cell><cell>64.9</cell><cell>86.6</cell><cell>84.0</cell><cell>87.7</cell></row><row><cell>Mehta [20]</cell><cell></cell><cell>93.0</cell><cell>68.8</cell><cell>84.5</cell><cell>65.1</cell><cell>57.6</cell><cell>72.0</cell><cell>63.6</cell><cell>69.9</cell></row><row><cell>Rogez [28]</cell><cell></cell><cell>91.7</cell><cell>64.7</cell><cell>70.7</cell><cell>54.6</cell><cell>44.7</cell><cell>61.1</cell><cell>53.7</cell><cell>61.2</cell></row><row><cell cols="2">Ours (Baseline)</cell><cell>105.7</cell><cell>70.2</cell><cell>89.6</cell><cell>69.1</cell><cell>61.7</cell><cell>80.6</cell><cell>66.9</cell><cell>73.0</cell></row><row><cell cols="2">Ours (Fine-Tuned)</cell><cell>93.6</cell><cell>63.1</cell><cell>79.3</cell><cell>63.9</cell><cell>51.5</cell><cell>71.4</cell><cell>53.5</cell><cell>65.2</cell></row><row><cell cols="5">truths based on 2D keypoints leads to matching errors and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">missed detections when two or more persons occlude each</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">other. Indeed, we observe that the algorithm's detection per-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">centage rose from 93% to 96%, thus improving the over-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">all 3DPCK. Interestingly, we observe that TS10 suffers un-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">der this protocol because all the three subjects bear similar</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">poses for many frames. Thus, we believe the two settings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>are complimentary.</cell><cell cols="4">Another evaluation metric used in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MuPots Test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of various training/testing settings on Human3.6M Protocol 1. The first column indicates the data used as the 2D input to the 3D pose module while training. The second column, likewise, indicates which datasets were used for training the HG-RCNN based 2D input.</figDesc><table><row><cell>2D-3D Training</cell><cell cols="2">HG-RCNN Training MPJPE</cell></row><row><cell>H36M GT</cell><cell>MS-COCO</cell><cell>135.5</cell></row><row><cell>H36M GT + noise</cell><cell>MS-COCO</cell><cell>119.7</cell></row><row><cell>H36M pred</cell><cell>MS-COCO</cell><cell>73.0</cell></row><row><cell>H36M pred</cell><cell>MS-COCO + H36M</cell><cell>65.2</cell></row><row><cell>MPI-INF GT</cell><cell>MS-COCO</cell><cell>118.3</cell></row><row><cell>MPI-INF GT + noise</cell><cell>MS-COCO</cell><cell>118.16</cell></row><row><cell cols="3">Human 3.6M: The results on Human 3.6M dataset are</cell></row><row><cell>detailed in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison of HG-RCNN and Mask-RCNN based models on MuPoTS 3D. The evaluation metric is 3DPCK. Mask-RCNN vs. HG-RCNN:Table 6 details the performance of HG-RCNN on MSCOCO Keypoints dataset.</figDesc><table><row><cell></cell><cell cols="2">Mask-RCNN HG-RCNN</cell></row><row><cell>all annotated joints</cell><cell>70.1</cell><cell>72.4</cell></row><row><cell>all occluded joints</cell><cell>61.0</cell><cell>64.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by Mercedes-Benz Research &amp; Development India (RD/0117-MBRDI00-001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Segment Every Thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SMPL: a skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00837</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ToG</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multiperson 2d and 3d pose detection in natural images. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sekii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
