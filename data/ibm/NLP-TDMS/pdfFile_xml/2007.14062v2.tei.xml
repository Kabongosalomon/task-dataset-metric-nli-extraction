<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Big Bird: Transformers for Longer Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
							<email>manzilz@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
							<email>gurug@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
							<email>avinavadubey@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Amr</roleName><forename type="first">Li</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Big Bird: Transformers for Longer Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Models based on Transformers <ref type="bibr" target="#b91">[91]</ref>, such as BERT <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b63">63]</ref>, are wildly successful for a wide variety of Natural Language Processing (NLP) tasks and consequently are mainstay of modern NLP research. Their versatility and robustness are the primary drivers behind the wide-scale adoption of Transformers. The model is easily adapted for a diverse range of sequence based tasks -as a seq2seq model for translation <ref type="bibr" target="#b91">[91]</ref>, summarization [66], generation [15], etc. or as a standalone encoders for sentiment analysis [83], POS tagging [65], machine reading comprehension [93], etc. -and it is known to vastly outperform previous sequence models like LSTM [37]. The key innovation in Transformers is the introduction of a self-attention mechanism, which can be evaluated in parallel for each token of the input sequence, eliminating the sequential dependency in recurrent neural networks, like LSTM. This parallelism enables Transformers to leverage the full power of modern SIMD hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets of unprecedented size. This ability to train on large scale data has led to surfacing of models like BERT [22]  and T5 <ref type="bibr" target="#b75">[75]</ref>, which pretrain transformers on large general purpose corpora and transfer the knowledge to down-stream task. The pretraining has led to significant improvement in low data regime downstream tasks [51] as well as tasks with sufficient data [101] and thus have been a major force behind the ubiquity of transformers in contemporary NLP.</p><p>The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement 34th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>translates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA <ref type="bibr" target="#b60">[60]</ref>, document classification, etc.</p><p>However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. <ref type="bibr" target="#b104">[104]</ref> showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, Pérez et al. <ref type="bibr" target="#b72">[72]</ref> showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?</p><p>In this paper, we address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. We systematically develop BIGBIRD, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We take inspiration from graph sparsification methods and understand where the proof for expressiveness of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern. This understanding helped us develop BIGBIRD, which is theoretically as expressive and also empirically useful. In particular, our BIGBIRD consists of three main part:</p><p>• A set of g global tokens attending on all parts of the sequence. • All tokens attending to a set of w local neighboring tokens. • All tokens attending to a set of r random tokens.</p><p>This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x).</p><p>To summarize, our main contributions are:</p><p>1. BIGBIRD satisfies all the known theoretical properties of full transformer (Sec. 3). In particular, we show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, we show that under standard assumptions regarding precision, BIGBIRD is Turing complete.</p><p>2. Empirically, we show that the extended context modelled by BIGBIRD benefits variety of NLP tasks. We achieve state of the art results for question answering and document summarization on a number of different datasets. Summary of these results are presented in Sec. 4.</p><p>3. Lastly, we introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoterregion and chromatin profile prediction (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>There have been a number of interesting attempts, that were aimed at alleviating the quadratic dependency of Transformers, which can broadly categorized into two directions. First line of work embraces the length limitation and develops method around it. Simplest methods in this category just employ sliding window <ref type="bibr" target="#b93">[93]</ref>, but in general most work fits in the following general paradigm: using some other mechanism select a smaller subset of relevant contexts to feed in the transformer and optionally iterate, i.e. call transformer block multiple time with different contexts each time.</p><p>Most prominently, SpanBERT <ref type="bibr" target="#b41">[42]</ref>, ORQA <ref type="bibr" target="#b53">[54]</ref>, REALM <ref type="bibr" target="#b33">[34]</ref>, RAG <ref type="bibr" target="#b56">[57]</ref> have achieved strong performance for different tasks. However, it is worth noting that these methods often require significant engineering efforts (like back prop through large scale nearest neighbor search) and are hard to train.</p><p>Second line of work questions if full attention is essential and have tried to come up with approaches that do not require full attention, thereby reducing the memory and computation requirements. Prominently, Dai et al. <ref type="bibr" target="#b20">[21]</ref>, Sukhbaatar et al. <ref type="bibr" target="#b82">[82]</ref>, Rae et al. <ref type="bibr" target="#b74">[74]</ref> have proposed auto-regresive models that work well for left-to-right language modeling but suffer in tasks which require bidirectional context. Child et al. <ref type="bibr" target="#b15">[16]</ref> proposed a sparse model that reduces the complexity to O(n √ n), Kitaev et al. <ref type="bibr" target="#b48">[49]</ref> further reduced the complexity to O(n log(n)) by using LSH to compute nearest neighbors. Ye et al. <ref type="bibr" target="#b103">[103]</ref> proposed binary partitions of the data where as Qiu et al. <ref type="bibr" target="#b73">[73]</ref> reduced complexity by using block sparsity. Recently, Longformer <ref type="bibr" target="#b7">[8]</ref> introduced a localized sliding window based mask with few global mask to reduce computation and extended BERT to longer sequence based tasks. Finally, our work is closely related to and built on the work of Extended Transformers Construction <ref type="bibr" target="#b3">[4]</ref>. This work was designed to encode structure in text for transformers. The idea of global tokens was used extensively by them to achieve their goals. Our theoretical work can be seen as providing a justification for the success of these models as well. It is important to note that most of the aforementioned methods are heuristic based and empirically are not as versatile and robust as the original transformer, i.e. the same architecture do not attain SoTA on multiple standard benchmarks.</p><p>(There is one exception of Longformer which we include in all our comparisons, see App. E.3 for a more detailed comparison). Moreover, these approximations do not come with theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BIGBIRD Architecture</head><p>In this section, we describe the BIGBIRD model using the generalised attention mechanism that is used in each layer of transformer operating on an input sequence X = (x 1 , ..., x n ) ∈ R n×d . The generalized attention mechanism is described by a directed graph D whose vertex set is [n] = {1, . . . , n}. The set of arcs (directed edges) represent the set of inner products that the attention mechanism will consider. Let N (i) denote the out-neighbors set of node i in D, then the i th output vector of the generalized attention mechanism is defined as</p><formula xml:id="formula_0">ATTND(X)i = xi + H h=1 σ Q h (xi)K h (X N (i) ) T · V h (X N (i) ) (AT)</formula><p>where Q h , K h : R d → R m are query and key functions respectively, V h : R d → R d is a value function, σ is a scoring function (e.g. softmax or hardmax) and H denotes the number of heads. Also note X N (i) corresponds to the matrix formed by only stacking {x j : j ∈ N (i)} and not all the inputs. If D is the complete digraph, we recover the full quadratic attention mechanism of Vaswani et al. <ref type="bibr" target="#b91">[91]</ref>. To simplify our exposition, we will operate on the adjacency matrix A of the graph D even though the underlying graph maybe sparse. To elaborate, A ∈ [0, 1] n×n with A(i, j) = 1 if query i attends to key j and is zero otherwise. For example, when A is the ones matrix (as in BERT), it leads to quadratic complexity, since all tokens attend on every other token. This view of self-attention as a fully connected graph allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the quadratic complexity of self-attention can now be seen as a graph sparsification problem. It is well-known that random graphs are expanders and can approximate complete graphs in a number of different contexts including in their spectral properties <ref type="bibr" target="#b80">[80,</ref><ref type="bibr" target="#b37">38]</ref>. We believe sparse random graph for attention mechanism should have two desiderata: small average path length between nodes and a notion of locality, each of which we discuss below.</p><p>Let us consider the simplest random graph construction, known as Erdős-Rényi model, where each edge is independently chosen with a fixed probability. In such a random graph with justΘ(n) edges, the shortest path between any two nodes is logarithmic in the number of nodes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>. As a consequence, such a random graph approximates the complete graph spectrally and its second eigenvalue (of the adjacency matrix) is quite far from the first eigenvalue <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6]</ref>. This property leads to a rapid mixing time for random walks in the grpah, which informally suggests that information can flow fast between any pair of nodes. Thus, we propose a sparse attention where each query attends over r random number of keys i.e. A(i, ·) = 1 for r randomly chosen keys (see <ref type="figure" target="#fig_0">Fig. 1a</ref>).</p><p>The second viewpoint which inspired the creation of BIGBIRD is that most contexts within NLP and computational biology have data which displays a great deal of locality of reference. In this phenomenon, a great deal of information about a token can be derived from its neighboring tokens. Most pertinently, Clark et al. <ref type="bibr" target="#b18">[19]</ref> investigated self-attention models in NLP tasks and concluded that that neighboring inner-products are extremely important. The concept of locality, proximity of tokens in linguistic structure, also forms the basis of various linguistic theories such as transformationalgenerative grammar. In the terminology of graph theory, clustering coefficient is a measure of locality of connectivity, and is high when the graph contains many cliques or near-cliques (subgraphs that are almost fully interconnected). Simple Erdős-Rényi random graphs do not have a high clustering coefficient <ref type="bibr" target="#b84">[84]</ref>, but a class of random graphs, known as small world graphs, exhibit high clustering coefficient <ref type="bibr" target="#b94">[94]</ref>. A particular model introduced by Watts and Strogatz <ref type="bibr" target="#b94">[94]</ref> is of high relevance to us as it achieves a good balance between average shortest path and the notion of locality. The generative process of their model is as follows: Construct a regular ring lattice, a graph with n nodes each connected to w neighbors, w/2 on each side.  In other words we begin with a sliding window on the nodes. Then a random subset (k%) of all connections is replaced with a random connection. The other (100 -k)% local connections are retained. However, deleting such random edges might be inefficient on modern hardware, so we retain it, which will not affect its properties. In summary, to capture these local structures in the context, in BIGBIRD, we define a sliding window attention, so that during self attention of width w, query at location i attends from i − w/2 to i + w/2 keys. In our notation, <ref type="figure" target="#fig_0">Fig. 1b</ref>). As an initial sanity check, we performed basic experiments to test whether these intuitions are sufficient in getting performance close to BERT like models, while keeping attention linear in the number of tokens. We found that random blocks and local window were insufficient in capturing all the context necessary to compete with the performance of BERT.</p><formula xml:id="formula_1">A(i, i − w/2 : i + w/2) = 1 (see</formula><p>The final piece of BIGBIRD is inspired from our theoretical analysis (Sec. 3), which is critical for empirical performance. More specifically, our theory utilizes the importance of "global tokens" (tokens that attend to all tokens in the sequence and to whom all tokens attend to (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). These global tokens can be defined in two ways:</p><p>• BIGBIRD-ITC: In internal transformer construction (ITC), we make some existing tokens "global", which attend over the entire sequence. Concretely, we choose a subset G of indices (with g := |G|), such that A(i, :) = 1 and A(:, i) = 1 for all i ∈ G.</p><p>• BIGBIRD-ETC: In extended transformer construction (ETC), we include additional "global" tokens such as CLS. Concretely, we add g global tokens that attend to all existing tokens. In our notation, this corresponds to creating a new matrix B ∈ [0, 1] (N +g)×(N +g) by adding g rows to matrix A, such that B(i, :) = 1, and B(:, i) = 1 for all i ∈ {1, 2, . . . g}, and B(g + i, g + j) = A(i, j)∀ i, j ∈ {1, . . . , N }. This adds extra location to store context and as we will see in the experiments improves performance.</p><p>The final attention mechanism for BIGBIRD ( <ref type="figure" target="#fig_0">Fig. 1d</ref>) has all three of these properties: queries attend to r random keys, each query attends to w/2 tokens to the left of its location and w/2 to the right of its location and they contain g global tokens (The global tokens can be from existing tokens or extra added tokens). We provide implementation details in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Results about Sparse Attention Mechanism</head><p>In this section, we will show that that sparse attention mechanisms are as powerful and expressive as full-attention mechanisms in two respects. First, we show that when sparse attention mechanisms are used in a standalone encoder (such as BERT), they are Universal Approximators of sequence to sequence functions in the style of Yun et al. <ref type="bibr" target="#b104">[104]</ref>. We note that this property was also explored theoretically in contemporary work Yun et al. <ref type="bibr" target="#b105">[105]</ref>. Second, unlike <ref type="bibr" target="#b105">[105]</ref>, we further show that sparse encoder-decoder transformers are Turing Complete (assuming the same conditions defined in <ref type="bibr" target="#b72">[72]</ref>). Complementing the above positive results, we also show that moving to a sparse-attention mechanism incurs a cost, i.e. there is no free lunch. In Sec. 3.4, we show lower bounds by exhibiting a natural task where any sufficiently sparse mechanism will require polynomially more layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>The complete Transformer encoder stack is nothing but the repeated application of a single-layer encoder (with independent parameters). We denote class of such Transformer encoders stack, defined using generalized encoder (Sec. 2), by T H,m,q D which consists of H-heads with head size m and q is the hidden layer size of the output network, and the attention layer is defined by the directed graph D.</p><p>The key difference between our proposed attention mechanism to that of Vaswani et al. <ref type="bibr" target="#b91">[91]</ref>, Yun et al. <ref type="bibr" target="#b104">[104]</ref> is that we add a special token at the beginning of each sequence and assign it a special vector. We will refer to this as x 0 . Therefore our graph D will have vertex set {0} ∪ [n] = {0, 1, 2, . . . , n}.</p><p>We will assume that this extra node and its respective vector will be dropped at the final output layer of transformer. To avoid cumbersome notation, we will still treat transformer as mapping sequences X ∈ R n×d to R n×d . We will also allow the transformer to append position embeddings E ∈ R d×n to matrix X in the input layer.</p><p>Finally, we need to define the function class and distance measure for proving universal approximation property. Let F CD denote the set of continuous functions f : [0, 1] n×d → R n×d which are continuous with respect to the topology defined by p norm. Recall for any p ≥ 1, the p distance is</p><formula xml:id="formula_2">d p (f 1 , f 2 ) = f 1 (X) − f 2 (X) p p dX 1/p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Universal Approximators</head><p>Definition 1. The star-graph S centered at 0 is the graph defined on {0, . . . , n}. The neighborhood of all vertices i is</p><formula xml:id="formula_3">N (i) = {0, i} for i ∈ {1 . . . n} and N (0) = {1, . . . n}.</formula><p>Our main theorem is that the sparse attention mechanism defined by any graph containing S is a universal approximator: Theorem 1. Given 1 &lt; p &lt; ∞ and &gt; 0, for any f ∈ F CD , there exists a transformer with sparse-attention, g ∈ T H,m,q D such that d p (f, g) ≤ where D is any graph containing star graph S.</p><p>To prove the theorem, we will follow the standard proof structure outlined in <ref type="bibr" target="#b104">[104]</ref>.</p><p>Step 1: Approximate F CD by piece-wise constant functions. Since f is a continuous function with bounded domain [0, 1) n×d , we will approximate it with a suitable piece-wise constant function. This is accomplished by a suitable partition of the region [0, 1) into a grid of granularity δ to get a discrete set G δ . Therefore, we can assume that we are dealing with a functionf :</p><formula xml:id="formula_4">G δ → R n×d , where d p (f,f ) ≤ 3 .</formula><p>Step 2: Approximate piece-wise constant functions by modified transformers. This is the key step of the proof where the self-attention mechanism is used to generate a contextual-mapping of the input. Informally, a contextual mapping is a unique code for the pair consisting of a matrix (X, x i ) and a column. Its uniqueness allows the Feed forward layers to use each code to map it to a unique output column.</p><p>The main technical challenge is computing the contextual mapping using only sparse attention mechanism. This was done in <ref type="bibr" target="#b104">[104]</ref> using a "selective" shift operator which shift up entries that are in a specific interval. Key to their proof was the fact that the shift, was exactly the range of the largest entry to the smallest entry.</p><p>Creating a contextual mapping with a sparse attention mechanism is quite a challenge. In particular, because each query only attends to a few keys, it is not at all clear that sufficient information can be corralled to make a contextual embedding of the entire matrix. To get around this, we develop a sparse shift operator which shifts the entries of the matrices if they lie in a certain range. The exact amount of the shift is controlled by the directed sparse attention graphg D. The second key ingredient is the use of additional global token. By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token.</p><p>Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax.</p><p>We provide the full details in App. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Turing Completeness</head><p>Transformers are a very general class. In the original paper of Vaswani et al. <ref type="bibr" target="#b91">[91]</ref>, they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? Pérez et al. <ref type="bibr" target="#b72">[72]</ref> showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete.</p><p>It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine.</p><p>To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of Pérez et al. <ref type="bibr" target="#b72">[72]</ref>, uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of Pérez et al. <ref type="bibr" target="#b72">[72]</ref> goes through for sparse attentions, except for their addressing scheme to point back in history (Lemma B.4 in <ref type="bibr" target="#b72">[72]</ref>). We show how to simulate this using a sparse attention mechanism and defer the details to App. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Limitations</head><p>We demonstrate a natural task which can be solved by the full attention mechanism in O(1)-layers.</p><p>However, under standard complexity theoretic assumptions, this problem requiresΩ(n)-layers for any sparse attention layers withÕ(n) edges (not just BIGBIRD). (HereÕ hides poly-logarthmic factors). Consider the simple problem of finding the corresponding furthest vector for each vector in the given sequence of length n. Formally, Task 1. Given n unit vectors {u 1 , . . . , u n }, find f (u 1 , . . . , u n ) → (u 1 * , . . . , u n * ) where for a fixed j ∈ [n], we define j * = arg max k u k − u j 2 2 . Finding vectors that are furthest apart boils down to minimize inner product search in case of unit vectors. For a full-attention mechanism with appropriate query and keys, this task is very easy as we can evaluate all pair-wise inner products.</p><p>The impossibility for sparse-attention follows from hardness results stemming from Orthogonal Vector Conjecture(OVC) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b96">96]</ref>. The OVC is a widely used assumption in fine-grained complexity. Informally, it states that one cannot determine if the minimum inner product among n boolean vectors is 0 in subquadratic time. In App. C, we show a reduction using OVC to show that if a transformer g ∈ T H=1,m=2d,q=0 D for any sparse directed graph D can evaluate the Task 1, it can solve the orthogonal vector problem. Proposition 1. There exists a single layer full self-attention g ∈ T H=1,m=2d,q=0 that can evaluate Task 1, i.e. g(u 1 , ..., u n ) = [u 1 * , . . . , u n * ], but for any sparse-attention graph D withÕ(n) edges (i.e. inner product evaluations), would requireΩ(n 1−o(1) ) layers. We give a formal proof of this fact in App. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments: Natural Language Processing</head><p>In this section our goal is to showcase benefits of modeling longer input sequence for NLP tasks, for which we select three representative tasks. We begin with basic masked language modeling (MLM; <ref type="bibr">Devlin et al. 22)</ref> to check if better contextual representations can be learnt by utilizing longer contiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25.   Finally, we tackle long document classification where discriminating information may not be located in first 512 tokens. Below we summarize the results for BIGBIRD using sequence length 4096 1 , while we defer all other setup details including computational resources, batch size, step size, to App. E.</p><p>Pretraining and MLM We follow <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b63">63]</ref> to create base and large versions of BIGBIRD and pretrain it using MLM objective. This task involves predicting a random subset of tokens which have been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9), warm-starting from the public RoBERTa checkpoint 2 . We compare performance in predicting the masked out tokens in terms of bits per character, following <ref type="bibr" target="#b7">[8]</ref>. As seen in App. E.1, Tab. 10, both BIGBIRD and Longformer perform better than limited length RoBERTa, with BIGBIRD-ETC performing the best. We note that we trained our models on a reasonable 16GB memory/chip with batch size of 32-64. Our memory efficiency is due to efficient blocking and sparsity structure of the sparse attention mechanism described in Sec. 2.</p><p>Question Answering (QA) We considered following four challenging datasets:</p><p>1. Natural Questions <ref type="bibr" target="#b51">[52]</ref>: For the given question, find a short span of answer (SA) from the given evidences as well highlight the paragraph from the given evidences containing information about the correct answer (LA).</p><p>smaller verified subset of question, the given evidence is guaranteed to contain the answer. Nevertheless, we model the answer as span selection problem in this case as well. <ref type="bibr" target="#b95">[95]</ref>: Chose correct option from multiple-choice questions (MCQ), by aggregating information spread across multiple documents given in the evidences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">WikiHop</head><p>As these tasks are very competitive, multiple highly engineered systems have been designed specific each dataset confirming to respective output formats. For a fair comparison, we had to use some additional regularization for training BIGBIRD, details of which are provided in App. E.2 along with exact architecture description. We experiment using the base sized model and select the best configuration on the development set for each dataset (as reported in Tab. 2). We can see that BIGBIRD-ETC, with expanded global tokens consistently outperforms all other models. Thus, we chose this configuration to train a large sized model to be used for evaluation on the hidden test set.</p><p>In Tab. 3, we compare BIGBIRD-ETC model to top-3 entries from the leaderboard excluding BIGBIRD. One can clearly see the importance of using longer context as both Longformer and BIGBIRD outperform models with smaller contexts. Also, it is worth noting that BIGBIRD submission is a single model, whereas the other top-3 entries for Natural Questions are ensembles, which might explain the slightly lower accuracy in exact answer phrase selection.</p><p>Classification We experiment on datasets of different lengths and contents, specifically various document classification and GLUE tasks. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder-Decoder Tasks</head><p>For an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to the full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at the encoder side. This is because, in practical generative applications, the length of output sequence is typically small as compared to the input. For example for text summarization, we see in realistic scenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is ∼ 200 where as the input  sequence's median length is &gt; 3000. For such applications, it is more efficient to use sparse attention mechanism for the encoder and full self-attention for the decoder.</p><formula xml:id="formula_5">Model Arxiv PubMed BigPatent R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L Prior</formula><p>Summarization Document summarization is a task of creating a short and accurate summary of a text document. We used three long document datasets for testing our model details of which are mention in Tab. <ref type="bibr" target="#b17">18</ref>. In this paper we focus on abstractive summarization of long documents where using a longer contextual encoder should improve performance. The reasons are two fold: First, the salient content can be evenly distributed in the long document, not just in first 512 tokens, and this is by design in the BigPatents dataset <ref type="bibr" target="#b78">[78]</ref>. Second, longer documents exhibit a richer discourse structure and summaries are considerably more abstractive, thereby observing more context helps.</p><p>As has been pointed out recently <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b107">107]</ref>, pretraining helps in generative tasks, we warm start from our general purpose MLM pretraining on base-sized models as well as utilizing state-of-the-art summarization specific pretraining from Pegasus <ref type="bibr" target="#b107">[107]</ref> on large-sized models. The results of training BIGBIRD sparse encoder along with full decoder on these long document datasets are presented in Tab. 4. We can clearly see modeling longer context brings significant improvement. Along with hyperparameters, we also present results on shorter but more widespread datasets in App. E.5, which show that using sparse attention does not hamper performance either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Genomics</head><p>There has been a recent upsurge in using deep learning for genomics data <ref type="bibr" target="#b86">[86,</ref><ref type="bibr" target="#b106">106,</ref><ref type="bibr" target="#b12">13]</ref>, which has resulted in improved performance on several biologically-significant tasks such as promoter site prediction <ref type="bibr" target="#b71">[71]</ref>, methylation analysis <ref type="bibr" target="#b54">[55]</ref>, predicting functional effects of non-coding variant <ref type="bibr" target="#b109">[109]</ref>, etc. These approaches consume DNA sequence fragments as inputs, and therefore we believe longer input sequence handling capability of BIGBIRD would be beneficial as many functional effects in DNA are highly non-local <ref type="bibr" target="#b11">[12]</ref>. Furthermore, taking inspiration from NLP, we learn powerful contextual representations for DNA fragments utilizing abundant unlabeled data (e.g. human reference genome, Saccharomyces Genome Database) via MLM pretraining. Next, we showcase that our long input BIGBIRD along with the proposed pretraining significantly improves performances in two downstream tasks. Detailed experimental setup for the two tasks are provided in App. F.   <ref type="bibr" target="#b58">[58]</ref>, instead of operating on base pairs, we propose to first segment DNA into tokens so as to further increase the context length (App. F, <ref type="figure">Fig. 7</ref>). In particular, we build a byte-pair encoding <ref type="bibr" target="#b49">[50]</ref> table for the DNA sequence of size 32K, with each token representing 8.78 base pairs on average. We learn contextual representation of these token on the human reference genome (GRCh37) 3 using MLM objective. We then report the bits per character (BPC) on a held-out set in Tab. 5. We find that attention based contextual representation of DNA does improve BPC, which is further improved by using longer context.  Promoter Region Prediction Promoter is a DNA region typically located upstream of the gene, which is the site of transcription initiation. Multiple methods have been proposed to identify the promoter regions in a given DNA sequence <ref type="bibr" target="#b99">[99,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b98">98,</ref><ref type="bibr" target="#b71">71]</ref>, as it is an important first step in understanding gene regulation. The corresponding machine learning task is to classify a given DNA fragment as promoter or non-promoter sequence. We use the dataset compiled by Oubounyt et al. <ref type="bibr" target="#b71">[71]</ref> which was built from Eukaryotic Promoter Database (EPDnew) <ref type="bibr" target="#b23">[24]</ref> 4 . We finetuned the pretrained BIGBIRD model from above, using the training data and report F1 on test dataset. We compare our results to the previously reported best method in Tab. <ref type="bibr" target="#b5">6</ref>. We see that BIGBIRD achieve nearly perfect accuracy with a 5% jump from the previous best reported accuracy.   <ref type="bibr" target="#b109">[109,</ref><ref type="bibr" target="#b45">46]</ref>. Thus, understanding the functional effects of non-coding regions of DNA is a very important task. An important step in this process, as defined by Zhou and Troyanskaya <ref type="bibr" target="#b109">[109]</ref>, is to predict large-scale chromatin-profiling from non-coding genomic sequence. To this effect, DeepSea <ref type="bibr" target="#b109">[109]</ref>, compiled 919 chromatin-profile of 2.4M non-coding variants from Encyclopedia of DNA Elements (ENCODE) <ref type="bibr" target="#b4">5</ref> and Roadmap Epigenomics projects 6 . The corresponding ML task is to predict, for a given non-coding region of DNA, these 919 chromatin-profile including 690 transcription factors (TF) binding profiles for 160 different TFs, 125 DNase I sensitivity (DHS) profiles and 104 histone-mark (HM) profiles. We jointly learn 919 binary classifiers to predict these functional effects from sequence of DNA fragments. On held-out chromosomes, we compare AUC with the baselines in Tab. 7 and see that we significantly improve on performance on the harder task HM, which is known to have longer-range correlations <ref type="bibr" target="#b26">[27]</ref> than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose BIGBIRD: a sparse attention mechanism that is linear in the number of tokens. BIGBIRD satisfies a number of theoretical results: it is a universal approximator of sequence to sequence functions and is also Turing complete. Theoretically, we use the power of extra global tokens preserve the expressive powers of the model. We complement these results by showing that moving to sparse attention mechanism do incur a cost. Empirically, BIGBIRD gives state-of-the-art performance on a number of NLP tasks such as question answering and long document classification. We further introduce attention based contextual language model for DNA and fine-tune it for down stream tasks such as promoter region prediction and predicting effects of non-coding variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Big Bird: Transformers for Longer Sequences -Appendix A Universal Approximators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Notation</head><p>We begin by setting up some notations following Pérez et al. <ref type="bibr" target="#b72">[72]</ref> to formally describe the complete architecture of Transformers. A single layer of Transformer encoder is a parametric function Enc receiving a sequence X = (x 1 , ..., x n ) of vectors in R d and returning a sequence Z = (z 1 , ..., z n ) of the same length. Each z i is a d dimensional vector as well. We interchangeably treat the sequence X as a matrix in R n×d . Enc has two components:</p><p>1. An attention mechanism ATTN that takes in the sequence X and returns sequence (a 1 , ..., a n ) of the same length and dimensionality; and 2. A two layer fully connected network O that takes in a vector in R d and returns a vector in R d .</p><p>Then i-th output vector of Enc(X) is computed as follows:</p><formula xml:id="formula_6">z i = O(a i ) + a i where a i = ATTN(X) i + x i<label>(1)</label></formula><p>Now it remains to define ATTN and O which we do next.</p><p>As described in Sec. 2, an attention mechanism is parameterized by three functions: Q, K, V :</p><formula xml:id="formula_7">R d → R m .</formula><p>In this paper, we assume that they are simply matrix products: </p><formula xml:id="formula_8">Q(x) = xW Q , K(x) = xW K , and V (x) = xW V , where W Q , W K , W V ∈ R d×m and W V ∈ R d×d .</formula><formula xml:id="formula_9">ATTN D (X) i = H h=1 σ (x i W h Q )(X N (i) W h K ) T · (X N (i) W h V ) (AT)</formula><p>where N (i) denote the out-neighbors set of node i in D. In other words, the set of arcs (directed edges) in D represents the set of inner products that our attention mechanism will consider. Also recall that σ is a scoring function such as softmax or hardmax.</p><p>Lastly, we define the output fully connected network as follows:</p><formula xml:id="formula_10">O(a i ) = ReLU (a i W 1 + b 1 ) W 2 · +b 2 (FF) Here W 1 ∈ R d×q , W 2 ∈ R q×d , b 1 ∈ R p , and b 2 ∈ R d are parameters of output network O.</formula><p>Additional Notation We introduce a few pieces of additional notation that will be useful. Let</p><formula xml:id="formula_11">[a, b) δ = {a, a + δ, . . . , a + b−a δ · δ}. Therefore, [0, 1) δ = {0, δ, 2δ, . . . , (1 − δ)}.</formula><p>We use 1[E] to denote the indicator variable; it is 1 if the event E occurs and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof</head><p>In this section, we will present the full proof of theorem 1. The proof will contain three parts. The first and the third part will largely follow standard techniques. The main innovation lies is in the second part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Approximate F CD by piece-wise constant functions</head><p>First, we consider a suitable partition of the region (0, 1) into a grid of granularity δ, which we denote by G δ . We do this using Lemma 8 from Yun et al. <ref type="bibr" target="#b104">[104]</ref>, which we restate for completeness: Lemma 1 (Lemma 8 <ref type="bibr" target="#b104">[104]</ref>). For any given f ∈ F CD and 1 ≤ p ≤ ∞, there exists a δ &gt; 0 such that there exists a piece-wise constant functionf with d p (f,f ) ≤ 3 . Concretely,f is defined as</p><formula xml:id="formula_12">f (X) = P ∈G δ f (P ) · 1 [ ReLU(X − P ) ∞ ≤ δ]</formula><p>Since transformers can learn a positional embedding E, without any loss of generality, we can consider the translated function. In particular, define</p><formula xml:id="formula_13">E =       0 0 0 . . . 0 δ −d δ −d δ −d . . . δ −d δ −2d δ −2d δ −2d . . . δ −2d . . . δ −(n−1)d δ −(n−1)d δ −(n−1)d . . . δ −(n−1)d       We will try to approximate g(X) = f (X − E) where g is defined on the domain [0, 1] d × [δ −d , δ −d + 1] d × · · · × [δ −(n−1)d , δ −(n−1)d + 1] d .</formula><p>To do so, we will apply a suitable modification of Lemma 1, which will consider the discretized grid</p><formula xml:id="formula_14">G E δ := [0, 1] d δ × [δ −d , δ −d + 1] d δ × · · · × [δ −(n−1)d , δ −(n−1)d + 1] d δ .</formula><p>Therefore, it suffices to approximate a functionf :</p><formula xml:id="formula_15">G E δ → R n×d defined as f (X) = P ∈G E δ f (P − E) · 1 [ ReLU(X − P ) ∞ ≤ δ] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Contextual Mappings and Sparse Attention Mechanisms</head><p>Throughout this section, we will assume that we are given a function that has an extra global token at index 0 and all vectors have an extra dimension appended to them. The latter assumption is without loss of generality as we can use the Feed-Forward Network to append sparse dimensions. In particular, we will associate X ∈ R (n+1)×(d+1) where we write X = (x 0 , x 1 , . . . , x n ). Although our function is only defined for G E δ ⊂ R n×d , we can amend the function in a natural way by making it ignore the first column. To avoid excessive clutter, we will assume that the function value is evaluated on the last n columns.</p><p>The main idea in this section is the use of contextual mapping to enable Transformers to compute any discretized function. A contextual mapping is an unique encoding of each tuple (X,</p><formula xml:id="formula_16">x i ) where X ∈ G E δ , and each column x i ∈ [δ −(i−1)d , δ −(i−1)d + 1) d δ for all i ∈ [n]</formula><p>. We restate the definition adapted to our setting below Definition 2 (Defn 3.1 <ref type="bibr" target="#b104">[104]</ref>). (Contextual Mapping) A contextual mapping is a function mapping q : G E δ → R n if it satisfies the following:</p><p>1. For any P ∈ G E δ , q(P ) contains distinct entries. 2. For any two P, P ∈ G E δ with P = P , all entries of q(P ) and q(P ) are distinct.</p><p>The key technical novelty of the proof is computing a contextual mapping using only the sparse attention mechanism. We create a "selective shift" operator which only shifts entries of a vector that lie in a certain range. We will use this shift operator strategically to ensure that we attain a contextual mapping at the end of the process. The lemma below, which is based on parts of the proof of Lemma 6 of <ref type="bibr" target="#b104">[104]</ref>, states that we can implement a suitable "selective" shift operator using a sparse attention mechanism. Lemma 2. Given a function ψ :</p><formula xml:id="formula_17">R (n+1)×(d+1) × R 2 → R (n+1)×1</formula><p>and a vector u ∈ R d+1 and a sparse attention mechanism based on the directed graph D, we can implement a selective shift operator that receives as input a matrix X ∈ R (n+1)×(d+1) and outputs</p><formula xml:id="formula_18">X + ρ · ψ u (X, b 1 , b 2 ) where ψ u (Z; b 1 , b 2 ) i = (max j∈N (i) u T Z j − min j∈N (i) u T Z j )e 1 if b 1 ≤ u T Z j ≤ b 2 0 else.</formula><p>Note that e 1 ∈ R d+1 denotes (1, 0, . . . , 0).</p><p>Proof. Consider the function , which can be implemented by a sparse attention mechanism :</p><formula xml:id="formula_19">ψ(X, b) i = σ H (u T · X i ) T · (u T X N (i) − b1 T N (i) )e (1) (u T X N (i) )</formula><p>This is because the Key, Query and Value functions are simply affine transformations of X.</p><p>Given any graph D, the above function will evaluate to the following:</p><formula xml:id="formula_20">ψ(Z; b) i = (max j∈N (i) u T Z j )e 1 if u T Z j &gt; b (min j∈N (i) u T Z j )e 1 if u T Z j &lt; b</formula><p>Therefore we can say thatψ</p><formula xml:id="formula_21">(Z; b Q ) −ψ(Z; b Q ) satisfies ψ(Z; b 1 , b 2 ) i = (max j∈N (i) u T Z j − min j∈N (i) u T Z j )e 1 if b 1 ≤ u T Z j ≤ b 2 0 else</formula><p>The following lemma, which is the heart of the proof, uses the above selective shift operators to construct contextual mappings. Lemma 3. There exists a function g c : R (n+1)×(d+1) → R (n+1) and a unique vector u, such that for all P ∈ G E δ g c (P ) := u, g(P ) satisfies the property that g c is a contextual mapping of P . Furthermore, g c ∈ T 2,1,1 D using a composition of sparse attention layers as long as D contains the star graph.</p><formula xml:id="formula_22">Proof. Define u ∈ R d+1 = [1, δ −1 , δ −2 , . . . , δ −d+1 , δ −nd ]</formula><p>and let X 0 = (0, . . . , 0, 1). We will assume that x i , x 0 = 0, by assuming that all the columns x 1 , . . . , x n are appended by 0.</p><p>To successfully encode the entire context in each token, we will interleave the shift operator to target the original columns 1, . . . , n and to target the global column 0. After a column i is targeted, its inner product with u will encode the entire context of the first i columns. Next, we will shift the global token to take this context into account. This can be subsequently used by the remaining columns.</p><p>For i ∈ {0, 1, . . . , n}, we will use l i to denote the innerproducts u, x i at the beginning. For f i = u, x i after the i th column has changed for i ∈ {1, . . . , n} and we will use f k 0 to denote u, x 0 after the k th phase. We need to distinguish the global token further as it's inner product will change in each phase. Initially, given X ∈ G E δ , the following are true:</p><formula xml:id="formula_23">δ −(i−1)d ≤ u, X i ≤ δ −id − δ for all i ∈ [n] δ −(n+1)d = u, X 0</formula><p>Note that all l i ordered in distinct buckets l 1 &lt; l 2 &lt; · · · &lt; l n &lt; l 0 .</p><p>We do this in phases indexed from i ∈ {1, . . . , n}. Each phase consists of two distinct parts:</p><p>The low shift operation: These operation will be of the form</p><formula xml:id="formula_24">X ← X + δ −d ψ (X, v − δ/2, v + δ/2) for values v ∈ [δ −id ), δ −(i+1)d ) δ .</formula><p>The range is chosen so that only l i will be in the range and no other l j j = i is in the range. This will shift exactly the i th column x i so that the new inner product f i = u, x i is substantially larger than l i . Furthermore, no other column of X will be affected.</p><p>The high shift operation: These operation will be of the form</p><formula xml:id="formula_25">X ← X + δ −nd · ψ (X, v − δ/2, v + δ/2) for values v ∈ [S i , T i ) δ . The range [S i , T i ) δ</formula><p>is chosen to only affect the column x 0 (corresponding to the global token) and no other column. In particular, this will shift the global token by a further δ −nd . Letf i 0 denote the value off i 0 = u, x 0 at the end of i th high operation. Each phase interleaves a shift operation to column i and updates the global token. After each phase, the updated i th column f i = u, x i will contain a unique token encoding the values of all the l 1 , . . . , l i . After the high update,f i 0 = u, x 0 will contain information about the first i tokens. Finally, we define the following constants for all k ∈ {0, 1, . . . , n}.</p><formula xml:id="formula_26">T k = (δ −(n+1)d + 1) k · δ −nd − k t=2 (δ −(n+1)d + 1) k−t (2δ −nd−d + δ −nd + 1)δ −td − (δ −(n+1)d + 1) k−1 (δ −nd−d + δ −nd )δ −d − δ −(k+1)d (UP) S k = (δ −(n+1)d + 1) k · δ −nd − k t=2 (δ −(n+1)d + 1) k−t (2δ −nd−d + δ −nd + 1)δ −(t−1)d − (δ −(n+1)d + 1) k−1 (δ −nd−d + δ −nd ) − δ −kd (LP)</formula><p>After each k phases, we will maintain the following invariants:</p><formula xml:id="formula_27">1. S k &lt;f k 0 &lt; T k for all k ∈ {0, 1, . . . , n}. 2. T k−1 ≤ f k &lt; S k 3.</formula><p>The order of the inner products after k th phase is</p><formula xml:id="formula_28">l k+1 &lt; l k+2 · · · &lt; l n &lt; f 1 &lt; f 2 &lt; · · · &lt; f k &lt;f k 0 .</formula><p>Base case The case k = 0, is trivial as we simply set</p><formula xml:id="formula_29">S 0 = δ −(n+1)d , T 0 = δ −(n+1)·d + δ.</formula><p>The first nontrivial case is k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inductive</head><p>Step First, in the low shift operation is performed in the range [δ −(k−1)d , δ −kd ) δ Due to the invariant, we know that there exists only one column x k that is affected by this shift. In particular, for column k, we will have max j∈N (k) u,</p><formula xml:id="formula_30">x j = u, x 0 =f k−1 0 . The minimum is l k . Thus the update will be f k = δ −d (f k−1 0 − l k ) + l k . Observe that for small enough δ, f k ≥f k−1 0</formula><p>. Hence the total ordering, after this operation is</p><formula xml:id="formula_31">l k + 1 &lt; l k+2 · · · &lt; l n &lt; f 1 &lt; f 2 &lt; · · · &lt;f k−1 0 &lt; f k<label>(2)</label></formula><p>Now when we operate a higher selective shift operator in the range [S k−1 , T k−1 ) δ . Since only global token's innerproductf k−1 0 is in this range, it will be the only column affected by the shift operator. The global token operates over the entire range, we know from Eq. . Expanding and simplifying we get,</p><formula xml:id="formula_32">f k 0 = δ −nd · (f k − l k+1 ) +f k−1 0 = δ −nd · (δ −d (f k−1 0 − l k ) + l k − l k+1 ) +f k−1 0 = δ −(n+1)d · (f k−1 0 − l k ) + δ −nd (l k − l k+1 ) +f k−1 0 = (δ −(n+1)d + 1)f k−1 0 − (δ −nd−d + δ −nd )l k − l k+1</formula><p>Expanding the above recursively, we get</p><formula xml:id="formula_33">= (δ −(n+1)d + 1) k ·f 0 0 − k t=2 (δ −(n+1)d + 1) k−t (2δ −nd−d + δ −nd + 1)l t − (δ −(n+1)d + 1) k−1 (δ −nd−d + δ −nd )l 1 − l k+1</formula><p>Since we know thatf 0 0 = δ −nd and each l i &lt; δ −id , we can substitute this to get Eq. (UP) and we can get an lower-bound Eq. (LP) by using l i ≥ δ −(i−1)d .</p><p>By construction, we know that S k ≤f k 0 &lt; T k . For sufficiently small δ, observe that S k ≤f k 0 &lt; T k all are essentially the dominant term ≈ O(δ −n(k+1)d−kd ) and all the lower order terms do not matter. As a result it is immediate to see that that f k &gt; δ −d (f k−1 0 − l k ) &gt; T k−1 and hence we can see that the invariant 2 is also satisfied. Since only column k and the global token are affected, we can see that invariant 3 is also satisfied.</p><p>After n iterations,f n 0 contains a unique encoding for any P ∈ G E δ . To ensure that all tokens are distinct, we will add an additional layer X = X + δ −n 2 d ψ(X, v − δ/2, v + δ/2) for all v ∈ [S 1 , T n ) δ . This ensures that for all P, P ∈ G E δ , each entry of q(P ) and q(P ) are distinct.</p><p>The previous lemma shows that we can compute a contextual mapping using only sparse transforms. We now use the following lemma to show that we can use a contextual mapping and feed-forward layers to accurately map to the desired output of the functionf . Lemma 4 (Lemma 7 <ref type="bibr" target="#b104">[104]</ref>). Let g c be the function in Lemma 3, we can construct a function</p><formula xml:id="formula_34">g v : R (n+1)×(d+1) → R (n+1)×d composed of O(nδ −nd ) feed-forward layers (with hidden dimension q = 1) with activations in Φ such that g v is defined as g v (Z) = [g tkn v (Z 1 ), . . . , g tkn v (Z n )], where for all j ∈ {1, . . . , n}, g tkn v (g c (L) j ) = f (L) j A.2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approximating modified Transformers by Transformers</head><p>The previous section assumed we used Transformers that used hardmax operator σ H and activations functions belonging to the set Φ. This is without loss of generality as following lemma shows. Lemma 5 (Lemma 9 <ref type="bibr" target="#b104">[104]</ref>). For each g ∈T 2,1,1 and 1 ≤ p ≤ ∞, ∃g ∈ T 2,1,4 such that d p (g,ḡ) ≤ /3</p><p>Combining the above lemma with the Lemma 3, we get our main result:</p><formula xml:id="formula_35">Theorem 2. Let 1 ≤ p ≤ ∞ and &gt; 0, there exists a transformer network g ∈ T 2,1,4 D which achieves a ratio of d p (f, g) ≤ where D is the sparse graph.</formula><p>Since the sparsity graph associated with BIGBIRD contains a star network, we know that it can express any continuous function from a compact domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contemporary work on Universal Approximability of Sparse Transformers</head><p>We would like to note that, contemporary work done by Yun et al. <ref type="bibr" target="#b105">[105]</ref>, also parallelly explored the ability of sparse transformers with linear connections to capture sequence-to-sequence functions on the compact domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Turing Completeness</head><p>In this section, we will extend our results to the setting of Pérez et al. <ref type="bibr" target="#b72">[72]</ref>. Our exposition will largely use their proof structure but we will make a few changes. We repeat some of the lemmas with the amendments to make the exposition self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Notation</head><p>Transformer Decoder We need both an encoder and a decoder in the transformer for simulating a Turing machine. We utilize the same notation used in App. A.1 for encoders. The decoder is similar to an encoder but with additional attention to an external pair of key-value vectors (K e ∈ R n×m , V e ∈ R n×d ), which usually come from the encoder stack. A single layer of Transformer decoder is a parametric function Dec receiving a sequence Y j = (y 1 , . . . , y j ) of vectors in R d plus the external (K e , V e ) and returning a sequence of vectors Z j = (z 1 , . . . , z j ) of the same length. Each z i is a d dimensional vector as well. Dec has three components, one more than Enc:</p><p>1. An attention mechanism ATTN that takes in the sequence Y j and returns sequence (p 1 , ..., p j ) of the same length and dimensionality; 2. A cross-attention mechanism CROSSATTN that takes in the sequence (p 1 , ..., p j ) plus the external (K e , V e ) and returns sequence (a 1 , ..., a j ), with each a i ∈ R d ; and</p><p>3. A two layer fully connected network O that takes in a vector in R d and returns a vector in R d . Then i-th output vector of Dec(Y j ; K e , V e ) is computed as follows:</p><formula xml:id="formula_36">z i = O(a i ) + a i (3) where a i = CROSSATTN(p i , K e , V e ) + p i (4) and p i = ATTN D (Y j ) i + y i<label>(5)</label></formula><p>ATTN D and O are as defined in App. A.1 and it remains to define CROSSATTN. The i th output vector of multi-head cross-attention attention is given by</p><formula xml:id="formula_37">CROSSATTN(Y j ) i = H h=1 σ (y i W h Q )(K (e) W h K ) T · (V (e) W h V )<label>(6)</label></formula><p>where</p><formula xml:id="formula_38">W h Q , W h K , W h V ∈ R d×m , W h V ∈ R d×d , for all h = 1, . . . H heads.</formula><p>Turning Machine We will use the same setup of Turning Machine that was used by Pérez et al. <ref type="bibr" target="#b72">[72]</ref> (see section B.4). Given a Turing Machine M = (Q, Σ, δ, q init , F ), we use the following notation Note that we use five extra dimension as compared to Pérez et al. <ref type="bibr" target="#b72">[72]</ref>. We follow the convention used in Pérez et al. <ref type="bibr" target="#b72">[72]</ref> and write a a vector v ∈ Q d arranged in four groups of values as follows where q i ∈ Q |Q| , s i ∈ Q |Σ| , and x i ∈ Q.</p><formula xml:id="formula_39">v = [ q 1 , s 1 , x 1 , q 2 , s 2 , x 2 , x 3 , x 4 , x 5 , x 6 ,<label>s</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details of the Simulation</head><p>In this section, we give more details on the architecture of the encoder and decoder needed to implement our simulation strategy.</p><p>High Level Overview: Given the Turing machine M , we will show that a transformer with an appropriate encoder and decoder T D can simulate each step of M 's execution. Our simulation strategy will mostly follow Pérez et al. <ref type="bibr" target="#b72">[72]</ref>, except we will use a sparse attention mechanism. The main idea is to maintain the current Turing machine state q (j) and symbol under the head s (j) as part of the decoder sequence Y for all time step j so that we can always simulate the corresponding Turing machine transition δ(q (j) , s (j) ) = (q (j) , v (j) , m (j) ). The key difference will rise in Lemma B.4 of Pérez et al. <ref type="bibr" target="#b72">[72]</ref>, where full attention is used to select the appropriate symbol from tape history in one step. To accomplish the same task with sparse attention, we will exploit the associative property of max and break down the symbol selection over multiple steps. Finally, at the next compute step we will make the transition to new state q g(i)+1 , new head movement m g(i) , and new output symbol v g(i) to be written. Thereby we are able to completely simulate the given Turing machine M . As a result, we can prove the following main theorem: Theorem 3. There exists a sparse attention mechanism using O(n) inner products such that the resulting class of Transformer Networks using this sparse attention mechanism is Turing Complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>As <ref type="bibr" target="#b72">[72]</ref>, we use the same trivial single layer encoder where resulting K (e) contains position embedding and V (e) contains one-hot symbol representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>Sparse Self-Attention mechanism for Decoder In this section, we will consider a particular instance of the sparse graph D at decoder. We define its edges to be given by the following relations: ∀j ∈ N + , 1 ≤ k ≤ j + 1, j(j + 1) 2 + k, k(k + 1) 2 and</p><formula xml:id="formula_40">j(j + 1) 2 + k, j(j + 1) 2 + k if k &gt; 1 else j(j + 1) 2 + 1, j(j + 1) 2 .</formula><p>This graph can be seen as a special case of BIGBIRD where first type of edges are realizations of random and second type of edges correspond to locality. Also note that this graph satisfies the left-to-right constraint of decoder, i.e. no node attends to a node in the future.  pos Dec (i) = [ 0, . . . , 0, 0, . . . , 0, 0, . . . , 0, 1, g(i)</p><formula xml:id="formula_41">+ 1, 1 g(i)+1 , 1 (g(i)+1) 2 , h(i), 0, 0, 0, 0 ] where g(i) = −1+ √ 1+8i 2</formula><p>and h(i) = g(i + 1) − g(i). Note that h(i) reduces to a binary indicator</p><formula xml:id="formula_42">variable 1 −1+ √ 1+8i 2 = −1+ √ 1+8i 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Induction Setup</head><p>We next show how to construct the decoder layers to produce the sequence of outputs y 1 , y 2 , . . ., where y i is given by:</p><formula xml:id="formula_43">y i = [ q g(i) , s g(i) , c g(i) , 0, . . . , 0, 0 s , 0, w (i) , 0, 0, 0, 0, 0, u (i) 1 , u (i) 2 , u (i) 3 , u (i) 4 ]</formula><p>That is, at step i of our sparse decoder y i , it will contain the information about the state of the turing machine M at time g(i), the symbol under the head of M at time g(i), and the current location of head of M at time g(i). We also have a placeholder symbol w and placeholder scalars u 1 , u 2 , u 3 , whose role will be clear from our construction.</p><p>We consider as the starting vector for the decoder the vector</p><formula xml:id="formula_44">y 1 = [ q init , # , 0, 0, . . . , 0, 0, . . . , 0, 0, . . . , 0 ]</formula><p>We assume that the start head is at c (0) = 0, the initial state is q (0) = q init , and s (0) = # as we initialize from clean tape. We show the correctness of our construction by an inductive argument: we describe the architecture piece by piece and at the same time will show for every r ≥ 0 , our architecture constructs y r+1 from the previous vectors (y 0 , . . . , y r ).</p><p>Thus, assume that y 1 , . . . , y r satisfy the properties stated above. Since we are using positional encodings, the actual input for the first layer of the decoder is the sequence y 1 + pos Dec (1), y 2 + pos Dec (2), . . . , y r + pos Dec (r).</p><p>We denote by y i the vector y i plus its positional encoding. Thus we have ∀ 1 ≤ i ≤ r that</p><formula xml:id="formula_45">y i = [ q g(i) , s g(i) , c g(i) , 0, . . . , 0, 0 s , 0, w (i) , 1, g(i) + 1, 1 g(i)+1 , 1 (g(i)+1) 2 , h(i), u (i) 1 , u (i) 2 , u (i) 3 , u (i) 4 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Layer 1: Simulate Transition Function</head><p>In this layer, we use the cross-attention between encoder and decoder to access the input string and a feed-forward network to simulate the transition function of M . The first self attention in Eq. (5) is not used in this layer and we just produce the identity. This identity function is achieved by setting all queries, keys, values to be 0 everywhere plus the residual connection. Thus, we have p 1 i = y i . Since p 1 i is of the form [ , . . . , , 1, g(i) + 1, , . . . , ], we know by Lemma B.1 of Pérez et al. <ref type="bibr" target="#b72">[72]</ref> that if we use p 1 i to attend over the encoder we obtain CROSSATTN(p 1 i , K e , V e ) = [ 0, . . . , 0, 0, . . . , 0, α g(i)+1 , β g(i)+1 , 0 s , 0, . . . , 0 ]</p><p>where α and β are as defined in Eq. (21) of <ref type="bibr" target="#b72">[72]</ref>. Thus in Eq. (4) we finally produce the vector a 1 i given by</p><formula xml:id="formula_46">a 1 i = CROSSATTN(p 1 i , K e , V e ) + p 1 i = [ q g(i) , s g(i) , c g(i) , 0, . . . , 0, α g(i)+1 , β g(i)+1 , w (i) , 1, g(i) + 1, 1 g(i)+1 , 1 (g(i)+1) 2 , h(i), u (i) 1 , u (i) 2 , u (i) 3 , u (i) 4 ]<label>(7)</label></formula><p>As the final piece of the first decoder layer we use a function O 1 (·) (Eq. <ref type="formula">(3)</ref>) that satisfies the following lemma. Lemma 6 (Lemma B.2 <ref type="bibr" target="#b72">[72]</ref>). There exists a two-layer feed-forward network O 1 : Q d → Q d such that with input vector a 1 i (Eq. <ref type="formula" target="#formula_46">(7)</ref>) produces as output O 1 (a 1 i ) = [ 0, . . . , 0, q g(i)+1 , v g(i) , m g(i) , 0, 0, 0, 0 0, . . . , 0, 0, . . . , 0 ]</p><p>That is, function O 1 (·) simulates transition δ(q g(i) , s g(i) ) to construct q g(i)+1 , v g(i) , and m g(i) besides some other linear transformations.</p><p>Thus, finally the output of the first decoder layer is</p><formula xml:id="formula_47">z 1 i = O 1 (a 1 i ) + a 1 i = [ q g(i) , s g(i) , c g(i) , q g(i)+1 , v g(i) , m g(i) , 0, 0, 0, 0, α g(i)+1 , β g(i)+1 , w (i) , 1, g(i) + 1, 1 g(i)+1 , 1 (g(i)+1) 2 , h(i), u (i) 1 , u (i) 2 , u (i) 3 , u (i) 4 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Layer 2: Finding Head Node</head><p>In this layer, we only use the feed-forward network to evaluate the next location of the head. The self-attention and cross-attention are set to be the identity function, so a 2 i = p 2 i = z 1 i . Recall that c g(i) is the cell to which M is pointing to at time g(i), and that it satisfies the following recursion c g(i)+1 = c g(i) + m g(i) , which can be expanded to see that that c g(i)+1 = m (0) + m (1) + · · · + m g(i) . Its not difficult to see that a two layer network with non-linearity can compute c g(i)+1 /(g(i) + 1) and c g(i) /(g(i) + 1) from c g(i) , m g(i) , and 1/(g(i) + 1) using the relation c g(i)+1 = c g(i) + m g(i) . At the end of layer 2, we obtain</p><formula xml:id="formula_48">z 2 i = O 2 (a 2 i ) + a 2 i = [ q g(i) , s g(i) , c g(i) , q g(i)+1 , v g(i) , c g(i)+1 , 1 g(i)+1 , 1 (g(i)+1) 2 , c g(i)+1 g(i)+1 , c g(i) g(i)+1 , α g(i)+1 , β g(i)+1 , w (i) , 1, g(i) + 1, 1 g(i)+1 , 1 (g(i)+1) 2 , h(i), u (i) 1 , u (i) 2 , u (i) 3 , u (i) 4 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Layer 3: Distinguishing Node Type</head><p>This is an additional layer (not present in the work of <ref type="bibr" target="#b72">[72]</ref>), where we propagate computations in our sparse graph. In particular, we will use this layer to "compute" or accumulate state in intermediate nodes. We make this clear below. The self-attention and cross-attention are all set to be the identity function, so a 3 i = p 3 i = z 2 i . In this layer, we only use the dense attention layers to select the newly computed states or to continue with previous states. Using idea similar to Lemma B.6 of <ref type="bibr" target="#b72">[72]</ref>, we can construct a dense network such that</p><formula xml:id="formula_49">O([x, y, z, b])) = [0, 0, 0, 0] if b = 1, [0, z − y, −z, 0] if b = 0.</formula><p>The negatives are generated to offset results from skip connection. We utilize such network to switch Turing machine state and position embedding for intermediate steps to the values received from previous time step and do nothing for compute nodes. We use h(i) as the flipping bit b. Thus, at end of layer 3, we obtain</p><formula xml:id="formula_50">z 3 i = O 3 (a 3 i ) + a 3 i = [ 0, . . . , 0, q (i) , v (i) ,ĉ (i) , 1 g(i)+1 , 1 (g(i)+1) 2 , c g(i)+1 g(i)+1 ,û (i) 4 , α (i) ,β (i) , 0 s , 1,û (i) 1 ,û (i) 2 ,û (i) 3 , h(i), 0, 0, 0, 0 ]</formula><p>where we used h(i) for selecting old states. In particular, • We copy the input state and head position as is for intermediate nodes. We do not need to transition to next Turing machine states in these nodes.</p><formula xml:id="formula_51">q (i) = q g(i)+1 if h(i) = 1 q g(i) if h(i) = 0 ,v (i) = v g(i) if h(i) = 1 w (i) if h(i) = 0 ,ĉ (i) = c g(i)+1 if h(i) = 1 c g(i) if h(i) = 0 .</formula><p>• To preserve the symbol under the head for intermediate nodes, we copy the previous symbol to α location and set β = g(i) + 1, as the symbol at α location will be copied as the symbol under head for next transformer step by the final transformation layer if β = g(i) + 1. Thus, we correctly preserve the previous symbol under head as Turing machine does not transition these nodes. For compute nodes, things happen as usual.</p><formula xml:id="formula_52">α (i) = α g(i)+1 if h(i) = 1 s g(i) if h(i) = 0 ,β (i) = β g(i)+1 if h(i) = 1 g(i) + 1 if h(i) = 0 .</formula><p>• Finally for the intermediate nodes, we copy the position embedding corresponding to current best symbol w, which is stored in u 1 , u 2 , u 3 . For compute node, we let the position embedding correspond to current Turing machine step.</p><formula xml:id="formula_53">u (i) 1 = g(i) + 1 if h(i) = 1 u (i) 1 if h(i) = 0 ,û (i) 2 = 1 (g(i)+1) if h(i) = 1 u (i) 2 if h(i) = 0 , u (i) 3 = 1 (g(i)+1) 2 if h(i) = 1 u (i) 3 if h(i) = 0 ,û (i) 4 = c g(i) g(i)+1 if h(i) = 1 u (i) 4 if h(i) = 0 .</formula><p>For further simplification note that g(i + 1) = g(i) if h(i) = 0 else g(i) + 1 when h(i) = 1. With this fact, we can conclude thatq (i) = q g(i+1) andĉ (i) = c g(i+1) . Thus, we can write,</p><formula xml:id="formula_54">z 3 i = [ 0, . . . , 0, q g(i+1) , v (i) , c g(i+1) , 1 g(i)+1 , 1 (g(i)+1) 2 , c g(i)+1 g(i)+1 ,û (i) 4 , α (i) ,β (i) , 0 s , 1,û (i) 1 ,û (i) 2 ,û (i) 3 , h(i), 0, 0, 0, 0 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Layer 4: Finding next symbol on tape</head><p>To find the symbol on tape under next head position c g(i)+1 , we try to find what was written last at the location c g(i)+1 . To facilitate this, following <ref type="bibr" target="#b72">[72]</ref>, we define (j) to be the last time (previous to j) in which M was pointing to position c (j) , or it is j − 1 if this is the first time that M is pointing to c (j) . Recall j is the Turing machine step counter, which is different from sparse transformer step i. <ref type="bibr" target="#b72">[72]</ref> could utilize full attention mechanism to find v (j+1) at one go, but we have to do it over multiple steps owing to our sparse attention mechanism.</p><p>We use similar query, key, value functions as used for full attention by <ref type="bibr" target="#b72">[72]</ref> ∀i:</p><formula xml:id="formula_55">Q 4 (z 3 i ) = [ 0, .</formula><p>. . , 0 0, . . . , 0, 0, . . . , 0, 0, c g(i)+1 g(i)+1 , 1 g(i)+1 , 1 3(g(i)+1) 2 , 0, 0, 0, 0, 0 ] K 4 (z 3 i ) = [ 0, . . . , 0 0, . . . , 0, 0, . . . , 0, 0,û</p><formula xml:id="formula_56">(i) 2 ,û (i) 4 ,û (i) 3 , 0, 0, 0, 0, 0 ] V 4 (z 3 i ) = [ 0, . . . , 0, 0, . . . , 0, 0 s , 0, v (i) , 0, 0, 0, 0, 0,û (i) 1 ,û (i) 2 ,û (i) 3 ,û (i) 4 ]</formula><p>It is clear that the three functions are linear transformations and thus they can be defined by feedforward networks. Notice that the query vector is always formed using current time step position embedding, whereas key and value vectors are formed using copied over entries for intermediate nodes and using current entries only for compute node.</p><p>Pérez et al. <ref type="bibr" target="#b72">[72]</ref> find the desired v l(j+1) as v m(j) using full attention, where m(t) = arg min m∈{0,...,t}</p><formula xml:id="formula_57">χ j t = arg min m∈{0,...,t} | Q 4 (z 3 j ), K 4 (z 3 m ) |</formula><p>Note the minimization is only over Turing machine steps, i.e. over compute nodes in our case. We show below that we can estimates m(j) by parts using sparse attention mechanism. The main idea is just to notice that minimization problem min m∈{0,...,t} χ j t can be expressed as min{· · · min{min{χ j 0 , χ j 1 }, χ j 2 }, ..., χ j t } by the associativity property. By definition of our graph D, at every intermediate node i of the form j(j + 1)/2 + k, i.e. where k &gt; 0, g(i) = j and h(i) = 0, we will attend over node k(k + 1)/2 and best till now copied from i − 1. The node k(k + 1)/2 is never an intermediate node as h(k(k + 1)/2) = 1 for all k and in fact corresponds to Turing machine's step k. This will help us select the key and value corresponding to min between node k(k + 1)/2 and i − 1. In other words, at node i of the form j(j + 1)/2 + k we would have evaluated m(k) and corresponding value selected:</p><formula xml:id="formula_58">w (j(j+1)/2+k+1) =v m(k−1)</formula><p>and similarly for u's. So after going through all the intermediate nodes, finally at the next compute node, i.e. when k = j + 1, we will obtain the minimum value over all of 0, 1, ..., j. This implies at a compute node will be able to recover (g(i) + 1) and its corresponding value as shown in Lemma B.4 of <ref type="bibr" target="#b72">[72]</ref>. Then we have that p 4 i is given by</p><formula xml:id="formula_59">p 4 i = ATTN D (Z 3 i ) + z 3 i = [ 0, . . . , 0, q g(i+1) , v (i) , c g(i+1) , 0, c g(i)+1 g(i)+1 ,û (i) 4 , α (i) ,β (i) , w (i+1) , 1,û (i) 1 ,û (i) 2 ,û (i) 3 , h(i), u (i+1) 1 , u (i+1) 2 , u (i+1) 3 , u (i+1) 4 ]<label>(8)</label></formula><p>The cross-attention and feed-forward network are set to be identity, so z 4 i = a 4 i = p 4 i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.5 Final transformation</head><p>We finish our construction by using the final transformation function F (·) from the corresponding lemma from Pérez et al. <ref type="bibr" target="#b72">[72]</ref>, with a slight modification. Lemma 7 (Lemma B.5 <ref type="bibr" target="#b72">[72]</ref>). There exists a function F : Q d → Q d defined by a feed-forward network such that</p><formula xml:id="formula_60">F (z 4 r ) = [ q g(r+1)</formula><p>, s g(r+1)) , c g(r+1) , 0, . . . , 0, 0 s , 0, w (r+1) , 0, 0, 0, 0, 0, u</p><formula xml:id="formula_61">(r+1) 1 , u (r+1) 2 , u (r+1) 3 , u (r+1) 4 ] = y r+1</formula><p>The modification is to let w, u 1 , u 2 , u 3 to pass through. This yields the desired input to transformer at next time step for both intermediate and compute node, thereby concluding our induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitations</head><p>Finally, we show that sparse attention mechanisms can not universally replace dense attention mechanisms, i.e. there is no free lunch. We demonstrate a natural task which can be solved by the full attention mechanism in O(1)-layers. However, under standard complexity theoretic assumptions, we show that this problem will requireΩ(n)-layers for any sparse attention layers withÕ(n) edges (not just BIGBIRD). (We use the standard notationΩ(n) to hide the dependence on poly-logarithmic factors. )</p><p>We consider the simple problem of finding the furthest vector for each vector in the given sequence of length n and dimension d ∈ Ω(log 2 n). The assumption on the dimension is mild , as in many situations the dimension d = 768 is actually comparable to the number of n. Finding vectors that are furthest apart boils down to minimizing inner product search in case of unit vectors. For a full-attention mechanism with appropriate query and keys, this task is very easy as we can evaluate all pair-wise inner products.</p><p>The impossibility for sparse-attention follows from hardness results stemming from Orthogonal Vector Conjecture (OVC) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b96">96,</ref><ref type="bibr" target="#b6">7]</ref>, which is a widely used assumption in fine-grained complexity. Informally, it states that one cannot determine if the minimum inner product among n Boolean vectors is 0 in subquadratic time. with graph D havingÕ(n) edges (i.e. inner product evaluations), would requireΩ(n 1−o(1) ) layers.</p><p>Proof. We will break this proof into two parts:</p><p>Part 1: The full attention mechanism can solve the problem in O(1) layer We begin by providing an explicit construction of a single layer full self-attention that can evaluate Task 1.</p><p>Step 1 We embed each u i in the input into R 2d as follows:</p><formula xml:id="formula_62">x i := E(u i ) = [u i ; 0]<label>(9)</label></formula><p>Step 2 Construct query, key, value functions as follows:</p><formula xml:id="formula_63">Q([a; b]) = −a K([a; b]) = a V ([a; b]) = [0; a]<label>(10)</label></formula><p>Then Attn(Q(x i ), K(X), V (X) = [0; u arg max j −ui,uj ]. Then,</p><formula xml:id="formula_64">a i = Attn(Q(x i ), K(X), V (X)) + x i = [u i ; u arg max j −ui,uj ] = [u i ; u i * ]<label>(11)</label></formula><p>Step 3 Let O(a i ) = 0, then the output z i = [u i ; u i * ] as desired.</p><p>To complete the argument, observe that it now only takes O(n) inner products to check if there is a pair of orthogonal vectors as we need only compare u i , u i * .</p><p>Part 2: Every Sparse Attention Mechanism will needΩ(n 1− ) layers We prove by contradiction that it is impossible to solve Task 1 by any g ∈ T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H=O(d),m=O(d),q=O(d) D</head><p>sparse-attention graph D withÕ(n) edges.</p><p>Suppose we can solve Task 1 using a network g ∈ T H=O(d),m=O(d),q=O(d) D that has l layers. Recall that all the computation we do in one layer is:</p><formula xml:id="formula_65">a i = ATTN D (Q(x i ), K(X N (i) ), V (X N (i) ) + x i x i = O(a i ) + a i<label>(12)</label></formula><p>where Attn D is defined in eq. (AT).</p><p>Thus, total computation per layer isÕ(nd 3 ) and consequentlyÕ(nld 3 ) for the whole network consisting of l layers.</p><p>We can use the result of Task 1 to solve the orthogonal vector (OV) problem (defined in Conjecture 1) in linear time. So in total, we will be able to solve any instance of OV inÕ(nld 3 ) time.</p><p>Now if l = O(n 1− ) for any &gt; 0 and d = Θ(log 2 n), then it appears that we are able to solve OV inÕ(n 2− ) which contradicts Conjecture 1. Therefore, we need at leastΩ(n 1−o(1) ) layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation details</head><p>We optimize the code for modern hardware. Hardware accelerators like GPUs and TPUs truly shine on coalesced memory operations which load blocks of contiguous bytes at once. Thus, its not very efficient to have small sporadic look-ups caused by a sliding window or random element queries. We alleviate this by "blockifying" the lookups.</p><p>GPU/TPU and Sparsity Ideally, if the adjacency matrix A described in Sec. 2 is sparse, one would hope this would be sufficient to speed up the implementation. Unfortunately, it is well known <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b102">102]</ref>, that such sparse multiplications cannot be efficiently implemented in GPUs. GPUs have thousands of cores performing operations in parallel. Thus, we cannot efficiently perform the sparse matrix multiplication mentioned in section Sec. 2.</p><p>As a result we propose to first blockify the attention pattern i.e. we pack sets of query and keys together and then define attention on these blocks. It is easier to explain this process using the example shown in <ref type="figure" target="#fig_7">Fig. 3</ref>. Suppose, there are 12 query and 12 key vectors to attend to. Using a block size of 2, we split the query matrix into 12/2 = 6 blocks and similarly the key matrix into 12/2 = 6 blocks. Then the three different building components of BIGBIRD are defined on the block matrix. In particular the three different components are:</p><p>1. Random attention: Each query block attends to r random key blocks. In <ref type="figure" target="#fig_7">Fig. 3a</ref>, r = 1 with block size 2. This implies that each query block of size 2 randomly attends to a key block of size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Window local attention:</head><p>While creating the block, we ensure that the number of query blocks and the number of key blocks are the same. This helps us in defining the block window attention. Every query block with index j attends to key block with index j − (w − 1)/2 to j + (w − 1)/2, including key block j. In <ref type="figure" target="#fig_7">Fig. 3b</ref>, w = 3 with block size 2. It means that each query block j (size 2 queries) attends to key block j − 1, j, j + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global attention:</head><p>Global attention remains the same as defined in Sec. 2, but we compute it in terms of blocks. In <ref type="figure" target="#fig_7">Fig. 3c</ref>, g = 1 with block size 2. For BIGBIRD-ITC this implies that one query and key block, attend to everyone.</p><p>The resulting overall attention matrix is shown in <ref type="figure" target="#fig_7">Fig. 3d</ref>. Unfortunately, simply trying to compute this attention score as multiplying arbitrary pairs of query and key vectors would require use of gather operation, which is inefficient. Upon closer examination of window and global attention, we observe that we can compute these attention scores without using a gather operation.</p><p>Recall, full dense attention scores can be calculated by simple matrix product of query and key matrix with a cost of O(n 2 d), as illustrated in <ref type="figure">Fig. 4a</ref>. Now note that if we blockify the query and key matrix and multiply, then with only O(nbd) cost we will obtain the block diagonal portion of the attention score, as depicted in <ref type="figure">Fig. 4b</ref>. To elaborate this lets assume that Q, K ∈ R n×d are the query and key matrix corresponding to n tokens such that Q i. = x i W Q and K i. = x i W K . We reshape n × d query  <ref type="table" target="#tab_1">d   21  22  23  24   17  18  19  20   1  2  3  4   5  6  7  8   9  10  11  12   13  14  15  16   b   d   b   1  2  3  4   5  6  7  8  9  10  11  12   13  14  15  16  17  18  19</ref>  (a) Full all pair attention can be obtained by direct matrix multiplication between the query and key matrix. Groupings just shown for guidance. (c) Window local attention obtained by "blockifying" the query/key matrix, copying key matrix, and rolling the resulting key tensor (Obtaining rolled key-block tensor is illustrated in detail in <ref type="figure">Fig. 5</ref>). This ensures that every query attends to at least one block and at most two blocks of keys of size b on each side.  matrix, Q, and key matrix, K, along the sequence length to obtain n/b × b × d tensors Q and K respectively. Now we multiply the two tensors as</p><formula xml:id="formula_66">A jst = u Q jsu K jtu , j = 0, 1, ..., n/b<label>(13)</label></formula><p>The resulting A tensor of size n/b × b × b can be reshaped to correspond to the block diagonal portion of the full attention pattern. Now to extend the attention from block diagonal to a window, i.e. where query block with index j attends to key block with index j − (w − 1)/2 to j + (w − 1)/2, we make w copies of the reshaped key tensor K . We "roll" each copy of key-block tensor incrementally along the first axis of length n/b as illustrated in <ref type="figure">Fig. 5</ref>. Multiplying these w rolled key-block tensors with the query-block tensor would yield the desired window attention scores <ref type="figure">(Fig. 4c</ref>). Likewise the global component, we can always include the first g blocks from key tensor corresponding to the global tokens. Finally, for the random attention, which is very small (r = 3 for all of our experiments), we resort to using gather ops <ref type="figure">(Fig. 4d</ref>). Also note by design, each query block attends to exactly r random blocks.</p><p>Thus, the result of all the three components is basically a compact dense tensor K of size n/b × (g + w + r)b × d as shown in <ref type="figure">Fig. 6</ref>. Computing the final attention score then just boils down to a dense tensor multiplication, at which TPU/GPU are very efficient. Specifically, we need to multiply Q (size: n/b × b × d) and K (size: n/b × (g + w + r)b × d) with a cost of O(n(g + w + r)bd) to yield the desired attention score tensor of size n/b × b × (g + w + r)b, which can be reshaped to obtain all the attention scores according to the BigBird pattern.  <ref type="figure">Figure 6</ref>: Overview of BIGBIRD attention computation. Structured block sparsity helps in compactly packing our operations of sparse attention, thereby making our method efficient on GPU/TPU. On the left, we depict the transformed dense query and key tensors. The query tensor is obtained by simply blocking and reshaping while the final key tensor by concatenating three transformations: The first green columns, corresponding to global attention, is fixed. The middle blue columns correspond to window local attention and can be obtained by appropriately rolling as illustrated in <ref type="figure">Fig. 5</ref>. For the final orange columns, corresponding to random attentions, we need to use computationally inefficient gather operation. Dense multiplication between the query and key tensors efficiently calculates the sparse attention pattern (except the first row-block, which is computed by direct multiplication), using the ideas illustrated in <ref type="figure">Fig. 4</ref>. The resultant matrix on the right is same as that shown in <ref type="figure" target="#fig_7">Fig. 3d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E NLP experiments details E.1 MLM Pretraining</head><p>We use four publicly available datasets Books <ref type="bibr" target="#b110">[110]</ref>, CC-News <ref type="bibr" target="#b33">[34]</ref>, Stories <ref type="bibr" target="#b89">[89]</ref> and Wikipedia to pretrain BIGBIRD. We borrow the sentencepiece vocabulary as RoBERTa (which is in turn borrowed from GPT2). We split any document longer than 4096 into multiple documents and we join documents that were much smaller than 4096. Following the original BERT training, we mask 15% of tokens in these four datasets, and train to predict the mask. We warm start from RoBERTa's checkpoint. We train two different models: BIGBIRD-ITC-base and BIGBIRD-ETC-base. The hyper-parameters for these two models are given in Tab. <ref type="bibr" target="#b7">8</ref>. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate.</p><p>Similar to the norm, we trained a large version of model as well, which has 24 layers with 16 heads and hidden dimension of 1024. Following the observation from RoBERTa, we pretrain on a larger batch size of 2048 for this size. For BIGBIRD-ITC the block length was kept same as base size, but for BIGBIRD-ETC the block length was almost doubled to 169. All the remaining parameters were the same.  <ref type="table">Table 8</ref>: Hyperparameters for the two BIGBIRD base models for MLM.</p><formula xml:id="formula_67">Parameter BIGBIRD-ITC BIGBIRD-ETC Block length, b 64 84 # of global token, g 2 × b 256 Window length, w 3 × b 3 × b # of random token, r 3 × b 0 Max. sequence</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Question Answering</head><p>The detailed statistics of the four datasets used are given in Tab. 11. All the hyperparameters for BIGBIRD, used for creating Tab. 2 are shown in Tab. 12 and those submitted to get Tab. 3 are shown in Tab. 13. We use two types of regularization in training:</p><p>• We used a variant of contrastive predictive coding <ref type="bibr" target="#b70">[70]</ref> as a dual encoder model. • We use position embedding for ITC and relative position encoding <ref type="bibr" target="#b79">[79]</ref> for ETC.</p><p>Next, we will mention the dataset/task specific part of the model.    HotpotQA The data consists of each question with multiple evidence paragraphs. We filtered <ref type="bibr" target="#b15">16</ref> QA where the answer was not in the given evidences. For BIGBIRD-ITC, we use first 128 global tokens. For BIGBIRD-ETC, we have one global token for each question token, one for each evidence paragraph, and one for each sentence within the paragraph, for a maximum of 256 global token. We use a dense layer on the output corresponding to global token of the evidence paragraph to predict whether its a supporting fact with a threshold over the output logits. The answer type (yes/no/span) is predicted with a single dense layer from the global CLS token. For span based answers, the spans are predicted with dense layers on the sequence with the distance between start and end positions to be no more than 30 words. The spans are ranked by sum of start and end logits.</p><p>Natural Questions Here also the data consists of question with supporting evidence, but in form of a single, potentially long, document and not multiple paragraphs. We largely follow the setup of <ref type="bibr" target="#b4">[5]</ref>. For documents, that are longer than 4096, a sliding window approach is used with stride of 2048. We use CLS token at the beginning, followed by the question followed by a separator token followed by the document as input. For BIGBIRD-ITC, we make the first 128 tokens as global. For BIGBIRD-ETC, we make a global token for CLS, question, and one token for each of the paragraphs. We train four predictors at the final layer to predict long answer start, long answer end, short answer start and short answer end respectively. Instead of independently predicting the start and end of answers we first predict the start and then predict the best end location beyond the start. For short answer, we limit the distance between start and end positions to be no more than 38 words. The answer type (null, yes, no, short, long) is predicted from CLS token output embedding. When the logit for a yes/no answer is higher than the logits for short, long or null answer, we replace the short answer with a corresponding yes/no text.</p><p>TriviaQA The data consists of question-answer pairs with Wikipedia articles as the "noisy" supporting evidence. We call them noisy because the given Wikipedia articles may or may not contain the answer. Moreover, the answer entities is not annotated to appropriate span in the article, rather all occurrences found using fuzzy string matching are listed. We use CLS token at the beginning, followed by the question followed by a separator token followed by the document as input. For BIGBIRD-ITC, we make the first 128 tokens as global. For BIGBIRD-ETC, we make a global token for CLS, question, and one token for each sentence up to a maximum of 320 global tokens. Given the noisy nature of answer span, we follow Clark and Gardner <ref type="bibr" target="#b17">[18]</ref> for training. We use a dense layer on the sequence to predict the answer span for each article independently, with the distance between start and end positions to be no more than 16 words. For each article the span with maximum start logit + end logit is chosen. Then we normalize over all the documents associated with that question.</p><p>WikiHop For each question in WikiHop, we are given upto 79 candidates, and 63 supporting paragraphs. In our BIGBIRD-ITC model, following Beltagy et al. <ref type="bibr" target="#b7">[8]</ref>, we concatenate the answer and the question with special tokens, [q] Question [/q] [ans] Ans1 [/ans] . . .</p><p>[ans] AnsN [/ans] along with the context. As the start of the text, always contains questions followed by answers, we make the first 128 token attend globally. In BIGBIRD-ETC model, we do not need to insert special [ans], [/ans] etc. as we design global tokens appropriately. Along with global tokens for question, we have one per candidate answer up to a maximum of 430. Further, we linked answer tokens to their mentions using relative position label. Lastly, we use a dense layer that takes in the output vector corresponding to a candidate answer, and predicts a score for the current candidate to be the correct answer. We apply this dense layer to each candidate independently and the candidate with the best score is picked as our final answer.</p><p>It is worthwhile to note that explicitly designed attention connection in ETC works slightly better, the random connection based ITC is pretty competative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Relationship to Contemporary Work</head><p>Longformer Child et al. <ref type="bibr" target="#b15">[16]</ref> introduced localized sliding window to reduce computation. A more recent version, which includes localized sliding windows and global tokens was introduced independently by Longofrmer <ref type="bibr" target="#b7">[8]</ref>. Although BIGBIRD contains additional random tokens, there are also differences in the way global and local tokens are realized. In particular even when there is no random token, as used to get SoTA in question answering, there are two key differences between Longformer and BIGBIRD-etc (see <ref type="bibr" target="#b3">[4]</ref>):</p><p>1. We use global-local attention with relative position encodings enables it to better handle structured inputs 2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Classification</head><p>We try two types of classification task.    GLUE The General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b92">[92]</ref>, test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 × b, w = 3 × b, r = 3 × b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet <ref type="bibr" target="#b101">[101]</ref> and RoBERTa in Tab. <ref type="bibr" target="#b15">16</ref>. We find that even on task that have a much smaller context, our performance is competitive to full attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Summarization</head><p>As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder.   Following success of several recent works <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b63">63]</ref>, we warm start our encoder-decoder BIGBIRD transformer model with pretrained weights and the weights between encoder and decoder are shared.</p><p>In particular, the query/key/value matrix of self-attention and all the feedforward layers are shared between encoder and decoder. The only variable that is initialized randomly is the encoder-decoder attention. For base sized model, we utilize our MLM pretrained model on 4096 sequence length from App. E.1, which is in turn initialized using the public RoBERTa checkpoint. For the large size model, we lift weight from the state-of-the-art Pegasus model <ref type="bibr" target="#b107">[107]</ref>, which is pretrained using an objective designed for summarization task.</p><p>To check if sparse attention causes significant degradation as compared to full attention, we further experiment on two shorter but popular datasets, where full attention can be used without significantly truncating the document. The statistics of these two datasets are in Tab. <ref type="bibr" target="#b18">19</ref>. We see that our performance is competitive, which shows that sparse attention can achieve similar performance to a full attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Genomics experiments details</head><p>In this section we provide details of the experimental setup for BIGBIRD on genomics data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Pretraining</head><p>We try to keep the experimental setup as close to a typical NLP pipeline. In this regard, we take human reference GRCh37 7 and convert it into documents D. Each document d ∈ D is a sequence of sentences, where each sentence is a sequence of fragments of DNA. We construct the documents as follows:</p><p>1. Start with empty document set D = ∅.</p><p>2. For each chromosome C, repeat the following procedure 10 times. (a) Pick uniformly at random a starting point q between base pairs 0 and 5000 from the 5' end.</p><p>(b) Repeat until q &gt; |C| i. Pick uniformly at random s a number between 50 and 100 to denote number of sentences per document.</p><p>ii. Constructs a document d containing s sentences using consecutive base pairs (bps). The length of each sentence is chosen uniformly at random between 500-1000. Thus the resulting document has 25, 000 -100, 000 bps.</p><p>iii. D = D d iv. q = q + |d| By this procedure we end-up with approximately 450K documents.</p><p>Next we run sentencepiece <ref type="bibr" target="#b49">[50]</ref> tokenization on the resulting documents. In particular, using 5 characters as the building blocks (four for bases -A, T, C, G and one for missing symbol N), we construct a byte pair encoding table of size 32k, with each token representing 8.78 base pairs on average.</p><p>Using the above constructed documents, we construct a dataset for two pretraining tasks following Devlin et al. <ref type="bibr" target="#b21">[22]</ref>:</p><p>• Masked Language Model (MLM): In order to train a deep bidirectional representation, BERT training introduces the MLM task, where we simply mask out 15% of the input tokens at random, and then predict those masked tokens. We can simply replace such masked out of the tokens with a [MASK] placeholder, but it leads to a distribution mis-match for downstream tasks which will not have such placeholders. To mitigate with this issue, out of the 15% of the tokens selected for masking: -80% of the tokens are actually replaced with the token [MASK].</p><p>-10% of the time tokens are replaced with a random token.</p><p>-10% of the time tokens are left unchanged, but are still predicted at output. We run this entire sequence through the BIGBIRD transformer encoder and then predict corresponding to the masked positions, based on the context provided by the other non-masked tokens in the sequence. . For 50% of the time the second sequence comes from true sequence after the first one. Remaining 50% of the time it is a a random sequence from the full dataset. The model is then required to predict this relationship using the output corresponding to the [CLS] token, which is fed into a simple binary classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T G G G C T A A C A A G C A A A T G A T C T G T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Create Document &amp; Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T G G G C T A A C A A G C A A A T G A T C T G T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentencepiece T G G G C T A A C A A G C A A A T G A T C T G T</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T G G G C T A A C A A G C A A A T G A T C T G T</head><p>Masking ...</p><p>... <ref type="figure">Figure 7</ref>: Visual description of how the masked language modeling data was generated from raw DNA dataset. The raw DNA sequences of GRCh37, where split at random positions to create documents with 50-100 sentences where each sentence was 500-1000 base pairs (bps). Thus each document had a continuous strand of 25000-100,000 bps of DNA. This process was repeated 10 times to create 10 sets of document for each chromosome of GRCH37. The resulting set of documents was then passed through Sentencepiece that created tokens of average 8bp. For pretraining we used masked language model and masked 10% of the tokens and trained on predicting the masked tokens. The sequence of steps is visually elaborated in <ref type="figure">Fig. 9</ref>. The model is trained with both MLM and NSP together. Training hyperparameter is provided in second columns of Tab. <ref type="bibr" target="#b20">21</ref>. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate.</p><p>We additionally performed a simple ablation study to validate the hypothesis, that similar to NLP, having a larger context improves performance. We use MLM task described above to test how BIG-BIRD performed with sequences of different length. Accuracy on MLM task with increasing sequence length is shown in <ref type="figure" target="#fig_14">Fig. 8</ref>. Not only longer context improves final accuracy, it also leads to faster learning, as we have now more opportunities for masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Promoter Region Prediction</head><p>The promoter region plays an important role in transcription initiation and thus its recognition is an important area of interest in the field of bioinformatics. Following Oubounyt et al. <ref type="bibr" target="#b71">[71]</ref>, we use datasets from Eukaryotic Promoter Database (EPDnew) <ref type="bibr" target="#b23">[24]</ref>, which contains 29,597 promoter region in the human genome. Around the transcription start site (TSS), we extract a sequence of 8000 bp (-5000 +3000 bp) from the human reference genome GRCh37. Since EPDnew uses newer GRCh38, we convert to GRCh37 coordinates using LiftOver <ref type="bibr" target="#b43">[44]</ref>.</p><p>Context 5000 bp</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T G G T A A C A G C A A T G C T G T</head><p>... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predict Epigenetic Features of 200 bp non-coding region</head><p>Context 3000 bp ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G G T A A C A G C A A T G C T G</head><p>... ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentencepiece</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G T A A C A G</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head><p>... C A A T G ... <ref type="figure">Figure 9</ref>: Visual description of the DNA segment from which we predict the chromatin profile for a given non-coding region of the raw DNA sequences of GRCh37. We take 8000 bps of DNA before and after the given non-coding region as context. The complete fragment of DNA including the context on both side, is then tokenized to form our input sequence of tokens. The task is to predict 919 chromatin profile including 690 transcription factors (TF) binding profiles for 160 different TFs, 125 DNase I sensitivity (DHS) profiles and 104 histone-mark (HM) profiles Following Oubounyt et al. <ref type="bibr" target="#b71">[71]</ref> for each promoter region example, a negative example (non-promoter sequences) with the same size of the positive one is constructed as follow: The positive sequence is divided into 20 subsequences. Then, 12 subsequences are picked randomly and substituted randomly. The remaining 8 subsequences are conserved. This process is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> of <ref type="bibr" target="#b71">[71]</ref>. Applying this process to the positive set results in new non-promoter sequences with conserved parts from promoter sequences (the unchanged subsequences, 8 subsequences out of 20). These parameters enable generating a negative set that has 32 and 40% of its sequences containing conserved portions of promoter sequences.</p><p>Zhou and Troyanskaya <ref type="bibr" target="#b109">[109]</ref>  <ref type="bibr" target="#b7">8</ref> , to train BIGBIRD to predict the chromatic profile.</p><p>Each training sample consists of a 8,000-bp sequence from the human GRCh37 reference genome centered on each 200-bp bin and is paired with a label vector for 919 chromatin features. As before, we prefix and append each example with [CLS] and [SEP] token respectively. The output corresponding to the [CLS] token from BIGBIRD transformer encoder is fed to a linear layer with 919 heads. Thus we jointly predict the 919 independent binary classification problems. We fine-tune the pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. 21. As the data is highly imbalanced data (way more negative examples than positive examples), we upweighted loss function for positive examples by factor of 8.</p><p>We used training and testing split provided by Zhou and Troyanskaya <ref type="bibr" target="#b109">[109]</ref> using chromosomes and strictly non-overlapping. Chromosome 8 and 9 were excluded from training to test chromatin feature prediction performances, and the rest of the autosomes were used for training and validation. 4,000 samples on chromosome 7 spanning the genomic coordinates 30,508,751-35,296,850 were used as the validation set.</p><p>As the predicted probability for each sequence in DeepSea Zhou and Troyanskaya <ref type="bibr" target="#b109">[109]</ref> was computed as the ensemble average of the probability predictions for the forward and complementary sequence pairs, we also predict using an ensemble of two BIGBIRD model trained independently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Building blocks of the attention mechanism used in BIGBIRD. White color indicates absence of attention. (a) random attention with r = 2, (b) sliding window attention with w = 3 (c) global attention with g = 2. (d) the combined BIGBIRD model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(2) that, f k = max i∈[n] u, x i and l k+1 = min i∈[n] u, x i . The new valuef k 0 = δ −nd · (f k − l k+1 ) +f k−1 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>q</head><label></label><figDesc>(j) : state of Turing machine M at time j. s (j) : symbol under the head of M at time j. v (j) : symbol written by M at time j. m (j) : head direction in the transition of M at time j. Vector representations For a symbol s ∈ Σ, s denotes its one-hot vector representation in Q |Σ| . All the transformer intermediate vectors used in our simulations have dimension d = 2|Q|+4|Σ|+16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Mapping between transformer step and original Turing machine step. Embeddings and positional encodings Our construction needs a different positional encoding pos Dec : N → Q d for decoder:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Task 1 .</head><label>1</label><figDesc>Given n unit vectors {u 1 , . . . , u n }, each in R d where d = Θ(log 2 n), compute f (u 1 , . . . , u n ) → (u 1 * , . . . , u n * ) where for a fixed j ∈ [n], we define j * = arg max k u k − u j 2 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Conjecture 1 (Proposition 2 .</head><label>12</label><figDesc>Orthogonal Vectors Conjecture). For every &gt; 0, there is a c ≥ 1 such that given n Boolean vectors in d dimension, cannot determine if there is a pair of orthogonal vectors in O(n 2− ) time on instances with d ≥ c log n. Using conjecture 1, we show a reduction to show that a transformer g ∈ T H=O(d),m=O(d),q=O(d) D for any sparse directed graph D which completes Task 1 must require a superlinear number of layers. There exists a single layer full-attention network g ∈ T H=1,m=2d,q=0 that can evaluate Task 1, i.e. g(u 1 , ..., u n ) = [u 1 * , . . . , u n * ], but for any sparse-attention network in T H=O(d),m=O(d),q=O(d) D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Building blocks of the block-attention mechanism used in BIGBIRD with block size = 2. This implies the attention matrix is split into blocks of size 2 × 2. All the previous BIGBIRD parameters work on each block as a unit. White color indicates absence of attention. (a) random attention with r = 1, (b) sliding window attention with w = 3 (c) global attention with g = 1. (d) the combined BIGBIRD model. H I J K L M N O P Q R S T U V X Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>20 A</head><label>20</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Block diagonal attention can be computed by "blockifying" the query and key matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Window + Random attention obtained by following the procedure above along with gathering some random key blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Idea behind fast sparse attention computation in BIGBIRD. Construction of rolled key-block tensor. Make w copies of the key matrix. Index the copies as −(w − 1)/2 ≤ j ≤ (w − 1)/2. Roll j th copy by j blocks. Positive roll means circular shift entries left and likewise for negative roll corresponds to right shift. Finally, reshape by grouping the blocks along a new axis to obtain the key-blocked tensor. For illustration purpose w = 3 is chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>•</head><label></label><figDesc>Next Sentence Prediction (NSP): In order to understand relationship between two sequences, BERT training introduces the NSP task, where we predict if a given pair of sequences are contiguous or not. During training the model gets as input pairs of sequences separated by [SEP] token along with a [CLS] token at the start. Overall the input pattern is: [CLS] sequence A [SEP] sequence B [SEP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>BIGBIRD accuracy with context length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Building block comparison @512</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>QA Dev results using Base size models. We report accuracy for WikiHop and F1 for HotpotQA, Natural Questions, and TriviaQA.</figDesc><table><row><cell>Model</cell><cell></cell><cell>HotpotQA</cell><cell></cell><cell cols="2">NaturalQ</cell><cell cols="2">TriviaQA</cell><cell>WikiHop</cell></row><row><cell></cell><cell cols="3">Ans Sup Joint</cell><cell>LA</cell><cell>SA</cell><cell cols="2">Full Verified</cell><cell>MCQ</cell></row><row><cell>HGN [26]</cell><cell cols="3">82.2 88.5 74.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GSAN</cell><cell cols="3">81.6 88.7 73.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ReflectionNet [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">77.1 64.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RikiNet-v2 [61]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">76.1 61.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fusion-in-Decoder [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.4</cell><cell>90.3</cell><cell>-</cell></row><row><cell>SpanBERT [42]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.1</cell><cell>86.6</cell><cell>-</cell></row><row><cell>MRC-GCN [87]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.3</cell></row><row><cell>MultiHop [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.5</cell></row><row><cell>Longformer [8]</cell><cell cols="3">81.2 88.3 73.2</cell><cell>-</cell><cell>-</cell><cell>77.3</cell><cell>85.3</cell><cell>81.9</cell></row><row><cell>BIGBIRD-ETC</cell><cell cols="3">81.2 89.1 73.6</cell><cell cols="2">77.8 57.9</cell><cell>84.5</cell><cell>92.4</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Fine-tuning results on Test set for QA tasks. The Test results (F1 for HotpotQA, Natural Questions, TriviaQA, and Accuracy for WikiHop) have been picked from their respective leaderboard. For each task the top-3 leaders were picked not including BIGBIRD-etc. For Natural Questions Long Answer (LA), TriviaQA, and WikiHop, BIGBIRD-ETC is the new state-of-the-art. On HotpotQA we are third in the leaderboard by F1 and second by Exact Match (EM).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Summarization ROUGE score for long documents.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>MLM BPC Pre-training and MLM As explored in Liang</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Chromatin-Profile PredictionChromatin-Profile Prediction Non-coding regions of DNA do not code for proteins. Majority of diseases and other trait associated single-nucleotide polymorphism are correlated to non-coding genomic variations</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>3 , x 7 , s 4 , x 8 , x 9 , x 10 , x 11 , x 12 , x 13 , x 14 , x 15 , x 16 ]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Thus, unlike Pérez et al.<ref type="bibr" target="#b72">[72]</ref> one decoding step of our sparse transformer T D does not correspond to one step of the Turing machine M . In particular, we will have two type of steps: compute step corresponding to update of M 's state and intermediate steps corresponding to aggregating the max (which in turn is used for symbol selection). Let i denote the step of T D and g(i) denote the step of M being simulated at step i of the decoder. At each decoding step we want to maintain the current Turing machine state q g(i) and symbol under the s g(i) in y i .For roughly O( √ i) intermediate steps the state will remain the same, while we aggregate information about relevant past output symbols through sparse attention. To maintain the same state for intermediate steps, we introduce an extra switching layer (App. B.2.3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>Dataset used for pre training.</figDesc><table><row><cell>Model</cell><cell>Base Large</cell></row><row><cell>RoBERTa (sqln: 512)</cell><cell>1.846 1.496</cell></row><row><cell>Longformer (sqln: 4096)</cell><cell>1.705 1.358</cell></row><row><cell>BIGBIRD-ITC (sqln: 4096)</cell><cell>1.678 1.456</cell></row><row><cell cols="2">BIGBIRD-ETC (sqln: 4096) 1.611 1.274</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>MLM performance on held-out set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters of base BIGBIRD model used for Question Answering i.e. the numbers reported in Tab. 2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters of large BIGBIRD model for Question Answering submitted for test i.e. the numbers reported in Tab. 3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>× 10 −5 3 × 10 −5 5 × 10 −5 5 × 10 −6 2 × 10 −5</figDesc><table><row><cell>Parameter</cell><cell>IMDb</cell><cell>Arxiv</cell><cell cols="2">Patents Hyperpartisan</cell><cell>Yelp-5</cell></row><row><cell>Batch size</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>32</cell></row><row><cell>Learning rate 1 Num epochs</cell><cell>40</cell><cell>10</cell><cell>3</cell><cell>15</cell><cell>2</cell></row><row><cell>TPUv3 slice</cell><cell>4 × 4</cell><cell>4 × 4</cell><cell>4 × 4</cell><cell>4 × 2</cell><cell>4 × 8</cell></row><row><cell># of heads</cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>16</cell></row><row><cell># of hidden layers</cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>24</cell></row><row><cell>Hidden layer size</cell><cell></cell><cell>768</cell><cell></cell><cell></cell><cell>1024</cell></row><row><cell>Block length, b</cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell></row><row><cell>Global token location</cell><cell></cell><cell></cell><cell>ITC</cell><cell></cell><cell></cell></row><row><cell># of global token, g</cell><cell></cell><cell></cell><cell>2 × b</cell><cell></cell><cell></cell></row><row><cell>Window length, w</cell><cell></cell><cell></cell><cell>3 × b</cell><cell></cell><cell></cell></row><row><cell># of random token, r</cell><cell></cell><cell></cell><cell>3 × b</cell><cell></cell><cell></cell></row><row><cell>Max. sequence length</cell><cell></cell><cell></cell><cell>4096</cell><cell></cell><cell></cell></row><row><cell>Vocab size</cell><cell></cell><cell></cell><cell>50358</cell><cell></cell><cell></cell></row><row><cell>Activation layer</cell><cell></cell><cell></cell><cell>gelu</cell><cell></cell><cell></cell></row><row><cell>Dropout prob</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>Attention dropout prob</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell></cell><cell cols="2">cross-entropy</cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell>Adam</cell><cell></cell><cell></cell></row></table><note>We experiment on datasets of different lengths and contents, as listed in Tab. 15. In particular, we look at sentiment analysis (IMDb [64] and Yelp-5 [108]) task and topic</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameters for document classification.</figDesc><table><row><cell>Model</cell><cell cols="5">IMDb [64] Yelp-5 [108] Arxiv [35] Patents [53] Hyperpartisan [47]</cell></row><row><cell># Examples</cell><cell>25000</cell><cell>650000</cell><cell>30043</cell><cell>1890093</cell><cell>645</cell></row><row><cell># Classes</cell><cell>2</cell><cell>5</cell><cell>11</cell><cell>663</cell><cell>2</cell></row><row><cell>Excess fraction</cell><cell>0.14</cell><cell>0.04</cell><cell>1.00</cell><cell>0.90</cell><cell>0.53</cell></row><row><cell>SoTA</cell><cell>[88] 97.4</cell><cell cols="2">[3] 73.28 [69] 87.96</cell><cell>[69] 69.01</cell><cell>[40] 90.6</cell></row><row><cell>RoBERTa</cell><cell>95.0 ± 0.2</cell><cell>71.75</cell><cell>87.42</cell><cell>67.07</cell><cell>87.8 ± 0.8</cell></row><row><cell>BIGBIRD</cell><cell>95.2 ± 0.2</cell><cell>72.16</cell><cell>92.31</cell><cell>69.30</cell><cell>92.2 ± 1.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 15 :</head><label>15</label><figDesc>Classification results. We report the F1 micro-averaged score for all datasets. Experiments on smaller IMDb and Hyperpartisan datasets are repeated 5 times and the average performance is presented along with standard deviation. assignment (Arxiv<ref type="bibr" target="#b34">[35]</ref>, Patents<ref type="bibr" target="#b52">[53]</ref>, and Hyperpartisan<ref type="bibr" target="#b46">[47]</ref>) task. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token from the BIGBIRD encoder consuming 4096 tokens. We report the results of document classification experiments in Tab. 15. We compare against state-of-the-art (SoTA) methods for each dataset and plain RoBERTa model with 512 tokens truncation. In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there</figDesc><table><row><cell>System</cell><cell cols="8">MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE</cell></row><row><cell></cell><cell>392k</cell><cell>363k</cell><cell>108k</cell><cell>67k</cell><cell>8.5k</cell><cell>5.7k</cell><cell>3.5k</cell><cell>2.5k</cell></row><row><cell>BERT</cell><cell>84.6/83.4</cell><cell>71.2</cell><cell>90.5</cell><cell>93.5</cell><cell>52.1</cell><cell>85.8</cell><cell>88.9</cell><cell>66.4</cell></row><row><cell>XLNet</cell><cell>86.8/-</cell><cell>91.4</cell><cell>91.7</cell><cell>94.7</cell><cell>60.2</cell><cell>89.5</cell><cell>88.2</cell><cell>74.0</cell></row><row><cell>RoBERTa</cell><cell>87.6/-</cell><cell>91.9</cell><cell>92.8</cell><cell>94.8</cell><cell>63.6</cell><cell>91.2</cell><cell>90.2</cell><cell>78.7</cell></row><row><cell>BIGBIRD</cell><cell>87.5/87.3</cell><cell>88.6</cell><cell>92.2</cell><cell>94.6</cell><cell>58.5</cell><cell>87.8</cell><cell>91.5</cell><cell>75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 16 :</head><label>16</label><figDesc>GLUE Dev results on base sized models. Number of training examples is reported below each task. MCC score is reported for CoLA, F1 score is reported for MRPC, Spearman correlation is reported for STS-B, and accuracy scores are reported for the other tasks.is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20. In all experiments, we use a learning rate warmup over the first 10,000 steps, and square root decay of the learning rate.</figDesc><table><row><cell></cell><cell></cell><cell>Instances</cell><cell></cell><cell cols="2">Input Length</cell><cell>Output Length</cell></row><row><cell>Dataset</cell><cell>Training</cell><cell>Dev</cell><cell>Test</cell><cell cols="2">Median 90%-ile</cell><cell>Median 90%-ile</cell></row><row><cell>BBC XSum [67]</cell><cell cols="3">204044 11332 11334</cell><cell>359</cell><cell>920</cell><cell>25</cell><cell>32</cell></row><row><cell>CNN/DailyMail [36]</cell><cell cols="3">287113 13368 11490</cell><cell>777</cell><cell>1439</cell><cell>59</cell><cell>93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 19 :</head><label>19</label><figDesc>Shorter summarization dataset statistics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>BBC XSum</cell><cell></cell><cell cols="2">CNN/DailyMail</cell></row><row><cell cols="2">Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>R1</cell><cell>R2</cell><cell>R-L</cell></row><row><cell></cell><cell>Lead</cell><cell>16.30</cell><cell cols="2">1.61 11.95</cell><cell cols="3">39.60 17.70 36.20</cell></row><row><cell></cell><cell>PtGen [77]</cell><cell>29.70</cell><cell cols="2">9.21 23.24</cell><cell cols="3">39.53 17.28 36.38</cell></row><row><cell>Prior Art</cell><cell>ConvS2S [28] MMN [48] Bottom-Up [29] TransLM [45] UniLM [23]</cell><cell cols="3">31.89 11.54 25.75 32.00 12.10 26.00 − − − − − − − − −</cell><cell cols="3">− − 41.22 18.68 38.34 − − − − 39.65 17.74 36.85 43.47 20.30 40.63</cell></row><row><cell></cell><cell>Extr-Abst-BERT [62]</cell><cell cols="3">38.81 16.50 31.27</cell><cell cols="3">42.13 19.60 39.18</cell></row><row><cell></cell><cell>BART [56]</cell><cell cols="3">45.14 22.27 37.25</cell><cell cols="3">44.16 21.28 40.90</cell></row><row><cell></cell><cell>Transformer [91]</cell><cell>29.61</cell><cell cols="2">9.47 23.17</cell><cell cols="3">34.89 13.13 32.12</cell></row><row><cell>Base</cell><cell>+ RoBERTa [76] + Pegasus [107]</cell><cell cols="3">39.92 17.33 32.63 39.79 16.58 31.70</cell><cell cols="3">39.44 18.69 36.80 41.79 18.81 38.93</cell></row><row><cell></cell><cell>BIGBIRD-RoBERTa</cell><cell cols="3">39.52 17.22 32.30</cell><cell cols="3">39.25 18.46 36.61</cell></row><row><cell>Large</cell><cell>Pegasus (Reported) [107] Pegasus (Re-eval) BIGBIRD-Pegasus</cell><cell cols="3">47.60 24.83 39.64 47.37 24.31 39.23 47.12 24.05 38.80</cell><cell cols="3">44.16 21.56 41.30 44.15 21.56 41.05 43.84 21.11 40.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 20 :</head><label>20</label><figDesc>Summarization ROUGE score for shorter documents.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. HotpotQA-distractor<ref type="bibr" target="#b100">[100]</ref>: Similar to natural questions, it requires finding the answer (Ans) as well as the supporting facts (Sup) over different documents needed for multi-hop reasoning from the given evidences.3. TriviaQA-wiki<ref type="bibr" target="#b40">[41]</ref>: We need to provide an answer for the given question using provided Wikipedia evidence, however, the answer might not be present in the given evidence. On a 1 code available at http://goo.gle/bigbird-transformer 2 https://github.com/pytorch/fairseq/tree/master/examples/roberta</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/ 4 https://epd.epfl.ch/human/human_database.php?db=human</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.encodeproject.org/ 6 http://www.roadmapepigenomics.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.<ref type="bibr" target="#b38">39</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://deepsea.princeton.edu/media/code/deepsea_train_bundle.v0.9.tar. gz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We prefix and append each example with [CLS] and [SEP] token respectively. The output corresponding to the [CLS] token from BIGBIRD transformer encoder is fed to a simple binary classification layer. We fine-tune the pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. <ref type="bibr" target="#b20">21</ref>. We note that high performance is not surprising due to the overlap in the nature of negative example generation and MLM pretraining.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances Instance Length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Training Dev Median Max</p><p>HotpotQA-distractor <ref type="bibr" target="#b100">[100]</ref> 90447 7405 1227 3560 Natural Questions <ref type="bibr" target="#b51">[52]</ref> 307373 7830 3258 77962 TriviaQA <ref type="bibr" target="#b40">[41]</ref> 61888 7993 4900 32755 WikiHop <ref type="bibr" target="#b95">[95]</ref> 43738 5129 1541 20337 </p><p>Max. encoder sequence length   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Chromatin-Profile Prediction</head><p>The first step of sequence-based algorithmic framework for predicting non-coding effects is to build a model to predict, large scale chromatic profile <ref type="bibr" target="#b109">[109]</ref>. In this paper, we use the dataset provided in   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Consequences of faster alignment of sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Weimann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="39" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tight hardness results for lcs and other sequence similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 56th Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="59" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical attentional hybrid neural networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macêdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zanchettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="396" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Etc: Encoding long and structured data in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08483</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08634</idno>
		<title level="m">A bert baseline for the natural questions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Extremal eigenvalues of critical erd\h {o} sr\&apos;enyi graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducatez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Edit distance cannot be computed in strongly subquadratic time (unless seth is false)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-seventh annual ACM symposium on Theory of computing</title>
		<meeting>the forty-seventh annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Largest eigenvalues of sparse inhomogeneous erdős-rényi graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benaych-Georges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bordenave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1653" to="1676" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral radii of sparse random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benaych-Georges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bordenave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;Institut Henri Poincaré, Probabilités et Statistiques</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2141" to="2161" />
		</imprint>
	</monogr>
	<note>Institut Henri Poincaré</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Promoterpredict: sequence-based modelling of escherichia coli σ70 promoter strength yields logarithmic dependence between promoter strength and sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bharanikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A R</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palaniappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">5862</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-range correlation properties of coding and noncoding dna sequences: Genbank analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buldyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Havlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantegna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5084</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep learning approach to pattern recognition for short dna sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Busia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fannjiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">353474</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-hop question answering via reasoning chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02610</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowledge of bert for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03829</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The average distances in random graphs with given expected degrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="15879" to="15882" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05685</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Epd and epdnew, high-quality promoter resources in the next-generation sequencing era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dreos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ambrosini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Cavin</forename><surname>Périer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="157" to="164" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03631</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histone marks in the &apos;driver&apos;s seat&apos;: functional roles in steering the transcription cycle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in biochemical sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="977" to="989" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10792</idno>
		<title level="m">Bottom-up abstractive summarization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhanced regulatory sequence prediction using gapped k-mer features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohammad-Noori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Beer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A divide-and-conquer approach to the summarization of academic articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06190</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reflectionnet</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/people/migon/" />
		<imprint>
			<date type="published" when="2020-06-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Gpu kernels for block-sparse weights</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrieval-augmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long document classification from local word glimpses via recurrent attention learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="40707" to="40718" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Expander graphs and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="561" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Team bertha von suttner at semeval-2019 task 4: Hyperpartisan news detection using elmo sentence representation convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distribution of shortest path lengths in subcritical erdős-rényi networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Katzav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Biham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The human genome browser at ucsc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Sugnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Roskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Pringle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Zahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="996" to="1006" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sample efficient text summarization using a single pre-trained transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08836</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Role of non-coding sequence variants in cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Demichelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semeval-2019 task 4: Hyperpartisan news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Abstractive summarization of reddit posts with multi-level memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00783</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02245</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Patent classification by fine-tuning bert language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Patent Information</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101965</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00300</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Methylnet: an automated and modular deep learning approach for dna methylation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Titus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocktäschel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Segmenting dna sequence into words based on statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Precedings</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Identifying sigma70 promoters with novel pseudo nucleotide composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What makes a good answer? the role of context in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2003)</title>
		<meeting>the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2003)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14560</idno>
		<title level="m">Rikinet: Reading wikipedia pages for natural question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08345</idno>
		<title level="m">Text summarization with pretrained encoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J O</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><forename type="middle">V</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
		<title level="m">Camembert: a tasty french language model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Leveraging bert for extractive text summarization on lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04165</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08745</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The impact of frequency on summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno>MSR-TR-2005</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">101</biblScope>
			<pubPlace>Microsoft Research, Redmond, Washington</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Adapting pretrained language models for long document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deepromoter: Robust promoter predictor using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oubounyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Louadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tayara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marinković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barceló</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03429</idno>
		<title level="m">On the turing completeness of modern neural network architectures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Blockwise self-attention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03741</idno>
		<title level="m">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Spectral sparsification of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="981" to="1025" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03186</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09588</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Lecture Notes for Boston University MA 882 Spring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sussman</surname></persName>
		</author>
		<ptr target="http://math.bu.edu/people/sussman/MA882_2017/2017-01-26-Lecture-2.html" />
		<imprint>
			<date type="published" when="2017-06-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Viraminer: Deep learning on raw dna sequences for identifying viral genomes in human samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bzhalava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Multi-hop reading comprehension across documents with path-based graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06478</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Sentiment classification using document embeddings trained with cosine similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thongtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phienthrakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Umarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Solovyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multitask benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08167</idno>
		<title level="m">Multi-passage bert: A globally normalized bert model for open-domain question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A new algorithm for optimal 2-constraint satisfaction and its implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08052</idno>
		<title level="m">Challenges in data-to-document generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">ipsw (2l)-pseknc: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo k-tuple nucleotide composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1785" to="1793" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Exploiting sequence-based features for predicting enhancer-promoter interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="252" to="260" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Balanced sparsity for efficient dnn inference on gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5676" to="5683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04070</idno>
		<title level="m">Bp-transformer: Modelling long-range context via binary partitioning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10077</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">o(n) connections are expressive enough: Universal approximability of sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Ncnet: Deep learning network models for predicting function of non-coding dna</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08777</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Predicting effects of noncoding variants with deep learningbased sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="931" to="934" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
