<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
							<email>inoue@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
							<email>furuta@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
							<email>yamasaki@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
							<email>aizawa@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets 1 containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a task to localize instances of particular object classes in an image. It is a fundamental task and has advanced rapidly due to the development of convolutional neural networks (CNNs). Best-performing detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> are fully supervised detectors (FSDs). They are highly data-hungry and typically learned from many images with instance-level annotations. An instance-level annotation is composed of a label (i.e., the object class of an instance) and a bounding box (i.e., the location of the instance).</p><p>While object detection in a natural image domain has achieved outstanding performance, less attention has been paid to the detection in other domains such as watercolor. This is because it is often difficult and unrealistic to construct <ref type="bibr" target="#b0">1</ref> Datasets and codes are available at https://naoto0804. github.io/cross_domain_detection a large dataset with instance-level annotations in many image domains. There are many obstacles such as lack of image sources, copyright issues, and the cost of annotation. We tackle a novel task, cross-domain weakly supervised object detection. The task is described as follows: (i) instance-level annotations are available in a source domain; (ii) only image-level annotations are available in a target domain; (iii) the classes to be detected in the target domain are all or a subset of those in the source domain. The objective of the task is to detect objects as accurately as possible in the target domain under these conditions by using sufficient instance-level annotations in the source domain and a small number of image-level annotations in the target domain. This assumption is reasonable as it is easier to collect image-level annotations than instance-level annotations from existing datasets or an image search engine.</p><p>We will describe a framework to solve the proposed task. Starting from an FSD trained on images with instance-level annotations in the source domain, we fine-tune the FSD in the target domain, as this is the most straightforward and promising approach. However, there are no instance-level annotations available in the target domain. Instead, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we present two methods to generate images with instance-level annotations artificially and automatically, and fine-tune the FSD on them. The first method, domain trans-fer (DT), is used to generate images that look like those in the target domain from images in the source domain having instance-level annotations. This generation is achieved by image-to-image translation methods from unpaired examples such as CycleGAN <ref type="bibr" target="#b39">[40]</ref>. The second method, pseudolabeling (PL), is used to generate pseudo instance-level annotations. Given images with image-level annotations in the target domain and the FSD which is fine-tuned on the artificially generated samples by DT, these annotations and predictions of the FSD are combined. We achieve a two-step progressive domain adaptation by sequentially fine-tuning the FSD on the artificially generated samples. Our framework is general to cross-domain weakly supervised object detection across any image domain and is relatively scalable to many classes and instances.</p><p>Since there is no dataset for the target domain that is suitable to evaluate the proposed task, we construct new datasets with instance-level annotations, which we call Cli-part1k, Watercolor2k, and Comic2k. Each dataset comprises 1,000, 2,000, and 2,000 images of clipart, watercolor, and comic, respectively. The validity of our methods is demonstrated using these datasets. We show that the proposed twostep adaptation achieves an improvement of approximately 5 to 20 percentage points as compared to the best-performing baselines' mAP across all datasets. We believe that this paper itself can be a strong baseline for cross-domain weakly supervised object detection.</p><p>Our main contributions are as follows:</p><p>• We propose a framework for a novel task, cross-domain weakly supervised object detection. We achieve a two-step progressive domain adaptation by sequentially fine-tuning the FSD on the artificially generated samples by the proposed domain transfer and pseudo-labeling.</p><p>• We construct novel, fully instance-level annotated datasets with multiple instances of various object classes across three domains that are far from natural images.</p><p>• Our experimental results show that our framework outperforms the best-performing baselines by approximately 5 to 20 percentage points in terms of mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fully Supervised Detection</head><p>Standard methods in fully supervised object detection, such as R-CNN <ref type="bibr" target="#b9">[10]</ref>, Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, and Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, are based on a two-stage approach: generating region proposals and then, classifying them. Recently, singlestage object detectors such as SSD <ref type="bibr" target="#b21">[22]</ref>, YOLOv2 <ref type="bibr" target="#b26">[27]</ref>, and RetinaNet <ref type="bibr" target="#b19">[20]</ref> have also emerged. All of these detectors require large datasets with instance-level annotations such as PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>, Microsoft Common Objects in Context (MSCOCO) <ref type="bibr" target="#b20">[21]</ref>, and OpenImages <ref type="bibr" target="#b16">[17]</ref>.</p><p>Dataset construction for a new image domain becomes harder as the number of images and classes increases. <ref type="bibr" target="#b31">[32]</ref> reported that it took 35 seconds for a worker to annotate one bounding box. Recently, <ref type="bibr" target="#b25">[26]</ref> reduced it to 7 seconds through extreme clicking, while it still takes much time and effort to obtain large-scale datasets. On the contrary, our framework does not require instance-level annotations for the new target domain at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weakly Supervised Detection</head><p>One possible approach addressing the lack of large-scale instance-level annotations for object detection is to use a weakly supervised detector (WSD). In weakly supervised object detection, only pairs of an image and an image-level annotation (i.e., labels of objects in each image) are provided for training. Many existing methods are built upon region-of-interest (RoI) extraction methods such as selective search <ref type="bibr" target="#b35">[36]</ref>. Feature extraction for each region, region selection, and classification of the selected region are performed through multiple instance learning (MIL) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> or two-stream CNN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref>. However, WSDs are poor at accurately localizing the object boundary. Our framework uses image-level annotations in the target domain by pseudo-labeling the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross-domain Object Detection</head><p>Using an object detector that is neither trained nor finetuned for the target domain causes a significant drop in performance as shown in <ref type="bibr" target="#b37">[38]</ref>. Therefore, adapting the detector with the help of some information on the target domain is essential. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref> are the some of the best works closely related to this paper. Our methods and <ref type="bibr" target="#b12">[13]</ref> are similar as they propose to learn from a combination of instance-and imagelevel annotations. However, we address the adaptation of the detector from one domain to another, whereas <ref type="bibr" target="#b12">[13]</ref> addresses the classifier-to-detector adaptation for weakly labeled object classes within one domain. This paper and <ref type="bibr" target="#b4">[5]</ref> are similar as they tackle the adaptation of the detector from one domain to another. However, only image-level annotations are available in the source domain in <ref type="bibr" target="#b4">[5]</ref>. This is the first work to propose the cross-domain weakly supervised object detection.</p><p>For evaluating the cross-domain object detection method, the existing datasets for detecting common objects in various domains seem to have limitations. People-Art <ref type="bibr" target="#b36">[37]</ref> is used only for single-class detection in an artwork. Photo-Art <ref type="bibr" target="#b38">[39]</ref> assumes only one instance per image, which is unrealistic. Besides, we introduce the fully instance-level annotated datasets for object detection which comprises multiple common classes to be detected various visual domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation (UDA) in an image is a task used for learning domain invariant models, where (a) Clipart1k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Watercolor2k</head><p>(c) Comic2k <ref type="figure">Figure 2</ref>: Examples of datasets that we collected across three domains; The images usually contain not only the target objects but also various other objects and complex backgrounds.</p><p>pairs of an image and annotation are available in the source domain while only images are available in the target domain. Previous works for UDA in image classification is mostly distribution-matching-based, in which features extracted from two domains are made to closely resemble each other using the maximum mean discrepancy (MMD) <ref type="bibr" target="#b11">[12]</ref> or a domain classifier network <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Although current distribution-matching-based methods are applicable, it is primarily challenging to fully align the distribution for tasks that require structured outputs, such as object detection. This is because it is essential to keep the spatial information in the feature map. Our framework employs image-to-image translation and fine-tuning to avoid this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Our objective is to detect objects in a target domain by adapting an FSD that is originally trained on a source domain. The classes to be detected in the target domain are all or a subset of the classes defined in the dataset which is in the source domain. In this paper, PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>, which contains twenty classes, was used for the source domain, natural image. As no suitable dataset for the target domain of our task was available, we constructed three original datasets, Clipart1k, Watercolor2k, and Comic2k using Amazon Mechanical Turk.</p><p>Examples of the images are shown in <ref type="figure">Fig. 2</ref>. These images usually contain multiple objects per image, and some instances are small or partially occluded by the other objects. The statistics of the three datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. We collected a total of 5,000 images and 12,869 instance-level annotations. We believe these datasets are good benchmarks not only for domain adaptation but also for fully and weakly supervised, semi-supervised detection tasks. For the more detailed statistics, please refer to the supplementary material. In the following subsection, we will briefly describe each dataset, and the data collection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Clipart1k</head><p>In Clipart1k, the target domain classes to be detected were the same as those in the source domain. All the images for a clipart domain were collected from one dataset (i.e., CMPlaces <ref type="bibr" target="#b3">[4]</ref>) and two image search engines (i.e., Openclipart 2 and Pixabay 3 ). When we collected the images from the search engines, we used queries of the 205 scene classes (e.g., pasture) used in CMPlaces to collect various objects and scenes with complex backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comic2k and Watercolor2k</head><p>In Comic2k and Watercolor2k, the classes to be detected in the target domain were the subset of those in the source domain. The images were collected from BAM! <ref type="bibr" target="#b37">[38]</ref>.  In BAM! , millions of images with slightly noisy (80%−90% in precision) image-level attributes regarding object classes, domain, and emotion are provided in a human-in-the-loop fashion. Specifically, the target classes are bicycle, bird, cat, car, dog, and person, which are representative of the intersection of the classes in VOC and those in BAM!. We chose the watercolor and comic domains as the other domains in BAM! are not suitable for object detection. For example, oil paint images are unsuitable as they usually depict a single person in the center of the image, making object detection a trivial task.</p><p>As collecting instance-level annotations for all images in BAM! is difficult, we annotated the images in the following way: (i) images that contained at least one of the six target classes were extracted. Note that we relied on the labels provided by BAM! and did not conduct any other filtering process. We obtained 17,814 and 52,790 images for watercolor and comic domains, respectively. (ii) as many as 2,000 images are randomly sampled and assigned instance-level annotations for each domain. The remaining 15,814 and 50,790 images, which we called extra dataset, are still useful as they possess many image-level annotations. Although they are noisy and incomplete, they provided room for further improvement with respect to detector performance as shown in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>We propose a framework to adapt an FSD that is pretrained on a source domain. The adaptation is achieved through fine-tuning the FSD on artificially generated samples with instance-level annotations in a target domain. We propose two methods to generate the samples as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>: (i) domain transfer (DT), transferring images with instance-level annotations from the source domain to the target domain, and (ii) pseudo-labeling (PL), pseudo-labeling the images with image-level annotations in the target domain. The samples generated by these two methods display different properties. Although the samples generated by (i) are not high-quality images with respect to their similarities to target-domain images, bounding boxes are correctly annotated. On the contrary, although the samples generated by (ii) do not have accurate bounding boxes, image quality is guaranteed as they are completely target-domain images.</p><p>We progressively fine-tune an FSD using these examples as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. First, we pre-train it while using instancelevel annotations in the source domain. Second, we fine-tune it while using the images obtained by DT. Lastly, we finetune it while using the images obtained by PL. We would like to emphasize that the sequential execution of the two fine-tuning steps is critical as the performance of PL highly depends on the used FSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Domain Transfer (DT)</head><p>The differences between the source and target domains tackled in this paper mainly lie in their low-level features, such as color and texture. We generate images that look like those in the target domain to capture such differences and then, make the FSD robust to such differences by fine-tuning the FSD on the generated images.</p><p>To achieve this goal, an unpaired image-to-image translation method called CycleGAN <ref type="bibr" target="#b39">[40]</ref> is employed. With CycleGAN, the goal is to learn the mapping functions between two image domains X and Y with unpaired examples. In practice, a mapping G : X → Y and an inverse mapping F : Y → X are jointly learned using CNN. We train CycleGAN to learn the mapping functions between the source domain, X s , and the target domain, X t . Once the mapping functions are trained, we convert images in the source domain that are used in the pre-training and obtain domain-transferred images that accompany instance-level annotations. Using these images, the FSD is fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pseudo-Labeling (PL)</head><p>In the target domain, if we use an FSD that is trained only on the source domain for object detection, then the FSD mainly fails because of confusion with other classes and backgrounds rather than inaccurate localization. We will later show this trend in <ref type="figure">Fig. 4</ref>. Fine-tuning FSD on images obtained by PL dramatically reduces such confusion. PL is simple and applicable in any FSD as it does not access intermediate layers of an FSD.</p><p>Formally, the objective of PL is to obtain a pseudo instance-level annotation G for each image x from the target domain and add (b, c) to G. We fine-tune the FSD using pairs of x and G. Note that no layers of the FSD are replaced to preserve the original network's detection ability. The FSD that is trained on images obtained by DT was subsequently fine-tuned on these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In Sec. 5.1, we explain the details of the implementations, the compared methods, and the evaluation metrics. In Sec. 5.2, we test our methods using Clipart1k and conduct error analysis and ablation studies on the FSDs. In Sec. 5.3, we confirm that our framework is generalized for a variety of domains using Watercolor2k and Comic2k. In Sec. 5.4, we show actual detection results and the generated domain-transferred images for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation and Evaluation Metrics</head><p>Our methods were implemented using Chainer <ref type="bibr" target="#b33">[34]</ref>. We evaluated our methods using average precision (AP) and its mean, i.e., mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Arrangement for Training and Evaluation</head><p>VOC2007-trainval and VOC2012-trainval <ref type="bibr" target="#b5">[6]</ref> were used as images in the source domain (i.e., natural image) in all the experiments. For the target-domain images, the ones with instance-level annotations were used when discussing the performance gap between our methods and the ideal case quantitatively. The target-domain images were split into training set and test set by a ratio of 1:1. For the training set, the bounding box information was ignored to meet the proposed situation. For the test set, the labels and the bounding boxes of the annotations were used to evaluate the performance of the compared methods and our methods.</p><p>Comparison We compared our methods against the following methods:</p><p>• Baseline: SSD300 <ref type="bibr" target="#b21">[22]</ref> was used as our baseline FSD. We used the implementation provided by ChainerCV <ref type="bibr" target="#b24">[25]</ref>, and we obtained an SSD300, which was pre-trained on VOC2007-trainval and VOC2012-trainval, and skipped the pre-training. We followed the original paper on hyperparameters of SSD300 unless specified. The input images were resized to 300 × 300 in SSD300. The IoU threshold for NMS (0.45) and the confidence threshold for discarding low confidence detections (0.01) were employed.</p><p>• Ideal case: In this case, we had access to the instancelevel annotations for the training set of the target domain dataset. We simply fine-tuned the baseline FSD using these annotations. This experiment was to confirm the weak upperbound performance of our methods.</p><p>• Weakly supervised detection (WSD): ContextLocNet (CLNet) <ref type="bibr" target="#b14">[15]</ref> and WSDDN <ref type="bibr" target="#b1">[2]</ref> were chosen as the compared WSDs. Each WSD was trained on the images with image-level annotations from the training set of the dataset in the target domain.</p><p>• Unsupervised domain adaptation (UDA): We tested one of the state-of-the-art UDA methods, ADDA <ref type="bibr" target="#b34">[35]</ref>. We aligned the distributions at the relu4_3 layer in SSD300. We trained the model with a batch size of 32 and a learning rate of 1.0 × 10 −6 for 1,000 iterations using Adam <ref type="bibr" target="#b15">[16]</ref>.</p><p>• Ensemble: In image classification, unweighted averaging, which uses the unweighted average of the output score / probability of all the base models as the output, is a reasonable approach to boost performance. We accumulated all the detections produced by multiple detectors and applied nonmaximum suppression (NMS) <ref type="bibr" target="#b3">4</ref> to them. The parameters of NMS were the same as those used in SSD300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Training</head><p>We trained CycleGAN with a learning rate of 1.0 × 10 −5 for the first ten epochs and a linear decaying rate to zero over the next ten epochs. We followed the original paper on the other hyper-parameters of CycleGAN. When fine-tuning SSD300, we employed a learning rate of 1.0 × 10 −5 , which is the same as the final learning rate for the original SSD300 training. Fine-tuning, using the images obtained by DT and PL, was conducted for one epoch and 10,000 iterations, respectively. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison of AP for each class and mAP among our methods against the baseline FSD and the comparable methods. We observe that SSD300 performs better than the WSDs in terms of mAP, although SSD300 is not trained to adapt to the target domain. On the contrary, WSDs perform poorer due to insufficient data and their poor localization ability. The ensembling of WSDs with SSD300 have almost no effect as shown in the case of Ensemble. Conventional distribution-matching-based methods do not work well as shown in the case of ADDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Results on Clipart1k</head><p>The results of our methods based on SSD300 are shown in the bottom half of <ref type="table" target="#tab_1">Table 2</ref>. To quantify the relative contribution of each step, the performances of our methods are examined using different configurations.</p><p>• DT+PL: the proposed two-step fine-tuning.</p><p>• DT: only fine-tuning using images obtained by DT.</p><p>• PL: only fine-tuning using images obtained by PL. Note that the baseline FSD is used for PL.</p><p>PL provides an improvement of 9.6 percentage points improvement from the baseline SSD300 in terms of mAP. Further, DT+PL achieves 46.0 % in terms of mAP. This result ensures that both of our methods work and are complementary. The mAP of the combination of DT+PL is 19.2 percentage points higher than that of the baseline SSD300 and is approximately 18 percentage points greater than the ensemble of the detectors. We emphasize that this performance is only 9.4 percentage points lower than Ideal case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We considered a setting where we can obtain only images with no annotation in the target domain. DT is applicable without any modification. DT provides an improvement of 11.2 percentage points improvement from the baseline SSD300 in terms of mAP in <ref type="table" target="#tab_1">Table 2.</ref> PL is not directly applicable as we do not have access to the image-label annotations. To address this problem, only one detection d best , which has the highest probability p among all detections, can be pseudo-labeled. The results are shown as PL w/o label and DT+PL w/o label in <ref type="table" target="#tab_1">Table 2</ref>. Fine-tuning the FSD on the images labeled by this method harms the performance as the result of pseudolabeling contains a lot of inaccuracy. Therefore, image-level annotations in the target domain are essential for PL, which greatly improves the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generality across Detectors</head><p>We investigated our framework on other FSDs such as Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> and YOLOv2 <ref type="bibr" target="#b26">[27]</ref>. Please refer to the supplementary material about details of the hyper-paramters. The result further emphasizes the generality of our framework across all baseline FSDs, as shown in <ref type="table" target="#tab_2">Table 3</ref>. We additionally found that the ensembling of SSD300 and Faster R-CNN yields 30.2 % in terms of mAP and that of all the three FSDs yields 31.0 % in terms of mAP, which is not so remarkable compared to the improvement by DT+PL. The performance gain is significant in SSD300 compared to YOLOv2 and Faster R-CNN. This result suggests the importance of data augmentation (e.g., the zoom-in and zoom-out features implemented in SSD300) during the process of training FSDs with pseudolabeled annotations, which are often noisy and incomplete.</p><p>Performance Analysis Focusing on Errors The tool from <ref type="bibr" target="#b13">[14]</ref> was used to understand the type of the detection error that is reduced by our methods. The classes within the brackets were regarded as the same category: {all vehi-cles}, {all animals including person}, {chair, dining table, sofa}(furniture), {aeroplane, bird}(air objects). Considering the class, the category, and the IoU between the predicted bounding box and the ground truth bounding box, the detections were classified into five groups as listed below:</p><p>• Correct (Cor): correct class and IoU &gt; .5</p><p>• Localization (Loc): correct class, misaligned bounding  <ref type="figure">Fig. 4</ref> shows the example of the error analysis in the Cli-part1ktest set. Comparing the baseline and DT, we observe that fine-tuning the FSD on images obtained by DT improves the detection performance, especially in less-confident detections. Comparing DT and DT+PL, we observe that the confusion, which emerged with the other classes (Sim and Oth), especially in more confident detections, is greatly reduced by PL, which uses the image-level annotations in the target domain to remove such confusions with the FSD.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative Results on Watercolor2k and Comic2k</head><p>The comparison among our methods against the baseline FSD and the comparable methods is shown in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref>. In Watercolor2k, the learning rate was set to 1.0 × 10 −6 as the fine-tuning overfitted in 1.0 × 10 −5 even in Ideal case. Both our methods work in the two domains.</p><p>+extra in both tables indicates the use of extra   BAM! images with raw noisy image-level labels of the target classes as described in Sec. 3.2. These images were pseudo-labeled and used for fine-tuning the FSD. With a substantial number of images, the training of +extra methods underwent 30000 iterations. The methods using extra noisy labels in BAM! significantly improved the detection performance and sometimes proved to be better than Ideal case trained on 1,000 clean instance-level annotations. Without any manual annotation, our framework can use large-scale images with noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head><p>Fig <ref type="figure" target="#fig_7">. 6</ref> shows the example images generated by DT. There was no mode collapse in the training of CycleGAN. Visibly, the perfect mapping is not accomplished in this experiment as the representation gap between a natural image domain and the other domains used in this paper is too wide as compared with the gap between synthetic and real images tackled in recent studies, such as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>. CycleGAN seems to transfer color and texture while keeping most of the edges and semantics of the input image. The result of fine-tuning the FSD on these domain-transferred images in <ref type="table" target="#tab_1">Table 2</ref>, <ref type="table" target="#tab_3">Table 4</ref>, and <ref type="table" target="#tab_4">Table 5</ref> confirms the validity of our domain transfer method. Moreover, our methods are valid for various depiction styles as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. For more results, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In PL, only the top-1 bounding box for each class is employed. The other instances, if any, can be considered as negative samples. This issue is our future work. Moreover, if we could extract features with the same size corresponding to each detection, using the standard MIL paradigm in WSD such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, we would improve the localization accuracy in pseudo-labeling, which is also our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed the novel task, cross-domain weakly supervised object detection, and the novel framework performing the two-step progressive domain adaptation to address this task. To evaluate our methods, we constructed original datasets comprising images with instance-level annotations in three visual domains. The results suggested that our methods were better than the other existing comparable methods and provided a simple but solid baseline.    is also seen in PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>. In <ref type="table" target="#tab_5">Table 7</ref>, the number of instances in Watercolor2k and Comic2k is shown. In all datasets, the person class is dominant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of Detections</head><p>We discuss the detection results produced by our methods. We will show the typical detection errors of our methods in <ref type="figure" target="#fig_11">Fig. 8</ref>. The errors are often caused due to ignoring small objects ( <ref type="figure" target="#fig_11">Fig. 8a)</ref>, merging highly-overlapped objects which belong to the same object class <ref type="figure" target="#fig_11">(Fig. 8b)</ref>, localizing only the most discriminative part of an object <ref type="figure" target="#fig_11">(Fig. 8c</ref>), or being unable to recognize highly-deformed objects <ref type="figure" target="#fig_11">(Fig. 8d</ref>). The detections results obtained by our methods are shown in <ref type="figure" target="#fig_0">Fig. 9, Fig. 10</ref>, and <ref type="figure" target="#fig_0">Fig. 11</ref>. We confirm that our method is generally applicable and valid for various depiction styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Domain Transfer</head><p>All the images were loaded and resized to 286 × 286. In the fine-tuning phase, the images were randomly cropped to the size 256 × 256. In the test phase, the images were loaded, transferred, and converted back to the original size. We used all 16,551 images in VOC2007-trainval and VOC2012trainval and obtained the domain-transferred images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Configurations for training FSDs</head><p>For YOLOv2 <ref type="bibr" target="#b26">[27]</ref>, we used the original implementation and employed a learning rate of 1.0×10 −5 . The input images were resized to 416 × 416. With the IoU threshold (0.45) and the confidence threshold (0.001) employed, YOLOv2 was fine-tuned for five epochs and one hundred epochs for the DT and other experiments, respectively.</p><p>For Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, we used the reimplementation provided in ChainerCV <ref type="bibr" target="#b24">[25]</ref>. We employed a learning rate of 1.0 × 10 −5 . The length of the shorter edge of the input image was scaled to 600. After the scaling, if the length of the longer edge was longer than 1,000, the image was scaled so that the length of the longer edge came down to 1,000. With the IoU threshold (0.3) and the confidence threshold (0.05) employed, Faster R-CNN was fine-tuned for one epoch and one hundred epochs for the DT and other experiments, respectively. <ref type="figure">Figure 9</ref>: Example outputs for our DT+PA using SSD300 as the baseline FSD in the test set of Clipart1k. We only showed windows whose scores are above 0.25 so as to maintain visibility. <ref type="figure" target="#fig_0">Figure 10</ref>: Example outputs for our DT+PA using SSD300 as the baseline FSD in the test set of Watercolor2k. We only showed windows whose scores are above 0.25 so as to maintain visibility. <ref type="figure" target="#fig_0">Figure 11</ref>: Example outputs for our DT+PA using SSD300 as the baseline FSD in the test set of Comic2k. We only showed windows whose scores are above 0.25 so as to maintain visibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: the situation of the cross-domain weakly supervised object detection; Right: Our methods to generate instance-level annotated samples in the target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The workflow of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>X t . Let x ∈ R H×W ×3 denote an RGB image, where H and W are the image's height and width, respectively. C indicates a set of object classes. z indicates an image-level annotation: the set of classes in x. Further, G comprises g = (b, c), where b ∈ R 4 is a bounding box, and c ∈ C. First, we obtain FSD outputs D. D comprises each detection d = (p, b, c), where c ∈ C and p ∈ R indicates the probability of b belonging to c. Second, for each class c ∈ z, we take the top-1 confident detection d = (p, b, c) ∈ D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 : 1 • 1 •</head><label>411</label><figDesc>Visualization of performance for various methods on animals and vehicles in the test set of Clipart1k using SSD300 as the baseline FSD. The solid red line and dashed red line reflect the change of recall with strong criteria (0.5 jaccard overlap) and weak criteria (0.1 jaccard overlap) as the number of detections increases, respectively. box (.1 &lt; IoU &lt; .5) • Similar (Sim): wrong class, correct category, IoU &gt; .Other (Oth): wrong class, wrong category, IoU &gt; .Background (BG): IoU &lt; .1 for any object</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Example outputs for our DT+PA in the test set of each dataset. We only show windows whose scores are over 0.25 to maintain visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Example images generated by DT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The number of classes per image in our datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The number of instances per image in our datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Number of classes and instances in our datasets.For PASCAL VOC, we used all the annotations including difficult boxes. Note that there are twenty object classes in PASCAL VOC and Clipart1k, and six object classes in Watercolor2k and Comic2k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Typical detection errors by DT+PA using SSD300 as the baseline FSD. The images are from the test set of Clipart1k and Comic2k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of the datasets that we constructed for the target domains in this paper.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#classes #images #instances</cell></row><row><cell>Clipart1k</cell><cell>20</cell><cell>1,000</cell><cell>3,165</cell></row><row><cell>Watercolor2k</cell><cell>6</cell><cell>2,000</cell><cell>3,315</cell></row><row><cell>Comic2k</cell><cell>6</cell><cell>2,000</cell><cell>6,389</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of all the methods in terms of AP [%] using SSD300 as the baseline FSD in Clipart1k. Ensemble denotes an ensemble of SSD300, CLNet, and WSDDN. AP for each class Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP Baseline 19.8 49.5 20.1 23.0 11.3 38.6 34.2 2.5 39.1 21.6 27.3 10.8 32.5 54.1 45.3 31.2 19.0 19.5 19.1 17.9 26.8</figDesc><table><row><cell>Compared</cell><cell></cell></row><row><cell>WSDDN [2]</cell><cell>1.6 3.6 0.6 2.3 0.1 11.7 4.5 0.0 3.2 0.1 2.8 2.3 0.9 0.1 14.4 16.0 4.5 0.7 1.2 18.3 4.4</cell></row><row><cell>CLNet [15]</cell><cell>3.2 22.3 2.2 0.7 4.6 4.8 17.5 0.2 4.8 1.6 6.4 0.6 4.7 0.6 12.5 13.1 14.1 4.1 8.0 29.7 7.8</cell></row><row><cell>Ensemble</cell><cell>20.6 49.6 20.5 23.4 11.3 39.3 35.2 2.6 39.0 22.8 27.3 11.2 33.2 54.7 34.0 30.7 21.0 20.3 20.3 18.3 26.7</cell></row><row><cell>ADDA [35]</cell><cell>20.1 50.2 20.5 23.6 11.4 40.5 34.9 2.3 39.7 22.3 27.1 10.4 31.7 53.6 46.6 32.1 18.0 21.1 23.6 18.3 27.4</cell></row><row><cell>Proposed</cell><cell></cell></row><row><cell>PL w/o label</cell><cell>18.6 40.3 17.1 16.7 4.9 35.3 36.1 1.1 36.0 22.9 29.1 14.7 31.5 52.6 43.8 28.6 13.3 14.6 32.8 15.1 25.3</cell></row><row><cell>PL</cell><cell>24.2 59.8 22.0 26.6 25.0 54.7 51.3 3.9 47.4 44.5 40.3 14.3 33.6 55.1 50.8 41.1 23.2 26.3 40.5 43.2 36.4</cell></row><row><cell>DT</cell><cell>23.3 60.1 24.9 41.5 26.4 53.0 44.0 4.1 45.3 51.5 39.5 11.6 40.4 62.2 61.1 37.1 20.9 39.6 38.4 36.0 38.0</cell></row><row><cell cols="2">DT+PL w/o label 16.8 53.7 19.7 31.9 21.3 39.3 39.8 2.2 42.7 46.3 24.5 13.0 42.8 50.4 53.3 38.5 14.9 25.1 41.5 37.3 32.7</cell></row><row><cell>DT+PL</cell><cell>35.7 61.9 26.2 45.9 29.9 74.0 48.7 2.8 53.0 72.7 50.2 19.3 40.9 83.3 62.4 42.4 22.8 38.5 49.3 59.5 46.0</cell></row><row><cell>Ideal case</cell><cell>50.5 60.3 40.1 55.9 34.8 79.7 61.9 13.5 56.2 76.1 57.7 36.8 63.5 92.3 76.2 49.8 40.2 28.1 60.3 74.4 55.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of our methods on the different baseline FSDs in terms of mAP [%] in Clipart1k.</figDesc><table><row><cell>Method</cell><cell cols="3">SSD300 YOLOv2 Faster R-CNN</cell></row><row><cell>Baseline</cell><cell>26.8</cell><cell>25.5</cell><cell>26.2</cell></row><row><cell>DT</cell><cell>38.0</cell><cell>31.5</cell><cell>32.1</cell></row><row><cell>PL</cell><cell>36.4</cell><cell>34.0</cell><cell>29.8</cell></row><row><cell>DT+PL</cell><cell>46.0</cell><cell>39.9</cell><cell>34.9</cell></row><row><cell>Ideal case</cell><cell>55.4</cell><cell>51.2</cell><cell>50.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison in terms of AP [%] using SSD300 as the baseline FSD in Watercolor2k.</figDesc><table><row><cell></cell><cell>AP for each class</cell></row><row><cell>Method</cell><cell>bike bird car cat dog person mAP</cell></row><row><cell>Baseline</cell><cell>79.8 49.5 38.1 35.1 30.4 65.1 49.6</cell></row><row><cell>Compared</cell><cell></cell></row><row><cell>WSDDN [2]</cell><cell>1.5 26.0 14.6 0.4 0.5 33.3 12.7</cell></row><row><cell>CLNet [15]</cell><cell>4.5 27.9 19.6 14.3 6.4 31.4 17.4</cell></row><row><cell>Ensemble</cell><cell>79.8 49.6 38.1 35.2 30.4 58.7 48.6</cell></row><row><cell>ADDA [35]</cell><cell>79.9 49.5 39.5 35.3 29.4 65.1 49.8</cell></row><row><cell>Proposed</cell><cell></cell></row><row><cell>PL</cell><cell>76.3 54.9 46.6 37.5 36.9 71.7 54.0</cell></row><row><cell>DT</cell><cell>82.8 47.0 40.2 34.6 35.3 62.5 50.4</cell></row><row><cell>DT+PL</cell><cell>76.5 54.9 46.0 37.4 38.5 72.3 54.3</cell></row><row><cell>PL (+extra)</cell><cell>84.8 57.7 48.0 44.9 46.6 72.6 59.1</cell></row><row><cell cols="2">DT+PL (+extra) 86.3 57.3 48.5 43.0 46.5 73.2 59.1</cell></row><row><cell>Ideal case</cell><cell>76.0 60.0 52.7 41.0 43.8 77.3 58.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison in terms of AP [%] using SSD300 as the baseline FSD in Comic2k.</figDesc><table><row><cell></cell><cell>AP for each class</cell></row><row><cell>Method</cell><cell>bike bird car cat dog person mAP</cell></row><row><cell>Baseline</cell><cell>43.9 10.0 19.4 12.9 20.3 42.6 24.9</cell></row><row><cell>Compared</cell><cell></cell></row><row><cell>WSDDN [2]</cell><cell>1.5 0.1 11.9 6.9 1.4 12.1 5.6</cell></row><row><cell>CLNet [15]</cell><cell>0.0 0.0 2.0 4.7 1.2 14.9 3.8</cell></row><row><cell>Ensemble</cell><cell>44.0 10.0 19.4 14.5 20.7 42.9 25.3</cell></row><row><cell>ADDA [35]</cell><cell>39.5 9.8 17.2 12.7 20.4 43.3 23.8</cell></row><row><cell>Proposed</cell><cell></cell></row><row><cell>PL</cell><cell>52.9 13.7 35.3 16.2 28.9 50.8 32.9</cell></row><row><cell>DT</cell><cell>43.6 13.6 30.2 16.0 26.9 48.3 29.8</cell></row><row><cell>DT+PL</cell><cell>55.2 18.5 38.2 22.9 34.1 54.5 37.2</cell></row><row><cell>PL (+extra)</cell><cell>53.4 19.0 35.0 30.0 30.5 53.7 36.9</cell></row><row><cell cols="2">DT+PL (+extra) 56.6 24.0 40.7 35.8 39.0 57.3 42.2</cell></row><row><cell>Ideal case</cell><cell>55.9 26.8 40.4 42.3 43.0 70.1 46.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>The number of instances in Watercolor2k and Comic2k.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Bicycle Bird Car Cat Dog Person Total</cell></row><row><cell cols="2">Watercolor2k 27</cell><cell>486 101 102 116 2483 3315</cell></row><row><cell>Comic2k</cell><cell>87</cell><cell>270 107 233 192 5500 6389</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://openclipart.org/ 3 https://pixabay.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Further details about NMS can be found in<ref type="bibr" target="#b6">[7]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Statistics of Our Datasets</head><p>An important characteristic of our datasets is that they contain a sufficient number of objects. The number of classes and instances per image is shown in <ref type="figure">Fig. 7</ref>. For comparison, the figure contains the statistics of PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>, which is designed for detecting objects of twenty classes in natural images. Clipart1k contains 1.7 classes and 3.2 instances per image. Clipart1k contains almost the same number of classes and instances per image as PASCAL VOC. The average number of classes and instances in Clipart1k is almost the same as that in PASCAL VOC, which ensures the difficulty for the process of object detection. Watercolor2k contains 1.1 classes and 1.7 instances per image. Comic2k contains 1.1 classes and 3.2 instances per image. Note that Water-color2k and Comic2k are for detecting the six classes.</p><p>As shown in <ref type="table">Table 6</ref>, the distribution of the number of the instances for each class in Clipart1k is unbalanced, as </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning aligned cross-modal representations from weakly aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>2010. 5</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>2016. 3</idno>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-LocNet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://github.com/openimages,2017.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ChainerCV: a library for deep learning in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotations for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chainer: a nextgeneration open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting people in artwork with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Westlake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">BAM! the behance artistic media dataset for recognition beyond photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning graphs to model visual objects across different depictive styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
