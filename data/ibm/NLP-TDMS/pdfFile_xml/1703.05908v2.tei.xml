<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Robust Visual-Semantic Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung</forename><forename type="middle">Hubert</forename><surname>Tsai</surname></persName>
							<email>yaohungt@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Kang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Robust Visual-Semantic Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and fewshot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past few years, due to the availability of large amount of data and the advancement of the training techniques, learning effective and robust representations directly from images or text becomes feasible <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. These learned representations have facilitated a number of high-level tasks, such as image recognition <ref type="bibr" target="#b39">[40]</ref>, sentence generation <ref type="bibr" target="#b16">[17]</ref>, and object detection <ref type="bibr" target="#b34">[35]</ref>. Despite useful representations being developed for specific domains, learning more comprehensive representations across different data modalities remains challenging. In practice, more complex tasks, such as image captioning <ref type="bibr" target="#b45">[45]</ref> and image tagging <ref type="bibr" target="#b22">[23]</ref> often involve data from different modalities (i.e., images and text). Additionally, the learning process would be faster, requiring fewer labeled examples, and hence more scalable to handling a large number of categories if we could transfer cross-domain knowledge more effectively <ref type="bibr" target="#b8">[9]</ref>. This motivates learning multi-modal em- beddings. In this paper, we consider learning robust joint embeddings across visual and textual modalities in an endto-end fashion under zero and few-shot setting. Zero-shot learning aims at performing specific tasks, such as recognition and retrieval of novel classes, when no label information is available during training <ref type="bibr" target="#b15">[16]</ref>. On the other hand, few-shot learning enables us to have few labeled examples in our of-interest categories <ref type="bibr" target="#b37">[38]</ref>. In order to compensate the missing information under the zero and fewshot setting, the model should learn to associate novel concepts in image examples with textual attributes and transfer knowledge from training to test classes. A common strategy for deriving the visual-semantic embeddings is to make use of images and textual attributes in a supervised way <ref type="bibr" target="#b40">[41,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref>. Specifically, one can learn transformations of images and textual attributes under the objective that the transformed visual and semantic vectors of the same class should be similar in the joint embeddings space. Despite good performance, this common strategy basically boils down to a supervised learning setting, learning from labeled or paired data only. In this paper, we show that to learn better joint embeddings across different data modalities, it is beneficial to combine supervised and unsupervised learning from both labeled and unlabeled data.</p><p>Our contributions in this work are as follows. First, to extract meaningful feature representations from both labeled and unlabeled data, one possible option is to train an auto-encoder <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5]</ref>. In this way, instead of learning representations directly to align the visual and textual inputs, we choose to learn representations in an auto-encoder using reconstruction objective. Second, we impose a crossmodality distribution matching constraint to require the embeddings learned by the visual and textual auto-decoders to have similar distributions. By minimizing the distributional mismatch between visual and textual domain, we show improved performance on recognition and retrieval tasks. Finally, to achieve better adaptation on the unlabeled data, we perform a novel unsupervised-data adaptation inference technique. We show that by adopting this technique, the accuracy increases significantly not only for our method but also for many of the existing other models. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates our overall end-to-end differentiable model.</p><p>To summarize, our proposed method successfully combines supervised and unsupervised learning objectives, and learns from both labeled and unlabeled data to construct joint embeddings of visual and textual data. We demonstrate improved performance on Animals with Attributes (AwA) <ref type="bibr" target="#b20">[21]</ref> and Caltech-UCSD Birds 200-2011 <ref type="bibr" target="#b47">[47]</ref> datasets on both image recognition and image retrieval tasks under zero and few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide an overview of learning multimodal embeddings across visual and textual domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero and Few-Shot Learning</head><p>Zero-shot <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">1,</ref><ref type="bibr">2]</ref> and few-shot learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20]</ref> are related problems, but somewhat different in the setting of the training data. While few-shot learning aims to learn specific classes through one or few examples, zero-shot learning aims to learn even when no examples of the classes are presented. In this setting, zero-shot learning should rely on the side information provided by other domains. In the case of image recognition, this often comes in the form of textual descriptions. Thus, the focus of zero-shot image recognition is to derive joint embeddings of visual and textual data, so that the missing information of specific classes could be transferred from the textual domain.</p><p>Since the relation between raw pixels and text descriptions is non-trivial, most of the previous work relied on learning the embeddings through a large amount of data. Witnessing the success of deep learning in extracting useful representations, much of the existing work mostly applies deep neural networks to first transform raw pixels and text into more informative representations, followed by using various techniques to further identify the relation between them. For example, Socher et al. <ref type="bibr" target="#b40">[41]</ref> used deep architectures <ref type="bibr" target="#b12">[13]</ref> to learn representations for both images and text, and then used a Bayesian framework to perform classification. Norouzi et al. <ref type="bibr" target="#b28">[29]</ref> introduced a simple idea that treated classification scores output by the deep network <ref type="bibr" target="#b18">[19]</ref> as weights in convex combination of word vectors. Fu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a method that learns projections from low-level visual and textual features to form a hypergraph in the embedding space and performed label propagation for recognition. A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes <ref type="bibr">[2,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>A number of recent approaches also attempt to learn the entire task with deep models in an end-to-end fashion. Frome et al. <ref type="bibr" target="#b8">[9]</ref> constructed a deep model that took visual embeddings extracted by CNN <ref type="bibr" target="#b18">[19]</ref> and word embeddings as input, and trained the model with the objective that the visual and word embeddings of the same class should be well aligned under linear transformations. Ba et al. <ref type="bibr" target="#b21">[22]</ref> predicted the output weights of both the convolutional and fully connected layers in a deep convolutional neural network. Instead of using textual attributes or word embeddings model, Reed et al. <ref type="bibr" target="#b33">[34]</ref> proposed to train a neural language model directly from raw text with the goal of encoding only the relevant visual concepts for various categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual and Semantic Knowledge Transfer</head><p>Liu et al. <ref type="bibr" target="#b23">[24]</ref> developed multi-task deep visualsemantic embeddings model for selecting video thumbnails based on side semantic information (i.e., title, description, and query). By incorporating knowledge about objects similarities between visual and semantic domains, Tang et al. <ref type="bibr" target="#b43">[44]</ref> improved object detection in a semisupervised fashion. Kottur et al. <ref type="bibr" target="#b17">[18]</ref> proposed to learn visually grounded word embeddings (vis-w2v) and showed improvements over text only word embeddings (word2vec) on various challenging tasks. Reed et al. designed a text-conditional convolutional GAN architecture to synthesize an image from text. Recently, Wang et al. <ref type="bibr" target="#b46">[46]</ref> introduced structure-preserving constraints in learning joint embeddings of images and text for image-to-sentence and sentence-to-image retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Multi-modal Representations Learning</head><p>One of our key contributions is to effectively combine supervised and unsupervised learning tasks for learning multi-modal embeddings. This is inspired and supported by several previous works that provided evidence of how unsupervised learning tasks could benefit cross-modal feature learning.</p><p>Ngiam et al. <ref type="bibr" target="#b27">[28]</ref> proposed various models based on Restricted Boltzmann Machine, Deep Belief Network, and Deep Auto-encoder to perform feature learning over multiple modalities. The derived multi-modal features demonstrated an improved performance over single-modal features on the audio-visual speech classification tasks. Srivastava and Salakhutdinov <ref type="bibr" target="#b41">[42]</ref> developed a Multimodal Deep Boltzmann Machine for fusing together multiple diverse modalities even when some of them are absent. Providing inputs of images and text, their generative model manifested noticeable performance improvement on classification and retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>First, we define the problem setting and the corresponding notation.</p><formula xml:id="formula_0">Let V tr = {v (tr) i } Ntr i=1 denote labeled training images from C tr classes, V ut = {v (ut) i } Nut i=1 denote unla- beled training images from C ut possibly different classes, and V te = {v (te) i } Nte i=1</formula><p>denote test images from C te novel classes. For each class, following <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b6">7]</ref>, its textual attributes are either provided from human annotated attributes <ref type="bibr" target="#b20">[21]</ref> or learned from unsupervised text corpora (Wikipedia) <ref type="bibr" target="#b31">[32]</ref>. We denote these class-specific textual attributes as T tr = {t Under zero-shot setting, our goal is to predict labels of the test images coming from novel, previously unseen, classes given textual attributes. That is, for a given test image v</p><formula xml:id="formula_1">(te) i , its label is determined by arg max c∈{1,...,Cte} P θ c t (te) c , v (te) i ,<label>(1)</label></formula><p>where θ denotes model parameters. We will also consider a few-shot learning, where a few labeled training images are available in each of the test classes. In the following, we omit the model parameters θ for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Formulation</head><p>The goal of learning multi-modal embeddings can be formulated as learning transformation functions f v and f t , such that given an image v and a textual attribute t, f v (v) should be similar to f t (t) if v and t are of the same class. Much of the previous work for learning multi-modal embeddings can be generalized to this formulation. For instance, in Cross-Modal Transfer (CMT) <ref type="bibr" target="#b40">[41]</ref>, f v (·) can be viewed as a pre-defined feature extraction model followed by a two-layer neural network, while f t (·) is set to an identity matrix. To be more specific, <ref type="bibr" target="#b40">[41]</ref> aim at learning a nonlinear projection directly from visual features to semantic word vectors.</p><p>Over the past few years, deep architectures have been shown to learn useful representations that could embed high-level semantics for both visual and textual data. This gives rise to the attempts of applying successful deep architectures to learn f v (·) and f t (·). For example, DeViSE <ref type="bibr" target="#b8">[9]</ref> designed f v (·) as a CNN model followed by a linear transformation matrix. On the other hand, they adopted the well known skip-gram text modeling architecture <ref type="bibr" target="#b26">[27]</ref> for learning f t (·) from raw text on Wikipedia. It is worth noting that, to further take advantage of previous success, these deep models are often pre-trained on large datasets where they have shown to learn effective representations. <ref type="figure" target="#fig_2">Figure 2</ref> shows the basic formulation of the visualsemantic embeddings model. Our method is built on top of this basic architecture by adding additional components as well as modifying existing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reconstructing Features from Auto-Encoder</head><p>Although the basic architecture provides a way to utilize label information during training, the learning process could further benefit if unlabeled data are provided at the same time. To be more specific, we propose to combine supervised and unsupervised learning objectives together by incorporating auto-encoders <ref type="bibr" target="#b3">[4]</ref> for both image and text data. Typical setting of auto-encoders consists of a symmetric encoder-decoder architecture, with the hidden representations in the middle being compact representations that could be used to reconstruct the original input data. In our model, the auto-encoders are added after the image and text data are processed by the pre-trained networks. For learning visual embeddings, we use contractive auto-encoder proposed by <ref type="bibr" target="#b35">[36]</ref>, which is able to learn more robust visual codes for the images of same class. Given a visual feature vectorṽ, the contractive auto-encoder mapsṽ to a hidden representatioñ v h , and seeks to reconstructṽ fromṽ h . Let us denote the reconstructed vector by r v (·). Model parameters are thus learned by minimizing the regularized reconstruction error</p><formula xml:id="formula_2">L v = 1 N tr Ntr i=1 ṽ i − r v (ṽ i ) 2 + γ J(ṽ i ) 2 F ,<label>(2)</label></formula><p>where J(·) is the Jacobian matrix <ref type="bibr" target="#b35">[36]</ref>.</p><p>On the other hand, for a given semantic feature vector or textual attribute t, we use a vanilla auto-encoder to first encode and then reconstruct from its hidden representation t h . We hence minimize the reconstruction error</p><formula xml:id="formula_3">L t = 1 C tr Ctr c=1 t c − r t (t c ) 2 .<label>(3)</label></formula><p>Combining <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref> gives us the reconstruction loss</p><formula xml:id="formula_4">L reconstruct = L v + L t .<label>(4)</label></formula><p>In practice, if we have access to a large unlabeled set or a set of test inputs (without labels), we can easily incorporate them into the reconstruction loss. In our experimental results, we find that with the visual and textual auto-encoders, image and text data are transformed into visual and textual embeddings with more meaningful representations. In order to further transfer the knowledge across modalities, we impose discriminative constraints on the hidden representations (ṽ h and t h ) learned by these auto-encoders, as we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Modality Distributions Matching</head><p>Distributions matching technique has been proven to be effective for transferring knowledge from one modality to another <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>. A common nonparametric way to analyze and compare distributions is to use Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b10">[11]</ref> criterion. We can view MMD as a two-sample test onṽ h and t h , and thus its loss can be formulated as</p><formula xml:id="formula_5">L M M D = E p [φ(ṽ h )] − E q [φ(t h )] 2 H k ,<label>(5)</label></formula><p>where p, q are the distributions of visual and textual embeddings (i.e.,ṽ h ∼ p and t h ∼ q), φ is the feature map with canonical form φ(x) = k(x, ·), and H k is the reproducing kernel Hilbert space (RKHS) endowed with a characteristic kernel k. Note that the kernel in the MMD criterion must be a universal kernel, and thus we empirically choose a Gaussian kernel:</p><formula xml:id="formula_6">k(x, x ) = exp −κ x − x 2 .<label>(6)</label></formula><p>We can now minimize the MMD criterion between visual and textual embeddings by minimize eq. (5). This can be further regarded as shrinking the gap between information across two data modalities. In our experiments, we find that the MMD loss helps improve model performance on both recognition and retrieval tasks in zero and few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning</head><p>After we derive the hidden representationsṽ h and t h , the transformation functions f v (·) and f t (·) can be reformulated as</p><formula xml:id="formula_7">f v (v) = f v (ṽ h ) and f t (t) = f t (t h ),<label>(7)</label></formula><p>where f v (·) and f t (·) are the mapping functions from the hidden representations to the visual and textual output.</p><p>To leverage the supervised information from labeled training images V tr and the corresponding textual attributes T tr , we minimize the binary prediction loss:</p><formula xml:id="formula_8">L supervised = − 1 N tr Ntr i=1 Ctr c=1 I i,c f v (ṽ (tr) h,i ), f t (t (tr) h,c ) ,<label>(8)</label></formula><p>where I i,c indicates a {0, 1} encoding of positive and negative classes and · denotes a dot-product. It is worth noting that we can adopt different loss functions, including binary cross-entropy loss or multi-class hinge loss. However, empirically, using the simple binary prediction loss results in the best performance of our model. Similar to eq. (8), we adopt the binary prediction loss for unlabeled training images V ut and the attributes T ut :</p><formula xml:id="formula_9">L unsup unlab = − 1 N ut Nut i=1 Cut c=1 I (ut) i,c f v (ṽ (ut) h,i ), f t (t (ut) h,c ) ,<label>(9)</label></formula><formula xml:id="formula_10">where I (ut) i,c =    1 if c = arg max c∈{1,...,Cut} f v (ṽ (ut) h,i ), f t (t (ut) h,c ) 0 otherwise.<label>(10)</label></formula><p>We refer to eq. (9) as unsupervised-data adaptation inference, which acts as a self-reinforcing strategy using the unsupervised data with unknown labels. The intuition is that by minimizing eq. (9), we can further adapt our unlabeled data into the learning of f v (·) and f t (·) based on the empirical predictions. The choice of λ does influence its effectiveness. However, we find that setting λ = 1.0 works quite well for many methods we considered in this work.</p><p>In sum, our model learns by minimizing the total loss from both supervised and unsupervised objectives:</p><formula xml:id="formula_11">L T otal = L supervised + αL unsupervised ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_12">L unsupervised = L reconstruct + λL unsup unlab + βL M M D ,<label>(12)</label></formula><p>with α, λ, and β representing the trade-off parameters for different components. Note that we can also view the unsupervised objective here as a regularizer for learning more robust visual and textual representations (see <ref type="figure" target="#fig_0">Figure 1</ref> for our overall model architecture). Before computing the loss, we find it useful to perform 2 normalization on the output scores f v (v) and f t (t) along the batch-wise direction. It can be viewed as a mixture of Batch Normalization <ref type="bibr" target="#b14">[15]</ref> and Layer Normalization <ref type="bibr">[3]</ref>. The idea is simple, we encourage the competence between different instances in the data batch, rather than across different categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the experiments, we denote our proposed method as ReViSE (Robust sEmi-supervised Visual-Semantic Embeddings). Extensive experiments on zero and few-shot image recognition and retrieval tasks are conducted using two benchmark datasets: Animals with Attributes (AwA) <ref type="bibr" target="#b20">[21]</ref> and Caltech-UCSD Birds 200-2011 (CUB) <ref type="bibr" target="#b47">[47]</ref>. CUB is a fine-grained dataset in which the objects are both visually and semantically very similar, while AwA is a more general concept dataset. We use the same training (+validation)/ test splits as in <ref type="bibr">[2,</ref><ref type="bibr" target="#b48">48]</ref>. <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of the datasets.</p><p>To verify the performance of our method, we consider two state-of-the-art deep-embeddings methods: CMT <ref type="bibr" target="#b40">[41]</ref> and DeViSE <ref type="bibr" target="#b8">[9]</ref>. CMT and DeViSE can be viewed as a special case of our proposed method with α = 0 (without using unsupervised objective in eq. (11)). The difference between them is that DeViSE learns a nonlinear transformation on raw visual images and textual attributes for the alignment purpose, while CMT only learns the nonlinear transformation from visual to semantic embeddings.</p><p>We choose GoogLeNet <ref type="bibr" target="#b42">[43]</ref> as the CNN model in De-ViSE, CMT, and our architecture. For the textual attributes of classes, we consider three alternatives: human annotated attributes (att) <ref type="bibr" target="#b20">[21]</ref>, Word2Vec attributes (w2v ) <ref type="bibr" target="#b26">[27]</ref>, and Glove attributes (glo) <ref type="bibr" target="#b31">[32]</ref>. att are continuous attributes judged by humans: CUB contains 312 attributes and AwA contains 85 attributes. w2v and glo are unsupervised methods for obtaining distributed text representations of words. We use the pre-extracted Word2Vec and Glove vectors from Wikipedia provided by <ref type="bibr">[2,</ref><ref type="bibr" target="#b48">48]</ref>. Both w2v and glo are 400dim. features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Design and Training Procedure</head><p>Please see Supplementary for the design details of Re-ViSE and its parameters. Note that we report results averaged over 10 random trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero-Shot Learning</head><p>Following the partitioning strategy of <ref type="bibr">[2,</ref><ref type="bibr" target="#b48">48]</ref>, we split AwA dataset into 30/10/10 classes and CUB dataset into 100/50/50 classes for labeled training/ unlabeled training/ test data. We adopt att attributes as a textual description of each class. For zero-shot learning, not only the labels of images are unknown in the unlabeled training and test set, but classes are also disjoint across labeled training/ unlabeled training/ test splits.  To verify how unlabeled training data could benefit the learning of ReViSE, we provide four variants: ReViSE a , ReViSE b , ReViSE c , and ReViSE. ReViSE a is when we only consider supervised objective. That is, α = 0 in eq. (11). ReViSE b is when we further take unsupervised objective in labeled training data into account; that is, only L reconstruct and L M M D are considered in L unsupervised (see eq. (12)) for labeled training data. Next, for ReViSE c , we consider unlabeled training data in L unsupervised without unsupervised-data adaptation technique (setting β = 0). Last, ReViSE denotes our complete training architecture.</p><p>For completeness, we also consider the technique of unsupervised-data adaptation inference (see section 3.4) for DeViSE <ref type="bibr" target="#b8">[9]</ref> and CMT <ref type="bibr" target="#b40">[41]</ref>. In other words, we also evaluate how DeViSE and CMT benefit from the unlabeled training data. We adopt the same procedure as in eq. (9) and report results as DeViSE* and CMT*, respectively.</p><p>Similar to <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref>, the results and comparisons are reported using top-1 classification accuracy (top-1) (see <ref type="table" target="#tab_1">Table 2</ref>) and mean average precision (mAP) (see <ref type="table" target="#tab_2">Table 3</ref>) for recognition and retrieval tasks, respectively, on the unlabeled training and test images. To be more specific, we define the prediction score asŷ (·)</p><formula xml:id="formula_13">i,c = f v (ṽ (·) h,i ), f t (t (·)</formula><p>h,c ) for a given image v i,c on all unlabeled training or test classes. <ref type="table" target="#tab_1">Table 2</ref> and 3 list the results for our zero-shot recognition and retrieval experiments. We first observe that NOT all the methods benefit from using unlabeled training data dur- to ReViSE c . This shows that the learning method of our proposed architecture can actually benefit from unlabeled training data V ut and T ut . Next, we examine different variants in our proposed architecture. Comparing the average results from ReViSE a to ReViSE b , we observe 3.4% recognition improvement and 3.9% retrieval improvement. This indicates that taking unsupervised objectives L reconstruct and L M M D into account results in learning better feature representations and thus yields a better recognition/ retrieval performance. Moreover, when unsupervised-data adaptation technique is introduced, we enjoy 5.0% average recognition improvement and 5.8% average retrieval improvement from ReViSE c to ReViSE. It is worth noting that the significant performance improvement for unlabeled training images V ut further verifies that our unsupervised-data adaptation technique leads to a more accurate prediction on V ut .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transductive Zero-Shot Learning</head><p>In this subsection, we extend our experiments to a transductive setting, where test data are available during training. Therefore, the test data can now be regarded as the unlabeled training data (V tr = V ut and T tr = T ut ). To perform the experiments, as in <ref type="table" target="#tab_0">Table 1</ref>, we split AwA dataset into 40/10 disjoint classes and CUB dataset into 150/50 disjoint classes for labeled training/ test data.</p><p>In order to evaluate different components in ReViSE, we further provide two variants: ReViSE † and ReViSE † † . ReViSE † is when we consider no distributional matching between the codes across modalities (β = 0). ReViSE † † is when we further consider no contractive loss in our visual auto-encoder (β = γ = 0). Similar to previous subsection, we also consider DeViSE*, CMT*, and ReViSE c to evaluate the effect of our unsupervised-data adaptation inference.</p><p>Zero-Shot Recognition: <ref type="table">Table 9</ref> reports top-1 classification accuracy. Observe that ReViSE clearly outperforms other state-of-the-art methods by a large margin. On average, we have at least 17% gain compared to the methods without using unsupervised objective and 7.5% gain compared to DeViSE* and CMT*. Note that all the methods work better on human annotated attributes (att) than on unsupervised attributes (w2v and glo) in CUB dataset. One possible reason is that for visually and semantically similar classes in a fine-grained dataset (CUB), attributes obtained in an unsupervised way (glo word vectors) cannot fully differentiate between them. Nonetheless, for the more general concept dataset AwA, using either supervised or unsupervised textual attributes, the performance does not differ by that much. For instance, our method achieves comparable performance using att, w2v, and glo (93.4%, 93.5%, and 92.2% top-1 classification accuracy) on AwA dataset.</p><p>The recognition performance for DeViSE* and CMT* (60.6% and 60.5% on average) compared to DeViSE and CMT (49.3% and 50.5% on average) further verifies that using unsupervised-data adaptation inference technique does benefit transductive zero-shot recognition. Furthermore, all of the variants of ReViSE using unsupervised-data adaptation inference (ReViSE † † , ReViSE † , and ReViSE itself) have noticeable improvement over DeViSE* and CMT*. This shows that the proposed model succeeds in leveraging unsupervised information in test data for constructing more effective cross-modal embeddings.</p><p>Next, we evaluate the effects of different components designed in our architecture. First of all, we compare the results between ReViSE † (set β = 0) and ReViSE. The performance gain (66.8% to 68.1% on average) indicates that minimizing MMD distance between visual and textual codes enables our model to learn more robust visualsemantic embeddings. In other words, we can better associate cross-modal information when we match the distributions across visual and textual domains (please refer to Supplementary for the study of MMD distance). Second, we observe that, without contractive loss, performance slightly drop from 66.8% (ReViSE † ) to 65.8% (ReViSE † † ). This is not surprising since the contractive auto-encoder aims at learning less varying features/codes with similar visual input, and therefore we can expect to learn more robust visual codes. Finally, similar to the observations found in comparing DeViSE/CMT to DeViSE*/CMT*, the unsupervised-data adaptation inference in ReViSE substantially improves the average top-1 classification accuracy from 53.6% (ReViSE c ) to 68.1% (ReViSE). Please see Supplementary material for more detailed comparisons to the following non-deep-embeddings methods: SOC <ref type="bibr" target="#b29">[30]</ref>, ConSE <ref type="bibr" target="#b28">[29]</ref>, SSE <ref type="bibr" target="#b49">[49]</ref>, SJE <ref type="bibr">[2]</ref>, ESZSL <ref type="bibr" target="#b36">[37]</ref>, JLSE <ref type="bibr" target="#b50">[50]</ref>, LatEm <ref type="bibr" target="#b48">[48]</ref>, Sync <ref type="bibr" target="#b6">[7]</ref>, MTE <ref type="bibr" target="#b5">[6]</ref>, TMV <ref type="bibr" target="#b9">[10]</ref>, and SMS <ref type="bibr" target="#b11">[12]</ref>.</p><p>Zero-Shot Retrieval: In <ref type="table" target="#tab_5">Table 5</ref>, we report zero-shot retrieval results by measuring the retrieval performance by mean average precision (mAP). On average, methods that   leverage unsupervised information yield better performance compared to the methods using no unsupervised objective. However, in few cases, the performance drops when we take unsupervised information into account. For example, on CUB dataset, DeViSE* performs unfavorably compared to DeViSE when w2v and glo word embeddings are used as textual attributes. Overall, our method does help improve zero-shot retrieval by at least 14.1% compared to CMT*/DeViSE* and 21.5% compared to CMT/DeViSE. It clearly demonstrates the effectiveness of leveraging unsupervised information for improving zero-shot retrieval (please see Supplementary for the plot of precision-recall curves).</p><p>In addition to quantitative results, we also provide qualitative results of ReViSE. <ref type="figure" target="#fig_4">Fig. 3</ref> is the image retrieval experiments for classes Chestnut sided Warbler and White eyed Vireo. Given a class embedding, the nearest image neighbors are retrieved based on the cosine similarity between transformed visual and textual features. We consider two conditions: images from the same class and images from all test classes. In Chestnut sided Warbler, most of the images (71.7%) are correctly classified, and we also observe that three nearest image neighbors are also in Chestnut sided Warbler. On the other hand, only 43.3% images are correctly classified in White eyed Vireo, and two of the three nearest image neighbors are form wrong class Wilson Warbler.</p><p>Availability of Unlabeled Test Images: We next evaluate the performance of our method w.r.t. to the availability of  test images for unsupervised objective (see <ref type="figure" target="#fig_6">Fig. 4</ref>) on CUB dataset with att attributes. We alter the fraction p of unlabeled test images used in the training stage from 0% to 100% by a step size of 10%. That is, in eq. (12), only p portion (randomly chosen) of test images contributes to L unsupervised . <ref type="figure" target="#fig_6">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">From Zero to Few-Shot Learning</head><p>In this subsection, we extend our experiments from transductive zero-shot to transductive few-shot learning. Compared to zero-shot learning, few-shot learning allows us to have a few labeled images in our test classes. Here, 3 images are randomly chosen to be labeled per test category. We use the same performance comparison metrics as in Sec. 4.2 to report the results.</p><p>Transductive Few-Shot Recognition and Retrieval: Tables 6 and 7 list the results of transductive few-shot recognition and retrieval tasks. Generally speaking, ReViSE achieves the best performance compared to its variants and other methods. Moreover, as expected, when we compare the results with transductive zero-shot recognition ( <ref type="table">Table 9</ref>) and retrieval <ref type="table" target="#tab_5">(Table 5</ref>), every methods perform better when few (i.e., 3) labeled images are observed in the test classes. For example, for CUB dataset with w2v attributes, there is a 22.5% recognition improvement for CMT* and a 32.3%    on AwA dataset with glo attributes under transductive zero-shot setting. First of all, observe that both the reconstructed features and the visual codes have more separate clusters over different classes, which suggest ReViSE has learned useful representations. Another interesting observation is that the affinities between classes might change after learning visual codes. For example, "leopard" images (green dots) are near "humpback whale" images (light purple dots) in the original CNN feature space. However, in the visual code space, leopard images are far from humpback whale images. One possible explanation is that we know leopard is semantically distinct from humpback whale, and thus their semantic attributes must also be very different. This leads to different image clusters in our designed framework. Next, we provide the t-SNE visualization on the output visual test scores f v (V te ) for DeViSE*, CMT*, and Re-ViSE in <ref type="figure">Fig. 6</ref>. Clearly, ReViSE can better separate instances from different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">t-SNE Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we showed how we can augment a typical supervised formulation with unsupervised techniques for learning joint embeddings of visual and textual data. We empirically evaluate our proposed method on both general and fine-grained image classification datasets, with comparisons against the state-of-the-art methods in zero-shot and few-shot recognition and retrieval tasks, from inductive to transductive setting. In all the experiments, our method consistently outperforms other methods, substantially improving performance in some cases. We believe that this work sheds light on the advantages of combining supervised and unsupervised learning techniques, and makes a step towards learning more useful representations from multimodal data. <ref type="figure" target="#fig_10">Fig. 7</ref> provides an easy-to-understand design of ReViSE. In all of our experiments, GoogLeNet is pre-trained on Im-ageNet <ref type="bibr">[1]</ref> images. Without fine-tuning, we directly extract the top layer activations (1024-dim) as our input image features followed by a common log(1+v) pre-processing step. For the textual attributes, we pre-process them through a standard l 2 normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Network Design</head><p>In ReViSE, we set α = 1.0 in eq. (11), so that we place equal importance on supervised and unsupervised objectives. For the visual auto-encoder, we fix the parameter of the contraction strength γ = 0.1 in eq. (2). In the following, we omit the bias term in each layer for simplicity. The encoding of visual features is parameterized by a twohidden layer fully-connected neural network with architecture d v1 −d v2 −d c , where d v1 = 1024 is the input dimension of the visual features, d v2 = 500 is the intermediate layer, and d c denotes the dimension of the visual codesṽ h . To encode textual attributes, we consider a single-hidden layer neural network d t1 −d c , where d t1 is the input dimension of the textual attributes. We choose d c = 100 when d t1 &gt; 100 and d c = 75 when d t1 &lt; 100. Furthermore, we do not tie the weights to be learned between the decoding and encoding parts. Parameters for associating distributions of visual and textual codes (MMD Loss) in eqs. (5) (12), and (6) are set as β = {0.1, 1.0} (chosen by cross-validation) and κ = 32.0. For the remaining part of our model, we set the architecture of visual and textual code mapping as a single-hidden layer fully-connected neural network with dimension d c − 50. We also adopt a dropout of 0.7.</p><p>During the first 100 iterations of training, we set λ = 0 so that no unsupervised-data adaptation is used while still updatingÎ <ref type="bibr">(ut)</ref> i,c . Note thatÎ <ref type="bibr">(ut)</ref> i,c are the inferred labels for unsupervised data, and not random at each iteration. Beginning with the 101th iteration, we set λ = {0.1, 1.0} (chosen by cross-validation), and the model typically converges within 2000 to 5000 iterations.</p><p>We implement ReViSE in TensorFlow <ref type="bibr">[2]</ref>. We use Adam <ref type="bibr">[3]</ref> for optimization with minibatches of size 1024. We choose tanh for all of our activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Parameters Choice</head><p>We have four parameters in our architecture: α, β, γ, and κ. We fix α = 1.0, γ = 0.1, κ = 32.0 for all the experiments. Then we set λ = 0.0 (no unsuperviseddata adaptation inference), and perform cross-validation on the splitting set as suggested by <ref type="bibr">[3,</ref><ref type="bibr" target="#b46">46]</ref> to determine β from {0.1, 1.0}. Next, with chosen β, we perform crossvalidation to choose λ from {0.1, 1.0}. <ref type="table">Table 8</ref> lists the statistics of β and λ.</p><p>Next, we study the power of unsupervised information. We now take CUB dataset with att attributes to test the ad- vantage of using unsupervised information, which can be viewed as tuning the parameter α for the unsupervised objective in eq. (11). Originally, α was set to 1.0, which equally weights the contribution of supervised and unsupervised loss. We now alter α as follows: 0.1 to 1.0 by step size of 0.1 and 0.5 to 5.0 by step size of 0.5. The results are shown in <ref type="figure">Fig. 8</ref>. We observe that when α increases from 0.1 to 1.0, the performance increases; however, when α increase from 1.0 to 5.0, the performance stays relatively unchanged. Empirically, we find that ReViSE does not perform better when α &gt; 1.0, which is expected, since we should not view unsupervised information more important than supervised information. <ref type="figure" target="#fig_0">Fig. 10</ref> is the precision-recall curve for zero-shot retrieval results on CUB dataset with att attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Precision-Recall Curve</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">MMD Distance</head><p>MMD distance in eq. (5) can be viewed as the distribution measurement <ref type="bibr" target="#b12">[13]</ref> between visual and textual code. For CUB dataset with att attributes under transductive zero-shot experiment, we calculate the MMD distance (on the test codes) in our method with (ReViSE) and without (ReViSE † ) L M M D . The results of MMD distance w.r.t. the number of iterations are shown in <ref type="figure">Fig. 9</ref>. We clearly observe that the red curve (ReViSE) has consistently lower value than the blue curve (ReViSE † ). Moreover, based on the previous results, ReViSE always performs better than ReViSE † . Hence aligning the distributions across visual and textual codes can better associate cross-modal information and thus lead to more robust visual-semantic embeddings.  <ref type="figure" target="#fig_0">Figure 10</ref>. Precision-recall curve comparison for zero-shot retrieval on CUB with human annotated attributes as textual attributes for classes. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Remarks on Contractive Loss</head><p>We find that adding contractive loss to textual autoencoder doesn't provide much benefit. One possible reason may be the limited number of textual features (200 for CUB). On the other hand, the number of visual features is large (11, 786 for CUB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Comparing with recent state-of-the-art methods</head><p>In our main paper, we focus on comparing with deepembeddings methods. In <ref type="table">Table 9</ref>, we compare other methods for inductive and transductive zero-shot learning. Note that SMS ESZSL adopts ESZSL for its initialization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our proposed ReViSE (Robust sEmi-supervised Visual-Semantic Embeddings) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ctr c=1 , T ut = {t (ut) c } Cut c=1 , and T te = {t (te) c } Cte c=1 for labeled training, unlabeled training, and test classes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of traditional visual-semantic embeddings model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Images-retrieval experiments for CUB with att attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Fraction p of test images used for training ReViSE on transductive (a) zero-shot recognition (b) zero-shot retrieval for CUB dataset with att attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>(a) Original CNN features (b) Reconstructed features (c) Visual codes for AwA dataset in ReViSE under transductive zero-shot setting. We use glo as our textual attributes for classes. Different colors denote different classes. Best viewed in colors. Output features of : (a) DeViSE* (b) CMT* (c) ReViSE. glo attributes are used on AwA dataset under transductive zero-shot setting. Different colors denote different classes. Best view in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5</head><label>5</label><figDesc>further shows the t-SNE<ref type="bibr" target="#b25">[26]</ref> visualization for the original CNN features, the reconstructed visual features r v (ṽ (te) ), and the visual codesṽ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Our designed architecture. Table 8. Value of β and λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Varying α in two scales: 0.1 to 1.0 and 0.5 to 5.0. (a),(c) display plots for transductive zero-shot recognition and (b),(d) display plots for transductive zero-shot retrieval. CUB dataset with att attributes are used in the experiments. ReViSE ReViSE † MMD distance w.r.t. # of iterations for our method with and without L M M D . The experiment is conducted on CUB dataset with att attributes under transdutive zero-shot setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The statistics of AwA and CUB datasets. Images and classes are disjoint across training(+ validation) / test split.</figDesc><table><row><cell>Dataset</cell><cell cols="4">training (+ validation) # of images # of classes # of images # of classes test</cell></row><row><cell>AwA [21]</cell><cell>24293</cell><cell>40</cell><cell>6180</cell><cell>10</cell></row><row><cell>CUB [47]</cell><cell>8855</cell><cell>150</cell><cell>2931</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Zero-shot recognition using top-1 classification accuracy (%). att attributes are used to describe each category.</figDesc><table><row><cell>Dataset</cell><cell cols="2">AwA</cell><cell cols="2">CUB</cell><cell>average</cell></row><row><cell>recognition for</cell><cell>V ut</cell><cell>V te</cell><cell>V ut</cell><cell>V te</cell><cell>top-1 acc.</cell></row><row><cell cols="5">using only labeled training data</cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="2">60.8 63.0</cell><cell cols="2">38.9 36.8</cell><cell>49.9</cell></row><row><cell>CMT [41]</cell><cell cols="2">59.3 61.6</cell><cell cols="2">41.1 40.6</cell><cell>50.7</cell></row><row><cell>ReViSE a</cell><cell cols="2">60.3 61.2</cell><cell cols="2">46.4 45.0</cell><cell>53.2</cell></row><row><cell>ReViSE b</cell><cell cols="2">64.5 65.0</cell><cell cols="2">49.6 47.3</cell><cell>56.6</cell></row><row><cell cols="6">using both labeled and unlabeled training data</cell></row><row><cell>DeViSE* [9]</cell><cell cols="2">76.0 63.7</cell><cell cols="2">37.2 36.2</cell><cell>53.3</cell></row><row><cell>CMT* [41]</cell><cell cols="2">77.8 58.5</cell><cell cols="2">39.9 39.8</cell><cell>54.0</cell></row><row><cell>ReViSE c</cell><cell cols="2">64.7 67.6</cell><cell cols="2">52.2 48.2</cell><cell>58.2</cell></row><row><cell>ReViSE</cell><cell cols="2">78.0 68.6</cell><cell cols="2">56.6 49.6</cell><cell>63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Zero-shot retrieval using mean Average Precision (mAP) (%). att attributes are used to describe each category.</figDesc><table><row><cell>Dataset</cell><cell cols="2">AwA</cell><cell cols="2">CUB</cell><cell>average</cell></row><row><cell>retrieval for</cell><cell>V ut</cell><cell>V te</cell><cell>V ut</cell><cell>V te</cell><cell>mAP</cell></row><row><cell></cell><cell cols="4">using only labeled training data</cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="2">60.5 61.6</cell><cell cols="2">32.6 31.5</cell><cell>46.6</cell></row><row><cell>CMT [41]</cell><cell cols="2">58.8 61.4</cell><cell cols="2">35.8 37.0</cell><cell>48.3</cell></row><row><cell>ReViSE a</cell><cell cols="2">60.2 60.1</cell><cell cols="2">33.1 32.0</cell><cell>46.4</cell></row><row><cell>ReViSE b</cell><cell cols="2">64.4 63.2</cell><cell cols="2">36.0 37.4</cell><cell>50.3</cell></row><row><cell cols="6">using both labeled and unlabeled training data</cell></row><row><cell cols="3">DeViSE* [9] 65.6 58.9</cell><cell cols="2">35.4 31.3</cell><cell>47.8</cell></row><row><cell>CMT* [41]</cell><cell cols="2">63.5 57.1</cell><cell cols="2">39.7 38.0</cell><cell>49.6</cell></row><row><cell>ReViSE c</cell><cell cols="2">66.8 63.6</cell><cell cols="2">39.4 37.5</cell><cell>51.8</cell></row><row><cell>ReViSE</cell><cell cols="2">74.2 68.1</cell><cell cols="2">47.6 40.4</cell><cell>57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Transductive zero-shot recognition using top-1 classification accuracy (%).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>AwA</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>average</cell></row><row><cell>attributes</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>top-1 acc.</cell></row><row><cell></cell><cell cols="5">test data not available during training</cell><cell></cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="6">67.4 67.0 66.7 40.8 28.8 25.6</cell><cell>49.3</cell></row><row><cell>CMT [41]</cell><cell cols="6">67.6 69.5 68.0 42.4 29.6 25.7</cell><cell>50.5</cell></row><row><cell></cell><cell cols="5">test data available during training</cell><cell></cell><cell></cell></row><row><cell cols="7">DeViSE* [9] 90.7 84.8 88.0 41.4 31.6 26.9</cell><cell>60.6</cell></row><row><cell>CMT* [41]</cell><cell cols="6">89.4 87.8 81.8 43.1 31.8 28.9</cell><cell>60.5</cell></row><row><cell>ReViSE  † †</cell><cell cols="6">92.1 92.3 90.3 62.4 30.0 27.5</cell><cell>65.8</cell></row><row><cell>ReViSE  †</cell><cell cols="6">92.8 92.6 91.7 62.7 31.8 28.9</cell><cell>66.8</cell></row><row><cell>ReViSE c</cell><cell cols="6">73.0 67.0 73.4 53.7 26.4 28.2</cell><cell>53.6</cell></row><row><cell>ReViSE</cell><cell cols="6">93.4 93.5 92.2 65.4 32.4 31.5</cell><cell>68.1</cell></row><row><cell cols="8">ing training. For example, in AwA dataset for test images</cell></row><row><cell cols="8">V te , there is a 2.7% retrieval deterioration from DeViSE to</cell></row><row><cell cols="8">DeViSE* and a 3.1% recognition deterioration from CMT</cell></row><row><cell cols="8">to CMT*. On the other hand, our proposed method enjoys</cell></row><row><cell cols="8">2.6% recognition improvement and 0.4% retrieval improve-</cell></row><row><cell cols="2">ment from ReViSE b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Chestnut_sided_Warbler (71.7% of them are correctly classified)</figDesc><table><row><cell cols="2">Within-class Nearest Neighbors</cell><cell></cell><cell>Overall Nearest Neighbors</cell><cell></cell></row><row><cell>Chestnut_sided_Warbler</cell><cell>Chestnut_sided_Warbler</cell><cell cols="3">Chestnut_sided_Warbler Chestnut_sided_Warbler Chestnut_sided_Warbler</cell></row><row><cell></cell><cell cols="3">White_eyed_Vireo (43.3% of them are correctly classified)</cell><cell></cell></row><row><cell cols="2">Within-class Nearest Neighbors</cell><cell></cell><cell>Overall Nearest Neighbors</cell><cell></cell></row><row><cell>White_eyed_Vireo</cell><cell>White_eyed_Vireo</cell><cell>Wilson_Warbler</cell><cell>White_eyed_Vireo</cell><cell>Wilson_Warbler</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Transductive zero-shot retrieval using mean Average Precision (mAP) (%).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>AwA</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>average</cell></row><row><cell>attributes</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>mAP</cell></row><row><cell></cell><cell cols="5">test data not available during training</cell><cell></cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="6">67.5 67.6 66.2 31.9 26.6 24.5</cell><cell>47.4</cell></row><row><cell>CMT [41]</cell><cell cols="6">66.3 70.6 69.5 39.3 25.2 21.9</cell><cell>48.8</cell></row><row><cell></cell><cell cols="5">test data not available during training</cell><cell></cell><cell></cell></row><row><cell cols="7">DeViSE* [9] 82.3 78.0 84.4 36.9 25.8 21.3</cell><cell>54.8</cell></row><row><cell>CMT* [41]</cell><cell cols="6">85.8 77.3 73.0 44.1 28.9 28.1</cell><cell>56.2</cell></row><row><cell>ReViSE  † †</cell><cell cols="6">96.7 96.8 95.1 60.7 29.4 27.2</cell><cell>67.7</cell></row><row><cell>ReViSE  †</cell><cell cols="6">97.2 96.9 96.4 62.0 29.8 28.2</cell><cell>68.4</cell></row><row><cell>ReViSE c</cell><cell cols="6">73.0 67.0 73.4 53.7 26.4 28.2</cell><cell>53.6</cell></row><row><cell>ReViSE</cell><cell cols="6">97.4 97.4 96.7 68.9 30.5 30.9</cell><cell>70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Expand the test-time search space: Note that most of the methods<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">49,</ref> 2,<ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> consider that, at test time, queries come from only test classes. For AwA dataset with att attributes, we expand the test-time search space to all training and test classes and perform transductive zero-shot recognition for DeViSE*, CMT*, and Re-ViSE. We discover severe performance drops from 90.7%, 89.4%, and 93.4% to 47.4%, 45.8%, and 42.5%.</figDesc><table><row><cell>clearly indicates the performance in-</cell></row><row><cell>creases when p increases. That is, with more unsupervised</cell></row><row><cell>information (test images) available, our model can better</cell></row><row><cell>associate the supervised and unsupervised data. Another</cell></row><row><cell>interesting observation is that with only 40% test images</cell></row><row><cell>available, ReViSE achieves favorable performance on both</cell></row><row><cell>transductive zero-shot recognition and retrieval.</cell></row><row><cell>Similar</cell></row><row><cell>results can also be observed in other non-deep-embeddings</cell></row><row><cell>methods. Although challenging, it remains interesting to</cell></row><row><cell>consider this generalized zero-/few-shot learning setting in</cell></row><row><cell>our future work.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="8">Few-shot recognition comparison using top-1 classification ac-</cell></row><row><cell cols="8">curacy (%). For each test class, 3 images are randomly labeled, while the</cell></row><row><cell>rest are unlabeled.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>AwA</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>average</cell></row><row><cell>attributes</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>top-1 acc.</cell></row><row><cell></cell><cell cols="5">test data not available during training</cell><cell></cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="6">80.9 75.3 79.4 54.0 45.7 46.0</cell><cell>63.6</cell></row><row><cell>CMT [41]</cell><cell cols="6">85.1 83.4 84.3 56.7 53.4 52.0</cell><cell>69.2</cell></row><row><cell></cell><cell cols="5">test data available during training</cell><cell></cell><cell></cell></row><row><cell cols="7">DeViSE* [9] 92.6 91.1 91.3 57.5 50.7 52.9</cell><cell>72.7</cell></row><row><cell>CMT* [41]</cell><cell cols="6">90.6 90.2 91.1 62.5 54.3 55.4</cell><cell>74.0</cell></row><row><cell>ReViSE  † †</cell><cell cols="6">93.3 93.3 93.1 66.9 57.6 59.0</cell><cell>77.2</cell></row><row><cell>ReViSE  †</cell><cell cols="6">93.3 93.8 93.5 67.7 59.6 60.0</cell><cell>78.0</cell></row><row><cell>ReViSE c</cell><cell cols="6">87.8 88.7 90.2 61.1 55.3 55.0</cell><cell>73.0</cell></row><row><cell>ReViSE</cell><cell cols="6">94.2 94.1 94.4 68.4 59.9 61.7</cell><cell>78.8</cell></row><row><cell cols="5">retrieval improvement for ReViSE.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">We also observe that the performance gap between our</cell></row><row><cell cols="8">proposed ReViSE and other methods is reduced compared</cell></row><row><cell cols="8">to transductive zero-shot learning. For instance, in average</cell></row><row><cell cols="8">retrieval performance, ReViSE has 15.5% mAP improve-</cell></row><row><cell cols="8">ment over DeViSE* under zero-shot experiments, while</cell></row><row><cell cols="8">only 9.3% improvement under few-shot experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Few-shot retrieval comparison using mean Average Precision (mAP) (%). For each test class, 3 images are randomly labeled, while the rest are unlabeled.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>AwA</cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>average</cell></row><row><cell>attributes</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>att</cell><cell>w2v</cell><cell>glo</cell><cell>mAP</cell></row><row><cell></cell><cell cols="5">test data not available during training</cell><cell></cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="6">85.0 79.3 84.9 46.4 42.6 42.9</cell><cell>63.5</cell></row><row><cell>CMT [41]</cell><cell cols="6">88.4 88.2 89.2 58.5 54.0 52.7</cell><cell>71.8</cell></row><row><cell></cell><cell cols="5">test data available during training</cell><cell></cell><cell></cell></row><row><cell cols="7">DeViSE* [9] 96.7 95.5 95.8 47.5 49.2 51.6</cell><cell>72.7</cell></row><row><cell>CMT* [41]</cell><cell cols="6">95.3 94.8 95.8 60.0 54.7 56.4</cell><cell>76.2</cell></row><row><cell>ReViSE  † †</cell><cell cols="6">97.2 97.1 97.1 71.2 59.4 61.4</cell><cell>80.6</cell></row><row><cell>ReViSE  †</cell><cell cols="6">97.3 97.5 97.4 72.5 61.4 62.5</cell><cell>81.4</cell></row><row><cell>ReViSE c</cell><cell cols="6">92.3 93.0 94.6 60.8 55.0 57.1</cell><cell>75.5</cell></row><row><cell>ReViSE</cell><cell cols="6">97.8 97.7 97.8 72.9 62.8 63.0</cell><cell>82.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multicue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive zeroshot recognition via shared model space learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning cross-domain landmarks for heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H. Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5081" to="5090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4985" to="4994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Oneshot learning by inverting a compositional causal process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting deep zeroshot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot image tagging by hierarchical semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-task deep visual-semantic embedding for video thumbnail selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3707" to="3715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Oneshot learning with a hierarchical nonparametric bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning with hierarchical-deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1958" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Inductive and transductive zero-shot recognition using top-1 classification accuracy (%)</title>
		<imprint/>
	</monogr>
	<note>Table 9</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from tensorflow. org</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR 2015. 11</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
