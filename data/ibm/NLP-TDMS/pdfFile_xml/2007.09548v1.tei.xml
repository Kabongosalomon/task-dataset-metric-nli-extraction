<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kinematic 3D Object Detection in Monocular Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Kinematic 3D Object Detection in Monocular Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Object Detection</term>
					<term>Monocular</term>
					<term>Video</term>
					<term>Physics-based</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Perceiving the physical world in 3D is fundamental for selfdriving applications. Although temporal motion is an invaluable resource to human vision for detection, tracking, and depth perception, such features have not been thoroughly utilized in modern 3D object detectors. In this work, we propose a novel method for monocular video-based 3D object detection which leverages kinematic motion to extract scene dynamics and improve localization accuracy. We first propose a novel decomposition of object orientation and a self-balancing 3D confidence. We show that both components are critical to enable our kinematic model to work effectively. Collectively, using only a single model, we efficiently leverage 3D kinematics from monocular videos to improve the overall localization precision in 3D object detection while also producing useful by-products of scene dynamics (ego-motion and per-object velocity). We achieve state-of-the-art performance on monocular 3D object detection and the Bird's Eye View tasks within the KITTI self-driving dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The detection of foreground objects is among the most critical requirements to facilitate self-driving applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, 3D object detection has made significant progress <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>, even while using only a monocular camera <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. Such works primarily look at the problem from the perspective of single frames, ignoring useful temporal cues and constraints.</p><p>Computer vision cherishes inverse problems, e.g., recovering the 3D physical motion of objects from monocular videos. Motion information such as object velocity in the metric space is highly desirable for the path planning of self-driving. However, single image-based 3D object detection can not directly estimate physical motion, without relying on additional tracking modules. Therefore, videobased 3D object detection would be a sensible choice to recover such motion information. Furthermore, without modeling the physical motion, image-based 3D object detectors are naturally more likely to suffer from erratic and unnatural changes through time in orientation and localization (as exemplified in <ref type="figure">Fig. 1(a)</ref>). RPN <ref type="figure">Fig. 1</ref>. Single-frame 3D detection <ref type="bibr" target="#b1">[2]</ref> often has unstable estimation through time (a), while our video-based method (b) is more robust by leveraging kinematic motion via a 3D Kalman Filter to fuse forecasted tracks τ t and measurements b into τt.</p><p>Therefore, we aim to build a novel video-based 3D object detector which is able to provide accurate and smooth 3D object detection with per-object velocity, while also prioritizing a compact and efficient model overall.</p><p>Yet, designing an effective video-based 3D object detector has challenges. Firstly, motion which occurs in real-world scenes can come from a variety of sources such as the camera atop of an autonomous vehicle or robot, and/or from the scene objects themselves -for which most of the safety-critical objects (car, pedestrian, cyclist <ref type="bibr" target="#b13">[14]</ref>) are typically dynamic. Moreover, using video inherently involves an increase in data consumption which introduces practical challenges for training and/or inference including with memory or redundant processing.</p><p>To address such challenges, we propose a novel framework to integrate a 3D Kalman filter into a 3D detection system. We find Kalman is an ideal candidate for three critical reasons: <ref type="bibr" target="#b0">(1)</ref> it allows for use of real-world motion models to serve as a strong prior on object dynamics, <ref type="bibr" target="#b1">(2)</ref> it is inherently efficient due to its recursive nature and general absence of parameters, (3) the resultant behavior is explainable and provides useful by-products such as the object velocity.</p><p>Furthermore, we observe that objects predominantly move in the direction indicated by their orientation. Fortunately, the benefit of Kalman allows us to integrate this real-world constraint into the motion model as a compact scalar velocity. Such a constraint helps maintain the consistency of velocity over time and enables the Kalman motion forecasting and fusion to perform accurately. However, a model restricted to only move in the direction of its orientation has an obvious flaw -what if the orientation itself is inaccurate? We therefore propose a novel reformulation of orientation in favor of accuracy and stability. We find that our orientation improves the 3D localization accuracy by a margin of 2.39% and reduces the orientation error by ≈ 20%, which collectively help enable the proposed Kalman to function more effectively.</p><p>A notorious challenge of using Kalman comes in the form of uncertainty, which is conventionally <ref type="bibr" target="#b40">[41]</ref> assumed to be known and static, e.g., from a sensor. However, 3D objects in video are intuitively dependent on more complex factors of image features and cannot necessarily be treated like a sensor measurement. For a better understanding of 3D uncertainty, we propose a 3D self-balancing confidence loss. We show that our proposed confidence has higher correlation with the 3D localization performance compared to the typical classification probability, which is commonly used in detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>To complete the full understanding of the scene motion, we elect to estimate the ego-motion of the capturing camera itself. Hence, we further narrow the work of Kalman to account for only the object's motion. Collectively, our proposed framework is able to model important scene dynamics, both ego-motion and per-object velocity, and more precisely detect 3D objects in videos using a stabilized orientation and 3D confidence estimation. We demonstrate that our method achieves state-of-the-art (SOTA) performance on monocular 3D Object Detection and Bird's Eye View (BEV) tasks in the KITTI dataset <ref type="bibr" target="#b13">[14]</ref>. In summary, our contributions are as follows:</p><p>• We propose a monocular video-based 3D object detector, leveraging realistic motion constraints with an integrated ego-motion and a 3D Kalman filter. • We propose to reformulate orientation into axis, heading and offset along with a self-balancing 3D localization loss to facilitate the stability necessary for the proposed Kalman filter to perform more effectively. • Overall, using only a single model our framework develops a comprehensive 3D scene understanding including object cuboids, orientation, velocity, object motion, uncertainty, and ego-motion, as detailed in <ref type="figure">Fig. 1 and 2</ref>. • We achieve a new SOTA performance on monocular 3D object detection and BEV tasks using comprehensive metrics within the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We first provide context of our novelties from the perspective of monocular 3D object detection (Sec. 2.1) with attention to orientation and uncertainty estimation. We next discuss and contrast with video-based object detection (Sec. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Monocular 3D Object Detection</head><p>Monocular 3D object detection has made significant progress <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Early methods such as <ref type="bibr" target="#b7">[8]</ref> began by generating 3D object proposals along a ground plane using object priors and estimated point clouds, culminating in an energy minimization approach. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43]</ref> utilize additional domains of semantic segmentation, object priors, and estimated depth to improve the localization. Similarly, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref> create a pseudo-LiDAR map using SOTA depth estimator <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, which is respectfully passed into detection subnetworks or LiDAR-based 3D object detection works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. In <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref> strong 2D detection systems are extended to add cues such as object orientation, then the remaining 3D box parameters are solved via 3D box geometry. <ref type="bibr" target="#b1">[2]</ref> extends the region proposal network (RPN) of Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> with 3D box parameters.</p><p>Orientation Estimation: Prior monocular 3D object detectors estimate orientation via two main techniques. The first method is to classify orientation via a series of discrete bins then regress a relative offset <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>The bin technique requires a trade-off between the quantity/coverage of the discretized angles and an increase in the number of estimated parameters (bin ×).</p><p>Other methods directly regress the orientation angle using quaternion <ref type="bibr" target="#b37">[38]</ref> or Euler <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> angles. Direct regression is comparatively efficient, but may lead to degraded performance and periodicity challenges <ref type="bibr" target="#b45">[46]</ref>, as exemplified in <ref type="figure">Fig. 1</ref>.</p><p>In contrast, we propose a novel orientation decomposition which serves as an intuitive compromise between the bin and direct approaches. We decompose the orientation estimation into three components: axis and heading classification, followed by an angle offset. Thus, our technique increases the parameters by a static factor of 2 compared to a bin hyperparameter, while drastically reducing the offset search space for each orientation estimation (discussed in Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty Estimation:</head><p>Although it is common to utilize the classification score to rate boxes in 2D object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref> or explicitly model uncertainty as parametric estimation <ref type="bibr" target="#b19">[20]</ref>, prior works in monocular 3D object detection realize the need for 3D box uncertainty/confidence <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref>. <ref type="bibr" target="#b24">[25]</ref> defines confidence using the 3D IoU of a box and ground truth after center alignment, thus capturing the confidence primarily of the 3D object dimensions.</p><p>[38] predicts a confidence by re-mapping the 3D box loss into a probability range, which intuitively represents the confidence of the overall 3D box accuracy.</p><p>In contrast, our self-balancing confidence loss is generic and self-supervised, with two benefits. (1) It enables estimation of a 3D localization confidence using only the loss values, thus being more general than 3D IoU. (2) It enables the network to naturally re-balance extremely hard 3D boxes and focus on relatively achievable samples. Our ablation (Sec. 4.4) shows the importance of both effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video-based Object Detection</head><p>Video-based object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> is generally less studied than singleframe object detection <ref type="bibr">[5, 33-35, 44, 45]</ref>. A common trend in video-based detection is to benefit the accuracy-efficiency trade-off via reducing the frame redundancy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. Such works are applied primarily on domains of ImageNet VID 2015 <ref type="bibr" target="#b35">[36]</ref>, which contain less ego-motion from the capturing camera than self-driving scenarios <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. As such, the methods are designed to use 2D transformations, which lack the consistency and realism of 3D motion modeling.</p><p>In comparison, to our knowledge this is the first work that utilizes video cues to improve the accuracy and robustness of monocular 3D object detection. In the domain of 2D/3D object tracking, <ref type="bibr" target="#b14">[15]</ref> experiments using Kalman Filters, Particle Filters, and Gaussian Mixture Models, and observe Kalman to be the most effective aggregation method for tracking. An LSTM with depth ordering and IMU camera ego-motion is utilized in <ref type="bibr" target="#b15">[16]</ref> to improve the tracking accuracy. In contrast, we explore how to naturally and effectively leverage a 3D Kalman filter to improve the accuracy and robustness of monocular 3D object detection. We propose novel enhancements including estimating ego-motion, orientation, and a 3D confidence, while efficiently using only a single model. <ref type="figure">Fig. 2</ref>. Overview. Our framework uses a RPN to first estimate 3D boxes (Sec. 3.1). We forecast previous frame tracks τt−1 into τ t using the estimated Kalman velocity. Self-motion is compensated for applying a global ego-motion (Sec. 3.2) to tracks τ t . Lastly, we fuse τ t with measurements b using a kinematic 3D Kalman filter (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Associate / Update Apply Ego Forecast</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our proposed kinematic framework is composed of three primary components: a 3D region proposal network (RPN), ego-motion estimation, and a novel kinematic model to take advantage of temporal motion in videos. We first overview the foundations of a 3D RPN. Then we detail our contributions of orientation decomposition and self-balancing 3D confidence, which are integral to the kinematic method. Next we detail ego-motion estimation. Lastly, we present the complete kinematic framework ( <ref type="figure">Fig. 2</ref>) which carefully employs a 3D Kalman <ref type="bibr" target="#b40">[41]</ref> to model realistic motion using the aforementioned components, ultimately producing a more accurate and comprehensive 3D scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region Proposal Network</head><p>Our measurement model is founded on the 3D RPN <ref type="bibr" target="#b1">[2]</ref>, enhanced using novel orientation and confidence estimations. The RPN itself acts as a sliding window detector following the typical practices outlined in Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b1">[2]</ref>. Specifically, the RPN consists of a backbone network and a detection head which predicts 3D box outputs relative to a set of predefined anchors.</p><formula xml:id="formula_0">Anchors: We define our 2D-3D anchor Φ to consist of 2D dimensions [Φ w , Φ h ] 2D in pixels, a projected depth-buffer Φ z in meters, 3D dimensions [Φ w , Φ h , Φ l ] 3D</formula><p>in meters, and orientations with respect to two major axes</p><formula xml:id="formula_1">[Φ 0 , Φ 1 ] 3D in radians. The term Φ z is related to the camera coordinate [x, y, z] 3D by the equation Φ z · [u, v, 1] T 2D = Υ · [x, y, z, 1] T 3D where Υ ∈ R 3×4</formula><p>is a known projection matrix. We compute the anchor values by taking the mean of each parameter after clustering all ground truth objects in 2D, following the process in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Box Outputs:</head><p>Since our network is based on the principles of a RPN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>, most of the estimations are defined as a transformation T relative to an anchor. Let us define n a as the number of anchors, n c as the number of object classes, <ref type="figure">Fig. 3</ref>. Orientation. Our proposed orientation formulation decomposes an object orientationθ (a) into an axis classificationθa (b), a heading classificationθ h (c), and an offsetθr (d). Our method disentangles the objectives of axis and heading classification while greatly reducing the offset region (red) by a factor of <ref type="bibr">1 4</ref> .</p><p>and w × h as the output resolution of the network. The RPN outputs a clas-</p><formula xml:id="formula_2">sification map C ∈ R (na·nc)×w×h , then 2D transformations [T x , T y , T w , T h ] 2D , 3D transformations [T u , T v , T z , T w , T h , T l , T θr ] 3D , axis and heading [Θ a , Θ h ],</formula><p>and lastly a 3D self-balancing confidence Ω. Each output has a size of R na×w×h . The outputs can be unrolled into</p><formula xml:id="formula_3">n b = (n a · w · h) boxes with (n c + 14)-dim, with parameters of c, [t x , t y , t w , t h ] 2D , [t u , t v , t z , t w , t h , t l , t θr ] 3D , [θ a , θ h ], and ω, which relate to the maps by c ∈ C, t 2D ∈ T 2D , t 3D ∈ T 3D , θ ∈ Θ and ω ∈ Ω.</formula><p>The regression targets for 2D ground truths (GTs) [x,ŷ,ŵ,ĥ] 2D are defined as:</p><formula xml:id="formula_4">t x2D =x 2D − i Φ w2D ,t y2D =ŷ 2D − j Φ h2D ,t w2D = logŵ 2D Φ w2D ,t h2D = logĥ 2D Φ h2D ,<label>(1)</label></formula><p>where (i, j) ∈ R w×h represent the pixel coordinates of the corresponding box. Similarly, following the equation ofẑ · [û,v, 1] T 2D = Υ · [x, y, z, 1] T 3D , the regression targets for the projected 3D center GTs are defined as:</p><formula xml:id="formula_5">t u =û − i Φ w2D ,t v =v − j Φ h2D ,t z =ẑ − Φ z .<label>(2)</label></formula><p>Lastly, the regression targets for 3D dimensions GTs [ŵ,ĥ,l] 3D are defined as:</p><formula xml:id="formula_6">t w3D = logŵ 3D Φ w3D ,t h3D = logĥ 3D Φ h3D ,t l3D = logl 3D Φ l3D .<label>(3)</label></formula><p>The remaining targets for our novel orientation estimation t θr , [θ a , θ h ], and 3D self-balancing confidence ω are defined in subsequent sections.</p><p>Orientation Estimation: We propose a novel object orientation formulation, with a decomposition of three components: axis, heading, and offset ( <ref type="figure">Fig. 3)</ref>. Intuitively, the axis estimation θ a represents the probability an object is oriented towards the vertical axis (θ a = 0) or the horizontal axis (θ a = 1), with its label formally defined as:θ a = |sinθ| &lt; |cosθ|, whereθ is the ground truth object orientation in radians from a bird's eyes view (BEV) with [−π, π) bounded range. We then compute an orientationθ r with a restricted range relative to its axis, e.g., [−π, 0) whenθ a = 0, and [− π 2 , π 2 ) whenθ a = 1. We start withθ r =θ then add or subtract π fromθ r until the desired range is satisfied.</p><p>Intuitively,θ r loses its heading since the true rotation may be {θ r ,θ r ± π}. We therefore preserve the heading using a separateθ h , which represents the probability ofθ r being rotated by π with its GT target defined as:</p><formula xml:id="formula_7">θ h = 0θ =θ r 1 otherwise.<label>(4)</label></formula><p>Lastly, we encode the orientation offset transformation which is relative to the corresponding anchor, axis, and restricted orientationθ r as:t θr =θ r − Φ θa . The reverse decomposition is θ = Φ θa + θ h · π + t θr where denotes round. In designing our orientation, we first observed that the visual difference between objects at opposite headings of [θ, θ ± π] is low, especially for far objects. In contrast, classifying the axis of an object is intuitively more clear since the visual features correlate with the aspect ratio. Note that [θ a , θ h , θ r ] disentangle these two objectives. Hence, while the axis is being determined, the heading classifier can focus on subtle clues such as windshields, headlights and shape.</p><p>We further note that our 2 binary classifications have the same representational power as 4 bins following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, bins of [0, π 2 , π, 3π 2 ]. However, it is more common to use considerably more bins (such as 12 in <ref type="bibr" target="#b18">[19]</ref>). An important distinction is that bin-based approaches require the network decide axis and heading simultaneously, whereas our method disentangles the orientation into the two distinct and explainable objectives. We provide ablations to compare our decomposition and the bin method using <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref> bins in Sec. 4.4.</p><p>Self-Balancing Loss: The novel 3D localization confidence ω follows a selfbalancing formulation closely coupled to the network loss. We first define the 2D and 3D loss terms which comprise the general RPN loss. We unroll and match all n b box estimations to their respective ground truths. A box is matched as foreground when sufficient (≥ k) 2D intersection over union (IoU) is met, otherwise it is considered background (ĉ = 0) and all loss terms except for classification are ignored. The 2D box loss is thus defined as:</p><formula xml:id="formula_8">L 2D = − log(IoU(b 2D ,b 2D ))[ĉ = 0] + CE(c,ĉ),<label>(5)</label></formula><p>where CE denotes a softmax activation followed by logistic cross-entropy loss over the ground truth classĉ, and IoU uses predicted b 2D and ground truthb 2D . Similarly, the 3D localization loss for only foreground (ĉ = 0) is defined as:</p><formula xml:id="formula_9">L 3D = L 1 (t 3D ,t 3D ) + λ a · BCE([θ a , θ h ], [θ a ,θ h ]),<label>(6)</label></formula><p>where BCE denotes a sigmoid activation followed by binary cross-entropy loss. Next we define the final self-balancing confidence loss with the ω estimation as:</p><formula xml:id="formula_10">L = L 2D + ω · L 3D + λ L · (1 − ω),<label>(7)</label></formula><p>where λ L is the rolling mean of the n L most recent L 3D losses per mini-batch.</p><p>Since ω is predicted per-box via a sigmoid, the network can intuitively balance whether to use the loss of L 3D or incur a proportional penalty of λ L · (1 − ω). Hence, when the confidence is high (ω ≈ 1) we infer that the network is confident in its 3D loss L 3D . Conversely, when the confidence is low (ω ≈ 0), the network is uncertain in L 3D , thus incurring a flat penalty is preferred. At inference, we fuse the self-balancing confidence with the classification score as µ = c · ω. The proposed self-balancing loss has two key benefits. Firstly, it produces a useful 3D localization confidence with inherent correlation to 3D IoU (Sec. 4.4). Secondly, it enables the network to re-balance samples which are exceedingly challenging and re-focus on the more reasonable targets. Such a characteristic can be seen as the inverse of hard-negative mining, which is important while monocular 3D object detection remains highly difficult and unsaturated (Sec. 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ego-motion</head><p>A challenge with the dynamics of urban scenes is that not only are most foreground objects in motion, but the capturing camera itself is dynamic. Therefore, for a full understanding of the scene dynamics, we design our model to additionally predict the self-movement of the capturing camera, e.g., ego-motion.</p><p>We define ego-motion in the conventional six degrees of freedom: translation [γ x , γ y , γ z ] in meters and rotation [ρ x , ρ y , ρ z ] in radians. We attach an ego-motion features layer E with ReLU to the concatenation of two temporally adjacent feature maps χ which is the final layer in our backbone with size R n h ×w×h architecture, defined as χ t−1 || χ t . We then attach predictions for translation [Γ x , Γ y , Γ z ] and rotation [P x , P y , P z ], which are of size R w×h . Instead of using a global pooling, we predict a spatial confidence map E c ∈ R w×h based on E. We then apply softmax over the spatial dimension of E c such that E c = 1. Hence, the pooling of prediction maps [Γ, P ] into [γ, ρ] is defined as:</p><formula xml:id="formula_11">γ = (i,j) Γ (i, j) · E c (i, j), ρ = (i,j) P (i, j) · E c (i, j),<label>(8)</label></formula><p>where (i, j) is the coordinate in R w×h . We show an overview of the motion features E, spatial confidence E c , and outputs [Γ, P ] → [γ, ρ] within <ref type="figure">Fig. 2</ref>. We use a L 1 loss against GTsγ andρ defined as L ego = L 1 (γ,γ) + λ r · L 1 (ρ,ρ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Kinematics</head><p>In order to leverage temporal motion in video, we elect to integrate our RPN and ego-motion into a novel kinematic model. We adopt a 3D Kalman <ref type="bibr" target="#b40">[41]</ref> due to its notable efficiency, effectiveness, and interpretability. We next detail our proposed motion model, the procedure for forecasting, association, and update.</p><p>Motion Model: The critical variables we opt to track are defined as 3D center</p><formula xml:id="formula_12">[τ x , τ y , τ z ], 3D dimensions [τ w , τ h , τ l ], orientation [τ θ , τ θ h ]</formula><p>, and scalar velocity τ v . We define τ θ as an θ orientation constrained to the range of [− π 2 , π 2 ), and θ h as in Sec. 3.1 3 . We constrain the motion model to only allow objects to move in the direction of their orientation. Hence, we define the state transition F ∈ R 9×9 as:</p><formula xml:id="formula_13">F =         I 9×8 cos(τ θ + π τ θ h ) 0 − sin(τ θ + π τ θ h ) 0 . . . 1         ,<label>(9)</label></formula><p>where I denotes the identity matrix and the state variable order is respectively</p><formula xml:id="formula_14">[τ x , τ y , τ z , τ w , τ h , τ l , τ θ , τ θ h , τ v ].</formula><p>We constrain the velocity to only move within its orientation to simplify the Kalman to work more effectively. Recall that since our measurement model RPN processes a single frame, it does not measure velocity. Thus, to map the tracked state space and measurement space, we also define an observation model H as a truncated identity map of size I ∈ R 8×9 . We define covariance P with 3D confidence µ, as P = I 9×9 · (1 − µ) · λ o where λ o is an uncertainty weighting factor. Hence, we avoid the need to manually tune the covariance, while being dynamic to diverse and changing image content.</p><p>Forecasting: The forecasting step aims to utilize the tracked state variables and covariances of time t − 1 to estimate the state of a future time t. The equation to forecast a state variable τ t−1 into τ t is:</p><formula xml:id="formula_15">τ t = F t−1 · τ t−1 , where F t−1</formula><p>is the state transition model at t − 1. Note that both objects and the capturing camera may have independent motion between consecutive frames. Therefore, we lastly apply the estimated ego-motion to all available tracks' 3D center [τ x , τ y , τ z ] by:</p><formula xml:id="formula_16">    τ x τ y τ z 1     t = R, T 0, 1 t t−1 ·     τ x τ y τ z 1     t , τ tθ = τ tθ + ρ y<label>(10)</label></formula><p>where R t t−1 ∈ R 3×3 denotes the estimated rotation matrix converted from Euler angles and T t t−1 ∈ R 3×1 the translation vector for ego-motion (as in Sec. 3.2). Finally, we forecast a tracked object's covariance P from t − 1 to t defined as:</p><formula xml:id="formula_17">P t = F t−1 · P t−1 · F T t−1 + I 9×9 · (1 − µ t−1 ),<label>(11)</label></formula><p>where µ t−1 denotes the average self-balancing confidence µ of a track's life. Hence, the resultant track states τ t and track covariances P t represent the Kalman filter's best forecasted estimation with respect to frame t.</p><p>Association: After the tracks have been forecasted from t − 1 to t, the next step is to associate tracks to corresponding 3D box measurements (Sec. 3.1). Let us denote boxes produced by the measurement RPN as b ∈ R 8 mimicking the tracked state as</p><formula xml:id="formula_18">[b x , b y , b z , b w , b h , b l , b θ , b θ h ] 4 .</formula><p>Our matching strategy consists of two steps. We first compute the 3D center distance between the tracks τ t and measurements b. The best matches with the lowest distance are iteratively paired and removed until no pairs remain with distance ≤ k d . Then we compute the projected 2D box IoU between any remaining tracks τ t and measurements b.</p><p>The best matches with the highest IoU are also iteratively paired and removed until no pairs remain with IoU ≥ k u . Measured boxes that were not matched are added as new tracks. Conversely, tracks that were not matched incur a penalty with hyperparameter k p , defined as µ t−1 = µ t−1 · k p . Lastly, any box who has confidence µ t−1 ≤ k m is removed from the valid tracks.</p><p>Update: After making associations between tracks τ t and measurements b, the next step is to utilize the track covariance P t and measured confidence µ to update each track to its final state τ t and covariance P t . Firstly, we formally define the equation for computing the Kalman gain as:</p><formula xml:id="formula_19">K = P H T (H P H T + I 8×8 (1 − µ) · λ o ) −1 ,<label>(12)</label></formula><p>where I 8×8 (1 − µ) · λ o represents the incoming measurement covariance matrix, and P the forecasted covariance of the track. Next, given the Kalman gain K, forecasted state τ t , forecasted covariance P t , and measured box b, the final track state τ t and covariance P t are defined as:</p><formula xml:id="formula_20">τ t = τ t + K (b − H τ t ), P t = (I 9×9 − K H) P t .<label>(13)</label></formula><p>We lastly aggregate each track's overall confidence µ t over time as a running average of µ t = 1 2 · (µ t−1 + µ), where µ is the measured confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our framework is implemented in PyTorch <ref type="bibr" target="#b29">[30]</ref>, with the 3D RPN settings of <ref type="bibr" target="#b1">[2]</ref>. We release source code at http://cvlab.cse.msu.edu/project-kinematic.html. We use a batch size of 2 and learning rate of 0.004. We set k = 0.5, λ o = 0.2, λ r = 40, n L = 100, λ a = k u = 0.35, k d = 0.5, k p = 0.75, and k m = 0.05.</p><p>To ease training, we implement three phases. We first train the 2D-3D RPN with L = L 2D + L 3D , then the self-balancing loss of Eq. 7, for 80k and 50k iterations. We freeze the RPN to train ego-motion using L ego for 80k. Our backbone is DenseNet121 <ref type="bibr" target="#b16">[17]</ref> where n h = 1,024. Inference uses 4 frames as provided by <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We benchmark our kinematic framework on the KITTI <ref type="bibr" target="#b13">[14]</ref> dataset. We comprehensively evaluate on 3D Object Detection and Bird's Eye View (BEV) tasks. We then provide ablation experiments to better understand the effects and justification of our core methodology. We show qualitative examples in <ref type="figure" target="#fig_1">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KITTI Dataset</head><p>The KITTI <ref type="bibr" target="#b13">[14]</ref> dataset is a popular benchmark for self-driving tasks. The official dataset consists of 7,481 training and 7,518 testing images including annotations for 2D/3D objects, ego-motion, and 4 temporally adjacent frames. We evaluate on the most widely used validation split as proposed in <ref type="bibr" target="#b7">[8]</ref>, which consists of 3,712 training and 3,769 validation images. We focus primarily on the car class.</p><p>Metric: Average precision (AP) is utilized for object detection in KITTI.</p><p>Following <ref type="bibr" target="#b37">[38]</ref>, the KITTI metric has updated to include 40 (↑ 11) recall points while skipping the first. The AP 40 metric is more stable and fair overall <ref type="bibr" target="#b37">[38]</ref>. Due to the official adoption of AP 40 , it is not possible to compute AP 11 on test. Hence, we elect to use the AP 40 metric for all reported experiments.  <ref type="table">Table 2</ref>. KITTI Validation. We compare with SOTA on KITTI validation <ref type="bibr" target="#b7">[8]</ref> split. Note that methods published prior to <ref type="bibr" target="#b37">[38]</ref> are unable to report the AP40 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Object Detection</head><p>We evaluate our proposed framework on the task of 3D object detection, which requires objects be localized in 3D camera coordinates as well as supplying the 3D dimensions and BEV orientation relative to the XZ plane. Due to the strict requirements of IoU in three dimensions, the task demands precise localization of an object to be considered a match (3D IoU ≥ 0.7). We evaluate our performance on the official test <ref type="bibr" target="#b13">[14]</ref> dataset in Tab. 1 and the validation <ref type="bibr" target="#b7">[8]</ref> split in Tab. 2. We emphasize that our method improves the SOTA on KITTI test by a significant margin of ↑ 1.98% compared to <ref type="bibr" target="#b26">[27]</ref> on the moderate configuration with IoU ≥ 0.7, which is the most common metric used to compare. Further, we note that <ref type="bibr" target="#b26">[27]</ref> require multiple encoder-decoder networks which add overhead compared to our single network approach. Hence, their runtime is ≈ 3× (Tab. 1) compared to ours, self-reported on similar but not identical GPU hardware. Moreover, <ref type="bibr" target="#b1">[2]</ref> is the most comparable method to ours as both utilize a single network and an RPN archetype. We note that our method significantly outperforms <ref type="bibr" target="#b1">[2]</ref> and many other recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref> by ≈ 3.01 − 11.21%.</p><p>We further evaluate our approach on the KITTI validation <ref type="bibr" target="#b7">[8]</ref> split using the AP 40 for available approaches and observe similar overall trends as in Tab. 2. For instance, compared to competitive approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref> our method improves the performance by ↑ 3.03% for the challenging IoU criteria of ≥ 0.7. Similarly, our performance on the more relaxed criteria of IoU ≥ 0.5 increases by ↑ 3.53%. We additionally visualize detailed performance characteristics on AP 3D at discrete depth <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">All]</ref>   <ref type="table">Table 3</ref>. Ablation Experiments. We conduct a series of ablation experiments with the validation <ref type="bibr" target="#b7">[8]</ref> split of KITTI, using diverse IoU matching criteria of ≥ 0.7/0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bird's Eye View</head><p>The Bird's Eye View (BEV) task is similar to 3D object detection, differing primarily in that the 3D boxes are firstly projected into the XZ plane then 2D object detection is calculated. The projection collapses the Y-axis degree of freedom and intuitively results in a less precise but reasonable localization. We note that our method achieves SOTA performance on the BEV task regarding the moderate setting of the KITTI test dataset as detailed in Tab. 1. Our method performs favorably compared with SOTA works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref> (e.g., ≈ 3.85 − 14.29%), and similarly to <ref type="bibr" target="#b26">[27]</ref> at a notably lower runtime cost. We suspect that our method, especially the self-balancing confidence (Eq. 7), prioritizes precise localization which warrants more benefit in full 3D Object Detection task compared to the Bird's Eye View task.</p><p>Our method performs similarly on the validation <ref type="bibr" target="#b7">[8]</ref> split of KITTI (Tab. 2). Specifically, compared to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref> our proposed method outperforms by a range of ≈ 4.10−7.14%, which is consistent to the same methods on test ≈ 3.85−4.33%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To better understand the characteristics of our proposed kinematic framework, we perform a series of ablation experiments and analysis, summarized in Tab. 3. We adopt <ref type="bibr" target="#b1">[2]</ref> without hill-climbing or depth-aware layers as our baseline method. Unless otherwise specified we use the experimental settings outlined in Sec. 3.4.</p><p>Orientation Improvement: The orientation of objects is intuitively a critical component when modeling motion. When the orientation is decomposed into axis, heading, and offset the overall performance significantly improves, e.g., by ↑ 2.39% in AP 3D and ↑ 3.45% in AP BEV , as detailed within Tab. 3. We compute the mean angle error of our baseline, orientation decomposition, and kinematics method which respectively achieve 13.4 • , 10.9 • , and 6.1 • (↓ 54.48%), suggesting our proposed methodology is significantly more stable.</p><p>We compare our orientation decomposition to bin-based methods following general idea of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. We specifically change our orientation definition into [θ b , θ o ] which includes a bin classification and an offset. We experiment with the number of bins set to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref> which are uniformly spread from [0, 2π). Note that 4 bins have the same representational power as using binary [θ a , θ h ]. We observe that the ablated bin-based methods achieve [9.47%, 10.02%, 10.76%] in AP 3D . In comparison, our decomposed orientation achieves 12.10% in AP 3D . We provide additional detailed experiments in our supplemental material. <ref type="figure">Fig. 4</ref>. We compare AP3D with <ref type="bibr" target="#b1">[2]</ref> by varying 3D IoU criteria and depth. Further, we find that our proposed kinematic motion model (as in Sec. 3.3) degrades in performance when a comparatively erratic baseline (Row 1. Tab. 3) orientation is utilized instead (14.10 → 11.47 on AP 3D ), reaffirming the importance of having a consistent/stable orientation when aggregating through time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-balancing Confidence:</head><p>We observe that the self-balancing confidence is important from two key respects. Firstly, its integration in Eq. 7 enables the network to re-weight box samples to focus more on reasonable samples and incur a flat penalty (e.g., λ L · (1 − ω) of Eq. 7) on the difficult samples. In a sense, the self-balancing confidence loss is the inverse of hard-negative mining, allowing the network to focus on reasonable estimations. Hence, the loss on its own improves performance for AP 3D by ↑ 0.67% and AP BEV by ↑ 0.59%.</p><p>The second benefit of self-balancing confidence is that by design ω has an inherent correlation with the 3D object detection performance. Recall that we fuse ω with the classification score c to produce a final box rating of µ = c · ω, which results in an additional gain of ↑ 0.78% in AP 3D and ↑ 0.80% in AP BEV . We further analyze the correlation of µ with 3D IoU, as is summarized in <ref type="figure" target="#fig_0">Fig. 5</ref>. The correlation coefficient with the classification score c is significantly lower than the correlation using µ instead (0.301 vs. 0.417). In summary, the use of the Eq. 7 and µ account for a gain of ↑ 1.45% in AP 3D and ↑ 1.39% in AP BEV .</p><p>Temporal Modeling: The use of video and kinematics is a significant motivating factor for this work. We find that the use of kinematics (detailed in Sec. 3.3) results in a gain of ↑ 0.55% in AP 3D and ↑ 0.90% in AP BEV , as shown in Tab. 3. We emphasize that although the improvement is less dramatic versus orientation and self-confidence, the former are important to facilitate temporal modeling. We find that if orientation decomposition and uncertainty are removed, by using the baseline orientation and setting µ to be a static constant, then the kinematic performance drastically reduces from 14.10% → 10.64% in AP 3D .</p><p>We emphasize that kinematic framework not only helps 3D object detection, but also naturally produces useful by-products such as velocity and ego-motion. Thus, we evaluate the respective average errors of each motion after applying the camera capture rate to convert the motion into miles per hour (MPH). We find that the per-object velocity and ego-motion speed errors perform reasonably at 7.036 MPH and 6.482 MPH respectively. We depict visual examples of all dynamic by-products in <ref type="figure" target="#fig_1">Fig. 6</ref> and additionally in supplemental video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present a novel kinematic 3D object detection framework which is able to efficiently leverage temporal cues and constraints to improve 3D object detection. Our method naturally provides useful by-products regarding scene dynamics, e.g., reasonably accurate ego-motion and per-object velocity. We further propose novel designs of orientation estimation and a self-balancing 3D confidence loss in order to enable the proposed kinematic model to work effectively. We emphasize that our framework efficiently uses only a single network to comprehensively understand a highly dynamic 3D scene for urban autonomous driving. Moreover, we demonstrate our method's effectiveness through detailed experiments on the KITTI <ref type="bibr" target="#b13">[14]</ref> dataset across the 3D object detection and BEV tasks.  <ref type="table" target="#tab_0">Table 1</ref>. Orientation. We compare our orientation decomposition to bin-based orientation following the high-level concepts within <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, using AP3D and APBEV. We evaluate our performances on the KITTI validation set <ref type="bibr" target="#b7">[8]</ref> using IoU ≥ 0.7/0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Orientation Ablations</head><p>We provide detailed experiments on 3D object detection and Bird's Eye View tasks to compare our orientation decomposition performance with bin-based approaches such as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> within Tab. 1. Recall that bin-based orientation first classifies the best bin for orientation then predicts an offset with respect to the bin. In contrast, our method disentangles the bin classification into a distinct explainable objectives such as an axis classification and a heading classification. For such experiments we change our formulation to use bins of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>, where 4 bins has a similar representational power as two binary classifications [θ a , θ h ]. The bins are spread uniformly from [0, 2π] and an offset is predicted afterwards. We use the settings in Sec. 3.4 in main paper. We emphasize that our method outperforms the bin-based approaches between ≈ 1.36 − 2.63% on AP 3D and ≈ 2.06 − 2.71% on AP BEV using the standard moderate setting and IoU ≥ 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kalman Forecasting</head><p>Since our method uses ego-motion and a 3D Kalman filter to aggregate temporal information, the approach can be modified to act as a box forecaster. Although our method was not strictly designed for the tracking and forecasting task, we evaluate the 3D object detection and Bird's Eye View performance after forecasting n f = [1, 2, 3, 4] frames into the future. We assume a static ego-motion for unknown frames and otherwise use the Kalman equations described in the main paper Sec. 3.3 to forecast the tracked boxes.</p><p>For all forecasting experiments we process 4 temporally adjacent frames before forecasting. Since KITTI only provides a current frame and 3 proceeding frames, we carefully map images back to the raw dataset in order to forecast. For instance, when n f = 2 we infer using frames [−5, −4, −3, −2] then forecast ego-motion and Kalman n f times. We then evaluate with respect to frame 0 which is the standard timestamp KITTI provides images and 3D labels for. We provide detailed performances on AP 3D in Tab. 2 and AP BEV in Tab. 3. We find that the forecasting performance degrades through time but performs reasonably 1 − 2 frames ahead, being competitive in magnitude to state-of-the-art methods on the KITTI test dataset as reported in Tab. 1 of the main paper. For instance, forecasting 1 and 2 frames results in 10.64% and 5.10% AP 3D respectively, which are competitive to methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>   <ref type="table">Table 3</ref>. Forecasting -Bird's Eye View. We evaluate our forecasting performance on APBEV within the KITTI validation <ref type="bibr" target="#b7">[8]</ref> set and using IoU ≥ 0.7/0.5/0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Qualitative Video</head><p>We further provide a qualitative demonstration video at http://cvlab.cse.msu. edu/project-kinematic.html. The video demonstrates our framework's ability to determine a full scene understanding including 3D object cuboids, per-object velocity and ego-motion. We compare to a related monocular work of M3D-RPN <ref type="bibr" target="#b1">[2]</ref>, plot ground truths, image view, Bird's Eye View, and the track history.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>We show the correlation of 3D IoU to classification c and 3D confidence µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative Examples. We depict the image view (left) and BEV (right). We show velocity vector in green, speed and ego-motion in miles per hour (MPH) on top of detection boxes and at the top-left corner, and tracks as dots in BEV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>KITTI Test. We compare with SOTA methods on the KITTI test dataset. We report performances using the AP40<ref type="bibr" target="#b37">[38]</ref> metric available on the official leaderboard. * the runtime is reported from the official leaderboard with slight variances in hardware. We indicate methods reported on CPU with †. Bold/italics indicate best/second AP.<ref type="bibr" target="#b13">14</ref> .53 /48 .56 11 .07 /35 .94 8 .65 /28 .59 20 .85 /53 .35 15 .62 /39 .60 11 .88 /31 .77 Ours 19.76/55.44 14.10/39.47 10.47/31.26 27.83/61.79 19.72/44.68 15.10/34.56</figDesc><table><row><cell></cell><cell cols="3">AP3D (IoU ≥ 0.7) Easy Mod Hard</cell><cell cols="4">APBEV (IoU ≥ 0.7) Easy Mod Hard</cell><cell>s/im  *</cell></row><row><cell>FQNet [25]</cell><cell>2.77</cell><cell>1.51</cell><cell>1.01</cell><cell>5.40</cell><cell></cell><cell>3.23</cell><cell>2.46</cell><cell>0.50  †</cell></row><row><cell>ROI-10D [28]</cell><cell>4.32</cell><cell>2.02</cell><cell>1.46</cell><cell>9.78</cell><cell></cell><cell>4.91</cell><cell>3.74</cell><cell>0.20</cell></row><row><cell>GS3D [22]</cell><cell>4.47</cell><cell>2.90</cell><cell>2.47</cell><cell>8.41</cell><cell></cell><cell>6.08</cell><cell>4.94</cell><cell>2.00  †</cell></row><row><cell>MonoPSR [19]</cell><cell>10.76</cell><cell>7.25</cell><cell>5.85</cell><cell>18.33</cell><cell cols="2">12.58</cell><cell>9.91</cell><cell>0.20</cell></row><row><cell>MonoDIS [38]</cell><cell>10.37</cell><cell>7.94</cell><cell>6.40</cell><cell>17.23</cell><cell cols="2">13.19</cell><cell>11.12</cell><cell>−</cell></row><row><cell>M3D-RPN [2]</cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell><cell>21.02</cell><cell cols="2">13.67</cell><cell>10.23</cell><cell>0 .16</cell></row><row><cell>AM3D [27]</cell><cell>16 .50</cell><cell>10 .74</cell><cell>9.52</cell><cell>25 .03</cell><cell cols="2">17 .32</cell><cell>14.91</cell><cell>0.40</cell></row><row><cell>Ours</cell><cell>19.07</cell><cell>12.72</cell><cell>9 .17</cell><cell>26.69</cell><cell cols="2">17.52</cell><cell>13 .10</cell><cell>0.12</cell></row><row><cell></cell><cell cols="3">AP3D (IoU ≥ [0.7/0.5])</cell><cell cols="5">APBEV (IoU ≥ [0.7/0.5])</cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell></cell><cell></cell><cell>Mod</cell><cell>Hard</cell></row><row><cell cols="2">MonoDIS [38] 11.06/ −</cell><cell>7.60/ −</cell><cell>6.37/ −</cell><cell cols="2">18.45/ −</cell><cell cols="2">12.58/ −</cell><cell>10.66/ −</cell></row><row><cell>M3D-RPN [2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>meters and IoU matching criterias 0.3 → 0.7 in Fig. 4. = c · ω 18 .28 /54 .70 13 .55 /39 .33 10 .13 /31.25 25 .72 /60 .87 18 .82 /44 .36 14 .48 /34.48 + kinematics 19.76/55.44 14.10/39.47 10.47/31 .26 27.83/61.79 19.72/44.68 15.10/34 .56</figDesc><table><row><cell></cell><cell cols="3">AP3D (IoU ≥ [0.7/0.5])</cell><cell cols="3">APBEV (IoU ≥ [0.7/0.5])</cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell></row><row><cell>Baseline</cell><cell>13.81/47.10</cell><cell>9.71/34.14</cell><cell cols="3">7.44/26.90 20.08/52.57 13.98/38.45</cell><cell>11.10/29.88</cell></row><row><cell cols="3">+ θ decomposition 16.66/51.47 12.10/38.58</cell><cell cols="4">9.40/30.98 23.15/56.48 17.43/42.53 13.48/34.37</cell></row><row><cell>+ self-confidence</cell><cell cols="2">16.64/52.18 12.77/38.99</cell><cell cols="4">9.60/31.42 24.22/58.52 18.02/42.95 13.92/34.80</cell></row><row><cell>+ µ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Michigan State University, Computer Science &amp; Engineering 2 Max Planck Institute for Informatics, Saarland Informatics Campus 1 {brazilga, liuxm}@msu.edu, 2 {gpons, schiele}@mpi-inf.mpg.de 46.46 9.47/33.78 7.93/26.85 19.17/52.06 14.72/37.54 11.38/31.16 4 bins 12.65/44.01 10.02/33.27 7.87/26.27 19.09/49.86 14.55/37.90 11.14/30.44 10 bins 14.27/49.71 10.74/36.12 8.29/28.62 21.12/54.70 15.37/39.72 11.60/31.75 Our decomp. 16.66/51.47 12.10/38.58 9.40/30.98 23.15/56.48 17.43/42.53 13.48/34.37</figDesc><table><row><cell></cell><cell cols="3">AP3D (IoU ≥ [0.7/0.5])</cell><cell cols="3">APBEV (IoU ≥ [0.7/0.5])</cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell></row><row><cell>2 bins</cell><cell>12.83/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>on test. / 28.97 / 58.46 2.32 / 18.05 / 37.82 1.75 / 13.88 / 29.80 Forecast → 2 7.84 / 39.40 / 68.87 5.10 / 25.48 / 48.30 4.14 / 20.20 / 37.84 Forecast → 1 16.09 / 49.66 / 75.88 10.64 / 34.18 / 55.26 8.14 / 26.62 / 44.01 No Forecast 19.76 / 55.44 / 79.81 14.10 / 39.47 / 60.57 10.47 / 31.26 / 48.95 Table 2. Forecasting -3D Object Detection. We evaluate our forecasting performance on AP3D within the KITTI validation [8] set and using IoU ≥ 0.7/0.5/0.3. / 29.40 / 54.52 3.54 / 18.13 / 36.13 2.90 / 14.71 / 28.49 Forecast → 3 11.03 / 39.08 / 64.87 6.89 / 24.01 / 43.52 5.67 / 18.85 / 34.91 Forecast → 2 17.02 / 47.07 / 72.33 10.76 / 31.62 / 51.67 8.37 / 25.47 / 40.79 Forecast → 1 23.58 / 55.99 / 77.48 15.79 / 39.33 / 58.05 12.54 / 31.22 / 46.59 No Forecast 27.83 / 61.79 / 81.20 19.72 / 44.68 / 63.44 15.10 / 34.56 / 49.84</figDesc><table><row><cell></cell><cell cols="2">AP3D (IoU ≥ [0.7/0.5/0.3])</cell><cell></cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell></row><row><cell>Forecast → 4</cell><cell>1.16 / 18.47 / 47.26</cell><cell>0.84 / 11.21 / 29.22</cell><cell>0.62 / 8.97 / 23.40</cell></row><row><cell>Forecast → 3</cell><cell cols="3">3.72 APBEV (IoU ≥ [0.7/0.5/0.3])</cell></row><row><cell></cell><cell>Easy</cell><cell>Mod</cell><cell>Hard</cell></row><row><cell>Forecast → 4</cell><cell>5.48</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We do not use the axis θa of Sec. 3.1, since we expect the orientation to change smoothly and do not wish the orientation is relative to a (potentially) changing axis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We apply the estimated transformations of Sec. 3.1 to their respective anchors with equations of<ref type="bibr" target="#b1">[2]</ref>, and backproject into 3D coordinates to match track variables.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments: Research was partially sponsored by the Army Research</head><p>Office under Grant Number W911NF-18-1-0330. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work is further partly funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -409792180 (Emmy Noether Programme, project: Real Virtual Humans).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Kinematic 3D Object</head><p>Detection in Monocular Video</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">M3D-RPN: Monocular 3D region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pedestrian detection with autoregressive network phases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep MANTA: A coarse-to-fine many-task network for joint 2D and 3D vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Monocular 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast point R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging shape completion for 3D siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Joint monocular 3D vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS. IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular 3D object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LUVLi face alignment: Estimating landmarks location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GS3D: An efficient 3D object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-task multi-sensor fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep continuous fusion for multi-sensor 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep fitting degree scoring network for monocular 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Accurate monocular 3D object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ROI-10D: Monocular lifting of 2D detection to 6D pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">PointRCNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Task-aware monocular depth estimation for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pseudo-LiDAR from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An introduction to the kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3D object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">STD: Sparse-to-dense 3D object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. IEEE</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
