<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">NVIDIA and Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasingly impressive results of generative adversarial networks (GAN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> are fueled by the seemingly unlimited supply of images available online. Still, it remains challenging to collect a large enough set of images for a specific application that places constraints on subject type, image quality, geographical location, time period, privacy, copyright status, etc. The difficulties are further exacerbated in applications that require the capture of a new, custom dataset: acquiring, processing, and distributing the ∼ 10 5 − 10 6 images required to train a modern high-quality, high-resolution GAN is a costly undertaking. This curbs the increasing use of generative models in fields such as medicine <ref type="bibr" target="#b46">[47]</ref>. A significant reduction in the number of images required therefore has the potential to considerably help many applications.</p><p>The key problem with small datasets is that the discriminator overfits to the training examples; its feedback to the generator becomes meaningless and training starts to diverge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>. In almost all areas of deep learning <ref type="bibr" target="#b39">[40]</ref>, dataset augmentation is the standard solution against overfitting. For example, training an image classifier under rotation, noise, etc., leads to increasing invariance to these semantics-preserving distortions -a highly desirable quality in a classifier <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. In contrast, a GAN trained under similar dataset augmentations learns to generate the augmented distribution <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53]</ref>. In general, such "leaking" of augmentations to the generated samples is highly undesirable. For example, a noise augmentation leads to noisy results, even if there is none in the dataset.</p><p>In this paper, we demonstrate how to use a wide range of augmentations to prevent the discriminator from overfitting, while ensuring that none of the augmentations leak to the generated images. We start by presenting a comprehensive analysis of the conditions that prevent the augmentations from leaking. We then design a diverse set of augmentations, and an adaptive control scheme that enables the same approach to be used regardless of the amount of training data, properties of the dataset, or the exact training setup (e.g., training from scratch or transfer learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34]</ref>). We demonstrate, on several datasets, that good results are now possible using only a few thousand images, often matching StyleGAN2 results with an order of magnitude fewer images. Furthermore, we show that the popular CIFAR-10 benchmark suffers from limited data and achieve a new record Fréchet inception distance (FID) <ref type="bibr" target="#b17">[18]</ref> of 2.42, significantly improving over the current state of the art of 5.59 <ref type="bibr" target="#b51">[52]</ref>. We also present METFACES, a high-quality benchmark dataset for limited data scenarios. Our implementation and models are available at https://github.com/NVlabs/stylegan2-ada 2 Overfitting in GANs</p><p>We start by studying how the quantity of available training data affects GAN training. We approach this by artificially subsetting larger datasets (FFHQ and LSUN CAT) and observing the resulting dynamics. For our baseline, we considered StyleGAN2 <ref type="bibr" target="#b20">[21]</ref> and BigGAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. Based on initial testing, we settled on StyleGAN2 because it provided more predictable results with significantly lower variance between training runs (see Appendix A). For each run, we randomize the subset of training data, order of training samples, and network initialization. To facilitate extensive sweeps over dataset sizes and hyperparameters, we use a downscaled 256 × 256 version of FFHQ and a lighter-weight configuration that reaches the same quality as the official StyleGAN2 config F for this dataset, but runs 4.6× faster on NVIDIA DGX-1. <ref type="bibr" target="#b0">1</ref> We measure quality by computing FID between 50k generated images and all available training images, as recommended by Heusel et al. <ref type="bibr" target="#b17">[18]</ref>, regardless of the subset actually used for training. <ref type="figure" target="#fig_0">Figure 1a</ref> shows our baseline results for different subsets of FFHQ. Training starts the same way in each case, but eventually the progress stops and FID starts to rise. The less training data there is, the earlier this happens. <ref type="figure" target="#fig_0">Figure 1b</ref>,c shows the discriminator output distributions for real and generated images during training. The distributions overlap initially but keep drifting apart as the discriminator becomes more and more confident, and the point where FID starts to deteriorate is consistent with the loss of sufficient overlap between distributions. This is a strong indication of overfitting, evidenced further by the drop in accuracy measured for a separate validation set. We propose a way to tackle this problem by employing versatile augmentations that prevent the discriminator from becoming overly confident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stochastic discriminator augmentation</head><p>By definition, any augmentation that is applied to the training dataset will get inherited to the generated images <ref type="bibr" target="#b13">[14]</ref>. Zhao et al. <ref type="bibr" target="#b52">[53]</ref> recently proposed balanced consistency regularization (bCR) as a solution that is not supposed to leak augmentations to the generated images. Consistency regularization states that two sets of augmentations, applied to the same input image, should yield the same output <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27]</ref>. Zhao et al. add consistency regularization terms for the discriminator loss, and enforce discriminator consistency for both real and generated images, whereas no augmentations or consistency loss terms are applied when training the generator <ref type="figure">(Figure 2a</ref>). As such, their approach</p><formula xml:id="formula_0">D Latents G -f (-x)</formula><p>Aug Aug p (b) Ours p = 0 p = 0.1 p = 0.2 p = 0.3 p = 0.5 p = 0.8 (c) Effect of augmentation probability p <ref type="figure">Figure 2</ref>: (a,b) Flowcharts for balanced consistency regularization (bCR) <ref type="bibr" target="#b52">[53]</ref> and our stochastic discriminator augmentations. The blue elements highlight operations related to augmentations, while the rest implement standard GAN training with generator G and discriminator D <ref type="bibr" target="#b13">[14]</ref>. The orange elements indicate the loss function and the green boxes mark the network being trained. We use the non-saturating logistic loss <ref type="bibr" target="#b13">[14]</ref> f (x) = log (sigmoid(x)). (c) We apply a diverse set of augmentations to every image that the discriminator sees, controlled by an augmentation probability p.</p><p>effectively strives to generalize the discriminator by making it blind to the augmentations used in the CR term. However, meeting this goal opens the door for leaking augmentations, because the generator will be free to produce images containing them without any penalty. In Section 4, we show experimentally that bCR indeed suffers from this problem, and thus its effects are fundamentally similar to dataset augmentation.</p><p>Our solution is similar to bCR in that we also apply a set of augmentations to all images shown to the discriminator. However, instead of adding separate CR loss terms, we evaluate the discriminator only using augmented images, and do this also when training the generator <ref type="figure">(Figure 2b</ref>). This approach that we call stochastic discriminator augmentation is therefore very straightforward. Yet, this possibility has received little attention, possibly because at first glance it is not obvious if it even works: if the discriminator never sees what the training images really look like, it is not clear if it can guide the generator properly ( <ref type="figure">Figure 2c</ref>). We will therefore first investigate the conditions under which this approach will not leak an augmentation to the generated images, and then build a full pipeline out of such transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Designing augmentations that do not leak</head><p>Discriminator augmentation corresponds to putting distorting, perhaps even destructive goggles on the discriminator, and asking the generator to produce samples that cannot be distinguished from the training set when viewed through the goggles. Bora et al. <ref type="bibr" target="#b3">[4]</ref> consider a similar problem in training GANs under corrupted measurements, and show that the training implicitly undoes the corruptions and finds the correct distribution, as long as the corruption process is represented by an invertible transformation of probability distributions over the data space. We call such augmentation operators non-leaking.</p><p>The power of these invertible transformations is that they allow conclusions about the equality or inequality of the underlying sets to be drawn by observing only the augmented sets. It is crucial to understand that this does not mean that augmentations performed on individual images would need to be undoable. For instance, an augmentation as extreme as setting the input image to zero 90% of the time is invertible in the probability distribution sense: it would be easy, even for a human, to reason about the original distribution by ignoring black images until only 10% of the images remain. On the other hand, random rotations chosen uniformly from {0 • , 90 • , 180 • , 270 • } are not invertible: it is impossible to discern differences among the orientations after the augmentation.</p><p>The situation changes if this rotation is only executed at a probability p &lt; 1: this increases the relative occurrence of 0 • , and now the augmented distributions can match only if the generated images have correct orientation. Similarly, many other stochastic augmentations can be designed to be non-leaking on the condition that they are skipped with a non-zero probability. Appendix C shows that this can be made to hold for a large class of widely used augmentations, including deterministic mappings (e.g., basis transformations), additive noise, transformation groups (e.g, image or color space rotations, flips and scaling), and projections (e.g., cutout <ref type="bibr" target="#b10">[11]</ref>). Furthermore, composing non-leaking augmentations in a fixed order yields an overall non-leaking augmentation.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref> we validate our analysis by three practical examples. Isotropic scaling with log-normal distribution is an example of an inherently safe augmentation that does not leak regardless of the value of p ( <ref type="figure" target="#fig_1">Figure 3a</ref>). However, the aforementioned rotation by a random multiple of 90 • must be skipped at least part of the time <ref type="figure" target="#fig_1">(Figure 3b</ref>). When p is too high, the generator cannot know which way the generated images should face and ends up picking one of the possibilities at random. As could be expected, the problem does not occur exclusively in the limiting case of p = 1. In practice, the training setup is poorly conditioned for nearby values as well due to finite sampling, finite representational power of the networks, inductive bias, and training dynamics. When p remains below ∼ 0.85, the generated images are always oriented correctly. Between these regions, the generator sometimes picks a wrong orientation initially, and then partially drifts towards the correct distribution. The same observations hold for a sequence of continuous color augmentations ( <ref type="figure" target="#fig_1">Figure 3c</ref>). This experiment suggests that as long as p remains below 0.8, leaks are unlikely to happen in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our augmentation pipeline</head><p>We start from the assumption that a maximally diverse set of augmentations is beneficial, given the success of RandAugment <ref type="bibr" target="#b8">[9]</ref> in image classification tasks. We consider a pipeline of 18 transformations that are grouped into 6 categories: pixel blitting (x-flips, 90 • rotations, integer translation), more general geometric transformations, color transforms, image-space filtering, additive noise <ref type="bibr" target="#b40">[41]</ref>, and cutout <ref type="bibr" target="#b10">[11]</ref>. Details of the individual augmentations are given in Appendix B. Note that we execute augmentations also when training the generator <ref type="figure">(Figure 2b</ref>), which requires the augmentations to be differentiable. We achieve this by implementing them using standard differentiable primitives offered by the deep learning framework.</p><p>During training, we process each image shown to the discriminator using a pre-defined set of transformations in a fixed order. The strength of augmentations is controlled by the scalar p ∈ [0, 1], so that each transformation is applied with probability p or skipped with probability 1 − p. We always use the same value of p for all transformations. The randomization is done separately for each augmentation and for each image in a minibatch. Given that there are many augmentations in the pipeline, even fairly small values of p make it very unlikely that the discriminator sees a clean image ( <ref type="figure">Figure 2c</ref>). Nonetheless, the generator is guided to produce only clean images as long as p remains below the practical safety limit.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> we study the effectiveness of stochastic discriminator augmentation by performing exhaustive sweeps over p for different augmentation categories and dataset sizes. We observe that it can improve the results significantly in many cases. However, the optimal augmentation strength depends heavily on the amount of training data, and not all augmentation categories are equally useful in practice. With a 2k training set, the vast majority of the benefit came from pixel blitting and geometric transforms. Color transforms were modestly beneficial, while image-space filtering, noise, and cutout were not particularly useful. In this case, the best results were obtained using strong augmentations. The curves also indicate some of the augmentations becoming leaky when p → 1.</p><p>With a 10k training set, the higher values of p were less helpful, and with 140k the situation was markedly different: all augmentations were harmful. Based on these results, we choose to use only pixel blitting, geometric, and color transforms for the rest of our tests. <ref type="figure" target="#fig_2">Figure 4d</ref> shows that while stronger augmentations reduce overfitting, they also slow down the convergence.</p><p>In practice, the sensitivity to dataset size mandates a costly grid search, and even so, relying on any fixed p may not be the best choice. Next, we address these concerns by making the process adaptive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive discriminator augmentation</head><p>Ideally, we would like to avoid manual tuning of the augmentation strength and instead control it dynamically based on the degree of overfitting. <ref type="figure" target="#fig_0">Figure 1</ref> suggests a few possible approaches for this. The standard way of quantifying overfitting is to use a separate validation set and observe its behavior relative to the training set. From the figure we see that when overfitting kicks in, the validation set starts behaving increasingly like the generated images. This is a quantifiable effect, albeit with the drawback of requiring a separate validation set when training data may already be in short supply. We can also see that with the non-saturating loss <ref type="bibr" target="#b13">[14]</ref> used by StyleGAN2, the discriminator outputs for real and generated images diverge symmetrically around zero as the situation gets worse. This divergence can be quantified without a separate validation set.</p><p>Let us denote the discriminator outputs by D train , D validation , and D generated for the training set, validation set, and generated images, respectively, and their mean over N consecutive minibatches by E[·]. In practice we use N = 4, which corresponds to 4 × 64 = 256 images. We can now turn our observations about <ref type="figure" target="#fig_0">Figure 1</ref> into two plausible overfitting heuristics:</p><formula xml:id="formula_1">r v = E[D train ] − E[D validation ] E[D train ] − E[D generated ] r t = E[sign(D train )]<label>(1)</label></formula><p>For both heuristics, r = 0 means no overfitting and r = 1 indicates complete overfitting, and our goal is to adjust the augmentation probability p so that the chosen heuristic matches a suitable target value. The first heuristic, r v , expresses the output for a validation set relative to the training set and generated images. Since it assumes the existence of a separate validation set, we include it mainly as a comparison method. The second heuristic, r t , estimates the portion of the training set that gets positive discriminator outputs. We have found this to be far less sensitive to the chosen target value and other hyperparameters than the obvious alternative of looking at E[D train ] directly.</p><p>We control the augmentation strength p as follows. We initialize p to zero and adjust its value once every four minibatches 2 based on the chosen overfitting heuristic. If the heuristic indicates too much/little overfitting, we counter by incrementing/decrementing p by a fixed amount. We set the adjustment size so that p can rise from 0 to 1 sufficiently quickly, e.g., in 500k images. After every step we clamp p from below to 0. We call this variant adaptive discriminator augmentation (ADA).</p><p>In <ref type="figure">Figure 5a</ref>,b we measure how the target value affects the quality obtainable using these heuristics.</p><p>We observe that r v and r t are both effective in preventing overfitting, and that they both improve the results over the best fixed p found using grid search. We choose to use the more realistic r t heuristic in all subsequent tests, with 0.6 as the target value. <ref type="figure">Figure</ref>   safety limit after which some augmentations become leaky, indicating that the augmentations were not powerful enough. Indeed, FID started deteriorating after p ≈ 0.5 in this extreme case. <ref type="figure">Figure 5d</ref> shows the evolution of r t with adaptive vs fixed p, showing that a fixed p tends to be too strong in the beginning and too weak towards the end. <ref type="figure">Figure 6</ref> repeats the setup from <ref type="figure" target="#fig_0">Figure 1</ref> using ADA. Convergence is now achieved regardless of the training set size and overfitting no longer occurs. Without augmentations, the gradients the generator receives from the discriminator become very simplistic over time -the discriminator starts to pay attention to only a handful of features, and the generator is free to create otherwise non-sensical images. With ADA, the gradient field stays much more detailed which prevents such deterioration. In an interesting parallel, it has been shown that loss functions can be made significantly more robust in regression settings by using similar image augmentation ensembles <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We start by testing our method against a number of alternatives in FFHQ and LSUN CAT, first in a setting where a GAN is trained from scratch, then by applying transfer learning on a pre-trained GAN. We conclude with results for several smaller datasets. <ref type="figure" target="#fig_5">Figure 7</ref> shows our results in FFHQ and LSUN CAT across training set sizes, demonstrating that our adaptive discriminator augmentation (ADA) improves FIDs substantially in limited data scenarios. We also show results for balanced consistency regularization (bCR) <ref type="bibr" target="#b52">[53]</ref>, which has not been studied in the context of limited data before. We find that bCR can be highly effective when the lack of data is not too severe, but also that its set of augmentations leaks to the generated images. In this example, we used only xy-translations by integer offsets for bCR, and <ref type="figure" target="#fig_5">Figure 7d</ref> shows that the generated images get jittered as a result. This means that bCR is essentially a dataset augmentation and needs to be limited to symmetries that actually benefit the training data, e.g., x-flip is often acceptable but  ADA matches the average of real data, whereas the xy-translation augmentation in bCR <ref type="bibr" target="#b52">[53]</ref> has leaked to the generated images, significantly blurring the average image.  y-flip only rarely. Meanwhile, with ADA the augmentations do not leak, and thus the same diverse set of augmentations can be safely used in all datasets. We also find that the benefits for ADA and bCR are largely additive. We combine ADA and bCR so that ADA is first applied to the input image (real or generated), and bCR then creates another version of this image using its own set of augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training from scratch</head><formula xml:id="formula_2">N N N N N N N ),' %DVHOLQH $'$2XUV E&amp;5 $'$E&amp;5 N N N N N N N ),' %DVHOLQH $'$2XUV E&amp;5 $'$E&amp;5</formula><p>Qualitative results are shown in Appendix A.</p><p>In <ref type="figure" target="#fig_6">Figure 8a</ref> we further compare our adaptive augmentation against a wider set of alternatives: PA-GAN <ref type="bibr" target="#b47">[48]</ref>, WGAN-GP <ref type="bibr" target="#b14">[15]</ref>, zCR <ref type="bibr" target="#b52">[53]</ref>, auxiliary rotations <ref type="bibr" target="#b5">[6]</ref>, and spectral normalization <ref type="bibr" target="#b30">[31]</ref>. We also try modifying our baseline to use a shallower mapping network, which can be trained with less data, borrowing intuition from DeLiGAN <ref type="bibr" target="#b15">[16]</ref>. Finally, we try replacing our augmentations with multiplicative dropout <ref type="bibr" target="#b41">[42]</ref>, whose per-layer strength is driven by our adaptation algorithm. We spent considerable effort tuning the parameters of all these methods, see Appendix D. We can see that ADA gave significantly better results than the alternatives. While PA-GAN is somewhat similar to our method, its checksum task was not strong enough to prevent overfitting in our tests. <ref type="figure" target="#fig_6">Figure 8b</ref> shows that reducing the discriminator capacity is generally harmful and does not prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer learning</head><p>Transfer learning reduces the training data requirements by starting from a model trained using some other dataset, instead of a random initialization. Several authors have explored this in the context of GANs <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34]</ref>, and Mo et al. <ref type="bibr" target="#b32">[33]</ref> recently showed strong results by freezing the highest-resolution layers of the discriminator during transfer (Freeze-D).</p><p>We explore several transfer learning setups in <ref type="figure" target="#fig_7">Figure 9</ref>, using the best Freeze-D configuration found for each case with grid search. Transfer learning gives significantly better results than from-scratch training, and its success seems to depend primarily on the diversity of the source dataset, instead of the similarity between subjects. For example, FFHQ (human faces) can be trained equally well from  CELEBA-HQ (human faces, low diversity) or LSUN DOG (more diverse). LSUN CAT, however, can only be trained from LSUN DOG, which has comparable diversity, but not from the less diverse datasets. With small target dataset sizes, our baseline achieves reasonable FID quickly, but the progress soon reverts as training continues. ADA is again able to prevent the divergence almost completely. Freeze-D provides a small but reliable improvement when used together with ADA but is not able to prevent the divergence on its own.</p><formula xml:id="formula_3">W 0 0 0 0 0 0 ),' ))+4N )UHH]H' ))+4N )UHH]H' ))+4N )UHH]H' W 0 0 0 0 0 0 ),' ))+4N )UHH]H' ))+4N )UHH]H' ))+4N )UHH]H' N N N N N ),' %DVHOLQH 7UDQVIHU )UHH]H' $'$2XUV 7UDQVIHU )UHH]H' N N N N N ),' /681&amp;DWIURP&amp;HOHE$+4 /681&amp;DWIURP))+4 /681&amp;DWIURP/681'RJ ))+4IURP&amp;HOHE$+4 ))+4IURP/681'RJ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Small datasets</head><p>We tried our method with several datasets that consist of a limited number of training images ( <ref type="figure" target="#fig_0">Figure 10</ref>). METFACES is our new dataset of 1336 high-quality faces extracted from the collection of Metropolitan Museum of Art (https://metmuseum.github.io/). BRECAHAD <ref type="bibr" target="#b0">[1]</ref> consists of only 162 breast cancer histopathology images (1360 × 1024); we reorganized these into 1944 partially overlapping crops of 512 2 . Animal faces (AFHQ) <ref type="bibr" target="#b6">[7]</ref> includes ∼5k closeups per category for dogs, cats, and wild life; we treated these as three separate datasets and trained a separate network for each of them. CIFAR-10 includes 50k tiny images in 10 categories <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref> reveals that FID is not an ideal metric for small datasets, because it becomes dominated by the inherent bias when the number of real images is insufficient. We find that kernel inception distance (KID) <ref type="bibr" target="#b2">[3]</ref> -that is unbiased by design -is more descriptive in practice and see that ADA provides a dramatic improvement over baseline StyleGAN2. This is especially true when training from scratch, but transfer learning also benefits from ADA. In the widely used CIFAR-10 benchmark, we improve the SOTA FID from 5.59 to 2.42 and inception score (IS) <ref type="bibr" target="#b36">[37]</ref> from 9.58 to 10.24 in the class-conditional setting <ref type="figure" target="#fig_0">(Figure 11b</ref>). This large improvement portrays CIFAR-10 as a limited data benchmark. We also note that CIFAR-specific architecture tuning had a significant effect.</p><formula xml:id="formula_4">Dataset Method Scratch Transfer + Freeze-D FID KID KID KID ×10 3 ×10 3 ×10 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METFACES</head><p>Baseline 57. <ref type="bibr" target="#b25">26</ref>  (a) Small datasets (b) CIFAR-10 <ref type="figure" target="#fig_0">Figure 11</ref>: (a) Several small datasets trained with StyleGAN2 baseline (config F) and ADA, from scratch and using transfer learning. We used FFHQ-140K with matching resolution as a starting point for all transfers. We report the best KID, and compute FID using the same snapshot. (c) Mean and standard deviation for CIFAR-10, computed from the best scores of 5 training runs. For the comparison methods we report the average scores when available, and the single best score otherwise. The best IS and FID were searched separately <ref type="bibr" target="#b21">[22]</ref>, and often came from different snapshots. We computed the FID for Progressive GAN <ref type="bibr" target="#b18">[19]</ref> using the publicly available pre-trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have shown that our adaptive discriminator augmentation reliably stabilizes training and vastly improves the result quality when training data is in short supply. Of course, augmentation is not a substitute for real data -one should always try to collect a large, high-quality set of training data first, and only then fill the gaps using augmentation. As future work, it would be worthwhile to search for the most effective set of augmentations, and to see if recently published techniques, such as the U-net discriminator <ref type="bibr" target="#b37">[38]</ref> or multi-modal generator <ref type="bibr" target="#b38">[39]</ref>, could also help with limited data.</p><p>Enabling ADA has a negligible effect on the energy consumption of training a single model. As such, using it does not increase the cost of training models for practical use or developing methods that require large-scale exploration. For reference, Appendix E provides a breakdown of all computation that we performed related to this paper; the project consumed a total of 325 MWh of electricity, or 135 single-GPU years, the majority of which can be attributed to extensive comparisons and sweeps.</p><p>Interestingly, the core idea of discriminator augmentations was independently discovered by three other research groups in parallel work: Z. Zhao et al. <ref type="bibr" target="#b53">[54]</ref>, Tran et al. <ref type="bibr" target="#b42">[43]</ref>, and S. Zhao et al. <ref type="bibr" target="#b50">[51]</ref>. We recommend these papers as they all offer a different set of intuition, experiments, and theoretical justifications. While two of these papers <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b50">51]</ref> propose essentially the same augmentation mechanism as we do, they study the absence of leak artifacts only empirically. The third paper <ref type="bibr" target="#b42">[43]</ref> presents a theoretical justification based on invertibility, but arrives at a different argument that leads to a more complex network architecture, along with significant restrictions on the set of possible augmentations. None of these works consider the possibility of tuning augmentation strength adaptively. Our experiments in Section 3 show that the optimal augmentation strength not only varies between datasets of different content and size, but also over the course of training -even an optimal set of fixed augmentation parameters is likely to leave performance on the <ref type="table">table.</ref> A direct comparison of results between the parallel works is difficult because the only dataset used in all papers is CIFAR-10. Regrettably, the other three papers compute FID using 10k generated images and 10k validation images (FID-10k), while we use follow the original recommendation of Heusel et al. <ref type="bibr" target="#b17">[18]</ref> and use 50k generated images and all training images. Their FID-10k numbers are thus not comparable to the FIDs in <ref type="figure" target="#fig_0">Figure 11b</ref>. For this reason we also computed FID-10k for our method, obtaining 7.01 ± 0.06 for unconditional and 6.54 ± 0.06 for conditional. These compare favorably to parallel work's unconditional 9.89 <ref type="bibr" target="#b50">[51]</ref> or 10.89 <ref type="bibr" target="#b42">[43]</ref>, and conditional 8.30 <ref type="bibr" target="#b53">[54]</ref> or 8.49 <ref type="bibr" target="#b50">[51]</ref>. It seems likely that some combination of the ideas from all four papers could further improve our results. For example, more diverse set of augmentations or contrastive regularization <ref type="bibr" target="#b53">[54]</ref> might be worth testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader impact</head><p>Data-driven generative modeling means learning a computational recipe for generating complicated data based purely on examples. This is a foundational problem in machine learning. In addition to their fundamental nature, generative models have several uses within applied machine learning research as priors, regularizers, and so on. In those roles, they advance the capabilities of computer vision and graphics algorithms for analyzing and synthesizing realistic imagery.</p><p>The methods presented in this work enable high-quality generative image models to be trained using significantly less data than required by existing approaches. It thereby primarily contributes to the deep technical question of how much data is enough for generative models to succeed in picking up the necessary commonalities and relationships in the data.</p><p>From an applied point of view, this work contributes to efficiency; it does not introduce fundamental new capabilities. Therefore, it seems likely that the advances here will not substantially affect the overall themes -surveillance, authenticity, privacy, etc. -in the active discussion on the broader impacts of computer vision and graphics.</p><p>Specifically, generative models' implications on image and video authenticity is a topic of active discussion. Most attention revolves around conditional models that allow semantic control and sometimes manipulation of existing images. Our algorithm does not offer direct controls for highlevel attributes (e.g., identity, pose, expression of people) in the generated images, nor does it enable direct modification of existing images. However, over time and through the work of other researchers, our advances will likely lead to improvements in these types of models as well.</p><p>The contributions in this work make it easier to train high-quality generative models with custom sets of images. By this, we eliminate, or at least significantly lower, the barrier for applying GAN-type models in many applied fields of research. We hope and believe that this will accelerate progress in several such fields. For instance, modeling the space of possible appearance of biological specimens (tissues, tumors, etc.) is a growing field of research that appears to chronically suffer from limited high-quality data. Overall, generative models hold promise for increased understanding of the complex and hard-to-pinpoint relationships in many real-world phenomena; our work hopefully increases the breadth of phenomena that can be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional results</head><p>In <ref type="figure" target="#fig_0">Figures 12, 13</ref>, 14, 15, and 16, we show generated images for METFACES, BRECAHAD, and AFHQ CAT, DOG, WILD, respectively, along with real images from the respective training sets (Section 4.3 and <ref type="figure" target="#fig_0">Figure 11a</ref>). The images were selected at random; we did not perform any cherrypicking besides choosing one global random seed. We can see that ADA yields excellent results in all cases, and with slight truncation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref>, virtually all of the images look convincing. Without ADA, the convergence is hampered by discriminator overfitting, leading to inferior image quality for the original StyleGAN2, especially in METFACES, AFHQ DOG, and BRECAHAD. <ref type="figure" target="#fig_0">Figure 17</ref> shows examples of the generated CIFAR-10 images in both unconditional and classconditional setting (See Appendix D.1 for details on the conditional setup). <ref type="figure" target="#fig_0">Figure 18</ref> shows qualitative results for different methods using subsets of FFHQ at 256×256 resolution. Methods that do not employ augmentation (BigGAN, StyleGAN2, and our baseline) degrade noticeably as the size of the training set decreases, generally yielding poor image quality and diversity with fewer than 30k training images. With ADA, the degradation is much more graceful, and the results remain reasonable even with a 5k training set. <ref type="figure" target="#fig_0">Figure 19</ref> compares our results with unconditional BigGAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> and StyleGAN2 config F <ref type="bibr" target="#b20">[21]</ref>. BigGAN was very unstable in our experiments: while some of the results were quite good, approximately 50% of the training runs failed to converge. StyleGAN2, on the other hand, behaved predictably, with different training runs resulting in nearly identical FID. We note that FID has a general tendency to increase as the training set gets smaller -not only because of the lower image quality, but also due to inherent bias in FID itself <ref type="bibr" target="#b2">[3]</ref>. In our experiments, we minimize the impact of this bias by always computing FID between 50k generated images and all available real images, regardless of which subset was used for training. To estimate the magnitude of bias in FID, we simulate a hypothetical generator that replicates the training set as-is, and compute the average FID over 100 random trials with different subsets of training data; the standard deviation was ≤2% in all cases. We can see that the bias remains negligible with ≥20k training images but starts to dominate with ≤2k. Interestingly, ADA reaches the same FID as the best-case generator with FFHQ-1k, indicating that FID is no longer able to differentiate between the two in this case. <ref type="figure">Figure 20</ref> shows additional examples of bCR leaking to generated images and compares bCR with dataset augmentation. In particular, rotations in range [−45 • , +45 • ] (denoted ±45 • ) serve as a very clear example that attempting to make the discriminator blind to certain transformations opens up the possibility for the generator to produce similarly transformed images with no penalty. In applications where such leaks are acceptable, one can employ either bCR or dataset augmentation -we find that it is difficult to predict which method is better. For example, with translation augmentations bCR was significantly better than dataset augmentation, whereas x-flip was much more effective when implemented as a dataset augmentation.</p><p>Finally, <ref type="figure" target="#fig_0">Figure 21</ref> shows an extended version of <ref type="figure" target="#fig_2">Figure 4</ref>, illustrating the effect of different augmentation categories with increasing augmentation probability p. Blit + Geom + Color yielded the best results with a 2k training set and remained competitive with larger training sets as well.  <ref type="figure" target="#fig_0">Figure 15</ref>: Uncurated 512×512 results generated for AFHQ DOG <ref type="bibr" target="#b6">[7]</ref> (4739 images) with and without ADA, along with real images from the training set. Both generators were trained from scratch. We recommend zooming in to inspect the image quality in detail. <ref type="figure" target="#fig_0">Figure 16</ref>: Uncurated 512×512 results generated for AFHQ WILD <ref type="bibr" target="#b6">[7]</ref> (4738 images) with and without ADA, along with real images from the training set. Both generators were trained from scratch. We recommend zooming in to inspect the image quality in detail.  . We show the results for the best generators trained in the context of <ref type="figure" target="#fig_0">Figure 11b</ref>, selected according to either FID or IS. The numbers refer to the single best model and are therefore slightly better than the averages quoted in the result table. It can be seen that the model with the lowest FID produces images with a wider variation in coloring and poses compared to the model with highest IS. This is in line with the common approximation (e.g., <ref type="bibr" target="#b4">[5]</ref>) that FID roughly corresponds to Recall and IS to Precision, two independent aspects of result quality <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26]</ref>.  <ref type="figure" target="#fig_0">Figure 18</ref>: Images generated for different subsets of FFHQ at 256×256 resolution using the training setups from <ref type="figure" target="#fig_0">Figures 7 and 19</ref>. We show the best snapshot of the best training run for each case, selected according to FID, so the numbers are slightly better than the medians reported in <ref type="figure" target="#fig_5">Figure 7c</ref>. In addition to FID, we also report the Recall metric <ref type="bibr" target="#b25">[26]</ref> as a more direct way to estimate image diversity. The bolded numbers indicate the lowest FID and highest Recall for each training set size. "BigGAN" corresponds to the unconditional variant of BigGAN <ref type="bibr" target="#b4">[5]</ref> proposed by Schönfeld et al. <ref type="bibr" target="#b37">[38]</ref>, and "StyleGAN2" corresponds to config F of the official TensorFlow implementation by Karras et al. <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_5">ADA (Ours), untruncated Original StyleGAN2 config F, untruncated FID 3.05 -KID 0.45×10 3 -Recall 0.147 FID 3.48 -KID 0.77×10 3 -Recall 0.143</formula><formula xml:id="formula_6">N N N N N N N N N ),' %LJ*$1 UXQV 6W\OH*$1 UXQV 2XUEDVHOLQH UXQV $'$ UXQV $SSUR[LPDWH ELDVLQ),' N N N N N N N N N ),' %LJ*$1 UXQV 6W\OH*$1 UXQV 2XUEDVHOLQH UXQV $'$ UXQV $SSUR[LPDWH ELDVLQ),'</formula><p>(a) Different subsets of FFHQ at 256×256 (b) Different subsets of LSUN CAT at 256×256 <ref type="figure" target="#fig_0">Figure 19</ref>: Comparison of our results with unconditional BigGAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> and StyleGAN2 config F <ref type="bibr" target="#b20">[21]</ref>. We report the median/min/max FID as a function of training set size, calculated over multiple independent training runs. The dashed red line illustrates the expected bias of the FID metric, computed using a hypothetical generator that outputs random images from the training set as-is.  </p><formula xml:id="formula_7">Integer translation ± 0px ± 4px ± 8px ± 16px ± 16px samples Arbitrary rotation ± 0 • ± 10 • ± 20 • ± 45 • ± 45 • samples N N N N N N ),' %DVHOLQH 'DWDWUDQV 'DWD[IOLS E&amp;5WUDQV E&amp;5WUDQVE&amp;5[IOLS E&amp;5WUDQVGDWD[IOLS N N N N N N ),' %DVHOLQH GDWD[IOLS $'$ GDWD[IOLS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Our augmentation pipeline</head><p>We designed our augmentation pipeline based on three goals. First, the entire pipeline must be strictly non-leaking (Appendix C). Second, we aim for a maximally diverse set of augmentations, inspired by the success of RandAugment <ref type="bibr" target="#b8">[9]</ref>. Third, we strive for the highest possible image quality to reduce unintended artifacts such as aliasing. In total, our pipeline consists of 18 transformations: geometric <ref type="bibr" target="#b6">(7)</ref>, color (5), filtering (4), and corruption <ref type="bibr" target="#b1">(2)</ref>. We implement it entirely on the GPU in a differentiable fashion, with full support for batching. All parameters are sampled independently for each image. <ref type="figure" target="#fig_13">Figure 22</ref> shows pseudocode for our geometric and color transformations, along with example images. In general, geometric transformations tend to lose high-frequency details of the input image due to uneven resampling, which may reduce the capability of the discriminator to detect pixel-level errors in the generated images. We alleviate this by introducing a dedicated sub-category, pixel blitting, that only copies existing pixels as-is, without blending between neighboring pixels. Furthermore, we avoid gradual image degradation from multiple consecutive transformations by collapsing all geometric transformations into a single combined operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Geometric and color transformations</head><p>The parameters for pixel blitting are selected on lines 5-15, consisting of x-flips (line 7), 90 • rotations (line 10), and integer translations (line 13). The transformations are accumulated into a homogeneous</p><formula xml:id="formula_8">3 × 3 matrix G, defined so that input pixel (x i , y i ) is placed at [x o , y o , 1] T = G · [x i , y i , 1] T in the output.</formula><p>The origin is located at the center of the image and neighboring pixels are spaced at unit intervals. We apply each transformation with probability p by sampling its parameters from uniform distribution, either discrete U{·} or continuous U(·), and updating G using elementary transforms:</p><formula xml:id="formula_9">SCALE2D(s x , s y ) = s x 0 0 0 s y 0 0 0 1 , ROTATE2D(θ) = cos θ − sin θ 0 sin θ cos θ 0 0 0 1 , TRANSLATE2D(t x , t y ) = 1 0 t x 0 1 t y 0 0 1<label>(2)</label></formula><p>General geometric transformations are handled in a similar way on lines 16-32, consisting of isotropic scaling (line 17), arbitrary rotation (lines 21 and 27), anisotropic scaling (line 24), and fractional translation (line 30). Since both of the scaling transformations are multiplicative in nature, we sample their parameter, s, from a log-normal distribution so that ln s ∼ N 0, (0.2 · ln 2) 2 . In practice, this can be done by first sampling t ∼ N (0, 1) and then calculating s = exp 2 (0.2t). We allow anisotropic scaling to operate in other directions besides the coordinate axes by breaking the rotation into two independent parts, one applied before the scaling (line 21) and one after it (line 27). We apply the rotations slightly less frequently than other transformations, so that the probability of applying at least one rotation is equal to p. Note that we also have two translations in our pipeline (lines 13 and 30), one applied at the beginning and one at the end. To increase the diversity of our augmentations, we use U(·) for the former and N (·) for the latter.</p><p>Once the parameters are settled, the combined geometric transformation is executed on lines 33-47. We avoid undesirable effects at image borders by first padding the image with reflection. The amount of padding is calculated dynamically based on G so that none of the output pixels are affected by regions outside the image (line 35). We then upsample the image to a higher resolution (line 40) and transform it using bilinear interpolation (line 45). Operating at a higher resolution is necessary to reduce aliasing when the image is minified, e.g., as a result of isotropic scaling -interpolating at the original resolution would fail to correctly filter out frequencies above Nyquist in this case, no matter which interpolation filter was used. The choice of the upsampling filter requires some care, however, because we must ensure that an identity transform does not modify the image in any way (e.g., when p = 0). In other words, we need to use a lowpass filter H(z) with cutoff f c = π 2 that satisfies DOWNSAMPLE2D UPSAMPLE2D Y, H(z −1 ) , H(z) = Y . Luckily, existing literature on wavelets <ref type="bibr" target="#b9">[10]</ref> offers a wide selection of such filters; we choose 12-tap symlets (SYM6) to strike a balance between resampling quality and computational cost.</p><p>Finally, color transformations are applied to the resulting image on lines 48-70. The overall operation is similar to geometric transformations: we collect the parameters of each individual transformation into a homogeneous 4 × 4 matrix C that we then apply to each pixel by computing </p><formula xml:id="formula_10">[r o , g o , b o , 1] T = C · [r i , g i , b i , 1] T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Select parameters for pixel blitting 6: G ← I3</p><p>Homogeneous 2D transformation matrix 7: apply x-flip with probability p 8:</p><p>sample i ∼ U {0, 1} 9:</p><p>G ← SCALE2D(1 − 2i, 1) · G 10: apply 90 • rotations with probability p 11:</p><p>sample i ∼ U {0, 3} 12:</p><p>G ← ROTATE2D − π 2 · i · G 13: apply integer translation with probability p 14:</p><p>sample tx, ty ∼ U (−0.125, +0.125) 15:</p><p>G ← TRANSLATE2D round(txw), round(tyh) · G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Select parameters for general geometric transformations 17: apply isotropic scaling with probability p 18:</p><p>sample s ∼ Lognormal 0, (0.2 · ln 2) 2 19:</p><p>G ← SCALE2D(s, s) · G 20: prot ← 1 − √ 1 − p P (pre ∪ post) = p 21: apply pre-rotation with probability prot 22:</p><p>sample θ ∼ U (−π, +π) 23:</p><p>G ← ROTATE2D(−θ) · G Before anisotropic scaling 24: apply anisotropic scaling with probability p 25:</p><p>sample s ∼ Lognormal 0, (0.2 · ln 2) 2 26:</p><p>G ← SCALE2D s, 1 s · G 27: apply post-rotation with probability prot 28:</p><p>sample θ ∼ U (−π, +π) 29:</p><p>G ← ROTATE2D(−θ) · G After anisotropic scaling 30: apply fractional translation with probability p 31:</p><p>sample tx, ty ∼ N 0, (0.125) 2 32:</p><p>G ← TRANSLATE2D(txw, tyh) · G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33:</head><p>Pad image and adjust origin 34:</p><formula xml:id="formula_11">H(z) ← WAVELET(SYM6) Orthogonal lowpass filter 35: (m lo , m hi ) ← CALCULATEPADDING G, w, h, H(z) 36: Y ← PAD(Y, m lo , m hi , REFLECT) 37: T ← TRANSLATE2D 1 2 w− 1 2 +m lo,x , 1 2 h− 1 2 +m lo,y 38: G ← T · G · T −1</formula><p>Place origin at image center 39:</p><formula xml:id="formula_12">Execute geometric transformations 40: Y ← UPSAMPLE2X2 Y, H(z −1 ) 41: S ← SCALE2D(2, 2) 42: G ← S · G · S −1</formula><p>Account for the upsampling 43: for each pixel (xo, yo) ∈ Y do 44:</p><formula xml:id="formula_13">[xi, yi, zi] T ← G −1 · [xo, yo, 1] T 45: Yx o ,yo ← BILINEARLOOKUP(Y , xi, yi) 46: Y ← DOWNSAMPLE2X2 Y, H(z) 47: Y ← CROP(Y, m lo , m hi )</formula><p>Undo the padding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>48:</head><p>Select parameters for color transformations 49: C ← I4</p><p>Homogeneous 3D transformation matrix 50: apply brightness with probability p 51:</p><p>sample b ∼ N 0, (0.2) 2 52:</p><p>C ← TRANSLATE3D(b, b, b) · C 53: apply contrast with probability p 54:</p><p>sample c ∼ Lognormal 0, (0.5 · ln 2) 2 55:</p><formula xml:id="formula_14">C ← SCALE3D(c, c, c) · C 56: v ← [1, 1, 1, 0] / √ 3</formula><p>Luma axis 57: apply luma flip with probability p 58:</p><formula xml:id="formula_15">sample i ∼ U {0, 1} 59: C ← I4 − 2v T v · i · C</formula><p>Householder reflection 60: apply hue rotation with probability p 61:</p><p>sample θ ∼ U (−π, +π) 62:</p><p>C ← ROTATE3D(v, θ) · C Rotate around v 63: apply saturation with probability p 64:</p><p>sample s ∼ Lognormal 0, (1 · ln 2) 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>65</head><p>:  </p><formula xml:id="formula_16">C ← v T v + I4 − v T v · s ·</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Select parameters for image-space filtering 6: b ← 0, π 8 , π 8 , π 4 , π 4 , π 2 , π 2 , π Freq. bands 7: g ← [1, 1, 1, 1]</p><p>Global gain vector (identity) 8: λ ← [10, 1, 1, 1] / 13 Expected power spectrum (1/f ) 9: for i = 1, 2, 3, 4 do 10:</p><p>apply amplification for bi with probability p 11:</p><p>t </p><formula xml:id="formula_17">H (z) ← H (z) + BANDPASS H(z), bi · gi 20: (m lo , m hi ) ← CALCULATEPADDING H (z) 21: Y ← PAD(Y, m lo , m hi , REFLECT) 22: Y ← SEPARABLECONV2D Y, H (z) 23: Y ← CROP(Y, m lo , m hi )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>Additive RGB noise 25: apply noise with probability p 26:</p><p>sample σ ∼ Halfnormal (0.1) 2 27:</p><p>for each pixel (x, y) ∈ Y do 28:</p><p>sample nr, ng, n b ∼ N (0, σ 2 ) 29:</p><p>Yx,y ← Yx,y + [nr, ng, n b ] 30: Cutout 31: apply cutout with probability p 32:</p><p>sample cx, cy ∼ U (0, 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33</head><p>:  x y denotes element-wise multiplication. <ref type="figure" target="#fig_1">Figure 23</ref> shows pseudocode for our image-space filtering and corruptions. The parameters for imagespace filtering are selected on lines 5-14. The idea is to divide the frequency content of the image into 4 non-overlapping bands and amplify/weaken each band in turn via a sequence of 4 transformations, so that each transformation is applied independently with probability p (lines 9-10). Frequency bands b 2 , b 3 , and b 4 correspond to the three highest octaves, respectively, while the remaining low frequencies are attributed to b 1 (line 6). We track the overall gain of each band using vector g (line 7) that we update after each transformation (line 14). We sample the amplification factor for a given band from log-normal distribution (line 12), similar to geometric scaling, and normalize the overall gain so that the total energy is retained on expectation. For the normalization, we assume that the frequency content obeys 1/f power spectrum typically seen in natural images (line 8). While this assumption is not strictly true in our case, especially when some of the previous frequency bands have already been amplified, it is sufficient to keep the output pixel values within reasonable bounds.</p><formula xml:id="formula_18">r lo ← round cx − 1 4 · w, cy − 1 4 · h 34: r hi ← round cx + 1 4 · w, cy + 1 4 · h 35: Y ← Y 1 − RECTANGULARMASK(r lo ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-space corruptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Image-space filtering and corruptions</head><p>The filtering is executed on lines 15-23. We first construct a combined amplification filter H (z) (lines <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> and then perform separable convolution for the image using reflection padding (lines 21-23). We use a zero-phase filter bank derived from 4-tap symlets (SYM2) <ref type="bibr" target="#b9">[10]</ref>. Denoting the wavelet scaling filter by H(z), the corresponding bandpass filters are obtained as follows (line 19): </p><formula xml:id="formula_19">BANDPASS H(z), b 1 = H(z)H(z −1 )H(z 2 )H(z −2 )H(z 4 )H(z −4 )/8<label>(3)</label></formula><p>Finally, we apply additive RGB noise on lines 24-29 and cutout on lines 30-35. We vary the strength of the noise by sampling its standard deviation from half-normal distribution, i.e., N (·) restricted to non-negative values (line 26). For cutout, we match the original implementation of DeVries and Taylor <ref type="bibr" target="#b10">[11]</ref> by setting pixels to zero within a rectangular area of size w 2 , h 2 , with the center point selected from uniform distribution over the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Non-leaking augmentations</head><p>The goal of GAN training is to find a generator function G whose output probability distribution x (under suitable stochastic input) matches a given target distribution y.</p><p>When augmenting both the dataset and the generator output, the key safety principle is that if x and y do not match, then their augmented versions must not match either. If the augmentation pipeline violates this principle, the generator is free to learn some different output distribution than the dataset, as these look identical after the augmentations -we say that the augmentations leak. Conversely, if the principle holds, then the only option for the generator is to learn the correct distribution: no other choice results in a post-augmentation match.</p><p>In this section, we study the conditions on the augmentation pipeline under which this holds and demonstrate the safety and caveats of various common augmentations and their compositions.</p><p>Notation Throughout this section, we denote probability distributions (and their generalizations) with lowercase bold-face letters (e.g., x), operators acting on them by calligraphic letters (T ), and variates sampled from probability distributions by upper-case letters (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Augmentation operator</head><p>A very general model for augmentations is as follows. Assume a fixed but arbitrarily complicated nonlinear and stochastic augmentation pipeline. To any image X, it assigns a distribution of augmented images, such as demonstrated in <ref type="figure">Figure 2c</ref>. This idea is captured by an augmentation operator T that maps probability distributions to probability distributions (or, informally, datasets to augmented datasets). A distribution with the lone image X is the Dirac point mass δ X , which is mapped to some distribution T δ X of augmented images. <ref type="bibr" target="#b2">3</ref> In general, applying T to an arbitrary distribution x yields the linear superposition T x of such augmented distributions.</p><p>It is important to understand that T is different from a function f (X; φ) that actually applies the augmentation on any individual image X sampled from x (parametrized by some φ, e.g., angle in case of a rotation augmentation). It captures the aggregate effect of applying this function on all images in the distribution and subsumes the randomization of the function parameters. T is always linear and deterministic, regardless of non-linearity of the function f and stochasticity of its parameters φ. We will later discuss invertibility of T . Here it is also critical to note that its invertibility is not equivalent with the invertibility of the function f it is based on; for an example, refer to the discussion in Section 2.2.</p><p>Specifically, T is a (Markov) transition operator. Intuitively, it is an (uncountably) infinitedimensional generalization of a Markov transition matrix (i.e. a stochastic matrix), with nonnegative entries that sum to 1 along columns. In this analogy, probability distributions upon which T operates are vectors, with nonnegative entries summing to 1. More generally, the distributions have a vector space structure and they can be arbitrarily linearly combined (in which case they may lose their validity as probability distributions and are viewed as arbitrary signed measures). Similarly, we can do algebra with the with the operators by linearly combining and composing them like matrices. Concepts such as null space and invertibility carry over to this setting, with suitable technical care. In the following, we will be somewhat informal with the measure theoretical and functional analytic details of the problem, and draw upon this analogy as appropriate. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Invertibility implies non-leaking augmentations</head><p>Within this framework, our question can be stated as follows. Given a target distribution y and an augmentation operator T , we train for a generated distribution x such that the augmented distributions match, namely T x = T y.</p><p>The desired outcome is that this equation is satisfied only by the correct target distribution, namely x = y. We say that T leaks if there exist distributions x = y that satisfy the above equation, and the goal is to find conditions that guarantee the absence of leaks.</p><p>There are obviously no such leaks in classical non-augmented training, where T is the identity I, whence T x = T y ⇒ Ix = Iy ⇒ x = y. For arbitrary augmentations, the desired outcome x = y does always satisfy Eq. 7; however, if also other choices of x satisfy it, then it cannot be guaranteed that the training lands on the desired solution. A trivial example is an augmentation that maps every image to black (in other words, T z = δ 0 for any z). Then, T x = T y does not imply that x = y, as indeed any choice of x produces the same set of black images that satisfies Eq. 7. In this case, it is vanishingly unlikely that the training finds the solution x = y.</p><p>More generally, assume that T has a non-trivial null space, namely there exists a signed measure n = 0 such that T n = 0, that is, n is in the null space of T . Equivalently, T is not invertible, because n cannot be recovered from T n. Then, x = y + αn for any α ∈ R satisfies Eq. 7. Therefore noninvertibility of T implies that measures in its null space may freely leak into the learned distribution (as long as the sum remains a valid probability distribution that assigns non-negative mass to all sets). Conversely, assume that some x = y satisfies Eq. 7. Then T (x − y) = T y − T y = 0, so x − y is in null space of T and therefore T is not invertible.</p><p>Therefore, leaking augmentations imply non-invertibility of the augmentation operator, which conversely implies the central principle: if the augmentation operator T is invertible, it does not leak. Such a non-leaking operator further satisfies the requirements of Lemma 5.1. of Bora et al. <ref type="bibr" target="#b3">[4]</ref>, where the invertibility is shown to imply that a GAN learns the correct distribution.</p><p>The invertibility has an intuitive interpretation: the training process can implicitly "undo" the augmentations, as long as probability mass is merely shifted around and not squashed flat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Compositions and mixtures</head><p>We only access the operator T indirectly: it is implemented as a procedure, rather than a matrix-like entity whose null space we could study directly (even if we know that such a thing exists in principle). Showing invertibility for an arbitrary procedure is likely to be impossible. Rather, we adopt a constructive approach, and build our augmentation pipeline from combinations of simple known-safe augmentations, in a way that can be shown to not leak. This calls for two components: a set of combination rules that preserve the non-leaking guarantee, and a set of elementary augmentations that have this property. In this subsection we address the former.</p><p>By elementary linear algebra: assume T and U are invertible. Then the composition T U is invertible, as is any finite chain of such compositions. Hence, sequential composition of non-leaking augmentations is non-leaking. We build our pipeline on this observation.</p><p>The other obvious combination of augmentations is obtained by probabilistic mixtures: given invertible augmentations T and U, perform T with probability α and U with probability 1 − α.</p><p>The operator corresponding to this augmentation is the "pointwise" convex blend αT + (1 − α)U. More generally, one can mix e.g. a continuous family of augmentations T φ with weights given by a non-negative unit-sum function α(φ), as α(φ)T φ dφ. Unfortunately, stochastically choosing among a set of augmentations is not guaranteed to preserve the non-leaking property, and must be analyzed case by case (which is the content of the next subsection). To see this, consider an multiplication of probability distributions in this sense (as opposed to e.g. addition of random variables), unless otherwise noted. Technically, one can consider the vector space of finite signed measures on R N , which is a Banach space under the Total Variation norm. Markov operators form a convex subset of linear operators acting on this space, and general linear combinations thereof form a subspace (and a subalgebra). The exact mathematical conditions under which some of the following findings apply may be intricate but have limited practical significance given the approximate nature of GAN training. extremely simple discrete probability space with only two elements. The augmentation operator T = 0 1 1 0 flips the elements. Mixed with probability α = 1 2 with the identity augmentation I (which keeps the distribution unchanged), we obtain the augmentation 1 2 T + 1 2 I = 1 2 1 1 1 1 which is a singular matrix and therefore not invertible. Intuitively, this operator smears any probability distribution into a degenerate equidistribution, from which the original can no longer be recovered. Similar considerations carry over to arbitrarily complicated linear operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Non-leaking elementary augmentations</head><p>In the following, we construct several examples of relatively large classes of elementary augmentations that do not leak and can therefore be used to form a chain of augmentations. Importantly, most of these classes are not inherently safe, as they are stochastic mixtures of even simpler augmentations, as discussed above. However, in many cases we can show that the degenerate situation only arises with specific choices of mixture distribution, which we can then avoid.</p><p>Specifically, for every type of augmentation, we identify a configuration where applying it with probability strictly less than 1 results in an invertible transformation. From the standpoint of this analysis, we interpret this stochastic skipping as modifying the augmentation operator itself, in a way that boosts the probability of leaving the input unchanged and reduces the probability of other outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Deterministic mappings</head><p>The simplest form of augmentation is a deterministic mapping, where the operator T f assigns to every image X a unique image f (X). In the most general setting f is any measurable function and T f x is the corresponding pushforward measure. When f is a diffeomorphism, T f acts by the usual change of variables formula with a density correction by a Jacobian determinant. These mappings are invertible as long as f itself is invertible. Conversely, if f is not invertible, then neither is T f .</p><p>Here it may be instructive to highlight the difference between f and T f . The former transforms the underlying space on which the probability distributions live -for example, if we are dealing with images of just two pixels (with continuous and unconstrained values), f is a nonlinear "warp" of the two-dimensional plane. In contrast, T f operates on distributions defined on this space -think of a continuous 2-dimensional function (density) on the aforementioned plane. The action of T f is to move the density around according to f , while compensating for thinning and concentration of the mass due to stretching. As long as f maps every distinct point to a distinct point, this warp can be reversed.</p><p>An important special case is that where f is a linear transformation of the space. Then the invertibility of T f becomes a simpler question of the invertibility of a finite-dimensional matrix that represents f .</p><p>Note that when an invertible deterministic transformation is skipped probabilistically, the determinism is lost, and very specific choices of transformation could result in non-invertibility (see e.g. the example of flipping above). We only use deterministic mappings as building blocks of other augmentations, and never apply them in isolation with stochastic skipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2 Transformation group augmentations</head><p>Many commonly used augmentations are built from transformations that act as a group under sequential composition. Examples of this are flips, translations, rotations, scalings, shears, and many color and intensity transformations. We show that a stochastic mixture of transformations within a finitely generated abelian group is non-leaking as long as the mixture weights are chosen from a non-degenerate distribution.</p><p>As an example, the four deterministic augmentations {R 0 , R 90 , R 180 , R 270 } that rotate the images to every one of the 90-degree increment orientations constitute a group. This is seen by checking that the set satisfies the axiomatic definition of a group. Specifically, the set is closed, as composing two of elements always results in an element of the same set, e.g. R 270 R 180 = R 90 . It is also obviously associative, and has an identity element R 0 = I. Finally, every element has an inverse, e.g. R −1 90 = R 270 . We can now simply speak of powers of the single generator element, whereby the four group elements are written as {R 0 90 , R 1 90 , R 2 90 , R 3 90 } and further (as well as negative) powers "wrap over" to the same elements. This group is isomorphic to Z 4 , the additive group of integers modulo 4.</p><p>A group of rotations is compact due to the wrap-over effect. An example of a non-compact group is that of translations (with non-periodic boundary conditions): compositions of translations are still translations, but one cannot wrap over. Furthermore, more than one generator element can be present (e.g. y-translation in addition to x-translation), but we require that these commute, i.e. the order of applying the transformations must not matter (in which case the group is called abelian).</p><p>Similar considerations extend to continuous Lie groups, e.g. that of rotations by any angle; here the generating element is replaced by an infinitesimal generator from the corresponding Lie algebra, and the discrete powers by the continuous exponential mapping. For example, continuous rotation transformations are isomorphic to the group SO(2), or U(1).</p><p>In the following subsections show that for finitely generated abelian groups whose identity element matches the identity augmentation, stochastic mixtures of augmentations within the group are invertible, as long as the appropriate Fourier transform of the probability distribution over the elements has no zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete compact one-parameter groups</head><p>We demonstrate the key points in detail with the simple but relevant case of a discrete compact one-parameter group and generalize later. Let G be a deterministic augmentation that generates the finite cyclic group {G i } N −1 i=0 of order N (e.g. the four 90-degree rotations above), such that the element G 0 is the identity mapping that leaves its input unchanged.</p><p>Consider a stochastic augmentation T that randomly applies an element of the group, with the probability of choosing each element given by the probability vector p ∈ R N (where p is nonnegative and sums to 1):</p><formula xml:id="formula_22">T = N −1 i=0 p i G i (8)</formula><p>To show the conditions for invertibility of T , we build an operator U that explicitly inverts T , namely UT = I = G 0 . Whenever this is possible, T is invertible and non-leaking. We build U from the same group elements with a different weighting 5 vector q ∈ R N :</p><formula xml:id="formula_23">U = N −1 j=0 q j G j<label>(9)</label></formula><p>We now seek a vector q for which UT = I, that is, for which U is the desired inverse. Now,</p><formula xml:id="formula_24">UT = N −1 i=0 p i G i   N −1 j=0 q j G j   (10) = N −1 i,j=0 p i q j G i+j<label>(11)</label></formula><p>The powers of the group operation, as well as the indices of the weight vectors, are taken as modulo N due to the cyclic wrap-over of the group element. Collecting the terms that correspond to each G k in this range and changing the indexing accordingly, we arrive at:</p><formula xml:id="formula_25">= N −1 k=0 N −1 l=0 p l q k−l G k (12) = N −1 k=0 [p ⊗ q] k G k<label>(13)</label></formula><p>where we observe that the multiplier in front of each G k is given by the cyclic convolution of the elements of the vectors p and q. This can be written as a pointwise product in terms of the Discrete Fourier Transform F, denoting the DFT's of p and q by a hat:</p><formula xml:id="formula_26">= N −1 k=0 [F −1 (p q)] k G k<label>(14)</label></formula><p>To recover the sought after inverse, assuming every element ofp is nonzero, we setq i = 1 pi for all i:</p><formula xml:id="formula_27">= N −1 k=0 [F −1 (p p −1 )] k G k (15) = N −1 k=0 [F −1 1] k G k<label>(16)</label></formula><formula xml:id="formula_28">= G 0 (17) = I<label>(18)</label></formula><p>Here, we take advantage of the fact that the inverse DFT of a constant vector of ones is the vector [1, 0, ..., 0].</p><p>In summary, the product of U and T effectively computes a convolution between their respective group element weights. This convolution assigns all of the weight to the identity element precisely when one hasq i = 1 pi , for all i, whereby U is the inverse of T . This inverse only exists when the Fourier transformp i of the augmentation probability weights has no zeros.</p><p>The intuition is that the mixture of group transformations "smears" probability mass among the different transformed versions of the distribution. Analogously to classical deconvolution, this smearing can be undone ("deconvolved") as long as the convolution does not destroy any frequencies by scaling them to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some noteworthy consequences of this are:</head><p>• Assume p is a constant vector 1 N 1, that is, the augmentation applies the group elements with uniform probability. In this casep = δ 0 and convolution with any zero-mean weight vector is zero. This case is almost certain to cause leaks of the group elements themselves. To see this directly, the mixed augmentation operator is now T := 1 N N −1 j=0 G j . Consider the true distribution of training samples y, and a version y = G k y into which some element of the transformation group has leaked. Now,</p><formula xml:id="formula_29">T y = T (G k y) = 1 N N −1 j=0 G j G k y = 1 N N −1 j=0 G j+k y = 1 N N −1 j=0 G j y = T y<label>(19)</label></formula><p>(recalling the modulo arithmetic in the group powers). By Eq. 7, this is a leak, and the training may equally well learn the distribution G k y rather than y. By the same reasoning, any mixture of transformed elements may be learned (possibly even a different one for each image). • Similarly, if p is periodic (with period that is some integer factor of N , other than N itself), the Fourier transform is a sparse sequence of spikes separated by zeros. Another viewpoint to this is that the group has a subgroup, whose elements are chosen uniformly. Similar to above, this is almost certain to cause leaks with elements of that subgroup. • With more sporadic zero patterns, the leaks can be seen as "conditional": while the augmentation operator has a null space, it is not generally possible to write an equivalent of Eq. 19 without setting conditions on the distribution y itself. In these cases, leaks only occur for specific kinds of distributions, e.g., when a sufficient amount of group symmetry is already present in the distribution itself. For example, consider a dataset where all four 90 degree orientations of any image are equally likely, and an augmentation that performs either a 0 or 90 degree rotation at equal probability. This corresponds to the probability vector p = [0.5, 0.5, 0, 0] over the four elements of the 90-degree rotation group. This distribution has a single zero in its Fourier transform. The associated leak might manifest as the generator only learning to produce images in orientations 0 and 180 degrees, and relying on the augmentation to fill the gaps.</p><p>Such a leak could not happen in e.g. a dataset depicting upright faces, and the failure of invertibility would be harmless in this case. However, this may no longer hold when the augmentation is a part of a composed pipeline, as other augmentations may have introduced partial invariances that were not present in the original data.</p><p>In our augmentations involving compact groups (rotations and flips), we always choose the elements with a uniform probability, but importantly, only perform the augmentation with some probability less than one. This combination can be viewed as increasing the probability of choosing the group identity element. The probability vector p is then constant, except for having a higher value at p 0 ; the Fourier transform of such a vector has no zeros.</p><p>Non-compact discrete one-parameter groups The above reasoning can be extended to groups which are not compact, in particular translations by integer offsets (without periodic boundaries).</p><p>In the discrete case, such a group is necessarily isomorphic to the additive group Z of all integers, and no modulo integer arithmetic is performed. The mixture density is then a two-sided sequence {p i } with i ∈ Z, and the appropriate Fourier transform maps this to a periodic function. By an analogous reasoning with the previous subsection, the invertibility holds as long as this spectrum has no zeros.</p><p>Continuous one-parameter groups With suitable technical care, these arguments can be extended to continuous groups with elements G φ indexed by a continuous parameter φ. In the compact case (e.g. continuous rotation), the group elements wrap over at some period L, such that G φ+L = G φ . In the non-compact case (e.g. translation (addition) and scaling (multiplication) by real-valued amounts) no such wrap-over occurs. The compact and non-compact groups are isomorphic to U(1), and the additive group R, respectively. Stochastic mixtures of these group elements are expressed by probability density functions p(φ), with φ ∈ [0, L) if the group is compact, and φ ∈ R otherwise. The Fourier transforms are replaced by the appropriate generalizations, and the invertibility holds when the spectrum has no zeros.</p><p>Here it is important to use the correct parametrization of the group. Note that one could in principle parametrize e.g. rotations in arbitrary ways, and it may seem ambiguous as to what parametrization to use, which would appear to render concepts like uniform distribution meaningless. The issue arises when replacing the sums in the earlier formulas with integrals, whereby one needs to choose a measure of integration. These findings apply specifically to the natural Haar measure and the associated parametrization -essentially, the measure that accumulates at constant rate when taking small steps in the group by applying the infinitesimal generator. For rotation groups, the usual "area" measure over the angular parametrization coincides with the Haar measure, and therefore e.g. uniform distribution is taken to mean that all angles are chosen equally likely. For translation, the natural Euclidian distance is the correct parametrization. For other groups, such as scaling, the choice is a bit more nuanced: when composing scaling operations, the scale factor combines by multiplication instead of addition, so the natural parametrization is the logarithm of the scale factor.</p><p>For continuous compact groups (rotation), we use the same scheme as in the discrete case: uniform probability mixed with identity at a probability greater than zero.</p><p>For continuous non-compact groups, the Fourier transform of the normal distribution has no zeros and results in an invertible augmentation when used to choose among the group elements. Other distributions with this property are at least the α-stable and more generally the infinitely divisible family of distributions. When the parametrization is logarithmic, we may instead use exponentiated values from these distributions (e.g. the log-normal distribution). Finally, stochastically mixing zero-mean normal distributed variables with identity does not introduce zeros to the FT, as it merely lifts the already positive values of the spectrum.</p><p>Multi-parameter abelian groups Finally, these findings generalize to groups that are products of a finite number of single-parameter groups, provided that the elements of the different groups commute among each other (in other words, finitely generated abelian groups). An example of this is the group of 2-dimensional translations obtained by considering x-and y-translations simultaneously. <ref type="bibr" target="#b5">6</ref> The Fourier transforms are replaced with suitable multi-dimensional generalizations, and the probability distributions and their Fourier transforms obtain multidimensional domains accordingly.</p><p>Discussion Invertibility is a sufficient condition to ensure the absence of leaks. However, it may not always be necessary: in the case of non-compact groups, a hypothesis could be made that even a technically non-invertible operator does not leak. For example, a shift augmentation with uniform distributed offset on a continuous interval is not invertible, as the Fourier transform of its density is a sinc function with periodic zeros (except at 0). This only allows for leaks of zero-mean functions whose FT is supported on this evenly spaced set of frequencies -in other words, infinitely periodic functions. Even though such functions are in the null space of the augmentation operator, they cannot be added to any density in an infinite domain without violating non-negativity, and so we may hypothesize that no leak can in fact occur. In practice, however, the near-zero spectrum values might allow for a periodic leak modulated by a wide window function to occur for very specific (and possibly contrived) data distributions.</p><p>In contrast, straightforward examples and practical demonstrations of leaks are easily found for compact groups, e.g. with uniform or periodic rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.3 Noise and image filter augmentations</head><p>We refer to Theorem 5.3. of Bora et al. <ref type="bibr" target="#b3">[4]</ref>, where it is shown that in a setting effectively identical to ours, addition of noise that is independent of the image is an invertible operation as long as the Fourier spectrum of the noise distribution does not contain zeros. The reason is that addition of mutually independent random variables results in a convolution of their probability distributions. Similar to groups, this is a multiplication in the Fourier domain, and the zeros correspond to irrevocable loss of information, making the inversion impossible. The inverse can be realized by "deconvolution", or division in the Fourier domain.</p><p>A potential source of confusion is that the Fourier transform is commonly used to describe spatial correlations of noise in signal processing. We refer to a different concept, namely the Fourier transform of the probability density of the noise, often called the characteristic function in probability literature (although correlated noise is also subsumed by this analysis).</p><p>Gaussian product noise In our setting, we also randomize the magnitude parameter of the noise, in effect stochastically mixing between different noise distributions. The above analysis subsumes this case, as the mixture is also a random noise, with a density that is a weighted blend between the densities of the base noises. However, the noise is no longer independent across points, so its joint distribution is no longer separable to a product of marginals, and one must consider the joint Fourier transform in full dimension.</p><p>Specifically, we draw the per-pixel noise from a normal distribution and modulate this entire noise field by a multiplication with a single (half-)normal random number. The resulting distribution has an everywhere nonzero Fourier transform and hence is invertible. To see this, first consider two standard normal distributed random scalars X and Y , and their product Z = XY (taken in the sense of multiplying the random variables, not the densities). Then Z is distributed according to the density p Z (Z) = K0(|Z|) π , where K 0 is a modified Bessel function, and has the characteristic function (Fourier transform)p Z (ω) = 1 √ ω 2 +1 , which is everywhere positive <ref type="bibr" target="#b45">[46]</ref>. Then, considering our situation with a product of a normal distributed scalar X and an independent normal distributed vector Y ∈ R N , the N entries of the product Z = XY become mutually dependent. The marginal distribution of each entry is nevertheless exactly the above product distribution p Z . By Fourier slice theorem, all one-dimensional slices through the main axes of the characteristic function of Z must then coincide with the characteristic functionp Z of this marginal distribution. Finally, because the joint distribution is radially symmetric, so is the characteristic function, and this must apply to all slices through the origin, yielding the everywhere positive Fourier</p><formula xml:id="formula_30">transformp Z (ω) = 1 √ |ω| 2 +1</formula><p>. When stochastically mixed with identity (as is our random skipping procedure), the Fourier Transform values are merely lifted towards 1 and no new zero-crossings are introduced.</p><p>Additive noise in transformed bases Similar notes apply to additive noise in a different basis: one can consider the noise augmentation as being flanked by an invertible deterministic (possibly also nonlinear) basis transformation and its inverse. It then suffices to show that the additive noise has a non-zero spectrum in isolation. In particular, multiplicative noise with a non-negative distribution can be viewed as additive noise in logarithmic space and is invertible if the logarithmic version of the noise distribution has no zeros in its Fourier transform. The image-space filters are a combination of a linear basis transformation to the wavelet basis, and additive Gaussian noise under a non-linear logarithmic transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.4 Random projection augmentations</head><p>The cutout augmentation (as well as e.g. the pixel and patch blocking in AmbientGAN <ref type="bibr" target="#b3">[4]</ref>) can be interpreted as projecting a random subset of the dimensions to zero.</p><p>Let P 1 , P 2 , ..., P N be a set of deterministic projection augmentation operators with the defining property that P 2 j = P j . For example, each one of these operators can set a different fixed rectangular region to zero. Clearly the individual projections have a null space (unless they are the identity projection) and they are not invertible in isolation.</p><p>Consider a stochastic augmentation that randomly applies one of these projections, or the identity. Let p 0 , p 1 , ..., p N denote the discrete probabilities of choosing the identity operator I for p 0 , and P k for the remaining p k . Define the mixture of the projections as: Assume that T is not invertible, i.e. there exists a probability distribution x = 0 such that T x = 0. Then</p><formula xml:id="formula_31">T = p 0 I + N j=1 p j P j<label>(20)</label></formula><formula xml:id="formula_32">0 = T x = p 0 x + N j=1 p j P j x<label>(21)</label></formula><p>and rearranging,</p><formula xml:id="formula_33">N j=1 p j P j x = −p 0 x<label>(22)</label></formula><p>Under reasonable technical assumptions (e.g. discreteness of the pixel intensity values, such as justified in Theorem 5.4. of Bora et al. <ref type="bibr" target="#b3">[4]</ref>), we can consider the inner product of both sides of this equation with x:</p><formula xml:id="formula_34">N j=1 p j x, P j x = −p 0 x, x<label>(23)</label></formula><p>The right side of this equation is strictly negative if the probability p 0 of identity is greater than zero, as x = 0. The left side is a non-negative sum of non-negative terms, as the inner product of a vector with its projection is non-negative. Therefore, the assumption leads to a contradiction unless p 0 = 0; conversely, random projection augmentation does not leak if there is a non-zero probability that it produces the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Practical considerations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5.1 Conditioning</head><p>In practical numerical computation, an operator that is technically invertible may nevertheless be so close to a non-invertible configuration that inversion fails in practice. Assuming a finite state space, this notion is captured by the condition number, which is infinite when the matrix is singular, and large when it is singular for all practical purposes. The same consideration applies to infinite state spaces, but the appropriate technical notion of conditioning is less clear.</p><p>The practical value of the analysis in this section is in identifying the conditions where exact noninvertibility happens, so that appropriate safety margin can be kept. We achieve this by regulating the probability p of performing a given augmentation, and keeping it at a safe distance from p = 1 which for many of the augmentations corresponds to a non-invertible condition (e.g. uniform distribution over compact group elements).</p><p>For example, consider applying transformations from a finite group with a uniform probability distribution, where the augmentation is applied with probability p. In a finite state space, a matrix corresponding to this augmentation has 1 − p for its smallest singular value, and 1 for the largest, resulting in condition number 1/(1 − p) which approaches infinity as p approaches one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5.2 Pixel-level effects and boundaries</head><p>When dealing with images represented on finite pixel grids, naive practical implementations of some of the group transformations do not strictly speaking form groups. For example, a composition of two continuous rotations of an image with angles φ and θ does not generally reproduce the same image as a single rotation by angle φ + θ, if the transformed image is resampled to the rectangular pixel grid twice. Furthermore, parts of the image may fall outside the boundaries of the grid, whereby their values are lost and cannot be restored even if a reverse transformation is made afterwards, unless special care is taken. These effects may become significant when multiple transformations are composed.</p><p>In our implementation, we mitigate these issues as much as possible by accumulating the chain of transformations into a matrix and a vector representing the total affine transformation implemented by all the grouped augmentations, and only then applying it on the image. This is possible because all the augmentations we use are affine transformations in the image (or color) space. Furthermore, prior to applying the geometric transformations, the images are reflection padded and scaled to double resolution (and conversely, cropped and downscaled afterwards). Effectively the image is then treated as an infinite tiling of suitably reflected finer-resolution copies of itself, and a practical target-resolution crop is only sampled at augmentation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation details</head><p>We implemented our techniques on top of the StyleGAN2 official TensorFlow implementation <ref type="bibr" target="#b6">7</ref> . We kept most of the details unchanged, including network architectures <ref type="bibr" target="#b20">[21]</ref>, weight demodulation <ref type="bibr" target="#b20">[21]</ref>, path length regularization <ref type="bibr" target="#b20">[21]</ref>, lazy regularization <ref type="bibr" target="#b20">[21]</ref>, style mixing regularization <ref type="bibr" target="#b19">[20]</ref>, bilinear filtering in all up/downsampling layers <ref type="bibr" target="#b19">[20]</ref>, equalized learning rate for all trainable parameters <ref type="bibr" target="#b18">[19]</ref>, minibatch standard deviation layer at the end of the discriminator <ref type="bibr" target="#b18">[19]</ref>, exponential moving average of generator weights <ref type="bibr" target="#b18">[19]</ref>, non-saturating logistic loss <ref type="bibr" target="#b13">[14]</ref> with R 1 regularization <ref type="bibr" target="#b29">[30]</ref>, and Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with β 1 = 0, β 2 = 0.99, and = 10 −8 .</p><p>We ran our experiments on a computing cluster with a few dozen NVIDIA DGX-1s, each containing 8 Tesla V100 GPUs, using TensorFlow 1.14.0, PyTorch 1.1.0 (for comparison methods), CUDA 10.0, and cuDNN 7.6.3. We used the official pre-trained Inception network 8 to compute FID, KID, and Inception score.  <ref type="figure" target="#fig_2">Figure 24</ref>: Hyperparameters used in each experiment. <ref type="figure" target="#fig_2">Figure 24</ref> shows the hyperparameters that we used in our experiments, as well as the original StyleGAN2 config F <ref type="bibr" target="#b20">[21]</ref>. We performed all training runs using 8 GPUs and continued the training until the discriminator had seen a total of 25M real images, except for CIFAR-10, where we used 2 GPUs and 100M images. We used minibatch size of 64 when possible, but reverted to 32 for METFACES in order to avoid running out of GPU memory. Similar to StyleGAN2, we evaluated the minibatch standard deviation layer independently over the images processed by each GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Hyperparameters and training configurations</head><p>Dataset augmentation We did not use dataset augmentation in any of our experiments with FFHQ, LSUN CAT, or CIFAR-10, except for the FFHQ-140k case and in <ref type="figure">Figure 20</ref>. In particular, we feel that leaky augmentations are inappropriate for CIFAR-10 given its status as a standard benchmark dataset, where dataset/leaky augmentations would unfairly inflate the results. METFACES, BRECAHAD, and AFHQ DOG are horizontally symmetric in nature, so we chose to enable dataset x-flips for these datasets to maximize result quality.</p><p>Network capacity We follow the original StyleGAN2 configuration for high-resolution datasets (≥ 512 2 ): a layer operating on N = w × h pixels uses min 2 16 / √ N , 512 feature maps. With CIFAR-10 we use 512 feature maps for all layers. In the 256 × 256 configuration used with FFHQ and LSUN CAT, we facilitate extensive sweeps over dataset sizes by decreasing the number of feature maps to min 2 15 / √ N , 512 .</p><p>Learning rate and weight averaging We selected the optimal learning rates using grid search and found that it is generally beneficial to use the highest learning rate that does not result in training instability. We also found that larger minibatch size allows for a slightly higher learning rate. For the moving average of generator weights <ref type="bibr" target="#b18">[19]</ref>, the natural choice is to parameterize the decay rate with respect to minibatches -not individual images -so that increasing the minibatch size results in a longer decay. Furthermore, we observed that a very long moving average consistently gave the best results on CIFAR-10. To reduce startup bias, we linearly ramp up the length parameter from 0 to 500k over the first 10M images.</p><p>R1 regularization Karras et al. <ref type="bibr" target="#b20">[21]</ref> postulated that the best choice for the R 1 regularization weight γ is highly dependent on the dataset. We thus performed extensive grid search for each column 7 https://github.com/NVlabs/stylegan2 8 http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz in <ref type="figure" target="#fig_2">Figure 24</ref>, considering γ ∈ {0.001, 0.002, 0.005, . . . , 20, 50, 100}. Although the optimal γ does vary wildly, from 0.01 to 10, it seems to scale almost linearly with the resolution of the dataset. In practice, we have found that a good initial guess is given by γ 0 = 0.0002 · N/M , where N = w × h is the number of pixels and M is the minibatch size. Nevertheless, the optimal value of γ tends to vary depending on the dataset, so we recommend experimenting with different values in the range γ ∈ [γ 0 /5, γ 0 · 5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed-precision training</head><p>We utilize the high-performance Tensor Cores available in Volta-class GPUs by employing mixed-precision FP16/FP32 training in all of our experiments (with two exceptions, discussed in Appendix D.2). We store the trainable parameters with full FP32 precision for the purposes of optimization but cast them to FP16 before evaluating G and D. The main challenge with mixed-precision training is that the numerical range of FP16 is limited to ∼ ±2 <ref type="bibr" target="#b15">16</ref> , as opposed to ∼ ±2 128 for FP32. Thus, any unexpected spikes in signal magnitude -no matter how transient -will immediately collapse the training dynamics. We found that the risk of such spikes can be reduced drastically using three tricks: first, by limiting the use of FP16 to only the 4 highest resolutions, i.e., layers for which N layer ≥ N dataset /(2 × 2) 4 ; second, by pre-normalizing the style vector s and each row of the weight tensor w before applying weight modulation and demodulation 9 ; and third, by clamping the output of every convolutional layer to ±2 8 , i.e., an order of magnitude wider range than is needed in practice. We observed about 60% end-to-end speedup from using FP16 and verified that the results were virtually identical to FP32 on our baseline configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>We enable class-conditional image generation on CIFAR-10 by extending the original StyleGAN2 architecture as follows. For the generator, we embed the class identifier into a 512dimensional vector that we concatenate with the original latent code after normalizing each, i.e., z = concat norm(z), norm(embed(c)) , where c is the class identifier. For the discriminator, we follow the approach of Miyato and Koyama <ref type="bibr" target="#b31">[32]</ref> by evaluating the final discriminator output as D(x) = norm embed(c) · D (x) T , where D (x) corresponds to the feature vector produced by the last layer of D. To compute FID, we generate 50k images using randomly selected class labels and compare their statistics against the 50k images from the training set. For IS, we compute the mean over 10 independent trials using 5k generated images per trial. As illustrated in <ref type="figure" target="#fig_0">Figures 11b and 24</ref>, we found that we can improve the FID considerably by disabling style mixing regularization <ref type="bibr" target="#b19">[20]</ref>, path length regularization <ref type="bibr" target="#b20">[21]</ref>, and residual connections in D <ref type="bibr" target="#b20">[21]</ref>. Note that all of these features are highly beneficial on higher-resolution datasets such as FFHQ. We find it somewhat alarming that they have precisely the opposite effect on CIFAR-10 -this suggests that some previous conclusions reached in the literature using CIFAR-10 may fail to generalize to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Comparison methods</head><p>We implemented the comparison methods shown in Figures 8a on top of our baseline configuration, identifying the best-performing hyperparameters for each method via extensive grid search. Furthermore, we inspected the resulting network weights and training dynamics in detail to verify correct behavior, e.g., that with the discriminator indeed learns to correctly handle the auxiliary tasks with PA-GAN and auxiliary rotations. We found zCR and WGAN-GP to be inherently incompatible with our mixed-precision training setup due to their large variation in gradient magnitudes. We thus reverted to full-precision FP32 for these methods. Similarly, we found lazy regularization to be incompatible with bCR, zCR, WGAN-GP, and auxiliary rotations. Thus, we included their corresponding loss terms directly into our main training loss, evaluated on every minibatch.</p><p>bCR We implement balanced consistency regularization proposed by Zhao et al. <ref type="bibr" target="#b52">[53]</ref> by introducing two new loss terms as shown in <ref type="figure">Figure 2a</ref>. We set λ real = λ fake = 10 and use integer translations on the range of [−8, +8] pixels. In <ref type="figure">Figure 20</ref>, we also perform experiments with x-flips and arbitrary rotations.</p><p>zCR In addition to bCR, Zhao et al. <ref type="bibr" target="#b52">[53]</ref> also propose latent consistency regularization (zCR) to improve the diversity of the generated images. We implement zCR by perturbing each component of the latent z by σ noise = 0.1 and encouraging the generator to maximize the L 2 difference between the generated images, measured as an average over the pixels, with weight λ gen = 0.02. Similarly, we encourage the discriminator to minimize the L 2 difference in D(x) with weight λ dis = 0.2.</p><p>PA-GAN Zhang and Khoreva <ref type="bibr" target="#b47">[48]</ref> propose to reduce overfitting by requiring the discriminator to learn an auxiliary checksum task. This is done by providing a random bit string as additional input to D, requiring that the sign of the output is flipped based on the parity of bits that were set, and dynamically increasing the number of bits when overfitting is detected. We select the number of bits using our r t heuristic with target 0.95. Given the value of p produced by the heuristic, we calculate the number of bits as k = p · 16 . Similar to Zhang and Khoreva, we fade in the effect of newly added bits smoothly over the course of training. In practice, we use a fixed string of 16 bits, where the first k − 1 bits are sampled from Bernoulli(0.5), the k th bit is sampled from Bernoulli min(p · 16 − k + 1, 0.5) , and the remaining 16 − k bits are set to zero.</p><p>WGAN-GP For WGAN-GP, proposed by Gulrajani et al. <ref type="bibr" target="#b14">[15]</ref>, we reuse the existing implementation included in the StyleGAN2 codebase with λ = 10. We found WGAN-GP to be quite unstable in our baseline configuration, which necessitated us to disable mixed-precision training and lazy regularization, as well as to settle for a considerably lower learning rate η = 0.0010.</p><p>Auxiliary rotations Chen et al. <ref type="bibr" target="#b5">[6]</ref> propose to improve GAN training by introducing an auxiliary rotation loss for G and D. In addition the main training objective, the discriminator is shown real images augmented with 90 • rotations and asked to detect their correct orientation. Similarly, the generator is encouraged to produce images whose orientation is easy for the discriminator to detect correctly. We implement this method by introducing two new loss terms that are evaluated on a 4× larger minibatch, consisting of rotated versions of the images shown to the discriminator as a part of the main loss. We extend the last layer of D to output 5 scalar values instead of one and interpret the last 4 components as raw logits for softmax cross-entropy loss. We weight the additional loss terms using α = 10 for G, and β = 5 for D.</p><p>Spectral normalization Miyato et al. <ref type="bibr" target="#b30">[31]</ref> propose to regularize the discriminator by explicitly enforcing an upper bound for its Lipschitz constant, and several follow-up works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b37">38]</ref> have found it to be beneficial. Given that spectral normalization is effectively a no-op when applied to the StyleGAN2 generator <ref type="bibr" target="#b20">[21]</ref>, we apply it only to the discriminator. We ported the original Chainer implementation <ref type="bibr" target="#b9">10</ref> to TensorFlow, and applied it to the main convolution layers of D. We found it beneficial to not use spectral normalization with the FromRGB layer, residual skip connections, or the last fully-connected layer.</p><p>Freeze-D Mo et al. <ref type="bibr" target="#b32">[33]</ref> propose to freeze the first k layers of the discriminator to improve results with transfer learning. We tested several different choices for k; the best results were given by k = 10 in <ref type="figure" target="#fig_7">Figure 9</ref> and by k = 13 in <ref type="figure" target="#fig_0">Figure 11b</ref>. In practice, this corresponds to freezing all layers operating at the 3 or 4 highest resolutions, respectively.</p><p>BigGAN BigGAN results in <ref type="figure" target="#fig_0">Figures 19 and 18</ref> were run on a modified version of the original BigGAN PyTorch implementation <ref type="bibr" target="#b10">11</ref> . The implementation was adapted for unconditional operation following Schönfeld et al. <ref type="bibr" target="#b37">[38]</ref> by matching their hyperparameters, replacing class-conditional BatchNorm with self-modulation, where the BatchNorm parameters are conditioned only on the latent vector z, and not using class projection in the discriminator.</p><p>Mapping network depth For the "Shallow mapping" case in <ref type="figure" target="#fig_6">Figure 8a</ref>, we reduced the depth of the mapping network from 8 to 2. Reducing the depth further than 2 yielded consistently inferior results, confirming the usefulness of the mapping network. In general, we found depth 2 to yield slightly better results than depth 8, making it a good default choice for future work.</p><p>Adaptive dropout Dropout <ref type="bibr" target="#b41">[42]</ref> is a well-known technique for combating overfitting in practically all areas of machine learning. In <ref type="figure" target="#fig_6">Figure 8a</ref>, we employ multiplicative Gaussian dropout for all layers of the discriminator, similar to the approach employed by Karras et al. <ref type="bibr" target="#b18">[19]</ref> in the context of LSGAN loss <ref type="bibr" target="#b27">[28]</ref>. We adjust the standard deviation dynamically using our r t heuristic with target 0.6, so that the resulting p is used directly as the value for σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 MetFaces dataset</head><p>We have collected a new dataset, MetFaces, by extracting images of human faces from the Metropolitan Museum of Art online collection. Dataset images were searched using terms such as 'paintings', 'watercolor' and 'oil on canvas', and downloaded via the https://metmuseum.github.io/ API. This resulted in a set of source images that depicted paintings, drawings, and statues. Various automated heuristics, such as face detection and image quality metrics, were used to narrow down the set of images to contain only human faces. A manual selection pass over the remaining images was performed to weed out poor quality images not caught by automated filtering. Finally, faces were cropped and aligned to produce 1,336 high quality images at 1024 2 resolution.</p><p>The whole dataset, including the unprocessed images, is available at https://github.com/NVlabs/metfaces-dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Energy consumption</head><p>Computation is a core resource in any machine learning project: its availability and cost, as well as the associated energy consumption, are key factors in both choosing research directions and practical adoption. We provide a detailed breakdown for our entire project in <ref type="table">Table 25</ref> in terms of both GPU time and electricity consumption. We report expended computational effort as single-GPU years (Volta class GPU). We used a varying number of NVIDIA DGX-1s for different stages of the project, and converted each run to single-GPU equivalents by simply scaling by the number of GPUs used.</p><p>We followed the Green500 power measurements guidelines <ref type="bibr" target="#b11">[12]</ref> similarly to Karras et al. <ref type="bibr" target="#b20">[21]</ref>. The entire project consumed approximately 300 megawatt hours (MWh) of electricity. Almost half of the total energy was spent on exploration and shaping the ideas before the actual paper production started. Subsequently the majority of computation was targeted towards the extensive sweeps shown in various figures. Given that ADA does not significantly affect the cost of training a single model, e.g., training StyleGAN2 <ref type="bibr" target="#b20">[21]</ref> with 1024 × 1024 FFHQ still takes approximately 0.7 MWh.  <ref type="figure">Figure 25</ref>: Computational effort expenditure and electricity consumption data for this project. The unit for computation is GPU-years on a single NVIDIA V100 GPU -it would have taken approximately 135 years to execute this project using a single GPU. See the text for additional details about the computation and energy consumption estimates. Early exploration includes all training runs that affected our decision to start this project. Paper exploration includes all training runs that were done specifically for this project, but were not intended to be used in the paper as-is. Setting up the baselines includes all hyperparameter tuning for the baselines. Figures provides a per-figure breakdown, and underlines that just reproducing all the figures would require over 50 years of computation on a single GPU. Results intentionally left out includes additional results that were initially planned, but then left out to improve focus and clarity. Wasted due to technical issues includes computation wasted due to code bugs and infrastructure issues. Code release covers testing and benchmarking related to the public release.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convergence of FFHQ (256 × 256) (b) Discriminator outputs, 50k (c) Discriminator outputs, 20k (a) Convergence with different training set sizes. "140k" means that we amplified the 70k dataset by 2× through x-flips; we do not use data amplification in any other case. (b,c) Evolution of discriminator outputs during training. Each vertical slice shows a histogram of D(x), i.e., raw logits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Leaking behavior of three example augmentations, shown as FID w.r.t. the probability of executing the augmentation. Each dot represents a complete training run, and the blue Gaussian mixture is a visualization aid. The top row shows generated example images from selected training runs, indicated by uppercase letters in the plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a-c) Impact of p for different augmentation categories and dataset sizes. The dashed gray line indicates baseline FID without augmentations. (d) Convergence curves for selected values of p using geometric augmentations with 10k training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5c shows the resulting p over time. With a 2k training set, augmentations were applied almost always towards the end. This exceeds the practical rv target sweep (b) rt target sweep (c) Evolution of p over training (d) Evolution of rt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Behavior of our adaptive augmentation strength heuristics in FFHQ. (a,b) FID for different training set sizes as a function of the target value for r v and r t . The dashed horizontal lines indicate the best fixed augmentation probability p found using grid search, and the dashed vertical line marks the target value we will use in subsequent tests. (c) Evolution of p over the course of training using heuristic r t . (d) Evolution of r t values over training. Dashes correspond to the fixed p values in (b). (a) Training curves for FFHQ with different training set sizes using adaptive augmentation. (b) The supports of real and generated images continue to overlap. (c) Example magnitudes of the gradients the generator receives from the discriminator as the training progresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a-c) FID as a function of training set size, reported as median/min/max over 3 training runs. (d) Average of 10k random images generated using the networks trained with 5k subset of FFHQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>BaselineFigure 8 :</head><label>8</label><figDesc>(a) We report the mean and standard deviation for each comparison method, calculated over 3 training runs. (b) FID as a function of discriminator capacity, reported as median/min/max over 3 training runs. We scale the number of feature maps uniformly across all layers by a given factor (x-axis). The baseline configuration (no scaling) is indicated by the dashed vertical line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Transfer learning FFHQ starting from a pre-trained CELEBA-HQ model, both 256 × 256. (a) Training convergence for our baseline method and Freeze-D [33]. (b) The same configurations with ADA. (c) FIDs as a function of dataset size. (d) Effect of source and target datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Example generated images for several datasets with limited amount of training data, trained using ADA. We use transfer learning with METFACES and train other datasets from scratch. See Appendix A for uncurated results and real images, and Appendix D for our training configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 17 :</head><label>17</label><figDesc>Generated and real images for CIFAR-10 in the unconditional setting (top) and each class in the conditional setting (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Mean images for bCR with FFHQ-5k (b) bCR vs. dataset augment (c) Effect of dataset x-flips</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 20 :Figure 21 :</head><label>2021</label><figDesc>(a) Examples of bCR leaking to generated images. (b) Comparison between dataset augmentation and bCR using ±8px translations and x-flips. (c) In general, dataset x-flips can provide a significant boost to FID in cases where they are appropriate. For baseline, the effect is almost equal to doubling the size of training set, as evidenced by the consistent 2× horizontal offset between the blue curves. With ADA the effect is somewhat weaker. Extended version of Figure 4, illustrating the individual and cumulative effect of different augmentation categories with increasing augmentation probability p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>The transformations include adjusting brightness (line 50), contrast (line 53), and saturation (line 63), as well as flipping the luma axis while keeping the chroma unchanged (line 57) and rotating the hue axis by an arbitrary amount (line 60). 1: input: original image X, augmentation probability p 2: output: augmented image Y 3: (w, h) ← SIZE(X) 4: Y ← CONVERT(X, FLOAT) Yx,y ∈ [−1, +1] 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 22 :</head><label>22</label><figDesc>Pseudocode and example images for geometric and color transformations (Appendix B.1). We illustrate the effect of each individual transformation (apply) using four sets of parameter values, representing the 5 th , 35 th , 65 th , and 95 th percentiles of their corresponding distributions (sample). 1: input: original image X, augmentation probability p 2: output: augmented image Y 3: (w, h) ← SIZE(X) 4: Y ← CONVERT(X, FLOAT) Yx,y ∈ [−1, +1] 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 23 :</head><label>23</label><figDesc>Pseudocode and example images for image-space filtering and corruptions (Appendix B.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>BANDPASS H(z), b 2 = H(z)H(z −1 )H(z 2 )H(z −2 )H(−z 4 )H(−z −4 )/8 (4) BANDPASS H(z), b 3 = H(z)H(z −1 )H(−z 2 )H(−z −2 )/4(5)BANDPASS H(z), b 4 = H(−z)H(−z −1 )/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Again, T</head><label></label><figDesc>is a mixture of operators, however unlike in earlier examples, some (but not all) of the operators are non-invertible. Under what conditions on the probability distribution p is T invertible?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ours), untruncated Original StyleGAN2 config F, untruncated FID 15.71 -KID 2.88×10 3 -Recall 0.340 FID 97.72 -KID 89.76×10 3 -Recall 0.</figDesc><table><row><cell></cell><cell>Real images from the training set</cell></row><row><cell></cell><cell>Real images from the training set</cell></row><row><cell>ADA (Ours), untruncated ADA (Ours), untruncated ADA (Ours), untruncated</cell><cell>Original StyleGAN2 config F, untruncated Original StyleGAN2 config F, untruncated Original StyleGAN2 config F, untruncated</cell></row><row><cell>FID 3.55 -KID 0.66×10 3 -Recall 0.430 FID 7.40 -KID 1.16×10 3 -Recall 0.454</cell><cell>FID 5.13 -KID 1.54×10 3 -Recall 0.215 FID 19.37 -KID 9.62×10 3 -Recall 0.196</cell></row></table><note>FID 15.34 -KID 0.81×10 3 -Recall 0.261 FID 19.47 -KID 3.16×10 3 -Recall 0.350 Figure 12: Uncurated 1024×1024 results generated for METFACES (1336 images) with and without ADA, along with real images from the training set. Both generators were trained using transfer learning, starting from the pre-trained StyleGAN2 for FFHQ. We recommend zooming in.ADA (027 Figure 13: Uncurated 512×512 results generated for BRECAHAD [1] (1944 images) with and without ADA, along with real images from the training set. Both generators were trained from scratch. We recommend zooming in to inspect the image quality in detail.Figure 14: Uncurated 512×512 results generated for AFHQ CAT [7] (5153 images) with and without ADA, along with real images from the training set. Both generators were trained from scratch. We recommend zooming in to inspect the image quality in detail.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use 2× fewer feature maps, 2× larger minibatch, mixed-precision training for layers at ≥ 32 2 , η = 0.0025, γ = 1, and exponential moving average half-life of 20k images for generator weights.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This choice follows from StyleGAN2 training loop layout. The results are not sensitive to this parameter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">These distributions are probability measures over a non-discrete high dimensional space: for example, in our experiments with 256 × 256 RGB images, this space is R 256 * 256 * 3 = R 196608 .<ref type="bibr" target="#b3">4</ref> The addition and scalar multiplication of measures is taken to mean that for any set S to which x and y assign a measure, [αx + βy](S) = αx(S) + βy(S). When the measures are represented by density functions, this simplifies to the usual pointwise linear combination of the functions. We always mean addition and scalar</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Unlike with p, there is no requirement for q to represent a nonnegative probability density that sums to 1, as we are establishing the general invertibility of T without regard to its probabilistic interpretation. Note that U is never actually constructed or evaluated when applying our method in practice, and does not need to represent an operation that can be algorithmically implemented; our interest is merely to identify the conditions for its existence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">However, for example the non-abelian group of 3-dimensional rotations, SO(3), is not obtained as a product of the single-parameter "Euler angle" rotations along three axes, and therefore is not covered by the present formulation of our theory. The reason is that the three different rotations do not commute. One may of course still freely compose the three single-parameter rotation augmentations in sequence, but note that the combined effect can only induce a subset of possible probability distributions on SO(3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Note that our pre-normalization only affects the intermediate results; it has no effect on the final output of the convolution layer due to the subsequent post-normalization performed by weight demodulation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/pfnet-research/sngan_projection 11 https://github.com/ajbrock/BigGAN-PyTorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank David Luebke for helpful comments; Tero Kuosmanen and Sabu Nadarajan for their support with compute infrastructure; and Edgar Schönfeld for guidance on setting up unconditional BigGAN.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BreCaHAD: A dataset for breast cancer histopathological annotation and diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aksac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Demetrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ozyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alhajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Research Notes</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Demystifying MMD GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AmbientGAN: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised GANs via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">RandAugment: Practical automated data augmentation with a reduced search space. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ten lectures on wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">61</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Power measurement tutorial for the Green500 list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pyla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://www.top500.org/green500/resources/tutorials/" />
		<imprint>
			<date type="published" when="2020-03-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AutoGAN: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeLiGAN: Generative adversarial networks for diverse and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">cGANs with multi-hinge loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">E-LPIPS: robust perceptual image similarity via random transformation ensembles. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kettunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Megapixel size image creation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchesi</surname></persName>
		</author>
		<idno>abs/1706.00082</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Freeze the discriminator: a simple baseline for fine-tuning GANs. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image generation from small datasets via batch statistics adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A U-net based discriminator for generative adversarial networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised multi-modal styled content generation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sendik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Amortised MAP inference for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On data augmentation for GAN training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MineGAN: Effective knowledge transfer from GANs to target domains with few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transferring GANs: Generating images from limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The distribution of second order moment statistics in a normal system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wishart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="459" />
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative adversarial network in medical imaging: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PA-GAN: Improving GAN training by progressive augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient GAN training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Feature quantization improves GAN training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Improved consistency regularization for GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2002.04724</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Image augmentations for GAN training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
