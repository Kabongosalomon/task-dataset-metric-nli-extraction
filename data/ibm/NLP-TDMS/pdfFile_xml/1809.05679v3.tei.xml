<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Networks for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
							<email>liang.yao@northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Chicago IL</orgName>
								<address>
									<postCode>60611</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
							<email>chengsheng.mao@northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Chicago IL</orgName>
								<address>
									<postCode>60611</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
							<email>yuan.luo@northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Chicago IL</orgName>
								<address>
									<postCode>60611</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Networks for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text classification is a fundamental problem in natural language processing (NLP). There are numerous applications of text classification such as document organization, news filtering, spam detection, opinion mining, and computational phenotyping <ref type="bibr" target="#b0">(Aggarwal and Zhai 2012;</ref><ref type="bibr" target="#b25">Zeng et al. 2018</ref>). An essential intermediate step for text classification is text representation. Traditional methods represent text with hand-crafted features, such as sparse lexical features (e.g., bag-of-words and n-grams). Recently, deep learning models have been widely used to learn text representations, including convolutional neural networks (CNN) (Kim 2014) and recurrent neural networks (RNN) such as long short-term memory (LSTM) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber 1997)</ref>. As CNN and RNN prioritize locality and sequentiality <ref type="bibr" target="#b2">(Battaglia et al. 2018)</ref>, these deep learning models can capture semantic and syntactic information in local consecutive word sequences well, but may ignore global word cooccurrence in a corpus which carries non-consecutive and long-distance semantics <ref type="bibr" target="#b18">(Peng et al. 2018)</ref>.</p><p>Recently, a new research direction called graph neural networks or graph embeddings has attracted wide attention <ref type="bibr" target="#b2">(Battaglia et al. 2018;</ref><ref type="bibr" target="#b3">Cai, Zheng, and Chang 2018)</ref>. Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph embeddings.</p><p>In this work, we propose a new graph neural networkbased method for text classification. We construct a single large graph from an entire corpus, which contains words and documents as nodes. We model the graph with a Graph Convolutional Network (GCN) <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref>, a simple and effective graph neural network that captures high order neighborhoods information. The edge between two word nodes is built by word co-occurrence information and the edge between a word node and document node is built using word frequency and word's document frequency. We then turn text classification problem into a node classification problem. The method can achieve strong classification performances with a small proportion of labeled documents and learn interpretable word and document node embeddings. Our source code is available at https://github. com/yao8839836/text_gcn. To summarize, our contributions are as follows:</p><p>• We propose a novel graph neural network method for text classification. To the best of our knowledge, this is the first study to model a whole corpus as a heterogeneous graph and learn word and document embeddings with graph neural networks jointly. • Results on several benchmark datasets demonstrate that our method outperforms state-of-the-art text classification methods, without using pre-trained word embeddings or external knowledge. Our method also learn predictive word and document embeddings automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Traditional Text Classification</head><p>Traditional text classification studies mainly focus on feature engineering and classification algorithms. For feature engineering, the most commonly used feature is the bagof-words feature. In addition, some more complex features have been designed, such as n-grams <ref type="bibr" target="#b22">(Wang and Manning 2012)</ref> and entities in ontologies <ref type="bibr" target="#b4">(Chenthamarakshan et al. 2011</ref>). There are also existing studies on converting texts to graphs and perform feature engineering on graphs and subgraphs <ref type="bibr" target="#b14">(Luo, Uzuner, and Szolovits 2016;</ref><ref type="bibr" target="#b18">Rousseau, Kiagias, and Vazirgiannis 2015;</ref><ref type="bibr" target="#b20">Skianis, Rousseau, and Vazirgiannis 2016;</ref><ref type="bibr" target="#b12">Luo et al. 2014;</ref><ref type="bibr" target="#b13">Luo et al. 2015)</ref>. Unlike these methods, our method can learn text representations as node embeddings automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning for Text Classification</head><p>Deep learning text classification studies can be categorized into two groups. One group of studies focused on models based on word embeddings <ref type="bibr" target="#b16">(Mikolov et al. 2013;</ref><ref type="bibr" target="#b18">Pennington, Socher, and Manning 2014)</ref>. Several recent studies showed that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings <ref type="bibr" target="#b7">Joulin et al. 2017;</ref>. Some authors aggregated unsupervised word embeddings as document embeddings then fed these document embeddings into a classifier <ref type="bibr" target="#b9">(Le and Mikolov 2014;</ref><ref type="bibr" target="#b7">Joulin et al. 2017)</ref>. Others jointly learned word/document and document label embeddings <ref type="bibr" target="#b22">(Tang, Qu, and Mei 2015;</ref>). Our work is connected to these methods, the major difference is that these methods build text representations after learning word embeddings while we learn word and document embeddings simultaneously for text classification. Another group of studies employed deep neural networks. Two representative deep networks are CNN and RNN. (Kim 2014) used CNN for sentence classification. The architecture is a direct application of CNNs as used in computer vision but with one dimensional convolutions. <ref type="bibr" target="#b26">(Zhang, Zhao, and LeCun 2015)</ref> and <ref type="bibr" target="#b4">(Conneau et al. 2017</ref>) designed character level CNNs and achieved promising results. <ref type="bibr" target="#b21">(Tai, Socher, and Manning 2015)</ref>, <ref type="bibr" target="#b12">(Liu, Qiu, and Huang 2016)</ref> and <ref type="bibr" target="#b14">(Luo 2017)</ref> used LSTM, a specific type of RNN, to learn text representation. To further increase the representation flexibility of such models, attention mechanisms have been introduced as an integral part of models employed for text classification <ref type="bibr" target="#b25">(Yang et al. 2016;</ref><ref type="bibr">Wang et al. 2016)</ref>. Although these methods are effective and widely used, they mainly focus on local consecutive word sequences, but do not explicitly use global word co-occurrence information in a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks</head><p>The topic of Graph Neural Networks has received growing attentions recently <ref type="bibr" target="#b3">(Cai, Zheng, and Chang 2018;</ref><ref type="bibr" target="#b2">Battaglia et al. 2018)</ref>. A number of authors generalized well-established neural network models like CNN that apply to regular grid structure (2-d mesh or 1-d sequence) to work on arbitrarily structured graphs <ref type="bibr" target="#b2">(Bruna et al. 2014;</ref><ref type="bibr" target="#b6">Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b4">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b8">Kipf and Welling 2017)</ref>. In their pioneering work, Kipf and Welling presented a simplified graph neural network model, called graph convolutional networks (GCN), which achieved state-of-the-art classification results on a number of benchmark graph datasets <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref>. GCN was also explored in several NLP tasks such as semantic role labeling , relation classification <ref type="bibr" target="#b11">(Li, Jin, and Luo 2018)</ref> and machine translation <ref type="bibr" target="#b1">(Bastings et al. 2017)</ref>, where GCN is used to encode syntactic structure of sentences. Some recent studies explored graph neural networks for text classification <ref type="bibr" target="#b6">(Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b4">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b8">Kipf and Welling 2017;</ref><ref type="bibr" target="#b18">Peng et al. 2018;</ref><ref type="bibr" target="#b25">Zhang, Liu, and Song 2018)</ref>. However, they either viewed a document or a sentence as a graph of word nodes <ref type="bibr" target="#b4">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b18">Peng et al. 2018;</ref><ref type="bibr" target="#b25">Zhang, Liu, and Song 2018)</ref> or relied on the not-routinely-available document citation relation to construct the graph <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref>. In contrast, when constructing the corpus graph, we regard the documents and words as nodes (hence heterogeneous graph) and do not require inter-document relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Graph Convolutional Networks (GCN)</head><p>A GCN (Kipf and Welling 2017) is a multilayer neural network that operates directly on a graph and induces embedding vectors of nodes based on properties of their neighborhoods. Formally, consider a graph G = (V, E), where V (|V | = n) and E are sets of nodes and edges, respectively. Every node is assumed to be connected to itself, i.e., (v, v) ∈ E for any v. Let X ∈ R n×m be a matrix containing all n nodes with their features, where m is the dimension of the feature vectors, each row x v ∈ R m is the feature vector for v. We introduce an adjacency matrix A of G and its degree matrix D, where D ii = j A ij . The diagonal elements of A are set to 1 because of self-loops. GCN can capture information only about immediate neighbors with one layer of convolution. When multiple GCN layers are stacked, information about larger neighborhoods are integrated. For a one-layer GCN, the new k-dimensional node feature matrix L (1) ∈ R n×k is computed as</p><formula xml:id="formula_0">L (1) = ρ(ÃXW 0 ) (1) whereÃ = D − 1 2 AD − 1 2</formula><p>is the normalized symmetric adjacency matrix and W 0 ∈ R m×k is a weight matrix. ρ is an activation function, e.g. a ReLU ρ(x) = max(0, x). As mentioned before, one can incorporate higher order neighborhoods information by stacking multiple GCN layers:</p><formula xml:id="formula_1">L (j+1) = ρ(ÃL (j) W j )<label>(2)</label></formula><p>where j denotes the layer number and L (0) = X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Graph Convolutional Networks (Text GCN)</head><p>We build a large and heterogeneous text graph which contains word nodes and document nodes so that global word co-occurrence can be explicitly modeled and graph convolution can be easily adapted, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The number of nodes in the text graph |V | is the number of documents (corpus size) plus the number of unique words (vocabulary size) in a corpus. We simply set feature matrix X = I as an identity matrix which means every word or document is represented as a one-hot vector as the input to Text GCN. We build edges among nodes based on word occurrence in documents (document-word edges) and word co-occurrence in the whole corpus (word-word edges). The weight of the edge between a document node and a word node is the term frequency-inverse document frequency (TF-IDF) of the word in the document, where term frequency is the number of times the word appears in the document, inverse document frequency is the logarithmically scaled inverse fraction of the number of documents that contain the word. We found using TF-IDF weight is better than using term frequency only. To utilize global word co-occurrence information, we use a fixed size sliding window on all documents in the corpus to gather co-occurrence statistics. We employ point-wise mutual information (PMI), a popular measure for word associations, to calculate weights between two word nodes. We also found using PMI achieves better results than using word co-occurrence count in our preliminary experiments. Formally, the weight of edge between node i and node j is defined as</p><formula xml:id="formula_2">A ij =        PMI(i, j) i, j are words, PMI(i, j) &gt; 0 TF-IDF ij i is document, j is word 1 i = j 0 otherwise<label>(3)</label></formula><p>The PMI value of a word pair i, j is computed as</p><formula xml:id="formula_3">PMI(i, j) = log p(i, j) p(i)p(j) (4) p(i, j) = #W (i, j) #W (5) p(i) = #W (i) #W<label>(6)</label></formula><p>where #W (i) is the number of sliding windows in a corpus that contain word i, #W (i, j) is the number of sliding windows that contain both word i and j, and #W is the total number of sliding windows in the corpus. A positive PMI value implies a high semantic correlation of words in a corpus, while a negative PMI value indicates little or no semantic correlation in the corpus. Therefore, we only add edges between word pairs with positive PMI values. After building the text graph, we feed the graph into a simple two layer GCN as in <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref>, the second layer node (word/document) embeddings have the same size as the labels set and are fed into a softmax classifier:</p><formula xml:id="formula_4">Z = softmax(Ã ReLU(ÃXW 0 )W 1 )<label>(7)</label></formula><p>whereÃ = D − 1 2 AD − 1 2 is the same as in equation 1, and softmax(</p><formula xml:id="formula_5">x i ) = 1 Z exp(x i ) with Z = i exp(x i ).</formula><p>The loss function is defined as the cross-entropy error over all labeled documents:</p><formula xml:id="formula_6">L = − d∈Y D F f =1 Y df ln Z df (8)</formula><p>where Y D is the set of document indices that have labels and F is the dimension of the output features, which is equal to the number of classes. Y is the label indicator matrix. The weight parameters W 0 and W 1 can be trained via gradient descent. In equation 7, E 1 =ÃXW 0 contains the first layer document and word embeddings and E 2 =Ã ReLU(ÃXW 0 )W 1 contains the second layer document and word embeddings. The overall Text GCN model is schematically illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. A two-layer GCN can allow message passing among nodes that are at maximum two steps away. Thus although there is no direct document-document edges in the graph, the two-layer GCN allows the information exchange between pairs of documents. In our preliminary experiment. We found that a two-layer GCN performs better than a onelayer GCN, while more layers did not improve the performances. This is similar to results in <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref> and <ref type="bibr" target="#b10">(Li, Han, and Wu 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>In this section we evaluate our Text Graph Convolutional Networks (Text GCN) on two experimental tasks. Specifically we want to determine:</p><p>• Can our model achieve satisfactory results in text classification, even with limited labeled data?</p><p>• Can our model learn predictive word and document embeddings?</p><p>Baselines. We compare our Text GCN with multiple stateof-the-art text classification and embedding methods as follows:</p><p>• TF-IDF + LR : bag-of-words model with term frequencyinverse document frequency weighting. Logistic Regression is used as the classifier.</p><p>• CNN: Convolutional Neural Network (Kim 2014). We explored CNN-rand which uses randomly initialized word embeddings and CNN-non-static which uses pre-trained word embeddings.</p><p>• LSTM: The LSTM model defined in <ref type="bibr" target="#b12">(Liu, Qiu, and Huang 2016)</ref> which uses the last hidden state as the representation of the whole text. We also experimented with the model with/without pre-trained word embeddings.</p><p>• Bi-LSTM: a bi-directional LSTM, commonly used in text classification. We input pre-trained word embeddings to Bi-LSTM.</p><p>• PV-DBOW: a paragraph vector model proposed by <ref type="bibr" target="#b9">(Le and Mikolov 2014)</ref>, the orders of words in text are ignored. We used Logistic Regression as the classifier.</p><p>• PV-DM: a paragraph vector model proposed by <ref type="bibr" target="#b9">(Le and Mikolov 2014)</ref>, which considers the word order. We used Logistic Regression as the classifier.</p><p>• PTE: predictive text embedding <ref type="bibr" target="#b22">(Tang, Qu, and Mei 2015)</ref>, which firstly learns word embedding based on heterogeneous text network containing words, documents and labels as nodes, then averages word embeddings as document embeddings for text classification.</p><p>• fastText: a simple and efficient text classification method <ref type="bibr" target="#b7">(Joulin et al. 2017)</ref>, which treats the average of word/n-grams embeddings as document embeddings, then feeds document embeddings into a linear classifier. We evaluated it with and without bigrams.</p><p>• SWEM: simple word embedding models , which employs simple pooling strategies operated over word embeddings.</p><p>• LEAM: label-embedding attentive models , which embeds the words and labels in the same joint space for text classification. It utilizes label descriptions.</p><p>• Graph-CNN-C: a graph CNN model that operates convolutions over word embedding similarity graphs (Defferrard, Bresson, and Vandergheynst 2016), in which Chebyshev filter is used.</p><p>• Graph-CNN-S: the same as Graph-CNN-C but using Spline filter <ref type="bibr" target="#b2">(Bruna et al. 2014</ref>). • Graph-CNN-F: the same as Graph-CNN-C but using Fourier filter <ref type="bibr" target="#b6">(Henaff, Bruna, and LeCun 2015)</ref>. We first preprocessed all the datasets by cleaning and tokenizing text as (Kim 2014). We then removed stop words defined in NLTK 6 and low frequency words appearing less than 5 times for 20NG, R8, R52 and Ohsumed. The only exception was MR, we did not remove words after cleaning and tokenizing raw text, as the documents are very short. The statistics of the preprocessed datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Settings. For Text GCN, we set the embedding size of the first convolution layer as 200 and set the window size as 20. We also experimented with other settings and found that small changes did not change the results much. We tuned other parameters and set the learning rate as 0.02, dropout  <ref type="table">Table 2</ref>: Test Accuracy on document classification task. We run all models 10 times and report mean ± standard deviation. Text GCN significantly outperforms baselines on 20NG, R8, R52 and Ohsumed based on student t-test (p &lt; 0.05).</p><p>Model 20NG R8 R52 Ohsumed MR TF-IDF + LR 0.8319 ± 0.0000 0.9374 ± 0.0000 0.8695 ± 0.0000 0.5466 ± 0.0000 0.7459 ± 0.0000 CNN-rand 0.7693 ± 0.0061 0.9402 ± 0.0057 0.8537 ± 0.0047 0.4387 ± 0.0100 0.7498 ± 0.0070 CNN-non-static 0.8215 ± 0.0052 0.9571 ± 0.0052 0.8759 ± 0.0048 0.5844 ± 0.0106 0.7775 ± 0.0072 LSTM 0.6571 ± 0.0152 0.9368 ± 0.0082 0.8554 ± 0.0113 0.4113 ± 0.0117 0.7506 ± 0.0044 <ref type="bibr">LSTM (pretrain)</ref> 0.7543 ± 0.0172 0.9609 ± 0.0019 0.9048 ± 0.0086 0.5110 ± 0.0150 0.7733 ± 0.0089 Bi-LSTM 0.7318 ± 0.0185 0.9631 ± 0.0033 0.9054 ± 0.0091 0.4927 ± 0.0107 0.7768 ± 0.0086 PV-DBOW 0.7436 ± 0.0018 0.8587 ± 0.0010 0.7829 ± 0.0011 0.4665 ± 0.0019 0.6109 ± 0.0010 PV-DM 0.5114 ± 0.0022 0.5207 ± 0.0004 0.4492 ± 0.0005 0.2950 ± 0.0007 0.5947 ± 0.0038 PTE 0.7674 ± 0.0029 0.9669 ± 0.0013 0.9071 ± 0.0014 0.5358 ± 0.0029 0.7023 ± 0.0036 fastText 0.7938 ± 0.0030 0.9613 ± 0.0021 0.9281 ± 0.0009 0.5770 ± 0.0049 0.7514 ± 0.0020 fastText (bigrams) 0.7967 ± 0.0029 0.9474 ± 0.0011 0.9099 ± 0.0005 0.5569 ± 0.0039 0.7624 ± 0.0012 SWEM 0.8516 ± 0.0029 0.9532 ± 0.0026 0.9294 ± 0.0024 0.6312 ± 0.0055 0.7665 ± 0.0063 LEAM 0.8191 ± 0.0024 0.9331 ± 0.0024 0.9184 ± 0.0023 0.5858 ± 0.0079 0.7695 ± 0.0045 Graph-CNN-C 0.8142 ± 0.0032 0.9699 ± 0.0012 0.9275 ± 0.0022 0.6386 ± 0.0053 0.7722 ± 0.0027 Graph-CNN-S -0.9680 ± 0.0020 0.9274 ± 0.0024 0.6282 ± 0.0037 0.7699 ± 0.0014 Graph-CNN-F -0.9689 ± 0.0006 0.9320 ± 0.0004 0.6304 ± 0.0077 0.7674 ± 0.0021 Text GCN 0.8634 ± 0.0009 0.9707 ± 0.0010 0.9356 ± 0.0018 0.6836 ± 0.0056 0.7674 ± 0.0020 rate as 0.5, L 2 loss weight as 0. We randomly selected 10% of training set as validation set. Following <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref>, we trained Text GCN for a maximum of 200 epochs using Adam (Kingma and Ba 2015) and stop training if the validation loss does not decrease for 10 consecutive epochs. For baseline models, we used default parameter settings as in their original papers or implementations. For baseline models using pre-trained word embeddings, we used 300dimensional GloVe word embeddings (Pennington, Socher, and Manning 2014) 7 .</p><p>Test Performance. <ref type="table">Table 2</ref> presents test accuracy of each model. Text GCN performs the best and significantly outperforms all baseline models (p &lt; 0.05 based on student t-test) on four datasets, which showcases the effectiveness of the proposed method on long text datasets. For more in-depth performance analysis, we note that TF-IDF + LR performs well on long text datasets like 20NG and can outperform CNN with randomly initialized word embeddings. When pre-trained GloVe word embeddings are provided, CNN performs much better, especially on Ohsumed and 20NG. CNN also achieves the best results on short text dataset MR with pre-trained word embeddings, which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short-distance semantics well. Similarly, LSTM-based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter. PV-DBOW achieves comparable results to strong baselines on 20NG and Ohsumed, but the results on shorter text are clearly inferior to others. This is likely due to the fact that word orders are important in short text or sentiment classification. PV-DM performs worse than PV-DBOW, the only comparable results are on MR, where word orders are more essential. The results of PV-DBOW and PV-DM indicate that unsupervised document embeddings are not very discriminative in text classification. PTE and fastText clearly outperform PV-DBOW and PV-DM because they learn document embeddings in a supervised manner so that label information can be utilized to learn more discriminative embeddings. The two recent methods SWEM and LEAM perform quite well, which demonstrates the effectiveness of simple pooling methods and label descriptions/embeddings. Graph-CNN models also show competitive performances. This suggests that building word similarity graph using pretrained word embeddings can preserve syntactic and semantic relations among words, which can provide additional information in large external text data. The main reasons why Text GCN works well are two fold: 1) the text graph can capture both document-word relations and global word-word relations; 2) the GCN model, as a special form of Laplacian smoothing, computes the new features of a node as the weighted average of itself and its second order neighbors <ref type="bibr" target="#b10">(Li, Han, and Wu 2018)</ref>. The label information of document nodes can be passed to their neighboring word nodes (words within the documents), then relayed to other word nodes and document nodes that are neighbor to the first step neighboring word nodes. Word nodes can gather comprehensive document label information and act as bridges or key paths in the graph, so that label information can be propagated to the entire graph. However, we also observed that Text GCN did not outperform CNN and LSTM-based models on MR. This is because GCN ignores word orders that are very useful in sentiment classification, while CNN and LSTM model consecutive word sequences explicitly. Another reason is that the edges in MR text graph are fewer than other text graphs, which limits the message passing among the nodes. There are only few document-word edges because the documents are very short. The number of word-word edges is also limited due to the small number of sliding windows. Nevertheless, CNN and LSTM rely on pre-trained word embeddings from external corpora while Text GCN only uses information in the target input corpus. Parameter Sensitivity. <ref type="figure">Figure 2</ref> shows test accuracies with different sliding window sizes on R8 and MR. We can see that test accuracy first increases as window size becomes larger, but the average accuracy stops increasing when window size is larger than 15. This suggests that too small window sizes could not generate sufficient global word cooccurrence information, while too large window sizes may add edges between nodes that are not very closely related. <ref type="figure" target="#fig_1">Figure 3</ref> depicts the classification performance on R8 and MR with different dimensions of the-first layer embeddings. We observed similar trends as in <ref type="figure">Figure 2</ref>. Too low dimensional embeddings may not propagate label information to the whole graph well, while high dimensional embeddings do not improve classification performances and may cost more training time.</p><p>Effects of the Size of Labeled Data. In order to evaluate the effect of the size of the labeled data, we tested several best performing models with different proportions of the training data. <ref type="figure">Figure 4</ref> reports test accuracies with 1%, 5%, 10% and 20% of original 20NG and R8 training set. We note that Text GCN can achieve higher test accuracy with limited labeled documents. For instance, Text GCN achieves a test accuracy of 0.8063 ± 0.0025 on 20NG with only 20% training documents and a test accuracy of 0.8830 ± 0.0027 on R8 with only 1% training documents which are higher than some baseline models with even the full training documents. These encouraging results are similar to results in <ref type="bibr" target="#b8">(Kipf and Welling 2017)</ref> where GCN can perform quite well with low label rate, which again suggests that GCN can propagate document label information to the entire graph well and our  word document graph preserves global word co-occurrence information.</p><p>Document Visualization. We give an illustrative visualization of the document embeddings leaned by Text GCN. We use t-SNE tool <ref type="bibr" target="#b15">(Maaten and Hinton 2008)</ref> to visualize the learned document embeddings. <ref type="figure">Figure 5</ref> shows the visualization of 200 dimensional 20NG test document embeddings learned by GCN (first layer), PV-DBOW and PTE. We also show 20 dimensional second layer test document embeddings of Text GCN. We observe that Text GCN can learn more discriminative document embeddings, and the second layer embeddings are more distinguishable than the first layer.</p><p>Word Visualization. We also qualitatively visualize word embeddings learned by Text GCN. <ref type="figure" target="#fig_3">Figure 6</ref> shows the t-SNE visualization of the second layer word embeddings learned from 20NG. We set the dimension with the highest value as a word's label. We can see that words with the same label are close to each other, which means most words are closely related to some certain document classes. We also show top 10 words with highest values under each class in <ref type="table" target="#tab_2">Table 3</ref>. We note that the top 10 words are interpretable. For example, "jpeg", "graphics" and "image" in column 1 can represent the meaning of their label "comp.graphics" well. Words in other columns can also indicate their label's meaning.</p><p>Discussion. From experimental results, we can see the proposed Text GCN can achieve strong text classification results and learn predictive document and word embeddings. However, a major limitation of this study is that the GCN model is inherently transductive, in which test document nodes (without labels) are included in GCN training. Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents. Possible solutions to the problem are introducing inductive <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref> or fast GCN model <ref type="bibr" target="#b4">(Chen, Ma, and Xiao 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this study, we propose a novel text classification method termed Text Graph Convolutional Networks (Text GCN). We build a heterogeneous word document graph for a whole corpus and turn document classification into a node classification problem. Text GCN can capture global word cooccurrence information and utilize limited labeled documents well. A simple two-layer Text GCN demonstrates promising results by outperforming numerous state-of-theart methods on multiple benchmark datasets. In addition to generalizing Text GCN model to inductive settings, some interesting future directions include improving the classification performance using attention mechanisms <ref type="bibr" target="#b22">(Veličković et al. 2018</ref>) and developing unsupervised text GCN framework for representation learning on largescale unlabeled text data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of Text GCN. Example taken from Ohsumed corpus. Nodes begin with "O" are document nodes, others are word nodes. Black bold edges are document-word edges and gray thin edges are word-word edges. R(x) means the representation (embedding) of x. Different colors mean different document classes (only four example classes are shown to avoid clutter). CVD: Cardiovascular Diseases, Neo: Neoplasms, Resp: Respiratory Tract Diseases, Immun: Immunologic Diseases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Test accuracy by varying embedding dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Test accuracy by varying training data proportions. The t-SNE visualization of test set document embeddings in 20NG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The t-SNE visualization of the second layer word embeddings (20 dimensional) learned from 20NG. We set the dimension with the largest value as a word's label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="7"># Docs # Training # Test # Words # Nodes # Classes Average Length</cell></row><row><cell>20NG</cell><cell>18,846</cell><cell>11,314</cell><cell>7,532</cell><cell>42,757</cell><cell>61,603</cell><cell>20</cell><cell>221.26</cell></row><row><cell>R8</cell><cell>7,674</cell><cell>5,485</cell><cell>2,189</cell><cell>7,688</cell><cell>15,362</cell><cell>8</cell><cell>65.72</cell></row><row><cell>R52</cell><cell>9,100</cell><cell>6,532</cell><cell>2,568</cell><cell>8,892</cell><cell>17,992</cell><cell>52</cell><cell>69.82</cell></row><row><cell>Ohsumed</cell><cell>7,400</cell><cell>3,357</cell><cell>4,043</cell><cell>14,157</cell><cell>21,557</cell><cell>23</cell><cell>135.82</cell></row><row><cell>MR</cell><cell>10,662</cell><cell>7,108</cell><cell>3,554</cell><cell>18,764</cell><cell>29,426</cell><cell>2</cell><cell>20.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Words with highest values for several classes in 20NG. Second layer word embeddings are used. We show top 10 words for each class.</figDesc><table><row><cell>comp.graphics</cell><cell>sci.space</cell><cell>sci.med</cell><cell>rec.autos</cell></row><row><cell>jpeg</cell><cell>space</cell><cell>candida</cell><cell>car</cell></row><row><cell>graphics</cell><cell>orbit</cell><cell>geb</cell><cell>cars</cell></row><row><cell>image</cell><cell>shuttle</cell><cell>disease</cell><cell>v12</cell></row><row><cell>gif</cell><cell>launch</cell><cell>patients</cell><cell>callison</cell></row><row><cell>3d</cell><cell>moon</cell><cell>yeast</cell><cell>engine</cell></row><row><cell>images</cell><cell>prb</cell><cell>msg</cell><cell>toyota</cell></row><row><cell>rayshade</cell><cell>spacecraft</cell><cell>vitamin</cell><cell>nissan</cell></row><row><cell>polygon</cell><cell>solar</cell><cell>syndrome</cell><cell>v8</cell></row><row><cell>pov</cell><cell>mission</cell><cell>infection</cell><cell>mustang</cell></row><row><cell>viewer</cell><cell>alaska</cell><cell>gordon</cell><cell>eliot</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://qwone.com/˜jason/20Newsgroups/ 2 http://disi.unitn.it/moschitti/corpora.htm 3 https://www.cs.umb.edu/˜smimarog/textmining/datasets/ 4 http://www.cs.cornell.edu/people/pabo/movie-review-data/ 5 https://github.com/mnqu/PTE/tree/master/data/mr 6 http://www.nltk.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by NIH grant R21LM012618.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of text classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: problems, techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang ;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
	</analytic>
	<monogr>
		<title level="m">Deep convolutional networks on graphstructured data</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Kim; Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classifying relations in clinical narratives using segment graph convolutional and recurrent neural networks (seggcrns)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocy157</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics As</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic lymphoma classification with sentence subgraph mining from pathology reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Sohani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
	<note>Recurrent neural network for text classification with multi-task learning</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Subgraph augmented non-negative tensor factorization (santf) for modeling clinical narrative text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1009" to="1019" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bridging semantics and syntax with graph algorithms -state-of-the-art of extracting biomedical relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Szolovits ; Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Briefings in bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
	<note>Maaten and Hinton</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>and Lee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1702" to="1712" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple wordembedding-based models and associated pooling mechanisms</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularizing text categorization with clusters of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rousseau</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1827" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei ;</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, 90-94. Association for Computational Linguistics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
	<note>ICLR. Wang et al. 2016</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint embedding of words and labels for text classification</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2321" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural language processing for ehr-based computational phenotyping</title>
		<idno type="DOI">10.1109/TCBB.2018.2849968</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
