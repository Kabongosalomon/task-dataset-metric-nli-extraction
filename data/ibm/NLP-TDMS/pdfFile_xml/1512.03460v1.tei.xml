<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Self Talk: Image Understanding via Continuous Questioning and Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Research Institute of North America</orgName>
								<address>
									<postCode>48105</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Fermuller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Self Talk: Image Understanding via Continuous Questioning and Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Acclaimed as "one of the last cognitive tasks to be performed well by computers" <ref type="bibr" target="#b20">[Stork, 1998]</ref>, exploring and analyzing novel visual scenes is a journey of continuous discovery, which requires not just passively detecting objects and segmenting the images, but arguably more importantly, actively asking the right questions and subsequently closing the semantic loop by answering the questions being asked.</p><p>This paper proposes a framework that can continuously discover novel questions on an image, and then provide legitimate answers. This "self talk" approach for image understanding goes beyond visual classification by introducing a theoretically infinite interaction between a natural language question generation module and a visual question answering module. Under this architecture, the "thought process" for image understanding can be revealed by a sequence of consecutive question and answer pairs <ref type="figure">(Fig. 1)</ref>.</p><p>Our "self talk" framework has two "executives" that takes their roles iteratively: 1) question generation, which is responsible for asking the right questions, and 2) question answering, which accepts the questions and generate potential answers. With the rapid development in computer vision and machine learning <ref type="bibr" target="#b15">[Mao et al., 2014;</ref><ref type="bibr" target="#b5">Donahue et al., 2014;</ref><ref type="bibr" target="#b9">Karpathy and Li, 2014;</ref><ref type="bibr" target="#b20">Vinyals et al., 2014;</ref><ref type="bibr"></ref> Figure 1: One example self talk by the presented system, while the affirmative or questionable answer is decided by confidence score from visual answering executive. <ref type="bibr" target="#b4">Chen and Zitnick, 2014]</ref> there are a few tools developed for this seemingly intuitive philosophy in Artificial Intelligence, but self-talk is certainly beyond the aggregation of tools, because it is fundamentally a challenging chicken egg problem.</p><p>1) Questions from a single image can be as diversified as possible. Researchers have attempted a few approaches that mostly centered on asking limited questions such as "what" (e.g., object and action recognition) and "where" (e.g., place recognition). Unfortunately, questions can be anything related or unrelated to the given picture. This puzzling issue of unconstrained questions can be traced back to the original Turing test 1 , and the solution is still elusive.</p><p>Luckily, researchers have advanced the viewpoint that if we are able to develop a semantic understanding of a visual scene, we should be able to produce natural language descriptions of such semantics. This "image captioning" perspective are indeed exciting achievements, but it is only limited to generate descriptive captions, thus we propose to consider the question "Can we generate questions, based on images?".</p><p>2) Evaluating the correctness of automatic questions answering is in the realm of Turing test. The "Visual Question Answering" <ref type="bibr" target="#b2">[Antol et al., 2015]</ref> problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge <ref type="bibr" target="#b14">[Malinowski and Fritz, 2014]</ref>. A few approaches <ref type="bibr" target="#b14">[Malinowski et al., 2015;</ref><ref type="bibr" target="#b18">Ren et al., 2015]</ref> have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.</p><p>3) The semantic loop between the above two "executives" must be closed. While the above two "executives" are very interesting entities, they cannot achieve the "self talking" by their own. On one hand, the image captioning task neglects the importance of the thought process behind the appearance. Also, the amount of information covered by a finite language description is limited. These limitations have been pointed out by several recent works <ref type="bibr">[Johnson et al., 2015;</ref><ref type="bibr" target="#b18">Schuster et al., 2015;</ref><ref type="bibr" target="#b0">Aditya et al., 2015]</ref> and have been addressed partially by introducing middle layer knowledge representations. On the other hand, the setting of the visual question answering task requires as input a related question given by human beings. These questions themselves inevitably contain information about the image, which are recognized by human beings and only available through human intervention. Several recent results on the image VQA benchmarks indicate that language only information seem to contribute to the most of the good performance and how important the role of the visual recognition is still unclear.</p><p>In our formalism, the input of the final deep trained system is solely an image. Both questions and answers are generated from the trained models. Also, we want to argue that the capability of raising relevant and reasonable questions actively is the key to intelligent machinery. Thus, the main contributions of this paper are twofold: 1) we propose to automatically generate "self talk" for arbitrary image understanding, a conceptually intuitive yet AI-challenging task; 2) we propose an image question generation module based on deep learning method. <ref type="figure">Figure.</ref> 2 illustrates the flow chart of our approach (Sec.3). In Sec.4, we report experiments on two publicly available datasets (DAQUAR for indoor domain <ref type="bibr" target="#b14">[Malinowski and Fritz, 2014]</ref> and COCO for arbitrary domain <ref type="bibr" target="#b2">[Antol et al., 2015]</ref>). Specificaly, we 1) evaluate the quality of the generated questions using standard language based metrics similar to image captioning and 2) use Amazon Mechanical Turk (AMT)-based evaluations of the generated question-answer pairs. We further discuss the insights from experimental results and challenges beyond them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is related mainly to three lines of research of natural image understanding: 1) question generation, 2) image captioning and 3) visual question answering.</p><p>Question Generation is one of the key challenges in natural languages. Previous approaches of question generation from natural language sentences are mainly through template matching in a conservative manner <ref type="bibr" target="#b3">[Brown et al., 2005;</ref><ref type="bibr" target="#b7">Heilman and Smith, 2010;</ref>. <ref type="bibr" target="#b18">[Ren et al., 2015]</ref> proposed to use parsing based approach to synthetically create question and answer pairs from image annota- tions. In this paper, we propose a visual question generation module through a technique directly adapted from image captioning system <ref type="bibr" target="#b9">[Karpathy and Li, 2014]</ref>, which is data driven and the potential output questions space is significantly larger than previous parsing or template based approaches, and the trained module only takes in image as input.</p><p>In Image Captioning, in addition to the deep neural nets based approaches mentioned in Sec. 1 we also share our roots with the works of generating textual descriptions. This includes the works that retrieves and ranks sentences from training sets given an image such as <ref type="bibr" target="#b8">[Hodosh et al., 2013]</ref>, <ref type="bibr" target="#b6">[Farhadi et al., 2010]</ref>, <ref type="bibr" target="#b17">[Ordonez et al., 2011]</ref>, <ref type="bibr" target="#b19">[Socher et al., 2014]</ref>. <ref type="bibr" target="#b5">[Elliott and Keller, 2013]</ref>, <ref type="bibr" target="#b10">[Kulkarni et al., 2011]</ref>, <ref type="bibr" target="#b11">[Kuznetsova et al., 2012]</ref>, , <ref type="bibr" target="#b21">[Yao et al., 2010]</ref> are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.</p><p>In the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models <ref type="bibr" target="#b2">[Antol et al., 2015;</ref><ref type="bibr" target="#b14">Malinowski et al., 2015;</ref><ref type="bibr" target="#b7">Gao et al., 2015;</ref><ref type="bibr" target="#b14">Ma et al., 2015]</ref>. Interestingly both <ref type="bibr" target="#b2">[Antol et al., 2015]</ref> and <ref type="bibr" target="#b7">[Gao et al., 2015]</ref> adapted MS-COCO  images and created an open domain dataset with human generated questions and answers. The creation of these visual question answering testbed costs more than 20 people year of effort using Amazon Turk platform, and some questions are very challenging which actually require logical reasoning in order to answer correctly. Both <ref type="bibr" target="#b14">[Malinowski et al., 2015]</ref> and <ref type="bibr" target="#b7">[Gao et al., 2015]</ref> use recurrent networks [] to encode the sentence and output the answer. Specifically, <ref type="bibr" target="#b14">[Malinowski et al., 2015]</ref> applies a single network to handle both encoding and decoding, while <ref type="bibr" target="#b7">[Gao et al., 2015]</ref> divides the task into an encoder network and a decoder one. More recently, the work from <ref type="bibr" target="#b18">[Ren et al., 2015]</ref> reported state-of-the-art VQA performance using multiple benchmarks. The progress is mainly due to formulating the task as a classification problem and focusing on the domain of questions that can be answered with one word. The visual question answering module adopts this approach.</p><p>3 Self talk: Theory and Practice</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theory and Motivation</head><p>The phenomenon of "self talk" has been studied in the field of psychology for hundreds of years. The term is defined as a special form of intrapersonal communication: a communicator's internal use of language or thought. Using the terms of computer science and engineering, it could be useful to envision intrapersonal communication occurring in the mind of the individual in a model which contains a sender, receiver, and a potential feedback loop. This process happens consciously or sub-consciously in our mind. The capability of self-raising questions and answer them is also crucial for learning. Question raising and answering facilitate the learning process. For example, in the field of education, reciprocal questioning has been studied as a strategy, where students take on the role of the teacher by formulating their own list of questions about a reading material. In this paper, we regard this as another challenge for computers, and we believe that one key to intelligence is raising the right questions.</p><p>The benefits of modeling scene understanding task as a revealing of the "self talk" of the intelligent agents are mainly twofold: 1) the understanding of the scene can be revealed step by step and the failure cases could be tracked to specific question answer pairs. In other words, the process is more transparent; 2) theoretically the number of questions could be infinite and the question and answer loop could be never ending. This is especially crucial for active agent, such as movable robots, while their view of the scene keeps changing by moving around space, and the "self talk" in this scenario is never-ending. For a specific task, such as scene category recognition, this formulation has been proven to be efficient <ref type="bibr">[Yu et al., 2011]</ref>.</p><p>From a practical point of view, the revealing of "self talk" makes computers more human like, and the presented system has application potential in creating robotic companions <ref type="bibr">[Yu et al., 2011]</ref>. Note that as human being, we make mistakes, and some of them are "cute" mistakes. In Sec. 4, we show that our system makes many "cute" mistakes too, which actually makes it more human-like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Approach</head><p>We have two hypotheses to validate in this work: 1) with the current progress in image captioning, a system can be trained to generate reasonable and relevant questions, and 2) by incorporating it with a visual question answering system, a system could be trained to generate human like "self talk" with promising success.</p><p>In this section, we introduce a frustratingly straightforward policy to generate a sequence of questions for the purpose of "self talk". We repeat this sampling process q = QuestionSampling(I) N times (five times typically in our experiments). For each question q i generated and the accompanied original image I, we pass it through the VQA module a = V isualAnswer(q, I) to achieve an answer a i . In such a manner we achieve the "self talk" question and answers pairs {(q 1 , a 1 ), ..., (q N , a N )}. The "self talk" is further evaluated by Amazon Mechanical Turk based human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Generation</head><p>In this section, we assume an input set of images and their questions raised by human annotators. In our scenario, these are full images and their questions set. We adopted the method from <ref type="bibr" target="#b9">[Karpathy and Li, 2014]</ref>, where a simple but Algorithm 1 A Primitive "Self Talk" Generation Algorithm  <ref type="figure">{(q1, a1)</ref>, ..., (q N , a N )} effective extension is introduced from previously developed Recurrent Neural Networks (RNNs) [] based language models to train image captioning model effectively. For the purpose of a self-contained work, we briefly go over the method here.</p><formula xml:id="formula_0">i = i + 1 return</formula><p>Specifically, during the training of our image question generation module, the multimodal RNN takes the image pixels I and a sequence of input vectors (x 1 , ..., x T ). It then computes a sequence of hidden states (h 1 , ..., h T ) and a sequence of outputs (y 1 , ..., y T ) by iterating the following recurrence relation from t = 1 to t = T .</p><formula xml:id="formula_1">b v = W hi [CN N θc (I)] (1) h t = f (W hx x t + W hh h t−1 + b h + 1(t = 1) b v ) (2) y t = sof tmax(W ho h t + b o ),<label>(3)</label></formula><p>In the equations above, W hi , W hx , W hh , W oh , x i and b h ,b o are learnable parameters, and CN N θc (I) is the last layer of a pre-trained Convolutional Neural Network (CNN) []. The output vector y t holds the (unnormalized) log probabilities of words in the dictionary and one additional dimension for a special END token. In the approach, the image context vector b v to the RNN is only given at the first iteration. A typical size of the hidden layer of the RNN is 512 neurons.</p><p>The RNN is trained to combine a word (x t ), the previous context (h t1 ) to predict the next word (y t ) in the generated question. The RNNs predictions on the image information b v via bias interactions on the first step. The training proceeds as follows (refer to <ref type="figure" target="#fig_2">Figure.3a)</ref>): First set h 0 = 0, x 1 to a special START vector, and the desired label y 1 as the first word in the training question. Then set x 2 to the word vector of the first word and expect the network to predict the second word, etc. Finally, on the last step when x T represents the last word, the target label is set to a special END token. The cost function is to maximize the log probability assigned to the target labels (here, a Softmax classifier).</p><p>During testing time, to generate one question, we first compute the image representation b v , and then set h 0 = 0, x 1 to the START vector and compute the distribution over the first word y 1 . We sample each word in the question from the distribution, set its embedding vector as x 2 , and repeat this process until the END token is generated. In the rest of the paper, we denote this question generation process as q = QuestionSampling(I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>In this section, we assume an input set of images and their annotated question answer pairs from human labelers. We adopted the approach from <ref type="bibr" target="#b18">[Ren et al., 2015]</ref>, which introduced a model builds directly on top of the long short-term memory (LSTM) <ref type="bibr" target="#b7">[Hochreiter and Schmidhuber, 1997</ref>] sen-  <ref type="bibr">et al., 2014]</ref>. In our experiments, the word embedding is kept dynamic (trained with the rest of the model). Please refer to <ref type="bibr" target="#b18">[Ren et al., 2015]</ref> for the details. In the rest of the paper, we denote this trained VQA module as a = V isualAnswer(q, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon Mechanical Turk based Evaluation</head><p>For the generated question answer ("self talk") pairs, since there are no groundtruth annotations that could be used for automatic evaluation, we designed a Amazon Mechanical Turk (AMT) based human evaluation metric to report.</p><p>We ask the Turkers to imagine they have a companion robot whose name is "self talker". Once they bring the robot to a place shown in the image give, the robot started to generate questions and then self-answer the questions as if he is talking to himself. Specifically we ask the Turkers to evaluate three metrics: 1) Readability: how readable the "self talker" 's "self talk". Scores range from 1: not readable at all, to 5: no grammatical errors. Grammatically sound "self-talk" have better readability ratings; 2) Correctness: how correct the "self talk" is. "self-talk" content that correctly describes the image content with higher precision have better correctness ratings (range from 1 to 5); 3) Human likeness: how human-like does the robot perform (range from 1 to 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR <ref type="bibr" target="#b14">[Malinowski and Fritz, 2014]</ref> and MSCOCO-VQA <ref type="bibr" target="#b2">[Antol et al., 2015]</ref>.</p><p>In the experiments on these two datasets, we first report the question generation performance using standard image captioning language based evaluation metric. Then, in order to evaluate the performance of the "self talk" we report the AMT results and provide further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We first briefly describe the two testing-beds we are using for the experiments.</p><p>DAQUAR: Indoor Scenes: DAQUAR [Malinowski and Fritz, 2014] vqa dataset contains 12,468 human question answer pairs on 1,449 images of indoor scene. The training set contains 795 images and 6,793 question answer pairs, and the testing set contains 654 images and 5,675 question answer pairs. We run experiments for the full dataset with all classes, instead of their reduced set where the output space is restricted to only 37 object categories and 25 test images in total. This is because the full dataset is much more challenging and the results are more meaningful in statistics.</p><p>COCO: General Domain: MSCOCO-VQA <ref type="bibr" target="#b2">[Antol et al., 2015]</ref> is the latest VQA dataset that contains openended questions about arbitrary images collect from the Internet. This dataset contains 369,861 questions and 3,698,610 ground truth answers based on 123,287 MSCOCO images. These questions and answers are sentence-based and openended. The training and testing split follows MSCOCO-VQA official split. Specifically, we use 82,783 images for training and 40,504 validation images for testing. The variation of the images in this dataset is large and till now it is considered as the largest general domian VQA dataset. The effort of collecting this dataset cost over 20 people year working time using Amazon Mechanical Turk interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Generation Evaluation</head><p>We now evaluate the ability of our RNN model to raise questions about a given image. We first trained our Multimodal RNN to generate questions on full images with the goal of verifying that the model is rich enough to support the mapping from image data to sequences of words. We report the BLEU <ref type="bibr" target="#b17">[Papineni et al., 2002]</ref>, METEOR <ref type="bibr" target="#b12">[Lavie, 2014]</ref>, ROUGE <ref type="bibr" target="#b13">[Lin, 2004]</ref> and CIDEr  scores computed with the coco-caption code <ref type="bibr" target="#b4">[Chen et al., 2015]</ref>. Each method evaluates a candidate generated question by measuring how well it matches a set of several reference questions (averagely eight questions for DAQUAR dataset, and three questions for MSCOCO-VQA) written by humans.</p><p>To further validate the performance of question generation, we further list the performance metrics reported in the stateof-the-art image captioning work <ref type="bibr" target="#b9">[Karpathy and Li, 2014]</ref>. From <ref type="table">Table.</ref> 1, except CIDEr score, the question generation performance is comparable with the state-of-the-art image captioning performance. Note that for CIDEr score is a consensus based metric. The facts that, 1) coco-VQA has three reference ground-truth questions while coco-Caption has five and 2) human annotated questions by its nature varies more than captions, makes it hard to achieve high CIDEr score for question generation task.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">"Self talk" Evaluation</head><p>In <ref type="table">Table.</ref> 2 we report the average score as well as its standard deviation for each metric. We randomly drawn 100 and 1000 testing samples from DAQUAR and MSCOCO-VQA testing sets for the human evaluation reported here. From the human evaluation, we can see that the questions generated have achieved close to human readability. The correctness of the generated "self talk" averagely has some relevance to the image and according to Turkers, the imagined companion robot acts averagely beyond "a bit like human being" but below the "half human, half machine" category.</p><p>We also asked the Turkers to choose from five immediate feelings after their companion robot's performance. <ref type="figure" target="#fig_6">Fig. 7</ref> and <ref type="figure" target="#fig_7">Fig. 8</ref> depicts the feedback we got from the users. Given the fact that the performance of the "self talker" robot is still far from human performance, most of the Turkers thought they like such a robot or feel its amusing. And only very few of the users felt scared, which indicates that our image understanding performance is far from being trapped into the so-called "uncanny valley" <ref type="bibr" target="#b16">[Mori et al., 2012]</ref> of machine intelligence. At the end of our evaluation, we also asked Turkers to comment about what the robot's performance. Some   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Readability Correctness</head><p>Human likeness DAQUAR 3.35 ± 0.92 2.5 ± 1.03 2.49 ± 1.02 coco-VQA 3.39 ± 1.18 2.7 ± 1.29 2.40 ± 1.33 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we consider the image understanding problem as a self-questioning and answer process and we present a primitive "self talk" generation method based on two deep neural network modules. From the experimental evaluation on both the performance of question generation and final "self talk" pairs, we show that the presented method achieved a decent amount of success. There are still several potential pathways to improve the performance of intelligent "self talk". The role of common-sense knowledge. Common-sense knowledge has a crucial role in question raising and answering process for human beings <ref type="bibr" target="#b0">[Aditya et al., 2015]</ref>. The experimental result shows that our system by learning the model from large annotated question answer pairs, it implicitly encodes a certain level of common-sense. The real challenge is to deal with situations that the visual input conflicts with the common-sense learned from context data. In our experiment, it seems that the model is biased towards to trust his common sense more than the visual input. How to incorporate either logical or numerical forms of common-sense into end-to-end based image understanding system is still an open problem.</p><p>Creating a story-line. When human beings perform intrapersonal communication, we tend to follow a logic flow or so-called story-line. This requires a question generation modules that takes in consideration the answers from previous questions for consideration. This indicates a more sophisticated dialogue generation process (such as a cognitive dialogue <ref type="bibr">[Aloimonos and Fermüller, 2015]</ref>), and it can also potentially prevent self-contradictions happened in this paper's generated results (see last comment in <ref type="figure" target="#fig_5">Fig. 6</ref>).</p><p>As indicated from AMT feedback, human users felt it is cute and fondness to have a robot companion that moves around and talkative. Another open avenue is to integrate the current trained model onto a robot platform and through interaction with users to continuously refine its trained model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The flow chart or our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The presented architecture of question generation module (part A) and question answering module (part B), and how they are connected. tence model and is called the VIS+LSTM model. It treats the image as one word of the question as shown in Figure.3b). The model uses the last hidden layer of the 19-layer Oxford VGG Conv Net [Simonyan and Zisserman, 2014] trained on ImageNet 2014 Challenge as the visual embeddings. The CNN part of the model is kept frozen during training. The model also uses word embedding model from general purpose skip-gram embedding [Pennington</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example "self talk" generated on DAQUAR testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example "self talk" generated on MSCOCO-VQA testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example Turkers' comments about the "self talk" robot. example comments could be found inFig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>User feedback on DAQUAR set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>User feedback on MSCOCO-VQA set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of question generation on DAQUAR and coco-VQA datasets. MAX: the generated questions have max probability from trained model. SAMPLE: the generated questions are randomly drawn from the trained probabilistic model.</figDesc><table><row><cell></cell><cell cols="7">CIDEr METEOR ROUGE L Bleu-1 Bleu-2 Bleu-3 Bleu-4</cell></row><row><cell>DAQUAR question MAX</cell><cell>.512</cell><cell>.361</cell><cell>.761</cell><cell>.81</cell><cell>.735</cell><cell>.635</cell><cell>.361</cell></row><row><cell>DAQUAR question SAMPLE</cell><cell>.143</cell><cell>.256</cell><cell>.631</cell><cell>.685</cell><cell>.561</cell><cell>.428</cell><cell>.337</cell></row><row><cell>coco-VQA question MAX</cell><cell>.331</cell><cell>.178</cell><cell>.493</cell><cell>.594</cell><cell>.422</cell><cell>.291</cell><cell>.193</cell></row><row><cell>coco-VQA question SAMPLE</cell><cell>.133</cell><cell>.127</cell><cell>.342</cell><cell>.388</cell><cell>.220</cell><cell>.117</cell><cell>.064</cell></row><row><cell cols="2">coco-Caption [Karpathy and Li, 2014] .66</cell><cell>.195</cell><cell>-</cell><cell>.625</cell><cell>.45</cell><cell>.321</cell><cell>.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>"Self talk" AMT human evaluation.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Chitta Baral, Cornelia Fermuller, and Yiannis Aloimonos. From images to sentences through scene description graphs using commonsense reasoning and knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aditya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03292</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aloimonos and Fermüller, 2015] Yiannis Aloimonos and Cornelia Fermüller. The cognitive dialogue: A new model for vision implementing common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="42" to="44" />
		</imprint>
	</monogr>
	<note>Automation of question generation from sentences</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic question generation for vocabulary assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitnick ; Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence Zitnick ;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<idno>arXiv:1504.00325</idno>
	</analytic>
	<monogr>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Donahue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
		<respStmt>
			<orgName>Elliott and Keller</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05612</idno>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hodosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Johnson et al., 2015] Justin Johnson, Ranjay Krishna, Michael Stark, Jia Li, Michael Bernstein, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="853" to="899" />
		</imprint>
	</monogr>
	<note>Image retrieval using scene graphs</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">;</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kiros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<idno>arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep visual-semantic alignments for generating image descriptions</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th CVPR</title>
		<meeting>the 24th CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kuznetsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lavie ; Michael Denkowski Alon Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to answer questions from image using convolutional neural network</title>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<idno>arXiv:1505.01121</idno>
	</analytic>
	<monogr>
		<title level="m">Ask your neurons: A neuralbased approach to answering questions about images</title>
		<editor>Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz</editor>
		<imprint>
			<publisher>Malinowski and Fritz</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards a visual turing challenge</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<title level="m">Explain images with multimodal recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The uncanny valley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>from the field</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The turing test: Verbal behavior as the hallmark of intelligence edited by stuart shieber</title>
		<editor>John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<idno>arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Fourth Workshop on Vision and Language</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ng ; David G Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedantam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5726</idno>
		<idno>arXiv:1411.4555</idno>
	</analytic>
	<monogr>
		<title level="m">Grounded compositional semantics for finding and describing images with sentences. TACL</title>
		<editor>Yang, Ching Lik Teo, Hal Daumé, III, and Yiannis Aloimonos</editor>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Alexander Toshev</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">I2t: Image parsing to text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Active scene recognition with vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Ching Lik Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="810" to="817" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
