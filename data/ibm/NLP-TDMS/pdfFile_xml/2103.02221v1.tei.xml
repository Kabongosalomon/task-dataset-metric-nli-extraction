<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Energy-Based Learning for Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
							<email>suhail33@cs.ubc.camrmittal@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Mittal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behjat</forename><surname>Siddiquie</surname></persName>
							<email>behjats@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Broaddus</surname></persName>
							<email>chrispb@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayan</forename><surname>Eledath</surname></persName>
							<email>eledathj@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
							<email>medioni@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Canada CIFAR AI</orgName>
								<address>
									<settlement>Chair 4 Amazon</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Energy-Based Learning for Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional scene graph generation methods are trained using cross-entropy losses that treat objects and relationships as independent entities. Such a formulation, however, ignores the structure in the output space, in an inherently structured prediction problem. In this work, we introduce a novel energy-based learning framework for generating scene graphs. The proposed formulation allows for efficiently incorporating the structure of scene graphs in the output space. This additional constraint in the learning framework acts as an inductive bias and allows models to learn efficiently from a small number of labels. We use the proposed energy-based framework 1 to train existing stateof-the-art models and obtain a significant performance improvement, of up to 21% and 27%, on the Visual Genome [9] and GQA [5] benchmark datasets, respectively. Furthermore, we showcase the learning efficiency of the proposed framework by demonstrating superior performance in the zero-and few-shot settings where data is scarce.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A scene graph is a graph-based representation of an image which encodes objects along with the relationships between them. Such a representation allows for a comprehensive understanding of images that is useful in several vision applications, including visual question answering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, image captioning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> and scene synthesis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>A typical scene graph generation model comprises of the object detection network, which extracts object regions and corresponding features, and a message passing network with nodes initialized with these region features and edges accounting for the potential relations among them. The features are refined, through context aggregation, and then classified to produce both object (node) and relation (edge) <ref type="bibr" target="#b0">1</ref> Code and pre-trained models available at https://github.com/ mods333/energy-based-scene-graph.  <ref type="bibr" target="#b21">[22]</ref> model trained using conventional cross-entropy loss (purple) and our proposed energy-based framework (green). We make two crucial observations. First, the model trained using cross-entropy loss is incapable of consistent structural reasoning (riding is not possible given the rest of the graph). Second, the trained model tends to be biased, favoring more frequent relations (e.g., on). Our proposed energy-based framework is designed, and able, to address these shortcomings. labels. These networks are often trained end-to-end by minimizing individual cross-entropy losses on both sets of labels. A major drawback of such an approach is that quality of prediction (loss) is simply proportional to the number of correctly predicted labels and ignores the rich structure of the scene graph output space (e.g., correlation or exclusion among object and relation label sets). In addition, the imbalance in the number of training samples for the relations results in dominant relations being heavily favored, leading to biased relation prediction at test time <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure" target="#fig_0">Figure 1 (b)</ref> illustrates the scene graph generated by a model <ref type="bibr" target="#b21">[22]</ref> trained using the cross-entropy loss. Both the aforementioned drawbacks are apparent in the output. First, the model predicts a relation &lt;man, riding, wave&gt;. A simple examination of the rest of the scene graph reveals that such a relationship is impossible given that the man is on a rock and holding a surfboard. Second, the model leans towards making generic relation predictions such as &lt;man, on, rock&gt; as opposed to more informative alternatives, e.g., &lt;man, standing on, rock&gt;.</p><p>The origin of these issues can be identified by examining the likelihood term. Cross-entropy based training treats objects (O) and relationships (R) in a scene graph as independent entities. This amounts to factorizing the likelihood of a scene graph (SG), given an image (I), as the product of the likelihoods for the individual objects and relations:</p><formula xml:id="formula_0">log p(SG|I) = i∈O log p(o i |I) + j∈R log p(r j |I). (1)</formula><p>Eq.(1) brings to light the underlying cause of the problem highlighted above. First, during loss computation, the loss for each relation term is independent of the relations predicted in the rest of the scene graph. Thus an incorrect relation such as &lt;man, riding, wave&gt; is penalized the same as &lt;man, behind, wave&gt; irrespective of the other relations (&lt;man, on, rock&gt;). However, using common sense reasoning, we can determine that &lt;man, riding, wave&gt; is highly improbable given &lt;man, carrying, surfboard&gt; and should be penalized heavily as opposed to a likely, albeit incorrect, relation behind. Second, due to the summation over individual relation terms, the model, in order to minimize the loss, is incentivized to predict relations which are more common in the training data.</p><p>While prior works have tried to address the issue of biased predictions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> in the context of scene graph generation, little progress has been made towards structured learning of scene graphs. In this work, we address both of these issues by proposing a novel generic loss formulation that incorporates the structure of scene graphs into the learning framework using an energy-based learning framework. This energy-based framework relies on graph message passing algorithm for energy computation, that is learned to model the joint conditional density of a scene graph, given an image. Such a formulation transforms the problem from maximizing sum of the individual likelihood terms to that of directly maximizing the joint likelihood of the objects and relations. Furthermore, this added structure acts as an inductive bias for the learning, allowing the model to efficiently learn relationship statistics from less data.</p><p>The proposed learning framework is general and hence can be used to train any off-the-shelf scene graph generation model. We experiment with various state-of-the-art models and demonstrate that our energy-based formulation achieves significant improvements in the performance over the corresponding models trained using the standard cross-entropy based formulation. We also demonstrate the enhanced generalization capability of models trained using our framework by evaluating zero shot relation retrieval performance. Finally, we demonstrate the ability of our energybased framework to learn from lesser amounts of training data by demonstrating an improvement in relative performance when evaluating on few-shot relation triplets. <ref type="figure" target="#fig_0">Figure 1</ref> (c), shows the scene graph generated by the proposed method. The generated scene graph is more granular, predicting relations such as &lt;man, standing on, rock&gt; as opposed to the biased and generic variant &lt;man, on, rock&gt;. The model is also able to preclude improbable relations (e.g., &lt;man, riding, wave&gt;) and instead predicts in front of between man and wave.</p><p>Contribution: Our main contribution is a novel energybased framework for scene graph generation that allows for direct incorporation of structure into the learning. We also propose a novel message passing algorithm that is used for computing the energy of scene graph configurations. This message passing algorithm is generic and can be used for other applications such as learning graph embeddings. Finally, we demonstrate the efficacy of our proposed framework by applying it to multiple state-of-the-art models and evaluating performance on two benchmark datasets -Visual Genome <ref type="bibr" target="#b8">[9]</ref> and GQA [5] -where we consistently outperform the cross-entropy based counterparts by up to 21% on Visual Genome and 27% on GQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene Graph Generation: Scene Graph generation has become increasingly popular in the vision community <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Early works, e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, focused on improving context aggregation modules that facilitated learning better representations, thereby improving performance. Recent works, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, identify the shortcomings of training using cross-entropy loss and propose handcrafted loss formulations to improve the performance. Our work can be considered as a generalization of the latter, where we allow the model to learn an approximate joint distribution over the scene graphs and images using an energy based formulation. Furthermore, to the best of our knowledge, our work is the first to incorporate structure in the output space, into the learning paradigm.</p><p>Energy Based Modeling: Energy Based Models(EBM) learn the unnormalized density of the data space, thereby avoiding the need to compute the partition function <ref type="bibr" target="#b9">[10]</ref>. EBMs have recently sparked interest in generative modeling. One line of work uses energy models to learn the underlying data distribution and then implicitly generates samples from the learned distribution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, they propose denoising score matching based training, where the model learns to generate data by denoising progressively in-creasing noisy signals. While there has been an increase in the use of energy-based modeling for generative tasks, they have been relatively unexplored for discriminative tasks. In our work, we show how one can formulate a discriminative task of scene graph generation using an EBM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We first provide an overview of current approaches for scene graph generation based on the standard cross entropy loss, followed by a description of our proposed energybased learning framework along with the architecture used for energy computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene Graph Generation</head><p>Scene graph generation methods typically adopt a two stage framework. In the first stage, given an image I, bounding boxes are obtained using a standard object detector such as Faster R-CNN <ref type="bibr" target="#b17">[18]</ref>. Features corresponding to these regions are extracted using RoIAlign along with an initial distribution over object labels. In the next stage, these detections are used as inputs to predict the scene graphs. The bounding-box features along with the object label and the spatial coordinates of the bounding boxes are used to initialize a set of node features. These features are refined using architectures such as LSTM <ref type="bibr" target="#b30">[31]</ref>, TreeLSTM <ref type="bibr" target="#b21">[22]</ref> or Graph Attention Networks <ref type="bibr" target="#b26">[27]</ref>, to incorporate contextual information. Object labels, O, are then obtained by classifying the refined features. Relationship labels, R, are obtained by extracting features from union of object bounding boxes, followed by state refinement using BiLSTMs <ref type="bibr" target="#b30">[31]</ref> or BiTreeLSTMs <ref type="bibr" target="#b21">[22]</ref> and subsequent classification.</p><p>These models are trained using standard cross-entropy loss on object and relation labels. Each object and relationship is considered in isolation when computing individual losses, which are then summed up to obtain the loss for the given image. Such a loss formulation ignores the fact that objects and relations in a scene graph are interdependent. Intuitively, incorporating such dependencies into the learning procedure should lead to an improvement in performance. However, it is not clear as to how one can exploit the rich structure in the output space. Most methods ( <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>) attempt to find a way around this by employing message passing algorithms in the input space that allow for aggregation of context information. However, this does not explicitly consider structure in the output space; neither in predictions, nor in the loss function used for learning. In this work, we propose a novel energybased learning framework that allows scene graph generation models to be trained using a "loss" that explicitly incorporates structure in the output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Energy Based Modeling</head><p>Energy-based models <ref type="bibr" target="#b9">[10]</ref> encode dependencies between variables by assigning a scalar energy value to an input configuration. Given a data point x ∈ X with corresponding label y ∈ Y, let E θ (x, y) ∈ R be a joint energy function. While energy models map inputs to unnormalized densities, we can define a probability distribution via the Boltzmann distribution,</p><formula xml:id="formula_1">p θ (x, y) = exp(−E θ (x,y)) Z(θ) , where Z(θ) = exp(−E θ (x, y)</formula><p>) is referred to as the normalization constant or partition function. Computing the normalization constant, Z θ for most parameterizations of the energy function is intractable. Therefore, learning the parameters θ using methods such as maximum likelihood is not straightforward. Most methods address this problem by rewriting the derivative of the log likelihood as</p><formula xml:id="formula_2">∇ θ log p θ (x, y) = E p θ (x ,y ) [∇ θ E θ (x, y )] − ∇ θ E θ (x, y),</formula><p>where the expectation is approximately estimated using MCMC methods that sample from the data distribution.</p><p>Unlike most prior works that train energy models for generative modeling, our focus is scene graph generation, a discriminative task. For such a task, we are only concerned with the relative energies of the various label configurations given an input x. Training with a carefully crafted loss function circumvents the need for estimating the partition function or computing expectations. Therefore we can parametrize the energy function using an arbitrary neural network architecture. For a more detailed discussion on energy loss formulation refer to Section 2 in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Energy Models for Scene Graphs Generation</head><p>We now describe our proposed energy-based learning framework for scene graph generation. In our formulation of the energy function, the data space X is the set of images I ∈ R W ×H×3 and the label space Y is the set of scene graphs SG. The scene graph, SG, is defined by a tuple (O, R), where O ∈ R n×d is the set of object labels and R ∈ R n×n×d is the set of relationship labels; n is the number of objects in an image; d and d is the total number of possible object and relation labels in the dataset.</p><p>A simple implementation of the joint energy function, would take an encoding of the image, I, and a scene graph, SG, and produce an scalar energy value. However, there are a few challenges with this. First, a simple global CNNbased encoding of the image may fail to highlight, potentially small, regions relevant for scene graph prediction. Second, scene graph representation is variable in length (n is not fixed) and high dimensional. The second challenge can be addressed by pooling object O and relations R across n and n × n dimensions respectively. We propose a more sophisticated and effective scene graph refinement and gated pooling formulation in Section 3.4. To facilitate the former challenge of image encoding, we extract a graph The region in light blue correspond to most traditional scene graph generation pipelines. The proposed energy-based learning framework is highlighted in light green. We initialize the image graph with the extracted object proposal features as the node states. We instantiate the scene graph with predictions from traditional pipeline (or ground truth annotation). The image graph and scene graph are fed into the energy model where they undergo state refinement using a Gated Graph Neural Network and a novel Edged Graph Neural Network, respectively. We then obtain vector representations of each graph using pooling layers. The representations are concatenated and passed as input to a multi-layer perceptron which predicts the energy of the joint input (image) -output (scene graph) configuration. The loss is computed from the energy values of the ground truth and predicted configuration.</p><p>based representation from the image. This representation is henceforth referred to as an image graph (G I ). The nodes of the image graph are instantiated using features extracted from the object bounding boxes. Given a scene graph generation model M and an image I, we predict a scene graph, G 0 SG and compute the image graph G I . The scene graph along with the image is provided as input to the energy model (E θ ) to compute the energy corresponding to the predicted configuration. Similarly, we compute the energy of the ground truth configuration using the ground truth scene graph (G + SG ) and image graph (G + I ) constructed from ground truth bounding boxes. These two energy values are then used to compute the energy loss</p><formula xml:id="formula_3">L e = E θ (G + I , G + SG ) − min G SG ∈SG E θ (G I , G SG ). (2)</formula><p>Computing this loss requires solving an optimization problem to find a scene graph configuration that minimizes the energy value (second term in Eq.(2)). Starting from G 0 SG we use Stochastic Gradient Langevine Dynamics (SGLD) <ref type="bibr" target="#b23">[24]</ref> which approximately solves the optimization problem in an iterative manner:</p><formula xml:id="formula_4">O τ +1 = O τ − λ 2 ∇ O E θ (G I , G τ SG ) + τ , R τ +1 = R τ − λ 2 ∇ R E θ (G I , G τ SG ) + τ<label>(3)</label></formula><p>where O t and R t are the node and edge states in the scene graph and t is sampled from a normal distribution N (0, λ). Conceptually, the predicted G 0 SG is used as an "initialization" to arrive at the low energy configuration through a series of steps defined by Eq.(3); each step similar to regular gradient descent with an added Gaussian noise. Differentiation through the optimization path guides parameter learning of the model M that generates G 0 SG in the first place. When training using the above loss, we observe that the energy values get arbitrarily large in magnitude leading to gradient overflow. We address this problem by adding an L2 regularization loss on the energy values:</p><formula xml:id="formula_5">L r = E θ (G + I , G + SG ) 2 + E θ (G I , G SG ) 2 .<label>(4)</label></formula><p>Finally, since the space of scene graphs is very high dimensional, we need to restrict the search space of the energy model in order to stabilize the learning. This is done by incorporating the task loss used by the underlying scene graph generation model, L t on the predicted output as an added regularization on the initial prediction. The total loss for training the scene graph generator and the the energy model is given by:</p><formula xml:id="formula_6">L total = λ e L e + λ r L r + λ t L t ,<label>(5)</label></formula><p>where λ e , λ r and λ t are the relative weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Energy Model Architecture</head><p>Given an image graph (G I ) and a scene graph (G SG ), the energy model first refines the state representations using graph neural networks. We use a novel Edged Graph Neural Networks (EGNN) and Graph Neural Network <ref type="bibr" target="#b12">[13]</ref> on the scene graph and image graph respectively to incorporate contextual information. This is followed by applying a pooling layer on each graph to obtain a vector representation summarizing the graph states. Finally, these two vectors are fed into a multi layer perceptron (MLP) to compute the energy value corresponding to the predicted scene graph configuration. We then repeat these operations for the ground truth scene graph and the input image. The energy model can be parametrized as:</p><formula xml:id="formula_7">E θ (G I , G SG ) = MLP [f (EGNN(G SG )); g(GNN(G I ))] ,<label>(6)</label></formula><p>where f and g are pooling functions.</p><p>Notation. We use n i to represent the features of the i th node, which for G SG corresponds to the i th object, initialized to the corresponding i th row of matrix O. For G I n i corresponds to the i th image region, initialized by the RoIAlign image features. We use e i→j to represent the feature of the directed edge from node i to j, initialized to (i, j) th column of R. N i denotes the neighbours of node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Edge Graph Neural Network</head><p>To allow for direct application of convolution operations on graph representations accommodating edge features such as scene graphs, we propose a variant of graph message passing algorithm. For each node n i , we aggregate the message from neighbouring node and edges by </p><p>where W nn and W en are the kernel matrices for node-tonode and node-to-edge communication and 0 ≤ α ≤ 1 is a hyper-parameter that controls the contribution of messages from edges and nodes. Similarly, the message passing for edges are given by</p><formula xml:id="formula_9">d t i→j = W ee [n t−1 i n t−1 j ],<label>(8)</label></formula><p>where W ee is the kernel matrix for node-to-edge communication. Note that the message passing for edges is direction aware i.e. d i→j = d j→i . This is crucial as the relationship between two nodes change depending on direction of the edge for example &lt;cat, has, tail&gt; and &lt;tail, on, cat&gt;. The incoming messages are combined with the states using a gating mechanism <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Pooling Layer</head><p>We use gated pooling layers to generate vector representations of the two graphs. The pooling operation is given by</p><formula xml:id="formula_10">N = k f gate (n k ) n k (9) E = ij g gate (e i→j ) e i→j<label>(10)</label></formula><p>where f gate and g gate are gating functions that map node and edge states to a scalar and represents element-wise multiplication. These two vectors are then passed through a linear layer, after concatenation, to obtain the final vector representation of the graph. In the image graph G I , since there are no edge features, we use the pooled node vector N as the vector representation of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experimental results on two datasets: the Visual Genome dataset <ref type="bibr" target="#b8">[9]</ref> and the GQA dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>Visual Genome: We use the pre-processed version of the dataset from <ref type="bibr" target="#b25">[26]</ref>. The dataset consists of 108k images and contains 150 object categories and 50 predicate categories We use the original split with 70% of the images in the training set and the remaining 30% in the test set, with 5k images from the training set held out for validation <ref type="bibr" target="#b30">[31]</ref>.</p><p>GQA: The GQA dataset <ref type="bibr" target="#b4">[5]</ref> is also constructed from images in the Visual Genome dataset. Starting from the scene graph annotations provided in Visual Genome, a normalization process is applied. This normalization process augments object and relation annotations and prunes inaccurate or unnatural relations. The resulting dataset contains a total of 1704 object categories, 311 relation categories We use the same 70 − 30 split for the train and the test set, with 5k images in the validation set. Compared to Visual Genome, the GQA dataset has denser graphs with a larger number of object and relation categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Scene Graph Generation Models</head><p>The energy-based training introduced in this paper is generic and does not make any assumptions on the underlying scene graph generation model. This allows freedom in choosing the model architecture for image to scene graph mapping. On the Visual Genome dataset, we experiment with VCTree <ref type="bibr" target="#b21">[22]</ref>, Neural Motifs <ref type="bibr" target="#b30">[31]</ref> and Iterative Message Passing <ref type="bibr" target="#b25">[26]</ref>. We also experiment with VCTree-TDE <ref type="bibr" target="#b20">[21]</ref>, where the inference involves counterfactual reasoning. On the GQA dataset, we experiment with Transformers <ref type="bibr" target="#b22">[23]</ref>, instead of VCTree, as the larger number of object classes in the GQA dataset leads to considerably larger memory requirement in VCTree. The ability to experiment with different models demonstrates the versatility of our approach.  <ref type="table">Table 1</ref>. Quantitative Results. We compare the proposed energy-based loss formulation against traditional cross-entropy loss using various state-of-the-art models. We report the mean Recall@K <ref type="bibr" target="#b21">[22]</ref> under all three experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>Relationship Recall (RR). We use the mean Recall@K (mR@K) metric <ref type="bibr" target="#b21">[22]</ref> to evaluate the performance of the scene graph generation models. We report the mean Re-call@K instead of the Regular Recall@K (R@K) due to data imbalance that leads to reporting bias as pointed out in recent works <ref type="bibr" target="#b20">[21]</ref>. The evaluation is performed under three settings, (1) Predicate Classification (PredCls): Predict the relationship labels, given the image, object bounding boxes and object labels. Zero-Shot Recall (zsR@K). Introduced in <ref type="bibr" target="#b14">[15]</ref>, zsR@K evaluates the ability to identify subject-predictae-object relation triplets that were not observed during training. We compute zsR@K for 3 settings: PredCls, SGCls and SGDet.</p><p>Few-Shot Recall. We introduce the few-shot Recall@K (fsR@K) metric that reports the Recall@K for relation triplets that occur a certain number of times in the training set. Unlike the conventional few shot metric, we use a range of values to generate the few shot triplet splits. Thus instead of splitting the triplets into 1-shot, 2-shot, etc., we split them into groups of 1 − 5-shot, 6 − 10-shot, etc. with triplets in a k 1 −k 2 -shot occurring between k 1 and k 2 times.</p><p>Sentence-to-Graph Retrieval (S2GR): Introduced in <ref type="bibr" target="#b20">[21]</ref>, S2GR was designed to address the inability of RR (Relationship Recall) and zsR (Zero-Shot Recall) to capture graph level coherence. In S2GR, the scene graph predicted for an image (obtained in the SGDet setting) is used as a semantic representation of the image. The task then is to retrieve images using their graph representation, with the image caption as a query. Note that retrieval task is based solely on the detected scene graph and no other visual information. As a consequence, any bias in the scene graph generation will result in a decrease in the S2GR performance. Similar to <ref type="bibr" target="#b20">[21]</ref>, we report Recall@20/100 on 1k/5k gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Detector. We pre-train a Faster R-CNN <ref type="bibr" target="#b17">[18]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> backbone and freeze the weights of the network prior to scene graph generation training. We obtain the weights of the pre-trained detector for Visual Genome from <ref type="bibr" target="#b19">[20]</ref>. For GQA, we pre-train the object detector using standard Faster-RCNN settings. The object detector has 28 mAP on Visual Genome and 10 mAP on GQA.</p><p>Scene Graph Generator. The baseline models, trained with standard cross entropy loss, as well as our proposed framework, are trained using an identical setup. We use an SGD optimizer with an initial learning rate of 10 −2 . For the Visual Genome models, we incorporate frequency bias <ref type="bibr" target="#b30">[31]</ref> into the training and inference. We do not use the frequency bias on GQA dataset, due to high memory requirements.</p><p>Energy Model. In the sampling step of SGLD (Eq. 3), we set the number of iterations (τ ) to 20 and the number of message passing iterations in EGNN and GNN to 3. We use a step size of 1 and clip the gradients within [−0.01, 0.01].</p><p>After every gradient update step, we normalize the node and edge states of the scene graph to the range of [0, 1].</p><p>Sentence-to-Graph Retrieval. We use the same formulation for S2GR as previous work <ref type="bibr" target="#b20">[21]</ref>. The problem is formulated as a graph matching problem where the image caption is converted to graph structure using <ref type="bibr" target="#b8">[9]</ref>. The scene graphs and text graphs are mapped into a joint embedding space for retrieval using a Bilinear Attention Network <ref type="bibr" target="#b6">[7]</ref>.  <ref type="table">Table 2</ref>. Zero-shot Recall. The zero shot recall performance comparison of model trained using cross-entopy (CE) and energybased loss (EB) on the Visual Genome (VG) and GQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>Quantitative Results: <ref type="table">Table 1</ref> compares performance of various state-of-the-art methods trained using cross-entropy and our energy-based loss on two datasets, Visual Genome and GQA. We observe that training using the proposed energy loss leads to a consistent improvement in the the mean Recall in all three tasks, for all the models. For the VC-Tree model we obtain a relative improvement of 12.7%, 22.3% and 5.6% on mR@20 for PredCls, SGCls and SGDet respectively when compared to cross entropy based training. We obtain relative improvements of similar magnitudes with the Motif, IMP and VCTree-TDE models. In addition, using our proposed method with the VCTree-TDE <ref type="bibr" target="#b20">[21]</ref> model, we achieve the new state-of-the-art performance on the Visual Genome dataset. On the GQA dataset, we present results of three models, Transformer, Motif and IMP. Similar to the Visual Genome dataset, we observe a consistent improvement in the mean Recall metric under PredCls and SGCls, with each of the models. We omit experiments on SGDet task due to low mAP of the underlying object detector. The proposed method leads to a relative improvement of 8.5% and 25.92% in the mR@20 metric for the Pred-Cls and SGCls task when using the Transformer model for scene graph generation. The absolute performance on the GQA dataset is lower, compared to Visual Genome dataset due to larger number of object and relationship classes. Due to memory constraints, we omit the frequency prior on the GQA dataset, which was shown to be highly effective on the Visual Genome dataset <ref type="bibr" target="#b30">[31]</ref>. We provide additional results along with individual relation recall in the supplementary material. We observe that energy-based models obtain larger improvement in relations that have fewer training annotations, compared to  <ref type="table">Table 3</ref>. Few-shot Recall@20.  <ref type="table">Table 4</ref>. Sentence to Graph Retrieval. We compare the scene graph retrieval performance on gallery of 1000 and 5000 images.</p><p>their cross-entropy based counterparts.</p><p>Zero-Shot Recall: <ref type="table">Table 2</ref> reports zero-shot recall (zsR@20 and zsR@50) for all models. Similar to mR@K, we note consistent improvement on the zero-shot recall. We attribute this behaviour to our energy-based structure aware framework, which facilitates learning of models that are capable of performing global scene graph reasoning.</p><p>Few-shot Recall: In Section 1, we hypothesized that the energy based learning bakes an inductive bias into the learning, thereby allowing the models to learn from less amounts of data. To test this hypothesis, we measure the few-shot Recall@20 for the VCTree model. We train a scene graph detection model using the proposed energy-based model as well as the standard cross-entropy loss. The result, as shown in <ref type="table">Table 3</ref>, demonstrate that our training framework is able to provide a significant boost in performance in fewshot scenarios, when less data is available. This shows that the energy formulation provides a data efficient method for learning scene graph generation.</p><p>Sentence-to-Graph Retrieval: <ref type="table">Table 4</ref> compares the results of sentence-to-graph retrieval experiments. We use VCTree and Motif as our scene graph generation architecture. For each model we observe relative improvements ranging from 5%-23% in the retrieval recall compared to the scene graphs generated using corresponding baselines. This improvement in performance can be attributed to the more coherent and informative scene graphs generated by models trained using the proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicate Classification</head><p>Ablation mR@20 mR@50 mR@100  <ref type="table">Table 5</ref>. Ablation. We experiment with the number of optimization steps (τ ) needed to estimate the energy loss and the effect of excluding image information in the energy model. All numbers were obtained using a VCTree <ref type="bibr" target="#b21">[22]</ref> model.</p><p>while training a predicate classification model using VC-Tree. Note that increasing the number of iterations corresponds to a more precise minima. We also investigated the effect of removing the image graph input to the energy model to determine the effectiveness of modeling the energy density over the joint space of scene and image graph. <ref type="table">Table 5</ref> summarizes the mean recall@K for both of these experiments. We find that with an increase in the number of optimization steps in the energy loss, the mean recall almost consistently increases. Intuitively, this is expected as a larger number of optimization steps means that we have better chance of convergence when trying to find the minima in Eq.</p><p>(2). This increase, however, comes with the added computation overhead and increase in training time. Similarly, we note that removing the image information from the joint modeling, there is a drop in performance as the model is now forced to learn from only the scene graph labels.</p><p>Qualitative Results: We visualize the qualitative results obtained from a VCTree model trained using the proposed energy-based framework as well as cross-entropy loss in <ref type="figure" target="#fig_4">Figure 3</ref>. The top two rows show results from the regular relation retrieval task. We observe that the models trained using our proposed framework can consistently generate instructive relationships such as mounted on, parked on, walking on, standing on as opposed to the less informative and biased variant on generated by the baseline model. Similarly, in the top-left image, the energy-based training generates spatially informative relations such as &lt;cat, in front of, door&gt; instead of &lt;cat, near, door&gt; and &lt;cat, looking at, dog&gt; as opposed &lt;dog, near, cat&gt;. The bottom row shows results of zero-shot relation retrieval. In the first image, due to the triplets of elephants with glasses not being present in the training data, the baseline model predicts &lt;women, wearing, glasses&gt; whereas our method generates the accurate prediction &lt;elephant, has, glasses&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a novel model-agnostic energy-based learning framework for training scene graph generation models. Unlike cross-entropy based training, the proposed method embraces structure in the output space allowing the model to perform structure aware learning. We show that scene graph generation models can benefit from the proposed training framework by performing experiments on the Visual Genome and GQA datasets. We observe significant improvement in performance as compared to traditional crossentropy based training. We also exhibit the generality and efficiency of our model through experiments in zero-shot and few-shot relationship settings. Finally, the proposed method does not make any assumptions on the underlying generation model and can be easily used with any model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Scene Graph Generation: Figure shows scene graphs generated by a VCTree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Model Overview of the Energy-based Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2 )</head><label>2</label><figDesc>Scene Graph Classification (SGCls): Predict the object and predicate labels, given the image and bounding boxes and (3) Scene Graph Detection (SGDet): Predict the scene graph from the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Ablation Studies:Figure 3 .</head><label>3</label><figDesc>We investigated the impact of optimization in Eq.(2) on the effectiveness of the energy model. We experimented with a different number of optimization steps Qualitative Results. Visualizations of scene graphs generated by a VCTree<ref type="bibr" target="#b21">[22]</ref> model trained using cross-entropy loss (in purple) and proposed energy-based loss (in green). The top two rows show visualization of relation retrieval. The bottom row shows zero-shot relation retrieval results with the zero shot triplet show in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>24.41 27.73 31.52 32.31 E.B.M. 18.55 25.22 28.1 32.05 32.57</figDesc><table><row><cell></cell><cell></cell><cell>Few-Shot Recall@20</cell></row><row><cell>k 1 − k 2 shot</cell><cell>1-5</cell><cell>6-10 11-15 16-20 20-25</cell></row><row><cell>C.E.</cell><cell>16.9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table compares</head><label>compares</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">the few short</cell></row><row><cell cols="8">recall performance of a VCTree [22] model trained using cross-</cell></row><row><cell cols="8">entropy and energy-based loss. We observe larger improvement in</cell></row><row><cell cols="4">performance when data is scarce.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Sentence to Graph Retrieval</cell><cell></cell><cell></cell></row><row><cell cols="2">Gallery Size</cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell>5000</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">R@20 R@50 R@100 R@20 R@50 R@100</cell></row><row><cell>VCTree</cell><cell>CE EBM</cell><cell>14 17.2</cell><cell>28.4 32.5</cell><cell>44.6 48.6</cell><cell>4.1 5</cell><cell>8.9 10.96</cell><cell>14.98 18.52</cell></row><row><cell>Motif</cell><cell>CE EBM</cell><cell>15.5 19.2</cell><cell>29.4 32</cell><cell>46.7 49.2</cell><cell>4.56 5.22</cell><cell>9.7 10.96</cell><cell>17 18.64</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3608" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unpaired image captioning via scene graph alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10323" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning canonical representations for scene graph to image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<title level="m">Gqa: A new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph density-aware losses for novel compositions in scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cȃtȃlina</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belilovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08230</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Predicting structured data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIStats</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gps-net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinquan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A scene graph generation codebase in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch.6" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="606" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning visual commonsense for robust scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graphical co ntrastive losses for scene graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11535" to="11543" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
