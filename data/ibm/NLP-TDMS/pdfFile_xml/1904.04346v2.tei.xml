<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
							<email>parmap1@unlv.nevada.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
							<email>brendan.morris@unlv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task -estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks -fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (https: //github.com/ParitoshParmar/MTL-AQA). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned.</p><p>Multitask AQA Action Quality Score:</p><p>83.25/100</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What score should an athlete receive on her dive/gymvault/skating/etc? Which med student has the highest surgical skill level? How well can he paint or draw? How is a patient progressing in their physical rehabilitation program? Answering these questions involves the quantification of the quality of the action -determining how well the action was carried out, also known as action quality assessment (AQA). Existing AQA <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref> and skills assessment <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> approaches use a single label, known as a final score or skill-level, to train the system using some kind of regression or ranking loss function. However, the performance of these systems is limited and it seems that a single score is not sufficient to characterize a complicated action. In AQA, the final score is dependent on what was done (this determines the let's go and go he does forward four-and-a-half somersault three point seven big opening dive from even Garcia just a really difficult dive to control the splash a little more than he might have liked nice tight tuck position spots the water the feet coming over just a little <ref type="figure">Figure 1</ref>: Multitask AQA concept. Recognizing an action instance in detail and verbally describing its good and bad points can be helpful in the process of quantifying the quality of that action instance. We propose to learn a model that delineates an action besides measuring its quality. To see the videos play, please download the manuscript and view in an Adobe Reader. difficulty level) and how was that done (this determines the quality of execution). We pose the following question: can learning to describe and commentate on the action instances help improve the performance on the AQA task?</p><p>We hypothesize that by forcing the network to learn to do so will help better characterize the action, and hence aid in AQA. So, rather than using just a single encompassing quality label to train the network, we introduce a multitask learning (MTL) approach <ref type="figure">(Fig. 1)</ref> to assess the quality of an action. Specifically, we propose to utilize 3D CNN's to learn spatio-temporal representations of salient motion and appearance; optimize those using loss functions which account for i) the action quality score, ii) factorized (detailed) action classification, and iii) generate a verbal commentary of performance; and are trained end-to-end. Note that the architectures are multitask and not multi-modal since the input does not use captions or action classification to produce the AQA score. Besides straight forward utility for AQA and action classification, automatic commentary or sports narrative generation has been viewed valuable and greatly applicable in a recent work by Yu et al. <ref type="bibr" target="#b28">[29]</ref>.</p><p>For AQA tasks, domain experts can provide detailed analysis of performance. In the professional sports setting, ground truth annotations for detailed action classification and commentary by former athletes are readily available in broadcast footage facilitating extraction of labels and descriptive captions. As such, to evaluate our approach, we introduce the first multitask AQA dataset with 1412 samples of diving which is also the largest AQA dataset to date.</p><p>Experimental evaluation show that performance of both the architectures improved as more tasks were added and the C3D-AVG-MTL variant outperforms all existing AQA approaches in literature. MTL was shown to outperform STL across various training set sizes. Further experiments explore the AQA-orientedness of the feature representations learned by our networks and find they outperform actionrecognition representations on unseen actions indicating that better generalized concepts of quality were learned.</p><p>Contributions: primary novelty of this works lies in the problem formulation -to learn spatio-temporal representations by optimizing networks end-to-end jointly for finegrained action description and AQA scoring. Task selection is intuitive. No previous work has done this; not just for AQA, but even action recognition and captioning tasks. We release a novel MTL-AQA dataset which is the largest AQA dataset so far, much more diverse, challenging, and richly annotated with factorized fine-grained action class and AQA-oriented captions; can help researchers in the field to examine new ideas for AQA and auxiliary tasks. We show that our MTL approach works across different architectures. Our approach is applicable to a wide range of problems. Proposed models are simple, yet intuitive, and effective in carrying out central idea. Our C3D-AVG-MTL surpasses all the existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>AQA: Pirsiavash et al. <ref type="bibr" target="#b17">[18]</ref> proposed the use of DFT/DCT of body pose as features for a support vector regressor (SVR) to map to a final action quality score. They introduced an action quality dataset containing two actions: Diving and Figure Skating. However, since their method relied solely on pose features, it neglected important visual quality cues, like splash in the case of Diving. Since accurate pose is especially difficult in sports scenarios where athletes undergo extremely convoluted poses, Venkataraman et al. <ref type="bibr" target="#b24">[25]</ref> better encoded using the approximate entropy of the poses to improve the results.</p><p>More recently, spatio-temporal features from 3D convolutional neural networks (C3D) <ref type="bibr" target="#b23">[24]</ref> proved to be very successful on a related task of action recognition since they captured appearance and salient motion. Seeing this as a desirable property that would help to take into account visual cues, Parmar and Morris <ref type="bibr" target="#b15">[16]</ref> proposed using C3D features for AQA. They proposed three frameworks, C3D-SVR, C3D-LSTM, and C3D-LSTM-SVR, which differed in their feature aggregation and regression scheme. All the frameworks worked better than previous models proving the efficacy of C3D features for AQA. Xiang et al. <ref type="bibr" target="#b25">[26]</ref> proposed breaking video clips into action specific segments and fusing segment-averaged features instead of over full videos. By adding finer segment labels to data samples performance was improved. Li et al. <ref type="bibr" target="#b12">[13]</ref> divide a sample into 9 clips and use 9 different C3D networks dedicated to different stags of Diving. Features are concatenated and further processed through conv and fc layers to produce a final AQA score using a ranking loss along with the more typical L2 loss. Xu et al. <ref type="bibr" target="#b26">[27]</ref> tackle AQA for longer action sequences using self-attentive and multiscale convolutional skip LSTM.</p><p>Skills assessment: Zia et al. <ref type="bibr" target="#b32">[33]</ref> extract spatio-temporal interest points (STIP's) in the frequency domain to classify a sample into novice, intermediate or expert skills level. Instead of using handcrafted STIP's Doughty et al. <ref type="bibr" target="#b3">[4]</ref> learn and use convolutional features with ranking loss as their objective function to evaluate surgical, drawing, chopstick use and dough rolling skills. In their subsequent work <ref type="bibr" target="#b4">[5]</ref>, they use temporal attention. Li et al. <ref type="bibr" target="#b13">[14]</ref>, make use of spatial attention in the assessment of hand manipulation skills. Bertasius et al. <ref type="bibr" target="#b0">[1]</ref> focus on measuring basketball skills but rely only on assessment of a single basketball coach making their dataset subjective to a particular evaluator.</p><p>All of the existing AQA and skills assessment frameworks are single task models and only give the final AQA score. Our proposed framework is a multitask model to recognize the action, measures its quality and also generates captions (or commentary).</p><p>Multi-modal approaches and captioning: Images and videos (especially sports) are often accompanied by a caption or commentary which can themselves serve as labels yet to be exploited for AQA or skill assessment. Quattoni   <ref type="table">Table 2</ref>: Classification of dives. Each combination of the presented sub-fields produces a different kind of maneuver.</p><p>et al. <ref type="bibr" target="#b18">[19]</ref> use large quantities of unlabeled images, with associated captions, to learn image representations. They found that this sort of pre-training with extra information could speed up the learning on a target task. Rather than using captions as groundtruth labels, Sonal et al. <ref type="bibr" target="#b5">[6]</ref> treated captions as a "view" and use them along with images to learn a classifier using co-training. They again used commentary as a "view" for action recognition with success. To train an activity classifier in an automated fashion, without the requirement of any manual labeling, Sonal and Mooney <ref type="bibr" target="#b6">[7]</ref> make use of broadcast closed captions and used the system for video retrieval. There are a few works which focus on captioning in sports settings. Yu et al. <ref type="bibr" target="#b28">[29]</ref> address the task of generating fine-grained video descriptions for basketball and evaluate performance using their novel metric. Commentary generation in cricket has been addressed in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, while Sukhwani addressed the problem of describing tennis videos in <ref type="bibr" target="#b22">[23]</ref>. While these works focus on captioning or improving captioning, we integrate a captioning task with an AQA task to provide stronger supervision as commentary is a verbal description of AQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multitask AQA Dataset</head><p>In order to facilitate research in the area of AQA, we release a new dataset. This is the first of a kind multitask AQA dataset. With 1412 samples, it is the largest AQA dataset to date. This particular dataset focuses only on Diving as it has seen the most usage recently. Data was compiled from 16 different events unlike the single main event (2012 Olympics Men's 10m Platform Diving competition) used for previous datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref> to provide significantly more variation. Diving samples in the new dataset were collected from various International competitions and include the 10m Platform as well as 3m Springboard, include both male and female athletes, individual or pairs of synchronized divers, and different views. A comparison of our new dataset with existing Diving AQA datasets is provided in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Since data was collected from televised international events, before the athletes perform their routines, information regarding their routine is displayed. This information includes the difficulty of the dive and a description of the dive. The AQA score is extracted from the judges' scores after the dive completion. The dataset uses the same dive classification strategy as Nibali et al. <ref type="bibr" target="#b14">[15]</ref>, where instead of using dive number (equivalent to an action class in action recognition) directly, we factorize a dive into its components such as the position of the dive, the number of somersaults (SS), and number of twists (TW). Full details for the dive classification is in <ref type="table">Table 2</ref>.</p><p>Further, during and after a diving routine, television analysts provide commentary. These analysts are often retired athletes and have deep understanding of the sport. This verbal account of the athlete's performance is recorded for the third type of action label. The commentary was considered an important indicator for performance since it was the only way to "watch" an event before telecast was available. Commentators say what the athlete performed, what was correct with the athlete's performance, and where and how athletes made mistakes. This provides deeper insight into the athlete's performance and can help an average person better understand the sport. We used Google's Speech-To-Text API to convert commentary audio to text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multitask Approach to AQA</head><p>MTL is a machine learning paradigm in which a single model caters to more than a single task. An example is to recognize road signs, roads, and vehicles together while an STL approach would require separate models for each object type. MTL tasks are generally chosen such that they are related to one another and their networks have a common body that branches into task-specific heads. The total network loss is the sum of individual task losses. When optimized end-to-end, the network is able to learn richer representation in the common body section since it must be able to serve/explain all tasks. With the use of related auxiliary tasks, which are complementary to the main task, the richer representation tends to help improve performance on the main task.</p><p>In general, not just for diving, action quality is a function of what action was carried out and how well that action was executed. This makes the choice of auxiliary tasks natural: detailed action recognition is the answer to the 'what' part and commentary, being a verbal description containing good and bad points about action execution, is an answer to the 'how well' part. AQA can be thought of as finding a function that maps input video to the AQA scores. Caruana in <ref type="bibr" target="#b1">[2]</ref> views supervision signals from auxiliary tasks as an inductive bias (assumptions). Inductive bias can be thought of as constraints that restrict the hypothesis/search space when finding the AQA function. Through inductive biases, MTL provides improved generalization as compared STL <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this work, the main task is to assess the action quality (AQA score) and the auxiliary tasks are to recognize the action (dive type classification) and to generate descriptive captions/commentary. Action recognition in turn consists of five fine-grained dive sub-recognition tasks: recognizing position and rotation type, detecting armstand, and counting somersaults and twists.</p><p>First, let us formalize the settings and objective functions. AQA is a regression problem where, generally, the Euclidean distance between the predicted quality score and the ground truth is used as the objective function to be minimized <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>. Initial experimentation found that using L1 distance in addition to L2 yielded better results on the AQA task</p><formula xml:id="formula_0">L AQA = − 1 N N i=1 (x i − y i ) 2 + |x i − y i |<label>(1)</label></formula><p>where x i is the predicted score and y i is the ground truth score for each of the N samples. For action recognition, we use cross-entropy loss between the predicted labels and ground truth label</p><formula xml:id="formula_1">L Cls = − 1 N N i=1 sa ksa j=1 y sa i,j log(x sa i,j )<label>(2)</label></formula><p>where k sa is the number of categories in sub-action class sa (as in <ref type="table">Table 2</ref>). Negative log likelihood is used as the loss function for the captioning task</p><formula xml:id="formula_2">L Cap = − 1 N N i=1 sl ln(x cap y cap )<label>(3)</label></formula><p>with sl is the sentence length. The overall objective function to be minimized is the summation of all the losses</p><formula xml:id="formula_3">L M T L = αL AQA + βL AR + γL Cap .<label>(4)</label></formula><p>where α, β, γ are loss the weights. Now, we will introduce two different architectures for MTL-AQA.</p><p>MTL-AQA architectures Unlike action recognition that may be accomplished by looking at as little evidence as just a single frame <ref type="bibr" target="#b10">[11]</ref>, for AQA the complete action sequence needs to be considered because the athlete can make or lose points at any point during the whole sequence. While spatio-temporal representations learnt using 3D CNN's capture appearance and salient motion patterns <ref type="bibr" target="#b23">[24]</ref>, which makes them one of the best candidates for action recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8]</ref> and also for AQA <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>, 3D CNN's require large memories which limits their application to small clips. We tackle this bottleneck in two ways:</p><p>1. divide the video (96 frames) into small clips (16 frames), and then aggregate clip-level representations to obtain video-level description (Sec. 4.1) 2. downsample the video into a small clip (Sec. 4.2) Networks designed for multitask learning generally two segments: common network backbone and task-specific heads. Common network backbone learns shared representations, which are then further processed through taskspecific heads to obtain more task-oriented features and outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Averaging as aggregation (C3D-AVG)</head><p>The first network we present is C3D-AVG <ref type="figure" target="#fig_1">(Fig. 2</ref>). Network backbone: Backbone consists of C3D network <ref type="bibr" target="#b23">[24]</ref> up to the fifth pooling layer. Aggregation scheme: An athlete gathering (or losing) points throughout the action can be seen as an addition operation. Combining this perspective with a good rule of thumb that when good representations are learned, linear operations on them become meaningful, we propose to enforce a linear combination of representations to be meaningful, in order to learn good representations. Specifically, we propose to use averaging as the linear combination. The network is optimized end-to-end for all three tasks.</p><p>C3D-AVG network up to Average layer can be considered as an encoder, which encodes input video-clips into representations that when averaged (in feature space) would correspond to the total AQA points gathered by the athlete. Subsequent layers can be thought of decoders for individual tasks.</p><p>Task-specific heads: For action-recognition and AQA tasks, clip-level pool-5 features are averaged elementwise to yield a video-level representation. Since captioning is a sequence-to-sequence task, the individual clip-level (96 * 112 * 112 * 3) shared weights </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common network backbone</head><p>Task-specific heads features are input to the captioning branch before averaging (individual clip-level features worked better in practice than averaged clip-level features for captioning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multiscale Context Aggregation with Dilated Convolutions (MSCADC)</head><p>Multiscale context aggregation with dilated convolutions (MSCADC) <ref type="bibr" target="#b27">[28]</ref> has been shown to improve the classification of dives in the work of Nibali et al. <ref type="bibr" target="#b14">[15]</ref>. Given its strong performance on an auxiliary task MSCADC was selected for MTL. Our MTL variant network has a backbone and multiple heads as illustrated in <ref type="table" target="#tab_3">Table 3</ref>. Network backbone: The MSCADC network is based on C3D network <ref type="bibr" target="#b23">[24]</ref> and incorporates improvements like using Batch Normalization <ref type="bibr" target="#b8">[9]</ref> to provide better regularization which is needed in AQA where data is quite limited. Additionally, pooling is removed from the last two convolutional groups of C3D and instead a dilation rate of 2 is used. This backbone structure is shared among all the MTL tasks. Task-specific heads: We use separate heads, one for each task. Heads consist of a context net followed by a few additional layers. The context net is where the feature maps are aggregated at multiple scales.</p><p>Dilated convolutions and multi-scale aggregation have shown improvements in the tasks involving dense predictions <ref type="bibr" target="#b27">[28]</ref>. We believe that removing pooling layers and using dilated convolutions better maintains the structure of the diving athlete without losing resolution. This helps in better assessment of the athlete's pose which is critical for AQA. For example, pose can identify when legs are aligned or split which is useful not only for diving but also other sports such as gymnastic vault, figure skating, skiing, snowboarding, etc.</p><p>Unlike the C3D-AVG network, we downsample the complete action into a short sequence of only 16 frames (something like key action snapshots) as done by Nibali et al. <ref type="bibr" target="#b14">[15]</ref>.  This reduces our 96-frames videos into key action snapshots which helps in processing the complete action sequence in a single pass. Processing an action sequence using this network can be thought of as distilling information from the input frames and putting it into feature maps, with different feature maps containing different kinds of pose information.</p><p>A natural benefit of downsampling the sequence is that there is a significant reduction in the the number of network parameters and memory which can be used instead to increase spatial resolution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Implementation: PyTorch <ref type="bibr" target="#b16">[17]</ref> is used to implement all the networks; common network backbones were pretrained on the UCF101 <ref type="bibr" target="#b21">[22]</ref> action recognition dataset. The captioning module utilized a GRU <ref type="bibr" target="#b2">[3]</ref> cell and a dropout rate of 0.2 in the encoder and decoder. Maximum caption length is set to 100 words. Full vocabulary size is 5779. The parameters α, β, and γ in Eq. 4 are set to 1, 1, and 0.01. All networks used the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> and were trained for 100 epochs with initial learning rate of 1e-4. Data augmentation is performed through center cropping with temporal augmentation and random horizontal flipping. The center crop was found to reliably capture both the athlete and other prominent visual cues such as splash. Batch-size was set to three samples. Additional architecture-specific implementation details are as follows: C3D-AVG: The model is trained end-to-end with a 112 × 112 center crop from the 171 × 128 pixel input video. Each dive sample was temporally normalized to a length of 96 frames. MSCADC: Since this architecture does not contain fullyconnected layers and all videos are downsampled to 16 frames, there are fewer model parameters allowing the use of higher resolution video input. Frames are resized to 640 × 360 pixels and 180 × 180 center cropping is used. Evaluation metrics: AQA is assessed using Spearman's rank correlation, dive classification uses accuracy, and commentary uses captioning metrics of Bleu, Meteor, Rouge, and CIDEr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single-task vs. Multi-task approach</head><p>We carry out an experiment to compare the performance of STL against that of MTL. We have a total of 3 tasks: AQA, detailed action recognition, and commentary generation. This experiment first considered the STL approach to AQA task and then measured the effect of including auxiliary tasks. The evaluation is summarized in <ref type="table" target="#tab_5">Table 4</ref>. We observe that MTL approaches perform better than STL approach for both the networks, which shows that our MTL approach is not limited to a network but is generalizable across networks. Other thing to note here is that MTL per-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sp. Corr.</p><p>Pose+DCT <ref type="bibr" target="#b17">[18]</ref> 26.82 C3D-SVR <ref type="bibr" target="#b15">[16]</ref> 77.16 C3D-LSTM <ref type="bibr" target="#b15">[16]</ref> 84   <ref type="table">Table 6</ref>: Performance on auxiliary tasks.</p><p>formance improves as we incorporate more tasks. Comparing both the architectures, we find that our C3D-AVG outperforms our MSCADC for both STL and MTL, while MSCADC has the advantage of being fast and lower memory requirement than C3D-AVG. For qualitative results, refer to <ref type="table" target="#tab_10">Table 7</ref> and supplementary material. Next, we compare our models with the existing methods in <ref type="table" target="#tab_7">Table 5</ref>. We obtain the results for all of the existing methods on our dataset. C3D-SVR was the best performing method in <ref type="bibr" target="#b15">[16]</ref> but it does not seem to benefit from the increased number of training samples. In <ref type="bibr" target="#b15">[16]</ref>, C3D-LSTM was reported to be performing worse than C3D-SVR due to insufficient amount of training data and does outperform C3D-SVR with the expanded training data. Our MSCADC-STL works better than most of the existing methods, whereas our C3D-AVG-STL is better performing than all the existing methods. Furthermore, C3D-AVG- GT Caps: Games but that shoulder issue it's not even paddling with that arm I hope he hasn't done something serious here and I think he has he looks even he is struggling he is in trouble and this is most concerning his hope he hasn't suffered a little dislocation there he's managed to get himself up onto the pool edge that looks like a dislocation to me what a way to end that's not the case but he's in trouble C3D-AVG Caps: that's good she will certainly keep that with a dive that we can certainly do it in that field very impressive it is very good but it was a good dive here playing a little bit low water which is a strong start one look at that closes vertical position it's gonna get seven and a half's I think super slow-motion just throwing up a little bit too much splash but she'll get over 60s mid sixties probably and that will be good enough to keep her middle of the pack which is where she needs to be to MSCADC Caps: well that's a really good dive from her Cheung just last night for that the dive and over she goes die are faltering and a good takeoff if she gets her feet are so she drills she's feet up and then she goes into the water with her chin on the end of the diving board on the end of the diving board squeezing into the Tuck shape as tight as you can see here we are here's the one and a half twist into the pike and that makes it down there have a lot GT labels: 63.07; [Free, Yes, Backwards, 2, 2.5] C3D-AVG labels: 65.79; [Free, Yes, Backwards, 2, 1.5] MSCADC labels: 63.23; [Free, Yes, Backwards, 2, 1.5] GT Caps: it's beautiful our balance he is cranked the start a little bit too much he giving too much beans and could not stop that rotation it's a tricky dive to do it many times it's just a real pain because you've got so many twists to do to get in with a rotation it's very easy to over rotate that's exactly what Mitchum's done he is lagging behind now this could prove to be critical dropped another 12 C3D-AVG Caps: well it's okay and the entry into the water not quite 100% vertical but he's just a little bit overcooked on the end obviously there's a few of the divers have you use themselves so if you're getting your hands out there we are rocking and rolling a little bit of a splash with technically a little bit of splash that's not the splash means that the judges will penalize him or only got to 17 from the two and a half somersaults before he goes into the water now that was a MSCADC Caps: well that's a really good dive yeah it's a good dive from enough so she needed to be faltering she knows to go in her hair it's a difficult dive to finish in the business end of the field and away in the pike position this is the reverse two and a half there she didn't get around but instead of she goes wrong with her hands on I think she has done very well I'm not surprised as Paul Espinoza she gets around the 310 mark maybe yeah she will do well do GT Caps: states very good seventy nine point two for the dive last night brings a smile to the US coaching staff great height off the platform so he's already threw his first somersault before he passes it on the way down give yourself time to get the entry angle just right which he did started off as a swimmer and said that to switch to diving because he looked more fun having a bit more fun C3D-AVG Caps: excellent excellent dive if you might he's got a lot of divers here with their hands together for him a lot of them here and take a little bit of an angle on the entry that does good through that would not quite a way over a vertical look at that perfect angle so much better judges will like that that angle so not too many MSCADC Caps: well that's a really good dive yeah it's a good dive from marginally short she goes in but it's not going to get more than a couple of those three point seven as you can see here we are here's to show diving consistently and here but she didn't falter but she had enough mediocre dives on the dive you need to be doing the back of the head there you know you need to flick it and then just about that toes on the head on this dive is there when you know the competition's are coming from GT Caps: final but she has benefited from the experience and again another diver going a long long way down the pool and away from the platform yeah just it's the technique if you haven't got the right timing on the jump and momentum of the run down the board just drags you forward high score so far is 46 there we C3D-AVG Caps: nice nice entry because the execution was fine and then just suggesting she went surfing over the end of the diving board anyway she's a safe distance from the diving board so that's a good dive in the prelims you can see the splash moving away from the diving board six and a half's sevens at best moving further away from the podium dive after dive star with a 58 and it with a 64 this MSCADC Caps: well that's a busy dive finish from the end but she certainly was better than the three point three she didn't really attack she was her fourth round dive she knows too far away from the diving board squeezing into the Tuck shape just bending at a little bit too early they're short of vertical an injury little bit too much splash but she'll be lucky by the toes on the water the divers would have been that towel on the end of the divers do that the divers would have touched the scoreboard but I think she's gonna   <ref type="table">Table 9</ref>: Performance of fitting linear regressors on the activations of all the convolutional layers.</p><p>MTL with 90.44% correlation achieves new state-of-the-art results.</p><p>Method proposed by Xiang et al. <ref type="bibr" target="#b25">[26]</ref> requires manual annotation to mark end points of all the segments which is not available in the new Diving-MTL data. Xiang et al. <ref type="bibr" target="#b25">[26]</ref> used the UNLV-Dive dataset <ref type="bibr" target="#b15">[16]</ref> so for a fair comparison with <ref type="bibr" target="#b25">[26]</ref> we train and test our models on UNLV-Dive <ref type="bibr" target="#b15">[16]</ref>. The results are enumerated in <ref type="table" target="#tab_7">Table 5</ref>. Our C3D-AVG-STL does not perform as well S3D <ref type="bibr" target="#b25">[26]</ref>. However, our C3D-AVG-MTL outperforms the S3D model. An important thing to note here is that UNLV-Dive dataset is quite a bit smaller than our newly introduced MTL-AQA dataset which should limit MTL performance. However, as pointed out in Section 4, MTL provides better generalization than STL, which allows C3D-AVG-MTL to learn effectively from fewer training samples.</p><p>Performance on the auxiliary tasks is presented in <ref type="table">Table  6</ref>. To the best of our knowledge there is only one work (by Nibali et al. <ref type="bibr" target="#b14">[15]</ref>) on detailed dive classification. Our C3D-AVG-MTL performed best on the classification task as well. We also give captioning metrics for the two networks though there is no baseline for comparison in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization provided by MTL:</head><p>To ascertain that MTL is providing more generalization, we train our C3D-AVG-STL and C3D-AVG-MTL models using fewer number of datapoints. Train set size and the corresponding STL/MTL performances are detailed in <ref type="table" target="#tab_11">Table 8</ref>. We see that MTL consistently outperforms STL, and also the gap seems to widen with fewer training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">AQA-orientedness of the learned representations</head><p>We trained our networks end-to-end to learn AQAspecific feature representation rather than relying on pretrained action-recognition oriented features (as done in   <ref type="bibr" target="#b15">[16]</ref>). However, we question if there is a utility in learning AQA-specific feature representation or are actionrecognition oriented features equally good? To answer this, we follow an evaluation scheme similar to Zhang et al. <ref type="bibr" target="#b29">[30]</ref>, where we train linear regressors on top of all the convolutional layers, and compare the performance obtained for AQA and action-recognition models. In particular, we consider two action-recognition baselines: C3D model trained on UCF-101 dataset <ref type="bibr" target="#b21">[22]</ref> (Baseline-1), and our model trained on our MTL-AQA dataset, but for factorized action recognition task (Baseline-2).</p><p>In the primary evaluation, we compare the representations for measuring the quality of diving action. Comparison is detailed in <ref type="table">Table 9</ref>. In comparison to both the baselines, we find that our C3D-AVG-MTL learns better representations at all the intermediate layers.</p><p>Further we compare the representations for measuring the quality of an unseen action class -Gymnastic vault <ref type="bibr" target="#b15">[16]</ref>. This helps in estimating the generalizability of the representations. We hypothesize that if our AQA network has learned better representations that actually capture the concept of quality in an action, then it should be able to measure the quality of an unseen action better than actionrecognition specific networks. We carry out 2 different evaluations: 1) Within-dataset evaluation and 2) Outof-dataset evaluation. In Within-dataset evaluation we randomly divide the samples into train set and test set, whereas in Out-of-dataset evaluation, train and test samples are drawn from different athletic competitions. Outof-dataset evaluation is more challenging and requires feature representations to be more generalizable and not suffer from dataset-bias. Like the previous experiment, to compare learned representations, we train linear regressors on top of all the convolutional layers. Train and test sets consist of 125 and 56 samples respectively. Results from both evaluations are presented in <ref type="table" target="#tab_1">Table 10</ref>.</p><p>In the Within-dataset evaluation, the representations learned by all the models seem to be working well, although C3D-AVG-MTL performs best. The difference in performance becomes clearer in the Out-of-dataset evaluation. As expected, Out-of-dataset evaluation is more challenging and performances of all the models drop. However, the performances of Baseline-2 and our model drop more gracefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We introduced a multitask learning approach to AQA and showed that MTL performs better than STL because of better generalization which is especially important in AQA and skill assessment since datasets are small. We showed that the representations learned by our MTL models are better able to capture the inherent concept of quality of actions. Our approach is scalable since the supervision required for the auxiliary tasks is readily available from the existing video footage with minimal extra effort compared to just AQA labeling. In addition, state-of-the-art performance was achieved without any finetuning of hyperparameters. Our best performing and recommended model, C3D-AVG-MTL, achieved 90.44% correlation with judged scores which still leaves a small gap to achieve humanexperts-level performance (96% <ref type="bibr" target="#b17">[18]</ref>).</p><p>Extension to other actions and skills assessment: Although this paper is geared specifically toward multitask diving AQA, the approach is general in nature. No design decisions were biased towards or specific to the diving tasks. Experiments even showed that the models trained on diving do work reasonably well for another action, gymnastic vault. This encouraging result hints at the direct application of our MTL approach on other actions and everyday skills assessment. Commentary and action class details are available almost all the of time in the sport footages. For non-sport skills assessment, such as surgery, needle passing, drawing, or painting, experts could be used to generate comments and definition of sub-actions for classification. Note that existing datasets can simply be augmented to include additional labels, instead of building new datasets from scratch. Also, our MTL approach is complementary to the existing AQA and skills assessment approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>C3D-AVG-MTL network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>GT labels: 84.15; [Tuck, No, Backwards, 3.5, 0] C3D-AVG labels: 81.94; [Tuck, No, Backwards, 3.5, 0] MSCADC labels: 87.04; [Tuck, No, Backwards, 3.5, 0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Details of our newly introduced dataset, and its comparison with the existing AQA datasets.</figDesc><table><row><cell cols="3">Position Armstand Rotation type</cell><cell># SS</cell><cell># TW</cell></row><row><cell>Free Tuck Pike</cell><cell>No Yes</cell><cell>Inward Forward Reverse Backward</cell><cell cols="2">0 to 4.5 0 to 3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>MSCADC-MTL architecture. C3(d,ch): 3D convolutions, ch-no. of channels, d-dilation rate. C1: 1 × 1 × 1 convolutions. BN: batch normalization. MP(kr): max pooling operation, kr-kernel size. Cntxt net: context net for multi-scale context aggregation. AP: average pooling across (2 × 11 × 11) volume.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>STL vs. MTL across different architectures. Cls -classifiction, Caps -captioning. First row shows STL results, while the remaining rows show MTL results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with the existing AQA approaches.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Nibali</cell><cell cols="3">Ours-MTL</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">et al. [15]</cell><cell cols="4">MSCADC C3D-AVG</cell></row><row><cell>Position</cell><cell></cell><cell cols="2">74.79</cell><cell>78.47</cell><cell></cell><cell>96.32</cell><cell></cell></row><row><cell>Amstand</cell><cell></cell><cell cols="2">98.30</cell><cell>97.45</cell><cell></cell><cell>99.72</cell><cell></cell></row><row><cell cols="2">Rotation type</cell><cell cols="2">78.75</cell><cell>84.70</cell><cell></cell><cell>97.45</cell><cell></cell></row><row><cell cols="2"># Somersaults</cell><cell cols="2">77.34</cell><cell>76.20</cell><cell></cell><cell>96.88</cell><cell></cell></row><row><cell># Twists</cell><cell></cell><cell cols="2">79.89</cell><cell>82.72</cell><cell></cell><cell>93.20</cell><cell></cell></row><row><cell>Model</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>M</cell><cell>R</cell><cell>C</cell></row><row><cell>C3D-AVG</cell><cell cols="7">0.26 0.10 0.04 0.02 0.11 0.14 0.06</cell></row><row><cell>MSCADC</cell><cell cols="7">0.25 0.09 0.03 0.01 0.11 0.13 0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>GT labels: 89.08; [Tuck, No, Backwards, 3.5, 0] C3D-AVG labels: 80.41; [Tuck, No, Backwards, 3.5, 0] MSCADC labels: 85.09; [Pike, No, Backwards, 2.5, 0]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Qualitative results. Labels are ordered as follows: AQA score; [Position, Armstand?, Rotation type, #SS, #TW].</figDesc><table><row><cell># samples</cell><cell>1059</cell><cell>450</cell><cell>280</cell><cell>140</cell></row><row><cell>STL</cell><cell cols="4">89.60 77.27 69.63 64.17</cell></row><row><cell>MTL</cell><cell cols="4">90.44 83.52 72.09 68.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>STL vs. MTL generalization. Training using increasingly reduced no. of training samples.</figDesc><table><row><cell></cell><cell>c1</cell><cell>c2</cell><cell>c3</cell><cell>c4</cell><cell>c5</cell></row><row><cell>Baseline-1</cell><cell cols="5">71.01 71.39 73.13 76.34 73.69</cell></row><row><cell>Baseline-2</cell><cell cols="5">72.43 70.15 70.35 57.20 37.63</cell></row><row><cell cols="6">C3D-AVG-MTL 74.26 77.95 82.78 86.18 85.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>41.10 32.06 36.53 46.86 44.78 Baseline-2 37.76 42.02 37.98 44.28 38.56 C3D-AVG-MTL 38.32 42.68 45.53 49.18 38.47 MTL -07.75 -02.77 23.51 29.56 -03.25</figDesc><table><row><cell></cell><cell>c1</cell><cell>c2</cell><cell>c3</cell><cell>c4</cell><cell>c5</cell></row><row><cell></cell><cell cols="3">Train/Test events overlapping</cell><cell></cell></row><row><cell>Baseline-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Train/Test events non-overlapping</cell></row><row><cell></cell><cell cols="3">(requires more generalization)</cell><cell></cell></row><row><cell>Baseline-1</cell><cell cols="5">-02.68 00.75 -03.91 -02.22 03.17</cell></row><row><cell>Baseline-2</cell><cell cols="5">-07.52 -02.44 05.07 24.09 25.80</cell></row><row><cell>C3D-AVG-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Performance of fitting linear regressors on the activations of all the convolutional layers for a novel action class, Gymnastic vault. Top rows: Within-dataset evaluation, bottom rows: Out-of-dataset evaluation.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We would like to thank Andy Squadra, Mark Wilbourne, Josh Rana for helping us with the dataset collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Am i a baller? basketball performance assessment from first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who&apos;s better? who&apos;s best? pairwise deep ranking for skill determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The pros and cons: Rank-aware temporal attention for skill determination in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05538</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Watch, listen &amp; learn: Co-training on captioned images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="457" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using closed captions to train activity recognizers that improve video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR-09 Workshop on Visual and Contextual Learning from Annotated Images and Videos (VCL)</title>
		<meeting>the CVPR-09 Workshop on Visual and Contextual Learning from Annotated Images and Videos (VCL)<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating surgical skills from kinematic data using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Manipulation-skill assessment from videos with spatial attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extraction and classification of diving clips from continuous video footage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning visual representations using images with captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Commbox: Utilizing sensors for realtime cricket shot identification and commentary generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritam</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidhartha</forename><surname>Satapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satadal</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankarshan</forename><surname>Mridha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication Systems and Networks (COM-SNETS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="428" />
		</imprint>
	</monogr>
	<note>2017 9th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grain annotation of cricket videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Rahul Anand Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding and Describing Tennis Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohak</forename><surname>Kumar Sukhwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>International Institute of Information Technology Hyderabad</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamical regularity for action analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan K</forename><surname>Turaga</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">S3d: Stacking segmental p3d for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trac D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to score the figure skating sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02774</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-grained video captioning for sports narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated surgical skill assessment in rmis training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="731" to="739" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Video and accelerometer-based motion analysis for automated surgical skills assessment. International journal of computer assisted radiology and surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="443" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated video-based assessment of surgical skills for training and evaluation in medical schools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ploetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1623" to="1636" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
