<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Girisha</forename><forename type="middle">S</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Verma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Manohara</forename><surname>Pai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">M</forename><forename type="middle">M *</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Radhika</forename><forename type="middle">M</forename><surname>Pai</surname></persName>
						</author>
						<title level="a" type="main">UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-UAV video</term>
					<term>semantic segmentation</term>
					<term>transfer learning</term>
					<term>U-Net</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.</p><p>In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature retainer module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE analysis of data collected from airborne sensors such as aerial images/videos are increasingly becoming a vital factor in many applications such as scene understanding, studying the ecological variations <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, tracking of vehicles/animals/humans <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b49">[50]</ref> surveying the urban development <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b51">[52]</ref>, surveillance <ref type="bibr" target="#b39">[40]</ref>, etc. Besides, aerial image analysis has been used for assessing the damage immediately after a natural disaster <ref type="bibr" target="#b16">[17]</ref>. Typically, the aerial images are captured by different imaging modalities such as Synthetic Aperture Radar (SAR) <ref type="bibr" target="#b45">[46]</ref>, hyper-spectral imaging <ref type="bibr" target="#b35">[36]</ref> which are present on-board a satellite. Recently, the Unmanned Aerial Vehicles (UAV) have also been widely used for various applications such as disaster management, urban planning, tracking of wildlife, agricultural planning etc <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Due to rapid deployment and a customized flight path, the UAV images/videos, could provide additional finer details and complement satellite-based image analysis approaches for * Corresponding Author † Equal Contribution critical applications such as disaster response <ref type="bibr" target="#b27">[28]</ref>. Besides, the UAV images could be utilized along with satellite images for better urban planning or geographical information updating. Typically, the UAV image/video analysis is limited for object detection <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref> and recognition <ref type="bibr" target="#b41">[42]</ref> tasks such as building detection, road segmentation etc. However, to the best of our knowledge, there are limited works on semantic segmentation of UAV images or videos <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Segmentation is a crucial task for scene understanding and has been used for various applications <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Semantic segmentation is a process of assigning predetermined class labels to all the pixels in an image. Semantic segmentation of an image is a widely studied topic in computer vision. However, the extension of semantic segmentation for video applications is a non-trivial task. One of the challenges in video semantic segmentation is to find a way to incorporate temporal information. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the importance of temporal information in the context of video acquired by UAV. The poor segmentation in the greenery class can be observed in the (i+1) th keyframe which can be improved by embedding temporal information from the past frames.</p><p>In a typical video semantic segmentation approach, a sequential model is added on top of the frame-wise semantic arXiv:2011.14284v1 [cs.CV] 29 Nov 2020 segmentation module, thus creating an overhead <ref type="bibr" target="#b13">[14]</ref>. Besides, features/label propagation <ref type="bibr" target="#b29">[30]</ref>, which re-utilizes features/labels from previous frames has also been utilized to capture the temporal information. However, these methods depend on the establishment of pixel correspondence between two frames. Recently, video prediction based approach <ref type="bibr" target="#b52">[53]</ref> has been used to generate new training images and has achieved state-of-the-art performance for video semantic segmentation. However, this approach uses an additional video prediction model to learn the motion information.</p><p>This work focuses on semantic segmentation of videos acquired using UAV. The proposed method demonstrates that a simple modification in the encoder branch of CNN is able to capture the temporal information from the video thus eliminating the need for an extra sequential model for computing correspondence for feature/label propagation.</p><p>A new encoder-decoder based CNN architecture (UVid-Net) proposed in this work has two parallel branches of CNN layers for feature extraction. This new encoding path captures the temporal dynamics of the video by extracting features from multiple frames. These features are further processed by the decoder for the estimation of class labels. The proposed algorithm utilizes a new decoding path that retains the features of encoder layers for decoders. The contribution of the paper can be summarized as,</p><p>• A new encoding path is presented consisting of two parallel branches for extracting temporal and spatial features for video semantic segmentation. • A modified up-sampling path is proposed which uses feature retainer module to capture fine-grain features for accurate classification of class boundary pixels. • An extended version of UAV video semantic segmentation dataset is presented. This dataset is an extension of ManipalUAVid dataset <ref type="bibr" target="#b15">[16]</ref> and contains additional videos captured at new locations. Fine pixel-level annotations are provided for four background classes namely greenery, roads, constructions and water bodies as per the policy adopted in <ref type="bibr" target="#b15">[16]</ref>. The dataset is available for download at https://github.com/uverma/ManipalUAVid • This work shows that UVid-Net trained on a larger urban street scene dataset for semantic segmentation can be fine-tuned for segmentation of UAV aerial videos. This paper is organized as follows: Section II summarizes the recent developments in video semantic segmentation. Section III describes the architecture of the proposed network UVid-Net and Section IV presents the various results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Video semantic segmentation is generally addressed by utilizing traditional energy-based algorithms such as CRF or deep learning-based algorithms such as CNN, RNN, LSTM, etc. One of the challenges in video semantic segmentation is to embed temporal information. Learning the dynamics of the video aids in improving the performance of video semantic segmentation by ensuring temporal consistency. Despite this interest, previous works such as <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b6">[7]</ref> extended the traditional image semantic segmentation approach for video semantic segmentation. These approaches segment all the frames independently of each other which fails to capture the dynamics of the video. Recent advances in video semantic segmentation by utilizing Spatio-temporal information can be categorized into roughly two groups: Deep Learning based methods and CRF based methods.</p><p>There exists several CNN based semantic segmentation approaches ( <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>). Popular CNN based algorithms like <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b4">[5]</ref> used encoder and decoder based architecture for learning the various patterns of the data and localizing the class labels. These algorithms are dependent on a large densely annotated dataset. However, obtaining a finely annotated large dataset is expensive, time-consuming and challenging. Few authors ( <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>) used GAN to learn the dynamics of the video and perform video scene parsing. GAN can be trained to parse future frames as well as label images as proposed by <ref type="bibr" target="#b20">[21]</ref>. Besides, temporal dynamics are also learnt using a sequential model like LSTM <ref type="bibr" target="#b43">[44]</ref>. Moreover, LSTM is also used to select keyframes for video scene parsing <ref type="bibr" target="#b28">[29]</ref>. Few authors explored the attention mechanism with CNN to perform video semantic segmentation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Optical flow is another popular choice for the establishment of temporal correspondence between two consecutive frames <ref type="bibr" target="#b19">[20]</ref>. Few studies such as <ref type="bibr" target="#b52">[53]</ref> and <ref type="bibr" target="#b32">[33]</ref> proposed to predict labels and images jointly to efficiently train deep learning models with less training data. However, the dependence of deep learning algorithms on large annotated datasets limits the development of deep learning algorithms for other contexts such as UAV, etc.</p><p>Many researchers have explored Conditional Random Field (CRF) for incorporating Spatio-temporal information in video semantic segmentation. CRF is a graphical model that captures a large spatial relationship between pixels. Hence it is widely used in literature for context-aware scene parsing <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b8">[9]</ref>. CRF can be extended to incorporate temporal information <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b50">[51]</ref> but it depends on the reliable estimation of temporal links. In general, optical flow is widely used to establish the temporal link and propagate features and labels. However, estimation of accurate optical flow is an overhead for real-time video semantic segmentation. Several authors explored higher-order potential energies for video semantic segmentation by defining potential energy on temporal links <ref type="bibr" target="#b8">[9]</ref>. Class labels in CRF are inferred by using an inference algorithm which is computationally intensive and impractical for video processing.</p><p>The existing state-of-the-art method for video semantic segmentation predicts frames and its labels from the historic data <ref type="bibr" target="#b52">[53]</ref>. However, this approach is dependent on a reliable estimation of temporal correspondence between two consecutive frames. Temporal links are generally established by utilizing dense optical flow-based methods <ref type="bibr" target="#b24">[25]</ref>. Optical flow estimation is an overhead and accuracy of semantic segmentation depends on the accuracy of optical flow estimation. Besides, the error in optical flow estimation can lead to misaligned predicted labels in the future frames, thus affecting the accuracy of the segmentation.</p><p>In this work, a new encoder module is proposed which can capture the temporal dynamics of the video. The proposed work eliminates the need for computing optical flow, thus reducing the overhead.</p><p>In this work, a two-branch encoder is proposed for incorporating temporal smoothness in video semantic segmentation. Multi-branch CNNs are popularly used in video processing due to their ability to capture the relationship between the sequence of frames. Several authors used multi-branch CNNs to perform video classification <ref type="bibr" target="#b42">[43]</ref>, action recognition <ref type="bibr" target="#b33">[34]</ref> and video captioning <ref type="bibr" target="#b48">[49]</ref>. Few authors utilized multi-branch CNN architecture to provide attention mechanism. Authors of <ref type="bibr" target="#b34">[35]</ref> explored multi-branch CNN to extract features from frames. To the best of our knowledge, multi-branch CNNs are not explored to perform video semantic segmentation of UAV videos and capture temporal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>This section describes the encoder (Section III-A) and decoder module (Section III-B) of the proposed approach. The <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 3</ref> shows the proposed architecture with U-Net and ResNet-50 feature extractor respectively. In a typical video, the changes between two consecutive frames are very minimum and hence processing every frame is redundant and time-consuming for video semantic segmentation. However, selecting keyframes at constant interval may result in loss of useful information required for temporal consistency. This would be detrimental for video semantic segmentation methods which depend on temporal features. Hence, in the present study, the keyframes are identified using the shot boundary detection approach presented in <ref type="bibr" target="#b15">[16]</ref> (on an average, a shot consists of 15-20 frames). The use of shot boundary detection method for dynamically identifying the keyframes ensures that the frames containing useful information are not ignored.</p><p>Let us represent the i th frame from the l th shot in a video as f l i . The inputs to the two branches of UVid-Net <ref type="figure" target="#fig_1">(Figures 2, 3</ref>) are the two frames from two consecutive shots: f (l−1) (n/2+1) (upper branch) and f l n/2 (lower branch) , where n represents the total number of frames in a shot. These two frames correspond to the next frame after the middle frame of the previous shot f (l−1) (n/2+1) and the middle frame from the current shot. These two input frames produce the semantic segmentation for the middle frame of the current shot f l n/2 . For the first shot, since there is no prior shot, the first frame (f 1 1 ) of the video and middle frame (f 1 n/2 ) of the first shot is considered as input to the network.</p><p>In the rest of this document, the middle frame of a shot is considered as the keyframe, as per the policy followed for UAV video semantic segmentation <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder</head><p>In this work, the performance of two different architectures (U-Net and ResNet-50 encoders) is studied for feature extraction. U-Net encoder consists of a convolutional layer and maxpool layers for feature extraction. The ResNet-50 feature extractor consists of residual blocks which helps in alleviating the vanishing gradient. These two feature extractors are different and comparing their performance on multi-branch CNN helps us in providing insight into the robustness of the model. In the following text, UVid-Net (U-Net encoder) and UVid-Net (ResNet-50 encoder) refer to the proposed architecture with U-Net encoder and ResNet-50 encoder module respectively. It may be noted that the decoder module is identical for both the architectures.  and ReLU activation function as in the encoder of U-Net <ref type="bibr" target="#b47">[48]</ref>. Finally, the activation is then passed through a 1 × 1 convolution layer which is additionally introduced to reduce the dimensions of the feature maps. Lastly, a maxpooling layer with stride (2, 2) is applied to extract the most prominent features for the subsequent layers. As in the traditional U-Net, the number of feature maps doubles after each max pooling operation, starting with 64 feature maps for the first block.</p><p>The lower branch of the encoder also consists of four blocks. Each block in the lower branch has a 3 × 3 convolution layer with batch normalization and ReLU activation function and the second set of 3 × 3 convolution layer with batch normalization and ReLU activation function. This is followed by a maxpooling layer which extracts most prominent features. Similar to the upper branch, the number of feature maps doubles after each max pooling operation.</p><p>The features extracted by the upper and lower branch of the encoder are fed to two separate bottleneck layers consisting of 3 × 3 convolution with 1024 features maps. Finally, the activation of both these branches is concatenated and fed to the decoder.</p><p>2) ResNet-50 encoder: Besides the UNet based encoder described above, the ResNet-50 architecture ( <ref type="figure" target="#fig_3">Figure 3</ref>) could also be used as a branch in the encoder. ResNet-50 is a CNN architecture proposed for image classification. This architecture proposed the idea of skipping a few layers to learn identity mapping. ResNet-50 has also been widely used as a feature extractor for transfer learning applications <ref type="bibr" target="#b17">[18]</ref>.</p><p>In the present study, the upper branch and lower branch consists of identical ResNet50 architecture to extract features ( <ref type="figure" target="#fig_3">Figure 3</ref>). This architecture consists of an initial convolution operation with kernel size (7x7) followed by batch normalization layer and ReLU activation function. Subsequently, a max pool operation with kernel size (3x3) is applied. Followed by the maxpool operation the architecture consists of four stages. The first stage consists of three residual blocks each containing three layers. Each of these residual blocks consists of 64, 64 and 128 filters. The second stage consists of 4 residual blocks with three layers each. These three layers use 128, 128 and 256 filters. The third stage consists of 6 residual blocks with three layers each. These layers use 256, 256 and 512 filters. The fourth stage consists of 3 residual blocks with three layers each. These layers use 512, 512 and 1024 filters. The first residual blocks of stage 2,3 and 4 utilizes stride operation to reduce the input dimension by 2 in terms of width and height. First and last layers in every residual block consist of (1x1) kernel size and the second layer consists of (3x3) kernel size. All residual block consists of identity connection which solves the vanishing gradient problem.</p><p>The activations of upper and lower ResNet50 branch are concatenated and is further used by the decoder to perform semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoder</head><p>In an encoder-decoder based architecture, the consecutive max pooling operations in encoder reduces feature maps size and results in the loss of spatial resolution. Hence to compensate for this loss of information, skip connections are popularly used from encoding layers to decoding layers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Networks like U-Net use concatenation operation where the feature maps from the last layer of each block in the encoder are stacked with the feature maps of corresponding decoding layers. Here, we argue that elementwise multiplication of the feature maps from the last layer of each block in encoder with the corresponding decoding layers results in better representation of feature maps. This module which performs element-wise multiplication of feature maps is called as feature retainer since it retains the features of the corresponding encoding path. In addition to the improvement in segmentation, the proposed feature retainer module reduces the number of learnable parameters as compared to the concatenation operation. For instance, the total number of parameters for UVid-Net (U-Net encoder) with multiplication is 23, 745, 032 whereas the total number of parameters for UVid-Net (U-Net encoder) with concatenation is 26, 878, 472. The experimental results (Section IV-C) show that the elementwise multiplication of the encoder feature map with the corresponding decoder feature map produces a more smoother segmentation map.</p><p>As discussed earlier, the decoder module is identical for both UVid-Net (U-Net encoder) and UVid-Net (ResNet-50 encoder) <ref type="figure" target="#fig_1">(Figure 2 and 3)</ref>. The decoder path of the proposed architecture contains four blocks. Each of these blocks consists of an upsampling layer with stride 2. This is followed by a convolution layer with filter size <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>. The output of this is passed through a feature retainer module which multiplies the corresponding feature maps of the encoder (lower branch) and the decoder. Note that the last layer of each stage/block of the lower branch encoder is merged with corresponding decoder layers. This is followed by convolution layers and the ReLU activation layer. At the last layer, the SoftMax layer is applied to obtain the probability of pixels belonging to each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND DISCUSSION</head><p>In the present study, an extended version of ManipalUAVid <ref type="bibr" target="#b15">[16]</ref> dataset is used to evaluate the performance of the UVid- Net for UAV video semantic segmentation. The proposed architecture is trained by utilizing categorical cross-entropy loss with Adam optimizer for learning the parameters of the model. In this section, it is shown experimentally that the proposed encoder module is able to incorporate temporal smoothness for video semantic segmentation (Section IV-B). Further, the effectiveness of the feature retainer in the decoder module is demonstrated in Section IV-C. Finally, the performance of the proposed architecture is compared with the state-of-the-art methods for video semantic segmentation (Section IV-D).</p><formula xml:id="formula_0">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset: ManipalUAVid</head><p>This paper presents an extended version of ManipalUAVid <ref type="bibr" target="#b15">[16]</ref> dataset for semantic segmentation of UAV videos. This extended dataset consists of new videos captured at additional locations. The extended dataset consists of 37 videos with annotations provided for 711 keyframes. The pixel-level annotations are provided for four background classes viz. greenery, construction, road and water bodies. The videos are captured at 29 frames per second and at a resolution of 1280 × 720 pixels. The keyframes are identified by following the shot boundary detection approach mentioned in <ref type="bibr" target="#b15">[16]</ref> and on an average, a shot consists of 15-20 frames. More details of this dataset can be found in <ref type="bibr" target="#b15">[16]</ref>. The ManipalUAVid presented in <ref type="bibr" target="#b15">[16]</ref> contains 33 videos and annotations were provided for 667 keyframes. Besides, the performance of semantic segmentation algorithms which analyses each keyframe individually was provided in <ref type="bibr" target="#b15">[16]</ref> on the ManipalUAVid dataset. The earlier version of ManipalUAVid dataset <ref type="bibr" target="#b15">[16]</ref> consists of last two keyframes of each video in the test split which might not be sufficient to observe the temporal smoothness or the error (if any) accumulated over the period of time in the video. Therefore, in this work, ManipalUAVid is extended by incorporating four new videos (total key frames: 44) which are entirely in the test split. Besides, the training-test split distribution is slightly modified so that a greater number of frames (4-5 frames) per video is included in the test split of this updated dataset. This aids in evaluating the video semantic segmentation models for temporal consistency.</p><p>Following the same protocol <ref type="bibr" target="#b15">[16]</ref>, the performance of UVid-Net is evaluated by comparing the keyframes segmented using UVid-Net with the ground truth. In ManipalUAVid, middle frames of a shot (f l (n/2) ) are considered as the keyframes. As discussed earlier, two frames (f (l−1) (n/2+1) and f l (n/2) ) are provided as the input to UVid-Net for semantic segmentation of f l (n/2) (l = 1). The dataset is divided into train, validation and test split which consists of 569, 71 and 71 keyframes respectively. The following metrics are computed to evaluate the performance of the proposed method: mean Intersection over Union (mIoU), Precision, Recall and F1-score. It may be noted that the values of the evaluation metrics obtained in this study are different from that reported in <ref type="bibr" target="#b15">[16]</ref> due to additional videos being added in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision  </p><note type="other">Recall F1-Score mIoU Learnable FLOPs Parameters</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of encoder</head><p>The proposed encoder part consists of two branches which extract features from two consecutive keyframes of a video simultaneously. Two variants of UVid-Net (U-Net encoder and ResNet-50 encoder) encoders are considered in this work. To evaluate the performance of the proposed architecture, we compare it with the traditional U-Net architecture (with a single encoder branch). <ref type="figure" target="#fig_4">Figure 4</ref> shows the comparison of the segmentation results obtained using a single branch U-Net and two branch UVid-Net. Since single branch U-Net is an image semantic segmentation algorithm, it fails to capture the temporal information and hence produces temporally inconsistent labels. In contrast, the proposed architecture is able to capture the temporal dynamics between the two keyframes and produces more accurate results. For instance, the U-Net with single branch encoder incorrectly classifies few pixels belonging to road/greenery class as construction (shown in yellow circles). However, the two branch encoder based proposed method correctly classifies these pixels as road/greenery, thus producing a temporally smoother segmentation result as shown in <ref type="figure" target="#fig_4">Figure 4</ref>   <ref type="table">Table I and Table II</ref> compares the performance of single branch encoder U-Net with the proposed UVid-Net in terms of mIoU, Precision, Recall and F1-score. It is observed that the per class IoU of UVid-Net (U-Net encoder) for all the four classes are higher than the single branch U-Net. Moreover, from <ref type="table">Table I</ref> it is observed the proposed method has higher recall and precision scores than single branch U-Net which indicates that it has produced lower false positives and false negatives. The above results demonstrate the effectiveness of two branch encoder module in acquiring temporal information and thus resulting in a more accurate segmentation as compared to the classical single branch encoder UNet. It may be noted that a single branch U-Net with ResNet50 encoder suffered from high variance (over-fitting), even in the presence of regularization, with a training and validation accuracy of 0.98 and 0.66 respectively. In addition to the qualitative and quantitative evaluation of the encoder, the softmax output of U-Net and UVid-Net (U-Net encoder) is also analysed in <ref type="figure" target="#fig_7">Figure 6</ref>. It can be observed that a high probability score is obtained for the pixels in their actual class in UVid-Net as compared to that of U-Net. The high probability score eliminates uncertainty and produces a more accurate segmentation. For example, a high probability score for greenery class is obtained for pixel belonging to trees using UVid-Net ( <ref type="figure" target="#fig_7">Figure 6</ref>). In addition, U-Net which lacks temporal information has produced higher construction class probability for pixel belonging to greenery at the boundaries (Refer the 6 × 6 representative regions in <ref type="figure" target="#fig_7">Figure 6</ref>). In contrast, the UVid-Net which utilizes features propagated from the previous frame has produced very low construction class probability for greenery pixels at the class boundaries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of decoder</head><p>The decoder of the proposed UVid-Net architecture consists of skip connections from the lower branch of the encoder to the corresponding decoder layers. Element-wise multiplication operation is utilized to combine the activations of the encoder and decoder layers. The experimental evaluation of the proposed feature retainer module with the concatenation approach suggests a marginal increase in mIoU for UVid-Net with U-Net encoder <ref type="table" target="#tab_1">( Tables III and IV )</ref>. It can be observed that the per class IoU is higher for road and water bodies for the multiplication operation as compared to concatenation. Further, the other two classes (greenery and construction) performs competitively in terms of per class mIoU. However, the qualitative evaluation shows that a more accurate segmentation is obtained using the proposed approach compared with the concatenation. <ref type="figure" target="#fig_6">Figure 5</ref> shows few images where finer segmentation boundaries are obtained using the UVid-Net (multiplication) with U-Net encoder as compared to UVid-Net (concatenation). It may be observed in <ref type="figure" target="#fig_6">Figure 5</ref> (First two rows), that the pixels from the road class have been misclassified as construction class using UVid-Net (concatenation), while a precise greenery-road boundary is obtained using UVid-Net (multiplication). The improvement obtained using the proposed feature retainer module is more prominent for UVid-Net (ResNet encoder). A mIoU of 0.72 is obtained with UVid-Net (ResNet encoder) with multiplication operation as compared to 0.53 with concatenation.</p><p>Moreover, the feature retainer module reduces the number of FLOPs along with a number of parameters. It is observed that UVid-Net (multiplication) results in 142,291,716 FLOPs while UVid-Net (concatenation) results in 161,093,892 FLOPs for U-Net encoder (∼ 11% less FLOPs). These results show that an accurate segmentation is obtained using UVid-Net (multiplication) with much less computation overhead. Besides, the element-wise multiplication operation in UVid-Net also reduces the number of learnable parameters <ref type="bibr" target="#b22">(23,</ref><ref type="bibr">745,</ref><ref type="bibr">032)</ref> in the network as compared to the concatenation in the UVid-Net <ref type="bibr" target="#b25">(26,</ref><ref type="bibr">862,</ref><ref type="bibr">856)</ref>. This result is significant since the proposed architecture produces higher mIoU (in the order of 0.79) with a reduced number of parameters. Indeed, the reduced complexity and the number of parameters of UVid-Net as compared to traditional concatenation operation makes it an ideal CNN architecture which can be used for UAV-based IoT applications. </p><formula xml:id="formula_1">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with state-of-art</head><p>The proposed approach is compared with the existing stateof-the-art image semantic segmentation methods viz. U-Net <ref type="bibr" target="#b36">[37]</ref>, FCN8 <ref type="bibr" target="#b26">[27]</ref> and DeepLabV3+ <ref type="bibr" target="#b7">[8]</ref>. However, these methods do not incorporate temporal information and segments each keyframe independently. Therefore, the proposed method is also compared with the state-of-the-art approach <ref type="bibr" target="#b52">[53]</ref> on CityScape dataset that include temporal information. This method uses video prediction model to propagate labels to the immediate neighbouring frames for creating create more image-label pairs <ref type="bibr" target="#b52">[53]</ref>. Besides, the performance of UVidNet is compared with the UAV video semantic segmentation approach proposed in <ref type="bibr" target="#b43">[44]</ref>. This approach uses a Convolution Long Short Term Memory (Conv-LSTM) module to capture the temporal dynamics of the video. It may be noted that the method proposed in <ref type="bibr" target="#b43">[44]</ref> independently segments each frames using FCN8, and then the resulting frames are passed through Conv-LSTM module as the post-processing step. However, in addition to combining FCN8 + Conv-LSTM, we also compare the performance by segmenting individual frames with U-Net/DeepLabV3+ and then post-processing it with Conv-LSTM module, resulting in two additional methods viz UNet + ConvLSTM and DeepLabV3+ + ConvLSTM.</p><p>The proposed architecture is quantitatively compared with the above mentioned existing approaches. <ref type="table">Table I compares</ref> the performance metrics such as precision, recall, F1-score and mean Intersection over Union (mIoU) while <ref type="table">Table II</ref> compares the per class IoU and mIoU of the existing methods with the proposed method. As discussed earlier, the image semantic segmentation approaches (UNet, FCN, DeepLabV3+) segments each keyframe independently and fails to capture temporal cues. It can be observed a mIoU of 0.79 is obtained by the proposed approach as compared to a mIoU of 0.75, 0.64 and 0.65 for UNet, FCN8 and DeepLabV3+ respectively. The proposed approach outperforms the existing image segmentation approach. Besides, it can be observed from <ref type="figure">Figure 8</ref> that UVid-Net produces a more accurate segmentation map with smoother segmentation boundaries as compared with other approaches. The proposed UVid-Net incorporates temporal information by merging the features extracted from two different frames of a video and thereby outperforms the existing image semantic segmentation algorithms.</p><p>In addition to the image segmentation algorithms, the proposed approach is also compared with the video semantic segmentation algorithms viz. Video Propagation /Label Relaxation <ref type="bibr" target="#b52">[53]</ref>, UNet-ConvLSTM, FCN8-ConvLSTM <ref type="bibr" target="#b43">[44]</ref>, and DeepLabV3+ -ConvLSTM. It can be seen <ref type="table">(Table I)</ref> that the UVid-Net (U-Net encoder) achieves a mIoU of 0.79 and F1-score of 0.91 outperforming the other video segmentation approaches. Besides, UVid-Net (ResNet50-encoder) performs competitively and achieves F1-score of 0.89 and a mIoU of 0.72. To study the performance of the proposed method for each class, the per-class IoU is computed as shown in <ref type="table">Table  II</ref>. It can be observed that the UVid-Net (U-Net encoder) achieves significantly higher IoU for road and water bodies class. It may be noted that the dataset is unbalanced with very few pixels corresponding to water bodies and construction class. Therefore, a significant higher IoU (0.86) for water bodies class as compared to other approaches demonstrate the robustness of the proposed method. Besides, the UVid-Net (U-Net encoder) performs competitively for construction class with IoU of 0.60 even in the presence of limited data. <ref type="figure">Figure 8</ref> compares the segmentation results obtained using the proposed approach and the existing methods. It can be observed that the more accurate segmentation is obtained using the proposed method as compared to the existing methods. For instance, the proposed method is able to accurately identify construction, greenery and water bodies especially in fifth and eight rows of <ref type="figure">Figure 8</ref>.</p><p>The UNet-ConvLSTM performs competitively on Mani-palUAVid dataset with a mIoU of 0.76. However, U-Net-ConvLSTM fails to capture the temporal dynamics as shown in <ref type="figure" target="#fig_8">Figure 7</ref>. In comparison, UVid-Net (U-Net encoder) produces a more accurate segmentation, especially for the water body class.</p><p>In addition to the significant improvement in the performance, the UVid-Net (U-Net encoder) has a lower number of parameters as compared to the FCN-8, FCN-8 + ConvLSTM as shown in <ref type="table">Table I</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of transfer learning</head><p>The availability of manually annotated training dataset of sufficient size is a challenge in supervised deep learning based approach. A widely used approach in this scenario is to train the CNN network on a huge dataset and then transfer the weights learned for the task at hand <ref type="bibr" target="#b46">[47]</ref>.</p><p>In this work, the transfer learning approach has been studied on UVid-Net (U-Net encoder) for semantic segmentation of UAV aerial videos. The UAVid-Net (U-Net encoder) is initially trained on Cityscape <ref type="bibr" target="#b11">[12]</ref> dataset to predict eight categorical classes (flat, human, vehicle, construction, object, nature, sky and void) by using Adam optimizer with a learning rate set to 0.0001. This dataset is selected due to its similarity in classes as compared to ManipalUAVid. Moreover, this dataset consists of 3000 training images which are greater than the ManipalUAVid dataset and helps in learning more generalized features. Subsequently, the last layer of the model is re-trained (with other layers frozen) on the ManipalUAVid dataset to predict four classes (greenery, road, construction and water bodies). The performance metrics of UVid-Net (U-Net encoder) by utilizing transfer learning is shown in <ref type="table">Table  I</ref> and II. It is observed that the UVid-Net has performed competitively on greenery, road and construction classes with a per class IoU of 0.89, 0.80 and 0.54 respectively. However, a low per class IoU is observed on water bodies class (0.20). This result was expected since the Cityscape dataset does not contain any images with water, and has no definition for water bodies class. <ref type="figure">Figure 8</ref> shows the segmentation result of transfer learning on UVid-Net (U-Net encoder). It can be observed that the transfer learning approach offers competitive results as compared to existing approaches on greenery, road and construction classes. Despite the limitation on unknown classes, pre-trained UVid-Net (U-Net encoder) could be the preferred choice especially in the case of limited availability of training dataset for UAV aerial videos segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presents a new encoder-decoder based CNN architecture for semantic segmentation of UAV aerial videos. The proposed architecture utilizes a new encoder consisting of two parallel encoding branches with two consecutive keyframes of the video as the input to the network. By integrating the features extracted from the two encoding branches, the network can learn temporal information eliminating the need for an extra sequential module. Besides, it uses a feature retainer module in the decoder path. This module produces smoother segmentation boundaries. The proposed architecture achieved a mIoU of 0.79 on ManipalUAVid dataset which outperforms the other state-of-the-art algorithms. This work also demonstrated that the proposed network UVid-Net trained on a larger semantic segmentation dataset for Urban street scenes (Cityscape) can be utilized for UAV aerial videos segmentation. This transfer learning approach shows that competitive results are obtained on ManipalUAVid dataset by re-training only the last layer of UVid-Net trained on Cityscape dataset. These results hold significance as it reduces the dependency on the availability of manually annotated training dataset which is a time consuming and laborious task. The improved efficiency of UVid-Net by incorporating temporal information, along with reduced dependency on the availability of training data, will provide better segmentation of aerial videos. The lightweight architecture of UVid-Net aids in reducing the computational complexity and number of trainable parameters which makes it an ideal CNN architecture for UAV-based IoT applications. This improved segmentation can be utilized for monitoring of environmental changes, urban planning, disaster management and other aerial surveillance tasks. In future, the developed system will be studied for realtime performance and be deployed in UAV drones for real-time scene analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Importance of temporal consistency in video scene labeling: First row represents the temporal inconsistent labels produced by image segmentation algorithm (U-Net) on videos. Second row depicts an example of temporally consistent labelling obtained using the proposed approach (UVid-Net).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>UVid-Net: Overview of the proposed architecture for UAV video semantic segmentation (U-Net encoder). The architecture consists of encoding path to extract spatio-temporal features and an upsampling path which produces smoother segmentation boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1) U-Net Encoder: The upper branch of the encoder (Figure 2) contains four blocks. Each block consists of two consecutive 3 × 3 convolution layers with batch normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>UVid-Net (ResNet-50 encoder): Overview of the proposed architecture for UAV video semantic segmentation with ResNet-50 encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparing the performance of the proposed two branch encoder module with single branch encoder: Column (a) shows four consecutive keyframes and column (b) shows its corresponding ground-truth images. Column (c) shows the results of single branch encoder viz. U-Net, while column (d) and (e) shows the results of two branch encoder architectures viz. UVid-Net (with U-Net encoder) and UVid-Net (ResNet50 encoder) respectively. Yellow circles highlights the temporal inconsistency produced by single branch U-Net in semantic segmentation. Here, green, gray, red and blue colour represents the greenery, road, construction and water bodies class respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(d) and Figure 4 (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Evaluating decoder: Segmentation results obtained using UVid-Net (U-Net encoder, Concatenation) and UVid-Net (U-Net encoder, Multiplication). Column (a) shows the original image, while column (b) represents the results of concatenation of feature maps (UVid-Net (Concatenation)) and the column (c) represents the results of element wise multiplication (UVid-Net (Multiplication)). Note the improvement in the segmentation by utilizing element wise multiplication (yellow circles). Here, green, gray, red and blue colour represents the greenery, road, construction and water bodies class respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The heat map of probability distributions produced by U-Net and UVid-Net algorithm. Row 1 shows the softmax output of U-Net while row 2 shows the softmax output of UVid-Net. Also, shown are the actual softmax output for a 6 x 6 region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Comparing performance of U-Net + ConvLSTM with UVid-Net (U-Net encoder). Column (a) and (b) shows two consecutive key frames and its corresponding ground-truth. Column (c) and (d) shows the results of U-Net with ConvLSTM and UVid-Net. Here, green, gray, red and blue colour represents the greenery, road, construction and water bodies class respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV PER</head><label>IV</label><figDesc>CLASS IOU AND MIOU OF UVID-NET FOR COMPARING PERFORMANCE OF UVID-NET (CONCATENATION) WITH UVID-NET (MULTLIPLICATION).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Further, UVid-Net (U-Net encoder) has a comparable number of parameters with other models with an exception of DeepLabV3+ which uses MobileNet-V2 backbone. The lower parameters of UVid-Net reduces the dependency on the availability of huge training data. UAV Video Semantic Segmentation Results on ManipalUAVid dataset<ref type="bibr" target="#b15">[16]</ref>: Column (a) and (b) shows the keyframes from UAV video and its corresponding ground truth. Column (c) and (d) shows the results of ConvLSTM with U-Net and FCN8 backbone architectures respectively. Column (e) shows the results of<ref type="bibr" target="#b52">[53]</ref>. Column (f) shows the results of UVid-Net with ResNet50 encoder and column (g) shows the results of UVid-Net with U-Net encoder. Column (h) shows the results of transfer learning.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell><cell>(g)</cell><cell>(h)</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recyclegan: Unsupervised video retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the operational use of uavs for video-derived bathymetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Pedro</forename><surname>Almar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa</forename><surname>Melo De Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Coastal Engineering</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">103527</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-based urban terrain reconstruction from uav-videos for geoinformation applications. International Archives of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Solbrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wernerus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endre</forename><surname>Repasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
	<note>/C22)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guided anisotropic diffusion and iterative learning for weakly supervised change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Caye</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporally consistent multi-class video-object segmentation with the video graph-shifts algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page" from="614" to="621" />
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting nonlocal spatiotemporal structure for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="741" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated detection of koalas using low-level aerial surveillance and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangeline</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bree</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An improved object tracking method in uav videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="634" to="638" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stfcn: spatiotemporal fully convolutional neural network for semantic segmentation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Hajizadeh</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fay</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="493" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic segmentation of uav aerial videos using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohara</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance analysis of semantic segmentation algorithms for finely annotated new uav aerial video dataset (manipaluavid)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Manohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136239" to="136253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Post disaster mapping with semantic change detection in satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Welburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accel: A corrective fusion network for efficient semantic segmentation on video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samvit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8866" to="8875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5580" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The shapetime random field for semantic video labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple moving object detection from uav videos using trajectories of matched regional adjacency graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kalantar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Mansor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Z M</forename><surname>Shafri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5198" to="5213" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When a few clicks make all the difference: Improving weakly-supervised wildlife detection in uav images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentionguided network for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="140680" to="140689" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unmanned aerial vehicles for disaster management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanif</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sally</forename><surname>Mcclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geyong</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geological Disaster Monitoring Based on Sensor Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="83" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Budget-aware deep semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient temporal consistency for streaming video scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="133" to="139" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Surveillance of panicle positions by unmanned aerial vehicle to reveal morphological features of rice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiro</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Tsunematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshio</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Kanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Nonoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ichi</forename><surname>Yonemaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic segmentation of river and land in sar images: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohara</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Aiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient video semantic segmentation with labels propagation and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2873" to="2882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep video code for efficient face video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shishi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="296" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneesh</forename><surname>Rangnekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilay</forename><surname>Mokashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Ientilucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08178</idno>
		<title level="m">Aerorit: A new scene for hyperspectral image analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multiclass object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of variational and cnn-based optical flow techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Remco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="9" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segmentation and size estimation of tomatoes from sequences of paired images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florence</forename><surname>Rossant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurasip Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A framework for moving target detection, recognition and tracking in uav videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Appearance-andrelation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for semantic segmentation of uav videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2459" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A dynamic conditional random field model for joint labeling of object and scene classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Polsar image semantic segmentation based on deep transfer learning-realizing smooth classification with small training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="977" to="981" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised polarimetric sar image segmentation and classification using region growing with edge penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1302" to="1317" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving semantic video segmentation by dynamic scene integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Farhad Ghazvinian Zanjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NCCV</title>
		<meeting>the NCCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">D-linknet: Linknet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="182" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
