<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonathan</forename><surname>Aflalo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
						</author>
						<title level="a" type="main">HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Realistic use of neural networks often requires adhering to multiple constraints on latency, energy and memory among others. A popular approach to find fitting networks is through constrained Neural Architecture Search (NAS), however, previous methods enforce the constraint only softly. Therefore, the resulting networks do not exactly adhere to the resource constraint and their accuracy is harmed. In this work we resolve this by introducing Hard Constrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate formulation of the expected resource requirement and a scalable search method that satisfies the hard constraint throughout the search. Our experiments show that HardCoRe-NAS generates state-of-theart architectures, surpassing other NAS methods, while strictly satisfying the hard resource constraints without any tuning required.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rise in popularity of Convolutional Neural Networks (CNN), the need for neural networks with fast inference speed and high accuracy, has been growing continuously. At first, manually designed architectures, such as VGG <ref type="bibr" target="#b53">(Simonyan &amp; Zisserman, 2015)</ref> or ResNet <ref type="bibr" target="#b22">(He et al., 2015)</ref>, targeted powerful GPUs as those were the common computing platform for deep CNNs. Many variants of those architectures were the golden standard until the need for deployment on edge devices and standard CPUs emerged. These are more limited computing platforms, requiring lighter architectures that for practical scenarios have to comply with hard constraints on the real time latency or power consumption. This has spawned a line of research aimed at finding architectures with both high performance and bounded resource demand. * Equal contribution: author ordering determined by a coin flip. 1 Alibaba Group, Tel Aviv, Israel. Correspondence to: Niv Nayman &lt;niv.nayman@alibaba-inc.com&gt;, Yonathan Aflalo &lt;johnaflalo@gmail.com&gt;. Top-1 accuracy on Imagenet vs Latency measured on Intel Xeon CPU for a batch size of 1. HardCoreNAS can generate a network for any given latency, with accuracy according to the red curve, which is higher than all previous methods. the architecture over the differentiable search space into the discrete space of architectures.</p><p>In this paper, we propose a search algorithm that produces architectures with high accuracy <ref type="figure">(Figure 1</ref>) that strictly satisfy any given hard latency constraint <ref type="figure" target="#fig_2">(Figure 3</ref>). The search algorithm is fast and scalable to a large number of platforms. The proposed algorithm is based on several key ideas, starting from formulating the NAS problem more accurately, accounting for hard constraints over resources, and solving every aspect of it rigorously. For clarity we focus in this paper on latency constraints, however, our approach can be generalized to other types of resources.</p><p>At the heart of our approach lies a suggested differentiable search space that induces a one-shot model <ref type="bibr" target="#b1">(Bender et al., 2018;</ref><ref type="bibr" target="#b9">Chu et al., 2019;</ref><ref type="bibr" target="#b16">Guo et al., 2020;</ref><ref type="bibr" target="#b6">Cai et al., 2019)</ref> that is easy to train via a simple, yet effective, technique for sampling multiple sub-networks from the one-shot model, such that each one is properly pretrained. We further suggest an accurate formula for the expected latency of every architecture residing in that space. Then, we search the space for sub-networks by solving a hard constrained optimization problem while keeping the one-shot model pretrained weights frozen. We show that the constrained optimization can be solved via the block coordinate stochastic Frank-Wolfe (BC-SFW) algorithm <ref type="bibr" target="#b19">(Hazan &amp; Luo, 2016a;</ref><ref type="bibr" target="#b38">Lacoste-Julien et al., 2013a)</ref>. Our algorithm converges faster than SGD, while tightly satisfying the hard latency constraint continuously throughout the search, including during the final discretization step.</p><p>The approach we propose has several advantages. First, the outcome networks provide high accuracy and closely comply to the latency constraint. In addition, our solution is scalable to multiple target devices and latency demands. This scalability is due to the efficient pretraining of the oneshot model as well as the fast search method that involves a relatively small number of parameters, governing only the structure of the architecture. We hope that our formulation of NAS as a constrained optimization problem, equipped with an efficient algorithm that solves it, could give rise to followup work incorporating a variety of resource and structural constraints over the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efficient Neural Networks are designed to meet the rising demand of deep learning models for numerous tasks per hardware constraints. Manually-crafted architectures such as MobileNets <ref type="bibr" target="#b26">(Howard et al., 2017;</ref><ref type="bibr" target="#b52">Sandler et al., 2018b)</ref> and ShuffleNet <ref type="bibr" target="#b68">(Zhang et al., 2018)</ref> were designed for mobile devices, while TResNet <ref type="bibr" target="#b46">(Ridnik et al., 2020)</ref> and ResNesT <ref type="bibr" target="#b66">(Zhang et al., 2020a)</ref> are tailor-made for GPUs. Techniques for improving efficiency include pruning of redundant channels <ref type="bibr" target="#b14">(Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b0">Aflalo et al., 2020)</ref> and layers <ref type="bibr" target="#b18">(Han et al., 2015b)</ref>, model compression <ref type="bibr" target="#b17">(Han et al., 2015a;</ref><ref type="bibr" target="#b23">He et al., 2018)</ref> and weight quantization methods <ref type="bibr" target="#b31">(Hubara et al., 2016;</ref><ref type="bibr" target="#b60">Umuroglu et al., 2017)</ref>. Dynamic neural networks adjust models based on their inputs to accelerate the inference, via gating modules , graph branching <ref type="bibr" target="#b29">(Huang et al., 2017)</ref> or dynamic channel selection <ref type="bibr" target="#b41">(Lin et al., 2017)</ref>. These techniques are applied on predefined architectures, hence cannot utilize or satisfy specific hardware constraints.</p><p>Neural Architecture Search methods automate models' design per provided constraints. Early methods like NAS-Net <ref type="bibr" target="#b69">(Zoph &amp; Le, 2016)</ref> and AmoebaNet <ref type="bibr" target="#b49">(Real et al., 2019)</ref> focused solely on accuracy, producing SotA classification models  at the cost of GPU-years per search, with relatively large inference times. DARTS  introduced a differential space for efficient search and reduced the training duration to days, followed by XNAS <ref type="bibr" target="#b45">(Nayman et al., 2019)</ref> and ASAP  that applied pruning-during-search techniques to further reduce it to hours. Hardware-aware methods such as ProxylessNAS <ref type="bibr" target="#b5">(Cai et al., 2018)</ref>, Mnasnet , FBNet  and TFNAS <ref type="bibr" target="#b28">(Hu et al., 2020)</ref> produce architectures that satisfy the required constraints by applying simple heuristics such as soft penalties on the loss function. OFA <ref type="bibr" target="#b6">(Cai et al., 2019)</ref> proposed a scalable approach across multiple devices by training an one-shot model <ref type="bibr" target="#b4">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref> for 1200 GPU hours. This provides a strong pretrained super-network being highly predictive for the accuracy of extracted subnetworks <ref type="bibr" target="#b16">(Guo et al., 2020;</ref><ref type="bibr" target="#b9">Chu et al., 2019;</ref><ref type="bibr" target="#b64">Yu et al., 2020)</ref>. This work relies on such one-shot model acquired within only 400 GPU hours in a much simpler manner and satisfies hard constraints tightly with less heuristics.</p><p>Frank-Wolfe (FW) algorithm <ref type="bibr" target="#b15">(Frank et al., 1956</ref>) is commonly used by machine learning applications  thanks to its projection-free property <ref type="bibr" target="#b10">(Combettes et al., 2020;</ref><ref type="bibr" target="#b21">Hazan &amp; Minasyan, 2020)</ref> and ability to handle structured constraints <ref type="bibr" target="#b33">(Jaggi, 2013)</ref>. Modern adaptations aimed at deep neural networks (DNNs) optimization include more efficient variants <ref type="bibr" target="#b67">(Zhang et al., 2020b;</ref><ref type="bibr" target="#b10">Combettes et al., 2020)</ref>, task-specific variants <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b59">Tsiligkaridis &amp; Roberts, 2020)</ref>, as well as improved convergence guarantees <ref type="bibr" target="#b37">(Lacoste-Julien &amp; Jaggi, 2015;</ref><ref type="bibr" target="#b12">d'Aspremont &amp; Pilanci, 2020)</ref>. Two prominent variants are the stochastic FW <ref type="bibr" target="#b20">(Hazan &amp; Luo, 2016b)</ref> and <ref type="bibr">Blockcoordinate FW (Lacoste-Julien et al., 2013b)</ref>. While FW excels as an optimizer for DNNs <ref type="bibr" target="#b2">(Berrada et al., 2018;</ref><ref type="bibr" target="#b48">Pokutta et al., 2020)</ref>, this work is the first to utilize it for NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In the scope of this paper, we focus on latency-constrained NAS, searching for an architecture with the highest validation accuracy under a predefined latency constraint, denoted by T . Our architecture search space S is parametrized by ζ ∈ S, governing the architecture structure in a fully differentiable manner, and w, the convolution weights. A latency-constrained NAS can be expressed as the following constrained bilevel optimization problem:</p><formula xml:id="formula_0">min ζ∈S E x, y ∼ D val ζ ∼ P ζ (S) [L CE (x, y | w * ,ζ)] s.t. LAT(ζ) ≤ T (1) w * = argmin w E x, y ∼ D train ζ ∼ P ζ (S) [L CE (x, y | w,ζ)]</formula><p>where D train and D val are the train and validation sets' distributions respectively, P ζ (S) is some probability measure over the search space parameterized by ζ, L CE is the cross entropy loss as a differentiable proxy for the negative accuracy and LAT (ζ) is the estimated latency of the model.</p><p>To solve problem (1), we construct a fully differentiable search space parameterized by ζ = (α, β) (Section 3.1), that enables the formulation of a differentiable closed form formula expressing the estimated latency LAT (α, β) (Section 3.2) and efficient acquisition of w * (Section 3.3). Finally, we introduce rigorous constrained optimization techniques for solving the outer level problem (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>Aiming at the most accurate latency, a flexible search space is composed of a micro search space that controls the internal structures of each block b ∈ {1, .., d = 4}, together with a macro search space that specifies the way those are connected to one another in every stage s ∈ {1, .., S = 5}.  An input feature map x s b to block b of stage s is processed as follows:</p><formula xml:id="formula_1">x s b+1 = c∈C α s b,c · O s b,c (x s b )</formula><p>where O s b,c (·) is the operation performed by the elastic MBInvRes block configured according to c = (er, k, se).</p><p>Having several configurations O s b,c (·) share the same values of α s b,er or α s b,k or α s b,se induces weight sharing between the common operations of the associated architectures. This weight sharing is beneficial for solving the inner problem (1) effectively, as will be discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">MACRO-SEARCH</head><p>Inspired by <ref type="bibr" target="#b28">(Hu et al., 2020;</ref><ref type="bibr" target="#b6">Cai et al., 2019)</ref>, the output of each block of every stage is also directed to the end of the stage as illustrated in the middle of <ref type="figure" target="#fig_1">Figure 2</ref>. Thus, the depth of each stage s is controlled by the parameters</p><formula xml:id="formula_2">β ∈ B = S s=1 d b=1 β s b , such that: β s b ∈ {0, 1} d ; Σ d b=1 β s b = 1 The depth is b s ∈ {b | β s b = 1, b ∈ {1, .., d}}, since: x s+1 1 = Σ d b=1 β s b · x s b+1 3.1.3. THE COMPOSED SEARCH SPACE</formula><p>The overall search space is composed of both the micro and macro search spaces parameterized by α ∈ A and β ∈ B, respectively, such that:</p><formula xml:id="formula_3">S =        (α, β) α ∈ A, β ∈ B α s b,c ∈ {0, 1} |C| ; Σ c∈C α s b,c = 1 β s b ∈ {0, 1} d ; Σ d b=1 β s b = 1 ∀s ∈ {1, .., S}, b ∈ {1, .., d}, c ∈ C        A continuous probability distribution is induced over the space, by relaxing α s b,c ∈ {0, 1} |C| → α s b,c ∈ R |C| + and β s b ∈ {0, 1} d → β s b ∈ R d +</formula><p>to be continuous rather than discrete. A sample sub-network is drawnζ = (α,β) ∼ P ζ (S) using the Gumbel-Softmax Trick <ref type="bibr" target="#b34">(Jang et al., 2016)</ref> such thatζ ∈ S, as specified in <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_10">(5)</ref>. In summary, one can view the parametrization (α, β) as a composition of probabilities in P ζ (S) or as degenerated one-hot vectors in S.</p><p>Effectively we include at least a couple of blocks in each stage by setting β s 1 ≡ 0, hence, the overall size of the search space is:</p><formula xml:id="formula_4">|S| = ( d b=2 |C| b ) S = ( d b=2 |A er | b · |A k | b · |A se | b ) S = ((3 × 2 × 2) 2 + (3 × 2 × 2) 3 + (3 × 2 × 2) 4 ) 5 ≈ 10 27</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulating the Latency Constraint</head><p>Aiming at tightly satisfying latency constraints, we propose an accurate formula for the expected latency of a subnetwork. The expected latency of a block b can be computed by summing over the latency t s b,c of every possible configu-</p><formula xml:id="formula_5">ration c ∈ A:¯ s b = Σ c∈C α s b,c · t s b,c</formula><p>Thus the expected latency of a stage of depth b is</p><formula xml:id="formula_6">s b = b b=1¯ s b<label>(2)</label></formula><p>Taking the expectation over all possible depths yields</p><formula xml:id="formula_7">s = d b =1 β s b · s b</formula><p>and summing over all the stages results in the following formula for the overall latency:</p><formula xml:id="formula_8">LAT (α, β) = S s=1 d b =1 b b=1 c∈C α s b,c · t s b,c · β s b<label>(3)</label></formula><p>The the summation originated in (2) differentiates our latency formulation (3) from that of <ref type="bibr" target="#b28">(Hu et al., 2020)</ref>. Remark: By replacing the latency measurements t s b,c with other resources, e.g., memory, FLOPS, MACS, etc., one can use formula (3) to add multiple hard constraints to the outer problem of (1).  <ref type="formula" target="#formula_8">(3)</ref> is very close to the latency measured in practice, on both CPU and GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Solution to the Inner Problem w *</head><p>Previous work proposed approximated solutions to the following unconstrained problem:</p><formula xml:id="formula_9">min ζ∈S E L CE x, y | w * ζ , ζ s.t. w * ζ = argmin w E ζ∼P ζ (S) [L CE (x, y | w, ζ)]</formula><p>typically by alternating or simultaneous updates of w and ζ <ref type="bibr" target="#b63">Xie et al., 2018;</ref><ref type="bibr" target="#b5">Cai et al., 2018;</ref><ref type="bibr" target="#b62">Wu et al., 2019;</ref><ref type="bibr" target="#b28">Hu et al., 2020)</ref>. This approach has several limitations. First, obtaining a designated w * ζ with respect to every update of ζ involves a heavy training of a neural network until convergence. Instead a very rough approximation is obtained by just a few update steps for w. In turn, this approximation creates a bias towards strengthening networks with few parameters since those learn faster, hence, get sampled even more frequently, further increasing the chance to learn in a positive feedback manner. Eventually, often overly simple architectures are generated, e.g., consisting of many skip-connections <ref type="bibr" target="#b40">Liang et al., 2019)</ref>. Several remedies have been proposed, e.g., temperature annealing, adding uniform sampling, modified gradients and updating only w for a while before the joint optimization begins <ref type="bibr" target="#b62">Wu et al., 2019;</ref><ref type="bibr" target="#b28">Hu et al., 2020;</ref><ref type="bibr" target="#b45">Nayman et al., 2019)</ref>. While those mitigate the bias problem, they do not solve it.</p><p>We avoid such approximation whatsoever. Instead we obtain w * of the inner problem of (1) only once, with respect to a uniformly distributed architecture, samplingζ from Pζ(S) = U (S). This is done by sampling multiple distinctive paths (subnetworks of the one-shot model) for every image in the batch  in an efficient way (just a few lines of code provided in the supplementary materials), using the Gumbel-Softmax Trick. For every feature map x that goes through block b of stage s,</p><formula xml:id="formula_10">distinctive uniform random variables U s b,c,x , U s b,x ∼ U (0, 1) are sampled, governing the path undertaken by this feature map:α s b,c,x = e log(α s b,c )−log(log(U s b,c,x )) c∈C e log(α s b,c )−log(log(U s b,c,x )) (4) β s b,x = e log(β s b )−log(log(U s b,x )) c∈C e log(α s b,c )−log(log(U s b,x ))<label>(5)</label></formula><p>Based on the observation that the accuracy of a sub-network with w * should be predictive for its accuracy when optimized as a stand-alone model from scratch, we aim at an accurate prediction. Our simple approach implies that, with high probability, the number of paths sampled at each update step is as high as the number of images in the batch. This is two orders of magnitude larger than previous methods that sample a single path per update step <ref type="bibr" target="#b16">(Guo et al., 2020;</ref><ref type="bibr" target="#b6">Cai et al., 2019)</ref>, while avoiding the need to keep track of all the sampled paths . Using multiple paths reduces the variance of the gradients with respect to the paths sampled by an order of magnitude 1 . Furthermore, leveraging the weight sharing implied by the structure of the elastic MBInvRes block (Section 3.1.1), the number of gradients observed by each operation is increased by a factor of at least |C| max(|Aer|,|A k |,|Ase|) = 3×2×2 max(3,2,2) ≈ 4. This further reduces the variance by half. <ref type="figure" target="#fig_5">Figure 4</ref> shows that we obtain a one-shot model w * with high correlation between the ranking of sub-networks directly extracted from it and the corresponding stand-alone counterpart trained from scratch. See more details in Section 4.2.2. This implies that w * captures well the quality of sub-structures in the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Solving the Outer Problem</head><p>Having defined the formula for the latency LAT(ζ) and obtained a solution for w * , we can now continue to solve the outer problem (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">SEARCHING UNDER LATENCY CONSTRAINTS</head><p>Most differentiable resource-aware NAS methods account for the resources through shaping the loss function with soft penalties <ref type="bibr" target="#b28">Hu et al., 2020)</ref>. This approach solely does not meet the constraints tightly. Experiments illustrating this are described in Section 4.2.3.</p><p>Our approach directly solves the constrained outer problem (1), hence, it enables the strict satisfaction of resource constraints by further restricting the search space, i.e.,</p><formula xml:id="formula_11">S LAT = {ζ | ζ ∈ P ζ (S), LAT(ζ) ≤ T }.</formula><p>As commonly done for gradient based approaches, e.g., , we relax the discrete search space S to be continuous by searching for ζ ∈ S LAT . As long as S LAT is convex, it could be leveraged for applying the stochastic Frank-Wolfe (SFW) algorithm <ref type="bibr" target="#b19">(Hazan &amp; Luo, 2016a)</ref> to directly solve the constrained outer problem:</p><formula xml:id="formula_12">min ζ∈S LAT E x,y∼D val [L CE (x, y | w * , ζ)]<label>(6)</label></formula><p>following the update step:</p><formula xml:id="formula_13">ζ t+1 = (1 − γ t ) · ζ t + γ t · ξ t (7) ξ t = argmin ζ∈S LAT ζ T · ∇ ζt L CE (x t , y t | w * , ζ t ) (8)</formula><p>where (x t , y t ) and γ t are the sampled data and the learning rate at step t, respectively. For S LAT of linear constraints, the linear program (8) can be solved efficiently, using the Simplex algorithm <ref type="bibr" target="#b44">(Nash, 2000)</ref>.</p><formula xml:id="formula_14">A convex S LAT together with γ t ∈ [0, 1] satisfy ζ t ∈ S LAT anytime, as long as ζ 0 ∈ S LAT .</formula><p>We provide a method for satisfying the latter in the supplementary materials.</p><p>The benefits of such optimization are demonstrated in Figure 5 through a toy problem, described in Section 4.2.3. While GD is sensitive to the trade-off involved with a soft penalty, FW converges faster to the optimum with zero penalty.</p><p>All this requires S LAT to be convex. While P ζ (S) is obviously convex, formed by linear constraints, the latency constraint LAT(ζ) ≤ T is not necessarily so. The latency formula (3) can be expressed as a quadratic constraint by constructing</p><formula xml:id="formula_15">a matrix Θ ∈ R S·d·|A|×S·d + from t s b,c , such that, LAT (ζ) = LAT (α, β) = α T Θβ ; ζ ∈ P ζ (S) (9)</formula><p>Since Θ is constructed from measured latency, it is not guaranteed to be positive semi-definite, hence, the induced quadratic constraint could make S LAT non-convex.</p><p>To overcome this, we introduce the Block Coordinate Stochastic Frank-Wolfe (BCSFW) Algorithm 1, that combines Stochastic Frank-Wolfe with Block Coordinate <ref type="bibr">Frank-Wolfe (Lacoste-Julien et al., 2013a)</ref>. This is done by forming separated convex feasible sets at each step, induced by linear constraints only:</p><formula xml:id="formula_16">α t ∈ S α t = {α | α ∈ A, β T t Θ T · α ≤ T } (10) β t ∈ S β t = {β | β ∈ B, α T t Θ · β ≤ T }<label>(11)</label></formula><p>This implies that ζ t = (α t , β t ) ∈ S LAT for all t. Moving inside the feasible domain at anytime avoids irrelevant infeasible structures from being promoted and hiding feasible structures. Most methods use the argmax operator:</p><formula xml:id="formula_17">Algorithm 1 Block Coordinate SFW (BCSFW) input (α 0 , β 0 ) ∈ S LAT for t = 0, . . . , K do Pick δ := α or δ := β at random Sample an i.i.d validation batch (x t , y t ) ∼ D val ξ t = argmin ξ∈S δ t ξ T · ∇ δt L CE (x t , y t | w * , δ t ) Update δ t+1 = (1 − γ t ) · δ t + γ t · ξ t with γ t = 4</formula><formula xml:id="formula_18">α s b,c := 1{c = argmax c∈C α s b,c } (12) β s b := 1{b = argmax b=2,..,d β s b } for all s ∈ {1, .., S}, b ∈ {1, .., d}, c ∈ C, where (ᾱ,β)</formula><p>is the solution to the outer problem of (1).</p><p>For resource-aware NAS methods, applying such projection results in possible violation of the resource constraints, due to the shift from the converged solution in the continuous space. Experiments showing that latency constraints are violated due to (12) are provided in Section 4.2.4.</p><p>While several methods mitigate this violation by promoting sparse probabilities during the search, e.g., <ref type="bibr" target="#b45">Nayman et al., 2019)</ref>, our approach completely eliminates it by introducing an alternative projection step, described next.</p><p>Viewing the solution of the outer problem (α * , β * ) as the credit assigned to each configuration, we introduce a projection step that maximizes the overall credit while strictly satisfying the latency constraints. It is based on solving the following two linear programs:</p><formula xml:id="formula_19">max α∈S α * α T · α * ; max β∈S β * β T · β *<label>(13)</label></formula><p>Note, that when there is no latency constraint, e.g., T → ∞, (13) coincides with (12).</p><p>We next provide a theorem that guarantees that the projection (13) yields a sparse solution, representing a valid sub-network of the one-shot model. Specifically, a single probability vector from those composing α and β contains up to two non-zero entries each, as all the rest are one-hot vectors.</p><p>Theorem 3.1. The solution (α, β) of (13) admits:</p><formula xml:id="formula_20">c∈C |α s b,c | 0 = 1 ∀(s, b) ∈ {1, .., S} ⊗ {1, .., d} \ {(s α , b α )} d b=1 |β s b | 0 = 1 ∀s ∈ {1, .., S} \ {s β } where | · | 0 = 1{· &gt; 0} and (s α , b α ), s β are single block</formula><p>and stage respectively, satisfying:</p><formula xml:id="formula_21">c∈C |α sα bα,c | 0 ≤ 2 ; d b=1 |β s β b | 0 ≤ 2<label>(14)</label></formula><p>Refer to the supplementary materials for the proof.</p><p>Remark: A negligible deviation is associated with taking the argmax (12) over the only two couples referred to in <ref type="formula" target="#formula_21">(14)</ref>. Experiments supporting this are described in Section 4.2.4. Furthermore, this can be entirely eliminated by solving an equivalent Multiple-Choice Knapsack Problem (MCKP) as described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search for State-of-the-Art Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">DATASET AND SETTING</head><p>For all of our experiments, we train our networks using SGD with a learning rate of 0.1 with cosine annealing, Nesterov momentum of 0.9, weight decay of 10 −4 , applying label smoothing <ref type="bibr" target="#b56">(Szegedy et al., 2016)</ref> of 0.1, mixup <ref type="bibr" target="#b65">(Zhang et al., 2017)</ref> of 0.2, Autoaugment <ref type="bibr" target="#b11">(Cubuk et al., 2018)</ref>, mixed precision and EMA-smoothing.</p><p>We obtain the solution of the inner problem w * as specified in sections 3.3 and 4.2.2 over 80% of a random 80-20 split of the ImageNet train set. We utilize the remaining 20% as a validation set and search for architectures with latencies of 40, 45, 50, 55, 60 and 25, 30, 40 milliseconds running with a batch size of 1 and 64 on an Intel Xeon CPU and and NVIDIA P100 GPU, respectively. The search is performed according to section 3.4 for only 2 epochs of the validation set, lasting for 8 GPU hours 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">COMPARISONS WITH OTHER METHODS</head><p>We compare our generated architectures to other state-of-theart NAS methods in <ref type="table">Table 1</ref> and <ref type="figure">Figure 1</ref>. For each model in the table, we use the official PyTorch implementation <ref type="bibr" target="#b47">(Paszke et al., 2019)</ref> and measure its latency running on a single thread with the exact same conditions as for our networks. We excluded further optimizations, such as Intel MKL-DNN (Intel, R), therefore, the latency we report may differ from the one originally reported. For the purpose of comparing the generated architectures alone, excluding the contribution of evolved pretraining techniques, all the models (but OFA 2 ) are trained from a random initialization with the same settings, specified in section 4.1.1. It can be seen that networks generated by our method meet the latency target closely, while at the same time surpassing all the others methods on the top-1 accuracy over ImageNet with a reduced scalable search time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Empirical Analysis of Key Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">VALIDATION OF THE LATENCY FORMULA</head><p>One of our goals is to provide a practical method to accurately meet the given resource requirements. Hence, we validate empirically the accuracy of the latency formula <ref type="formula" target="#formula_8">(3)</ref>, by comparing its estimation with the measured latency. Experiments were performed on two platforms: Intel Xeon CPU and NVIDIA P100 GPU, and applied to multiple networks. Results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which confirms a linear relation between estimated and measured latency, with a ratio of 1.003 and a coefficient of determination of R 2 = 0.99. This supports the accuracy of the proposed formula. The ultimate quality measure for a generated architecture is arguably its accuracy over a test set when trained as a stand-alone model from randomly initialized weights. To evaluate the quality of our one-shot model w * we compare the accuracy of networks extracted from it with the accuracy of the corresponding architectures when trained from scratch. Naturally, when training from scratch the accuracy could increase. However, a desired behavior is that the ranking of the accuracy of the networks will remain the same with and without training from scratch. The correlation can be calculated via the Kendall-Tau <ref type="bibr" target="#b43">(Maurice, 1938</ref>) and Spearman's <ref type="bibr" target="#b54">(Spearman, 1961)</ref> rank correlation coefficients, denoted as τ and ρ, respectively.</p><p>To this end, we first train for 250 epochs a one-shot model w * using the heaviest possible configuration, i.e., a depth of 4 for all stages, with er = 6, k = 5 × 5, se = on for all the blocks. Next, to obtain w * , we apply the multi-path training of Section 3.3 for additional 100 epochs of fine-tuningw * over 80% of a 80-20 random split of the ImageNet train set <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>. The training settings are specified in Section 4.1.1. The first 250 epochs took 280 GPU hours 3 and the additional 100 fine-tuning epochs took 120 GPU hours 4 , summing to a total of 400 hours on NVIDIA V100 GPU to obtain w * . To further demonstrate the effectiveness of our multi-path technique, we repeat this procedure also without it, sampling a single path for each batch.</p><p>For the evaluation of the ranking correlations, we extract 18 sub-networks of common configurations for all stages of depths in {2, 3, 4} and blocks with an expansion ratio in A er = {3, 4, 6}, a kernel size in A k = {3 × 3, 5 × 5} and Squeeze and Excitation being applied. We train each of those as stand-alone from random initialized weights for 200 epochs over the full ImageNet train set, and extract their final top-1 accuracy over the validation set of ImageNet. <ref type="figure" target="#fig_5">Figure 4</ref> shows for each extracted sub-network its accuracy without and with stand-alone training. It further shows results for both multi-path and single-path sampling. It can be seen that the multi-path technique improves τ and ρ by 0.35 and 0.17 respectively, leading to a highly correlated rankings of τ = 0.95 and ρ = 0.99.</p><p>2 for 1w * Bootstrap: A nice benefit of the training scheme described in this section is that it further shortens the generation of trained models. We explain this next.</p><p>The common approach of most NAS methods is to re-train the extracted sub-networks from scratch. Instead, we leverage having two sets of weights:w * and w * . Instead of retraining the generated sub-networks from a random initialization we opt for fine-tuning w * guided by knowledge distillation <ref type="bibr" target="#b24">(Hinton et al., 2015)</ref> from the heaviest model w * . Empirically, we observe that this surpasses the accuracy obtained when training from scratch at a fraction of the time. More specifically, we are able to generate a trained model within a small marginal cost of 15 GPU hours. The total cost for generating N trained models is 400 + 15N , much lower than the 1200 + 25N reported by OFA <ref type="bibr" target="#b6">(Cai et al., 2019)</ref>. See <ref type="table">Table 1</ref>. This makes our method scalable for many devices and latency requirements. Note, that allowing for longer training further improves the accuracy significantly (see the supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">OUTER PROBLEM: HARD VS SOFT</head><p>Next, we evaluate our method's ability to satisfy accurately a given latency constraint. We compare our hard-constrained formulation (1) with the common approach of adding soft penalties to the loss function <ref type="bibr" target="#b28">(Hu et al., 2020;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref>. The experiments were performed over a simple and 3 Running with a batch size of 200 on 8×NVIDIA V100 4 Running with a batch size of 16 on 8×NVIDIA V100 intuitive toy problem:</p><formula xml:id="formula_22">min x x 2 s.t. Σ d i=1 x i = 1 ; x ∈ R d<label>(15)</label></formula><p>Our approach solves this iteratively, using the Frank-Wolfe (FW) <ref type="bibr" target="#b15">(Frank et al., 1956)</ref> update rule:</p><formula xml:id="formula_23">x t+1 = (1 − γ t ) · x t + γ t · ξ t (16) ξ t = argmin x x 2 s.t. Σ d i=1 x i = 1<label>(17)</label></formula><p>starting from an arbitrary random feasible point, e.g. sample a random vector and normalize it. The soft-constraint approach minimizes x 2 + λ ( i x i − 1) 2 using gradient descent (GD), where λ is a coefficient that controls the trade-off between the objective function and the soft penalty representing the constraint. <ref type="figure" target="#fig_6">Figure 5</ref>, shows the objective value for d = 10 and the corresponding soft penalty value along the optimization for both FW and GD with several values of λ. It can be seend that GD is very sensitive to the trade-off tuned by λ, often violating the constraint or converging to a sub-optimal objective value. On the contrary, FW converges faster to the optimal solution (x * = 1/d = 0.1), while strictly satisfying the constraint throughout the optimization.  <ref type="table" target="#tab_2">Table 2</ref>. The effect of discretization method on the actual latency shows that argmax method is harmful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">EVALUATING THE DISCRETIZING PROJECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The problem of resource-aware differentiable NAS is formulated as a bilevel optimization problem with hard constraints. Each level of the problem is addressed rigorously for efficiently generating well performing architectures that strictly satisfy the hard resource constraints. HardCoRe-NAS turns to be a fast search method, scalable to many devices and requirements, while the resulted architectures perform better than architectures generated by other state-of-the-art NAS methods. We hope that the proposed methodologies will give rise to more research and applications utilizing constrained search for inducing unique structures over a variety of search spaces and resource specifications. <ref type="table">Table 3</ref>. Macro architecture of the one-shot model. "MBInvRes" is the basic block in <ref type="bibr" target="#b51">(Sandler et al., 2018a)</ref>. "ElasticMBInvRes" denotes our elastic blocks (Section 3.1.1) to be searched for. "Cout" stands for the output channels. Act denotes the activation function used in a stage. "b" is the number of blocks in a stage, where [b,b] is a discrete interval. If necessary, the down-sampling occurs at the first block of a stage.</p><p>The configurations of the ElasticMBInvRes blocks c ∈ C are sorted according to their expected latency as specified in <ref type="table">Table 4</ref>. c er k se 1 2 3 × 3 off 2 2 5 × 5 on 3 2 3 × 3 off 4 2 5 × 5 on 5 3 3 × 3 off 6</p><p>3 5 × 5 on 7 3 3 × 3 off 8</p><p>3 5 × 5 on 9 6 3 × 3 off 10 6 5 × 5 on 11 6 3 × 3 off 12 6 5 × 5 on <ref type="table">Table 4</ref>. Specifications for each indexed configuration c ∈ C. "er" stands for the expansion ratio of the point-wise convolutions, "k" stands for the kernel size of the depth-wise separable convolutions and "se" stands for Squeeze-and-Excitation (SE) with on and of f denoting with and without SE respectively. The configurations are indexed according to their expected latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Searching for the Expansion Ratio</head><p>Searching for expansion ration (er), as specified in Section 3.1.1, involves the summation of feature maps of different number of channels:</p><formula xml:id="formula_24">y s b,er = er∈Aer α s b,er · P W C s b,er (x s b )<label>(18)</label></formula><p>where P W C s b,er is the point-wise convolution of block b in stage s with expansion ratio er.</p><p>The summation in (18) is made possible by calculating P W C s b,ēr only once, whereēr = max A er , and masking its output several times as following:</p><formula xml:id="formula_25">y s b,er = er∈Aer α s b,er · P W C s b,ēr (x s b ) 1 C≤er×Cin (19)</formula><p>where is a point-wise multiplication, C in is the number of channels of x s b and the mask tensors 1 C≤er×Cin are of the same dimensions as of P W C s b,ēr (x s b ) with ones for all channels C satisfying C ≤ er × C in and zeros otherwise.</p><p>Thus, all of the tensors involved in the summation have the same number of channels, i.e.ēr × C in , while the weights of the point-wise convolutions are shared. Thus we gain the benefits of weight sharing, as specified in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multipath Sampling Code</head><p>We provide a simple PyTorch <ref type="bibr" target="#b47">(Paszke et al., 2019)</ref> implementation for sampling multiple distinctive paths (subnetworks of the one-shot model) for every image in the batch, as specified in Section 3.3. The code is presented in figure 6. 1 import torch 2 from torch.nn.functional import gumbel_softmax </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. A Brief Derivation of the FW Step</head><p>Suppose D is a compact convex set in a vector space and f : D → R is a convex, differentiable real-valued function. The Frank-Wolfe algorithm <ref type="bibr" target="#b15">(Frank et al., 1956)</ref> iteratively solves the optimization problem:</p><formula xml:id="formula_26">min x∈D f (x).<label>(20)</label></formula><p>To this end, at iteration k + 1 it aims at solving:</p><formula xml:id="formula_27">min x k +∆∈D f (x k + ∆).<label>(21)</label></formula><p>Using a first order taylor expansion of f , (21) is approximated in the neighborhood of f (x k ), and thus the problem can be written as:</p><formula xml:id="formula_28">min x k +∆∈D ∆ T ∇f (x k )<label>(22)</label></formula><p>Replacing ∆ with γ(s − x k ) for γ ∈ [0, 1], problem (22) is equivalent to:</p><formula xml:id="formula_29">min x k +γ(s−x k )∈D γ(s − x k ) T ∇f (x k ).<label>(23)</label></formula><p>Assuming that</p><formula xml:id="formula_30">x k ∈ D, since D is convex, x k +γ(s−x k ) ∈ D holds for all γ ∈ [0, 1] iff s ∈ D.</formula><p>Hence, (23) can be written as following:</p><formula xml:id="formula_31">min s∈D s T ∇f (x k ).<label>(24)</label></formula><p>Obtaining the minimizer s k of (24) at iteration k + 1, the FW update step is:</p><formula xml:id="formula_32">x k+1 ← x k + γ(s k − x k ).<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Obtaining a Feasible Initial Point</head><p>Algorithm 1 requires a feasible initial point (u 0 , β 0 ) ∈ S lat . assuming such a point exists, i.e. as t is large enough, a trivial initial point (u 0 , β 0 ) := (ũ,β) is associated with the lightest structure in the search space S ⊂ P ζ (S), i.e. setting:α</p><formula xml:id="formula_33">s b,c = 1{c = 1} ;β s b = 1{b = 2}<label>(26)</label></formula><p>for all s ∈ {1, .., S}, b ∈ {1, .., d}, c ∈ C, where 1{·} is the indicator function. However, starting from this point condemns the gradients with respect to all other structures to be always zero due to the way paths are sampled from the space using the Gumbel-Softmax trick, section 3.1.3.</p><p>Hence for a balanced propagation of gradients, the closest to uniformly distributed structure in S LAT is encouraged. For this purpose we solve the following quadratic programs (QP) alternately,</p><formula xml:id="formula_34">min u∈Sũ s s=1 d−1 b=1 |C|−1 c=1 (α s b,c+1 − α s b,c ) 2 (27) min β∈Sβ s s=1 d−1 b=1 (β s b+1 − β s b ) 2</formula><p>using a QP solver at each step.</p><p>Sorting the indices of configurations according to their expected latency (see <ref type="table">Table 4</ref>), the objectives in <ref type="formula" target="#formula_6">(27)</ref> promote probabilities of consecutive indices to be close to each other, forming chains of non-zero probability with a balanced distribution up to an infeasible configuration, there a chain of zero probability if formed. Illustrations of the formation of such chains are shown in <ref type="figure" target="#fig_9">Figure 7</ref> for several latency constraints. Preferring as many blocks participating as possible over different configurations, the alternating optimization in (27) starts with β. This yields balanced β probabilities as long as the constraint allows it.</p><p>The benefits from starting with such initial point are quantified by averaging the relative improvements in top-1 accuracy for several latency constraints T = {35, 40, 45, 50, 55} milliseconds as following:</p><formula xml:id="formula_35">100 |T | T ∈T Acc T balance init − Acc T lighest init Acc T lighest init<label>(28)</label></formula><p>where Acc T balance init and Acc T lighest init are the top-1 accuracy measured for fine-tuned models generated by searching the space initialized with <ref type="formula" target="#formula_6">(27)</ref> and <ref type="formula" target="#formula_6">(26)</ref> respectively, under latency constraint T . The calculation in (28) yields 8.3% of relative improvement in favour of (27) on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Proof of Theorem 3.1</head><p>In order to proof 3.1, we start with proving auxiliary lemmas. To this end we define the relaxed Multiple Choice Knapsack Problem (MCKP):</p><p>Definition F.1. Given n ∈ N, and a collection of k distinct covering subsets of {1, 2, · · · , n} denoted as N i , i ∈ {1, 2, · · · , k}, such that ∪ k i=1 N i = {1, 2, · · · , n} and ∩ k i=1 N i = ∅ with associated values and costs p ij , t ij ∀i ∈ {1, . . . , k}, j ∈ N i respectively, the relaxed Multiple Choice Knapsack Problem (MCKP) is formulated as following: <ref type="bibr">. . . , k}, j ∈ N i</ref> where the binary constraints u ij ∈ {0, 1} of the original MCKP formulation <ref type="bibr">(Kellerer et al., 2004)</ref> are replaced with u ij ≥ 0.</p><formula xml:id="formula_36">max vu k i=1 j∈Ni p ij u ij subject to k i=1 j∈Ni t ij u ij ≤ T (29) j∈Ni u ij = 1 ∀i ∈ {1, . . . , k} u ij ≥ 0 ∀i ∈ {1,</formula><p>Definition F.2. An one-hot vector u i satisfies:</p><formula xml:id="formula_37">||u * i || 0 = j∈Ni |u * ij | 0 = j∈Ni 1 u * ij &gt;0 = 1</formula><p>where 1 A is the indicator function that yields 1 if A holds and 0 otherwise.  <ref type="table">(Table 4)</ref>. Each couple of frames shows the initial point for a different latency constraint. Rows in each frames stand for different stages (top) and blocks (bottom). The lightest feasible initial point <ref type="formula" target="#formula_6">(26)</ref> involves only a single configuration of the first block in each stage, avoiding gradients from propagating to others (left). The balanced initial points <ref type="formula" target="#formula_6">(27)</ref> form chains of similar non-zero probability followed by chains of zero probabilities, such that gradients are propagated through feasible paths with a balanced distribution.</p><p>Lemma F.1. The solution u * of the relaxed MCKP (29) is composed of vectors u * i that are all one-hot but a single one.</p><p>Proof. Suppose that u * is an optimal solution of (29), and two indices i 1 , i 2 exist such that u * i1 , u * i2 are not one-hot vectors. As a consequence, we show that four indices, j 1 , j 2 , j 3 , j 4 exist, such that u</p><formula xml:id="formula_38">* i1j1 , u * i1j2 , u * i2j3 , u * i2j4 / ∈ {0, 1}. Define q = t i2j2 − t i1j1 t i2j3 − t i2j4<label>(30)</label></formula><p>and</p><formula xml:id="formula_39">f = (t i1j1 − t i1j2 ) p i1j1 − p i1j2 t i1j1 − t i1j2 − p i2j3 − p i2j4 t i2j3 − t i2j4</formula><p>and assume, without loss of generality, that f &gt; 0, otherwise one could swap the indices j 1 and j 2 so that this assumption holds.</p><p>Set</p><formula xml:id="formula_40">∆ = min (1 − u * i1j1 ), 1 − u * i2j3 |q| , u * i1j2 , u * i2j4 |q|<label>(31)</label></formula><p>such that ∆ &gt; 0 and construct another feasible solution of (29) u ij ← u * ij for all i, j but for the following indices:</p><formula xml:id="formula_41">u i1j1 ← u * i1j1 + ∆ u i1j2 ← u * i2j2 − ∆ u i2j3 ← u * i2j3 + q∆ u i2j4 ← u * i2j4 − q∆</formula><p>The feasibility of u is easily verified by the definitions in <ref type="formula" target="#formula_8">(30)</ref> and <ref type="formula" target="#formula_8">(31)</ref>, while the objective varies by:</p><formula xml:id="formula_42">k i=1 j∈Ni p ij (u ij − u * ij ) = ∆(p i1j1 − p i1j2 ) + q∆(p i2j3 − p i2j4 ) = ∆(t i1j1 − t i1j2 ) p i1j1 − p i1j2 t i1j1 − t i1j2 − p i2j3 − p i2j4 t i2j3 − t i2j4 = ∆f &gt; 0<label>(32)</label></formula><p>where the last inequality holds due to (31) together with the assumption f &gt; 0. Equation (32) holds in contradiction to u * being the optimal solution of (29). Hence all the vectors of u * but one are one-hot vectors.</p><p>Lemma F.2. The single non one-hot vector of the solution u * of the relaxed MCKP (29) has at most two nonzero elements.</p><p>Proof. Suppose that u * is an optimal solution of (29) and an indexî and three indices j 1 , j 2 , j 3 exist such that</p><formula xml:id="formula_43">u * ij1 , u * ij2 , u * ij3 / ∈ {0, 1}.</formula><p>Consider the variables ∆ = (∆ 1 , ∆ 2 , ∆ 3 ) T ∈ R 3 and the following system of equations:</p><p>tî j1 · ∆ 1 + tî j2 · ∆ 2 + tî j3 · ∆ 3 = 0 (33)</p><formula xml:id="formula_44">∆ 1 + ∆ 2 + ∆ 3 = 0</formula><p>At least one non-trivial solution ∆ * to (33) exists, since the system consists of two equations and three variables.</p><p>Assume, without loss of generality, that pî j1 · ∆ 1 + pî j2 · ∆ 2 + pî j3 · ∆ 3 &gt; 0</p><p>Otherwise replace ∆ * with −∆ * .</p><p>Scale ∆ * such that</p><formula xml:id="formula_46">0 &lt; u * ij1 + ∆ * k &lt; 1 ∀k ∈ {1, 2, 3}<label>(35)</label></formula><p>and construct another feasible solution of (29) u ij ← u * ij for all i, j but for the following indices: <ref type="formula" target="#formula_8">(33)</ref> and <ref type="formula" target="#formula_8">(35)</ref>, the feasibility of u is easily verified while the objective varies by:</p><formula xml:id="formula_47">uî j1 ← u * ij1 + ∆ 1 uî j2 ← u * ij2 + ∆ 2 uî j3 ← u * ij3 + ∆ 3 Since ∆ * satisfies</formula><formula xml:id="formula_48">k i=1 j∈Ni p ij (u ij − u * ij )</formula><p>= pî j1 · ∆ 1 + pî j2 · ∆ 2 + pî j3 · ∆ 3 &gt; 0 (36)</p><p>where the last inequality is due to (34). Equation (36) holds in contradiction to u * being the optimal solution of (29).</p><p>Hence the single non one-hot vector of u * has at most two nonzero entries.</p><p>In order to prove Theorem 3.1, we use Lemmas F.1 and F.1 for α and β separately, based on the observation that each problem in (13) forms a relaxed MCKP (29). Thus replacing u in (29) with α and β, p is replaced with α * and β * and the elements of t are replaced with the elements of β * T Θ T and α * T Θ respectively.</p><p>Remark One can further avoid the two nonzero elements by applying an iterative greedy solver as introduced in <ref type="bibr">(Kellerer et al., 2004)</ref>, instead of solving a linear program, with the risk of obtaining a sub-optimal solution.</p><p>G. 2 for 1: w * Bootstrap -Accuracy vs Cost</p><p>In this section we compare the accuracy and total cost for generating trained models in three ways:</p><p>1. Training from scratch 2. Fine-tuning w * for 10 epochs with knowledge distillation from the heaviest model loaded withw * .</p><p>3. Fine-tuning w * for 50 epochs with knowledge distillation from the heaviest model loaded withw * .</p><p>The last two procedures are specified in Section 4.2.2.</p><p>The results, presented in <ref type="figure" target="#fig_10">Figure 8</ref>, show that with a very short fine-tuning procedure of less than 7 GPU hours (10 epochs) as specified in Section 4.1.1, in most cases, the resulted accuracy surpasses the accuracy obtained by training from scratch. Networks of higher latency benefit less from the knowledge distillation, hence a longer training is required. A training of 35 GPU hours (50 epochs) results in a significant improvement of the accuracy for most of the models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Solving the Mathematical Programs Requires a Negligible Computation Time</head><p>In this section we measure the computation time for solving the mathematical programs associated with the initialization point, the LP associated with the FW step and the LP associated with our projection. We show that the measured times are negligible compared to the computation time attributed to backpropagation.</p><p>The average time, measured during the search, for solving the linear programs specified in Algorithm 1 and in Section 3.4.2 and the quadratic program specified in Appendix E is 1.15 × 10 −5 CPU hours.</p><p>The average time, measured during the search, for a single backpropagation of gradients through the one-shot model is 2.15 × 10 −3 GPU Hours.</p><p>The overall cost of solving the mathematical programs for generating N networks is about 0.02N CPU hours, which is negligible compared to the overall 400 + 15N GPU hours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The official implementation of HardCoRe-NAS is available at: https://github.com/Alibaba-MIIL/HardCoReNAS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A search space view via the one-shot model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>provides empirical validation of (3), showing that in practice the actual and estimated latency are very close on both GPU and CPU. More details on the experiments are provided in Section 4.2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Experimental results showing that the latency calculated with formula</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Top-1 accuracy on Imagenet of networks trained from scratch v.s. corresponding sub-networks extracted from our oneshot model. τ and ρ represent the Kendall-Tau and Spearman correlation coefficients, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Objective loss and soft penalty for FW and GD for different values of the penalty coefficient λ. Solid and dashed curves represent the objective loss (left y-axis) and soft penalty (right y-axis), respectively. FW converges to the optimum much faster than GD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.2. PROJECTION BACK TO THE DISCRETE SPACE As differentiable NAS methods are inherently associated with a continuous search space, a final discretizaiton step P : P ζ (S) → S is required for extracting a single architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>PyTorch Multipath Code</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Heat maps representing the initial probability of picking each stage depth (top) and block configuration (bottom), sorted from the lightest to the heaviest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Top-1 accuracy on Imagenet vs Latency measured on Intel Xeon CPU for a batch size of 1, for architectures found with our method trained from scratch and fine-tuned from the pretrained super-network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">evaluates the projection of architectures to the dis-</cell></row><row><cell cols="2">crete space, as proposed in Section 3.4.2. While the com-</cell></row><row><cell cols="2">monly used argmax projection violates the constraints by</cell></row><row><cell cols="2">up to 10%, those are strictly satisfied by our proposed pro-</cell></row><row><cell>jection.</cell><cell></cell></row><row><cell>Constraint</cell><cell>35 40 45 50 55 60</cell></row><row><cell>argmax</cell><cell>36 42 50 54 58 66</cell></row><row><cell cols="2">Our Projection 35 40 45 49 54 60</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A typical batch consists of hundreds of i.i.d paths, thus a variance reduction of the square root of that is in place.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. More Specifications of the Search Space Inspired by EfficientNet  and TF-NAS <ref type="bibr" target="#b28">(Hu et al., 2020)</ref>, we build a layer-wise search space, as explained in Section 3.1 and depicted in <ref type="figure">Figure 2</ref> and in Table 3. The input shapes and the channel numbers are the same as EfficientNetB0. Similarly to TF-NAS and differently from EfficientNet-B0, we use ReLU in the first three stages. As specified in Section 3.1.1, the ElasticMBInvRes block is our elastic version of the MBInvRes block, introduced in <ref type="bibr" target="#b51">(Sandler et al., 2018a)</ref>. Those blocks of stages 3 to 8 are to be searched for, while the rest are fixed. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knapsack pruning with inner distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aflalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08258</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep frankwolfe for neural network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07591</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Online algorithms and stochastic approxima-p tions. Online learning and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<title level="m">one-shot model architecture search through hypernetworks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proxylessnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Oncefor-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A frank-wolfe framework for efficient and effective adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3486" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<title level="m">Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Projectionfree adaptive gradients for large-scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pokutta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14114</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Global convergence of frank wolfe on one hidden layer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pilanci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02208</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="760" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variance-reduced and projection-free stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1263" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variance-reduced and projection-free stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1263" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster projection-free online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Minasyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1877" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.02531" />
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00140</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00140" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South), Oc</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tf-Nas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05314</idno>
		<title level="m">Rethinking three search freedoms of latency-constrained differentiable neural architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09844</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Intel(r) math kernel library for deep neural networks (intel(r) mkl-dnn)</title>
		<ptr target="https://github.com/rsdubtso/mkl-dnn" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Intel(R)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Multiple-Choice Knapsack Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kellerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pferschy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pisinger</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="317" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Heidelberg</title>
		<idno type="DOI">10.1007/978-3-540-24777-7_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-24777-7_11" />
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the global linear convergence of frank-wolfe optimization variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05932</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Block-coordinate frank-wolfe optimization for structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Block-coordinate frank-wolfe optimization for structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darts+</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2181" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maurice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The (dantzig) simplex method for linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engg</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="31" />
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Xnas: Neural architecture search with expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Asap: Architecture search, anneal and prune</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="493" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alché-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pokutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07243</idno>
		<title level="m">Deep neural network training with frank-wolfe</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpudedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF</title>
		<meeting>the IEEE/CVF</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">general intelligence&quot; objectively determined and measured</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A survey of optimization methods from a machine learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3668" to="3681" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On frank-wolfe optimization for adversarial robustness and interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsiligkaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12368</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A framework for fast, scalable binarized neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Umuroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skipnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hardwareaware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fbnet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01099</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">How to train your super-net: An analysis of training heuristics in weightsharing nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04276</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Quantized frank-wolfe: Faster optimization, lower communication, and projection free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3696" to="3706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
