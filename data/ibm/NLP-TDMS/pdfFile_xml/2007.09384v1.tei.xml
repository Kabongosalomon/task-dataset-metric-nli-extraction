<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-18">18 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
							<email>wujiaxi@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">BAIC for BDBC</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SKLSDE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SCSE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
							<email>liusongtao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">BAIC for BDBC</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SKLSDE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SCSE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<email>dhuang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">BAIC for BDBC</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SKLSDE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SCSE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">BAIC for BDBC</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SCSE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-18">18 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Few-Shot Object Detection, Multi-Scale Refinement</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection (FSOD) helps detectors adapt to unseen classes with few training instances, and is useful when manual annotation is time-consuming or data acquisition is limited. Unlike previous attempts that exploit few-shot classification techniques to facilitate FSOD, this work highlights the necessity of handling the problem of scale variations, which is challenging due to the unique sample distribution. To this end, we propose a Multi-scale Positive Sample Refinement (MPSR) approach to enrich object scales in FSOD. It generates multi-scale positive samples as object pyramids and refines the prediction at various scales. We demonstrate its advantage by integrating it as an auxiliary branch to the popular architecture of Faster R-CNN with FPN, delivering a strong FSOD solution. Several experiments are conducted on PASCAL VOC and MS COCO, and the proposed approach achieves state of the art results and significantly outperforms other counterparts, which shows its effectiveness. Code is available at https://github.com/jiaxi-wu/MPSR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection makes great progress these years following the success of deep convolutional neural networks (CNN) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref>. These CNN based detectors generally require large amounts of annotated data to learn extensive numbers of parameters, and their performance significantly drops when training data are inadequate. Unfortunately, for object detection, labeling data is quite expensive and the samples of some object categories are even hard to collect, such as endangered animals or tumor lesions. This triggers considerable attentions to effective detectors dealing with limited training samples. Few-shot learning is a popular and promising direction to address this issue. However, the overwhelming majority of the existing few-shot investigations focus on object/image classification, while the efforts on the more challenging few-shot object detection (FSOD) task are relatively rare.</p><p>With the massive parameters of CNN models, training detectors from scratch with scarce annotations generally incurs a high risk of overfitting. Preliminary research <ref type="bibr" target="#b3">[4]</ref> tackles this problem in a transfer learning paradigm. Given a set of base classes with sufficient annotations and some novel classes with only a few samples, the goal is to acquire meta-level knowledge from base classes and then apply it to facilitating few-shot learning in detection of novel classes. Subsequent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref> strengthen this pipeline by bringing more advanced methods on few-shot image classification, and commonly emphasize to improve classification performance of Region-of-Interest (RoI) in FSOD by using metric learning techniques. With elaborately learned representations, they ameliorate the similarity measurement between RoIs and marginally annotated instances, reporting better detection results. Meanwhile, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> also attempt to deliver more general detectors, which account for all the classes rather than the novel ones only, by jointly using their samples in the training phase.  The prior studies demonstrate that the FSOD problem can be alleviated in a similar manner as few-shot image classification. Nevertheless, object detection is much more difficult than image classification, as it involves not only classification but also localization, where the threat of varying scales of objects is particularly evident. The scale invariance has been widely explored in generic supervised detectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>, while it remains largely intact in FSOD. Moreover, restricted by the quantity of annotations, this scale issue is even more tricky. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the lack of labels of novel classes leads to a sparse scale space (green bars) which may be totally divergent from the original distribution (yellow bars) of abundant training data. One could assume to make use of current effective solutions from generic object detection to enrich the scale space. For instance, Feature Pyramid Network (FPN), which builds multi-scale feature maps to detect objects at different scales, applies to situations where significant scale variations exist <ref type="bibr" target="#b21">[22]</ref>. This universal property does contribute to FSOD, but it will not mitigate the difference of the scale distribution in the data of novel classes. Regarding image pyramids <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>, they build multi-scale representations of an image and allow detectors to capture objects in it at different scales. Although they are expected to narrow such a gap between the two scale distributions, the case is not so straightforward. Specifically, multi-scale inputs result in an increase in improper negative samples due to anchor matching. These improper negative samples contain a part of features belonging to the positive samples, which interferes their recognition. With abundant data, the network learns to extract diverse contexts and suppress the improper local patterns. But it is harmful to FSOD where both semantic and scale distributions are sparse and biased.</p><p>In this work, we propose a Multi-scale Positive Sample Refinement (MPSR) approach to few-shot object detection, aiming at solving its unique challenge of sparse scale distribution. We take the reputed Faster R-CNN as the basic detection model and employ FPN in the backbone network to improve its tolerance to scale variations. We then exploit an auxiliary refinement branch to generate multi-scale positive samples as object pyramids and further refine the prediction. This additional branch shares the same weights with the original Faster R-CNN. During training, this branch classifies the extracted object pyramids in both the Region Proposal Network (RPN) and the detector head. To keep scale-consistent prediction without introducing more improper negatives, we abandon the anchor matching rules and adaptively assign the FPN stage and spatial locations to the object pyramids as positives. It is worth noting that as we use no extra weights in training, our method achieves remarkable performance gains in an inference cost-free manner and can be conveniently deployed on different detectors.</p><p>The contributions of this study are three-fold:</p><p>1. To the best of our knowledge, it is the first work to discuss the scale problem in FSOD. We reveal the sparsity of scale distributions in FSOD with both quantitative and qualitative analysis. 2. To address this problem, we propose the MPSR approach to enrich the scale space without largely increasing improper negatives. 3. Comprehensive experiments are carried out, and significant improvements from MPSR demonstrate its advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Few-Shot Image Classification. There are relatively many historical studies in the area of few-shot image classification that targets recognition of objects with only a handful of images in each class <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b8">[9]</ref> learns to initialize weights that effectively adapt to unseen categories. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> aim to predict network parameters without heavily training on novel images. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35]</ref> employ metric learning to replace linear classifiers with learnable metrics for comparison between query and support samples. Although few-shot image classification techniques are usually used to advance the phase of RoI classification in FSOD, they are different tasks, as FSOD has to consider localization in addition.</p><p>Generic Object Detection. Recent object detection architectures are mainly divided into two categories: one-stage detectors and two-stage detectors. One-stage detectors use a single CNN to directly predict bounding boxes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>, and two-stage ones first generate region proposals and then classify them for decision making <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>. Apart from network design, scale invariance is an important aspect to detectors and many solutions have recently been proposed to handle scale changes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>. For example, <ref type="bibr" target="#b21">[22]</ref> builds multi-scale feature maps to match objects at different scales. <ref type="bibr" target="#b32">[33]</ref> performs scale normalization to detect scale-specific objects and adopts image pyramids for multi-scale detection. These studies generally adapt to alleviate large size differences of objects. Fewshot object detection suffers from scale variations in a more serious way where a few samples sparsely distribute in the scale space.</p><p>Object Detection with Limited Annotations. To relieve heavy annotation dependence in object detection, there exist two main directions without using external data. One is weakly-supervised object detection, where only image-level labels are provided and spatial supervision is unknown <ref type="bibr" target="#b1">[2]</ref>. Research basically concentrates on how to rank and classify region proposals with only coarse labels through multiple instance learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Another is semi-supervised object detection that assumes abundant images are available while the number of bounding box annotations is limited <ref type="bibr" target="#b25">[26]</ref>. In this case, previous studies confirm the effectiveness of adopting extra images by pseudo label mining <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> or multiple instance learning <ref type="bibr" target="#b20">[21]</ref>. Both the directions reduce manual annotation demanding to some extent, but they heavily depend on the amount of training images. They have the difficulty in dealing with constrained conditions where data acquisition is inadequate, i.e., few-shot object detection.</p><p>Few-Shot Object Detection. Preliminary work <ref type="bibr" target="#b3">[4]</ref> on FSOD introduces a general transfer learning framework and presents the Low-Shot Transfer Detector (LSTD), which reduces overfitting by adapting pre-trained detectors to fewshot scenarios with limited training images. Following this framework, RepMet <ref type="bibr" target="#b16">[17]</ref> incorporates a distance metric learning classifier into the RoI classification head in the detector. Instead of categorizing objects with fully-connected layers, RepMet extracts representative embedding vectors by clustering and calculates distances between query and annotated instances. <ref type="bibr" target="#b7">[8]</ref> is motivated by <ref type="bibr" target="#b18">[19]</ref> which scores the similarity in a siamese network and computes pair-wise object relationship in both the RPN and the detection head. <ref type="bibr" target="#b15">[16]</ref> is a single-stage detector combined with a meta-model that re-weights the importance of features from the base model. The meta-model encodes class-specific features from annotated images at a proper scale, and the features are viewed as reweighting coefficients and fed to the base model. Similarly, <ref type="bibr" target="#b40">[41]</ref> delivers a two-stage detection architecture and re-weights RoI features in the detection head. Unlike previous studies where spatial influence is not considered, we argue that scale invariance is a challenging issue to FSOD, as the samples are few and their scale distribution is sparse. We improve the detector by refining object crops rather than masked images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> or siamese inputs <ref type="bibr" target="#b7">[8]</ref> for additional training, which enriches the scale space and ensures the detector being fully trained at all scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Before introducing MPSR, we briefly review the standard protocols and the basic detector we adopt for completeness. As it is the first work that addresses the challenge of sparse scale distribution in FSOD, we conduct some preliminary attempts with the current effective methods from generic object detection (i.e., FPN and image pyramids) to enrich the scale space and discuss their limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Few-Shot Object Detection</head><p>Few-Shot Object Detection Protocols. Following the settings in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, object classes are divided into base classes with abundant data and novel classes with only a few training samples. The training process of FSOD generally adopts a two-step paradigm. During base training, the detection network is trained with a large-scale dataset that only contains base classes. Then the detection network is fine-tuned on the few-shot dataset, which only contains a very small number of balanced training samples for both base and novel classes. This two-step training schedule avoids the risk of overfitting with insufficient training samples on novel classes. It also prevents the detector from extremely imbalanced training if all annotations from both base and novel classes are exploited together <ref type="bibr" target="#b40">[41]</ref>. To build the balanced few-shot dataset, <ref type="bibr" target="#b15">[16]</ref> employs the k-shot sampling strategy, where each object class only has k annotated bounding boxes. Another work <ref type="bibr" target="#b3">[4]</ref> collects k images for each class in the few-shot dataset. As k images actually contain an arbitrary number of instances, training and evaluation under this protocol tend to be unstable. We thus use the former strategy following <ref type="bibr" target="#b15">[16]</ref>.</p><p>Basic Detection Model. With the fast development in generic object detection, the base detector in FSOD has many choices. <ref type="bibr" target="#b15">[16]</ref> is based on YOLOv2 <ref type="bibr" target="#b29">[30]</ref>, which is a single-stage detector. <ref type="bibr" target="#b40">[41]</ref> is based on a classical two-stage detector, Faster R-CNN <ref type="bibr" target="#b30">[31]</ref>, and demonstrates that Faster R-CNN provides consistently better results. Therefore, we take the latter as our basic detection model. Faster R-CNN consists of the RPN and the detection head. For a given image, the RPN head generates proposals with objectness scores and bounding-box regression offsets. The RPN loss function is:</p><formula xml:id="formula_0">L RP N = 1 N obj N obj i=1 L i Bcls + 1 N obj N obj i=1 L i P reg .<label>(1)</label></formula><p>For the ith anchor in a mini-batch, L i Bcls is the binary cross-entropy loss over background and foreground and L i P reg is the smooth L 1 loss defined in <ref type="bibr" target="#b30">[31]</ref>. N obj is the total number of chosen anchors. These proposals are used to extract RoI features and then fed to the detection (RoI) head that outputs class-specific scores and bounding-box regression offsets. The loss function is defined as:</p><formula xml:id="formula_1">L RoI = 1 N RoI NRoI i=1 L i Kcls + 1 N RoI NRoI i=1 L i Rreg ,<label>(2)</label></formula><p>where L i Kcls is the log loss over K classes and N RoI is the number of RoIs in a mini-batch. Different from the original implementation in <ref type="bibr" target="#b30">[31]</ref>, we employ a class-agnostic regression task in the detection head, which is the same as <ref type="bibr" target="#b3">[4]</ref>. The total loss is the sum of L RP N and L RoI .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary Attempts</head><p>FPN for Multi-Scale Detection. As FPN is commonly adopted in generic object detection to address the scale variation issue <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>, we first consider applying it to FSOD in our preliminary experiments. FPN generates several different semantic feature maps at different scales, enriching the scale space in features. Our experiments validate that it is still practically useful under the restricted conditions in FSOD. We thus exploit Faster R-CNN with FPN as our second baseline. However, FPN does not change the distribution in the data of novel classes and the sparsity of scale distribution remains unsolved in FSOD. Image Pyramids for Multi-Scale Training. To enrich object scales, we then consider a multi-scale training strategy which is also widely used in generic object detection for multi-scale feature extraction <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref> or data augmentation <ref type="bibr" target="#b29">[30]</ref>. In few-shot object detection, image pyramids enrich object scales as data augmentation and the sparse scale distribution can be theoretically solved. However, this multi-scale training strategy acts differently in FSOD with the increasing number of improper negative samples. As in <ref type="figure" target="#fig_2">Fig. 2</ref> Motivated by the above discussion, we employ FPN in the backbone of Faster R-CNN as the advanced version of baseline. To enrich scales of positive samples without largely increasing improper negative samples, we extract each object independently and resize them to various scales, denoted as object pyramids. Specifically, each object is cropped by a square window (whose side is equal to the longer side of the bounding box) with a minor random shift. It is then resized to 32 2 , 64 2 , 128 2 , 256 2 , 512 2 , 800 2 pixels, which is similar to anchor design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPN Head Refinement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Head Refinement</head><p>Object Pyramids Feature Pyramids In object pyramids, each image only contains a single instance, which is inconsistent to the standard detection pipeline. Therefore, we propose an extra positive sample refinement branch to adaptively project the object pyramids into the standard detection network. For a given object, the standard FPN pipeline samples the certain scale level and the spatial locations as positives for training, operated by anchor matching. However, performing anchor matching on cropped single objects is wasteful and also incurs more improper negatives that hurt the performance for FSOD. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, instead of anchor matching, we manually select the corresponding scale level of feature maps and the fixed center locations as positives for each object, keeping it consistent with the standard FPN assigning rules. After selecting specific features from these feature maps, we feed them directly to the RPN head and the detection head for refinement. In the RPN head, the multi-scale feature maps of FPN {P 2 , P 3 , P 4 , P 5 , P 6 } represent anchors whose areas are 32 2 , 64 2 , 128 2 , 256 2 , 512 2 pixels respectively. For a given object, only one feature map with the consistent scale is activated, as shown in <ref type="table" target="#tab_1">Table 1</ref>. To simulate that each proposal is predicted by its center location in RPN, we select centric 2 2 features for object refinement. We also put anchors with {1 : 2, 1 : 1, 2 : 1} aspect ratios on the sampled locations. These selected anchors are viewed as positives for the RPN classifier.</p><p>To extract RoI features for the detection head, only {P 2 , P 3 , P 4 , P 5 } are used and the original RoI area partitions in the standard FPN pipeline are: 0 2 , 112 2 , 112 2 , 224 2 , 224 2 , 448 2 , 448 2 , ∞ <ref type="bibr" target="#b21">[22]</ref>. We also select one feature map at a specific scale for each object to keep the scale consistency, as shown in <ref type="table" target="#tab_1">Table 1</ref>. As the randomly cropped objects tend to have larger sizes than the orignal ground truth bounding boxes, we slightly increase the scale range of each FPN stage for better selection. Selected feature maps are adaptively pooled to the same RoI size and fed to the RoI classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Framework</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the whole detection framework for training consists of Faster R-CNN with FPN and the refinement branch working in parallel while sharing the same weights. For a given image, it is processed by the backbone network, RPN, RoI Align layer, and the detection head in the standard two-stage detection pipeline <ref type="bibr" target="#b30">[31]</ref>. Simultaneously, an independent object extracted from the original image is resized to different scales as object pyramids. The object pyramids are fed into the detection network as described above. The outputs from RPN and detection heads in the MPSR branch include objectness scores and class-specific scores similar to the definitions in Section 3.1. The loss function of the RPN head containing Faster R-CNN and the MPSR branch is defined as:</p><formula xml:id="formula_2">L RP N = 1 N obj +M obj N obj +M obj i=1 L i Bcls + 1 N obj N obj i=1 L i P reg ,<label>(3)</label></formula><p>where M obj is the number of selected positive anchor samples for refinement. The loss function of the detection head is defined as:</p><formula xml:id="formula_3">L RoI = 1 N RoI NRoI i=1 L i Kcls + λ M RoI MRoI i=1 L i Kcls + 1 N RoI NRoI i=1 L i Rreg ,<label>(4)</label></formula><p>where M RoI is the number of selected RoIs in MPSR. Unlike the RPN head loss where M obj is close to N obj , the number of positives from object pyramids is quite small compared to N RoI in the RoI head. We thus add a weight parameter λ to the RoI classification loss of the positives from MPSR to adjust its magnitude, which is set to 0.1 by default. After the whole network is fully trained, the extra MPSR branch is removed and only Faster R-CNN with FPN is used for inference. Therefore, the MPSR approach that we propose benefits FSOD training without extra time cost at inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Settings</head><p>We evaluate our method on the PASCAL VOC 2007 <ref type="bibr" target="#b6">[7]</ref>, 2012 <ref type="bibr" target="#b5">[6]</ref> and MS COCO <ref type="bibr" target="#b22">[23]</ref> benchmarks. For fair quantitative comparison with state of the art (SOTA) methods, we follow the setups in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>   <ref type="bibr" target="#b15">[16]</ref> to use the same three class splits, where the unseen classes are {"bird", "bus", "cow", "motorbike" ("mbike"), "sofa"}, {"aeroplane" ("aero"), "bottle", "cow", "horse", "sofa"}, {"boat", "cat", "motorbike", "sheep", "sofa"}, respectively. For FSOD experiments, the few-shot dataset consists of images where only k object instances are available for each category and k is set as 1/3/5/10.</p><p>MS COCO. COCO has 80 object categories, where the 20 categories overlapped with PASCAL VOC are denoted as novel classes. 5,000 images from the val set, denoted as minival, are used for evaluation while the left images in the train and val sets are used for training. Base and few-shot dataset construction is the same as that in PASCAL VOC except that k is set as 10/30. Implementation Details. We train and test detection networks on images of a single scale. We resize input images so that their shorter sides are set to 800 pixels and the longer sides are less than 1,333 pixels while maintaining the aspect ratio. Our backbone is ResNet-101 <ref type="bibr" target="#b14">[15]</ref> with the RoI Align <ref type="bibr" target="#b12">[13]</ref> layer and we use the weights pre-trained on ImageNet <ref type="bibr" target="#b31">[32]</ref> in initialization. For efficient training, we randomly sample one object to generate the object pyramid for each image. After training on base classes, only the last fully-connected layer (for classification) of the detection head is replaced. The new classification layer is randomly initialized and none of the network layers is frozen during few-shot fine-tuning. We train our networks with a batchsize of 4 on 2 GPUs, 2 images per GPU. We run the SGD optimizer with the momentum of 0.9 and the parameter decay of 0.0001. For base training on VOC, models are trained for 240k, 8k, and 4k iterations with learning rates of 0.005, 0.0005 and 0.00005 respectively. For few-shot fine-tuning on VOC, we train models for 1,300, 400, 300 iterations and the learning rates are 0.005, 0.0005 and 0.00005, respectively. Models are trained on base COCO classes for 56k, 14k, and 10k iterations. For COCO fewshot fine-tuning, the 10-shot dataset requires 2,800, 700, and 500 iterations, while the 30-shot dataset requires 5,600, 1,400, 1,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We compare our results with two baseline methods (denoted as Baseline and Baseline-FPN) as well as two SOTA few-shot detection counterparts. Baseline and Baseline-FPN are our implemented Faster R-CNN and Faster R-CNN with FPN described in Section 3. YOLO-FS <ref type="bibr" target="#b15">[16]</ref> and Meta R-CNN <ref type="bibr" target="#b40">[41]</ref> are the SOTA few-shot detectors based on DarkNet-19 and ResNet-101, respectively. It should be noted that due to better implementation and training strategy, our baseline achieves higher performance than SOTA, which is also confirmed by the very recent work <ref type="bibr" target="#b39">[40]</ref>.</p><p>PASCAL VOC. MPSR achieves 82.1%/82.7%/82.9% on base classes of three splits respectively before few-shot fine-tuning. The main results of few-shot ex- periments on VOC are summarized in <ref type="table" target="#tab_3">Table 2</ref>. It can be seen from this table that the results of the two baselines (i.e. Baseline and Baseline-FPN) are close to each other when the number of instances is extremely small (e.g. 1 or 3), and Baseline-FPN largely outperforms the other as the number of images increases. This demonstrates that FPN benefits few-shot object detection as in generic object detection. Moreover, our method further improves the performance of Baseline-FPN with any number of training samples in all the three class splits. Specifically, by solving the sparsity of object scales, we achieve a significant increase in mAP compared to the best scores of the two baselines, particularly when training samples are extremely scarce, e.g. 16.2% on 1-shot split-1. It clearly highlights the effectiveness of the extra MPSR branch. Regarding other counterparts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, the proposed approach outperforms them by a large margin, reporting the state of the art scores on this dataset. Following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, we display the detailed results of 3-/10-shot detection in the first split on VOC in <ref type="table" target="#tab_4">Table 3</ref>. Consistently, our Baseline-FPN outperforms the existing methods on both the novel and base classes. This confirms that FPN addresses the scale problem in FSOD to some extent. Furthermore, our method improves the accuracies of Baseline-FPN in all the settings by integrating MPSR, illustrating its advantage.</p><p>MS COCO. We evaluate the method using 10-/30-shot setups on MS COCO with the standard COCO metrics. The results on novel classes are provided in <ref type="table" target="#tab_5">Table 4</ref>. Although COCO is quite challenging, we still achieve an increase of 0.4% on 30-shot compared with Baseline-FPN while boosting the SOTA mAP from 12.4% (Meta R-CNN) to 14.1%. Specifically, our method improves the recognition of small, medium and large objects simultaneously. This demonstrates that our balanced scales of input objects are effective. MS COCO to PASCAL VOC. We conduct cross-dataset experiments on the standard VOC 2007 test set. In this setup, all the models are trained on the base COCO dataset and finetured with 10-shot objects in novel classes on VOC. Results of Baseline and Baseline-FPN are 38.5% and 39.3% respectively. They are worse than 10-shot results only trained on PASCAL VOC due to the large domain shift. Cross-dataset results of YOLO-FS and Meta R-CNN are 32.3% and 37.4% respectively. Our MPSR achieves 42.3%, which indicates that our method has better generalization ability in cross-domain situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Sparse Scales</head><p>We visualize the scale distribution of two categories on the original dataset (Pascal VOC) and 10-shot subset in <ref type="figure" target="#fig_1">Fig. 1</ref>. It is obvious that the scale distribution in the few-shot dataset is extremely sparse and distinct from the original ones.</p><p>To quantitatively analyze the negative effect of scale sparsity, we evaluate detectors on two specific 10-shot datasets. We carefully select the bus and cow instances with the scale between 128 2 and 256 2 pixels to construct the "limited" few-shot datasets. As shown in <ref type="table" target="#tab_6">Table 5</ref>, such extremely sparse scales lead to a significant drop in performance (e.g. for bus, -28.9% on Baseline-FPN). Therefore, it is essential to solve the extremely sparse and biased scale distribution in FSOD. With our MPSR, the reduction of performance is relieved. As in <ref type="table" target="#tab_7">Table 6</ref>, we compare MPSR with several methods that are used for scale invariance. SNIPER <ref type="bibr" target="#b33">[34]</ref> shows a lower accuracy on novel classes and a higher accuracy on base classes than the baseline. As SNIPER strictly limits the scale range in training, it actually magnifies the sparsity of scales in FSOD. Such low performance also indicates the importance of enriching scales. We also evaluate the scale augmentation and image pyramids with a shorter side of {480, 576, 688, 864, 1200} <ref type="bibr" target="#b13">[14]</ref>. We can see that our MPSR achieves better results than those two multi-scale training methods on the novel classes. When only one instance is available for each object category, our method exceeds multi-scale training by ∼12%, demonstrating its superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>We conduct some ablation studies to verify the effectiveness of the proposed manual selection and refinement method in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Manual Selection. From the first two lines in <ref type="table" target="#tab_8">Table 7</ref>, we see that applying anchor matching to object pyramids on both RPN and RoI heads achieves better performance than Baseline-FPN. However, when compared to the last three lines with manual selection rules, anchor matching indeed limits the benefits of object pyramids, as it brings more improper negative samples to interfere few-shot training. It confirms the necessity of the proposed manual refinement rules.</p><p>RPN and Detection Refinement. As in the last three lines of <ref type="table" target="#tab_8">Table 7</ref>, we individually evaluate RPN refinement and detection (RoI) refinement to analyze their credits in the entire approach. Models with only the RPN and RoI refinement branches exceed Baseline-FPN in all the settings, which proves their effectiveness. Our method combines them and reaches the top score, which indicates that the two branches play complementary roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper targets the scale problem caused by the unique sample distribution in few-shot object detection. To deal with this issue, we propose a novel approach, namely multi-scale positive sample refinement. It generates multi-scale positive samples as object pyramids and refines the detectors at different scales, thus enlarging the scales of positive samples while limiting improper negative samples. We further deliver a strong FSOD solution by integrating MPSR to Faster R-CNN with FPN as an auxiliary branch. Experiments are extensively carried out on PASCAL VOC and MS COCO, and the proposed approach reports better scores compared to current state of the arts, which shows its advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Ablation of Refinement in the Two-Step Training By default, we apply the refinement branch to detectors during both training steps for consistency. To reveal the effectiveness of our method in the two steps, we evaluate detectors with refinement only during base training or few-shot fine-tuning individually. As shown in <ref type="table" target="#tab_9">Table 8</ref>, refining detectors only during base training gets better results than Baseline-FPN, which means detection at various scales benefits from our method to some extent. Refining detectors only during few-shot fine-tuning exceeds refining base only and Baseline-FPN by a large margin when the number of instances is extremely small (e.g. 1 or 3), which demonstrates that our method relieves the unique sparsity of scales in FSOD. Besides, refining detectors during both base training and few-shot fine-tuning achieves the best results, indicating that the two steps play complementary roles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Complete Results on PASCAL VOC</head><p>As shown in <ref type="table" target="#tab_10">Table 9</ref>, we present the complete results on PASCAL VOC as in <ref type="bibr" target="#b40">[41]</ref>. In this table, we also supply the 2-shot experimental results for consistency. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of scale distributions of two specific classes: (a) bus and (b) cow, in PASCAL VOC (Original) and a 10-shot subset (Few-shot). Images are resized with the shorter size at 800 pixels for statistics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An example of improper negative samples in FSOD. Negative samples (NS), positive samples (PS) and ground-truth (GT) bounding boxes are annotated. The improper negative samples significantly increase as more scales are involved (top right), while they may even be true positives in other contexts (bottom right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Multi-scale positive sample feature extraction. The positive sample is extracted and resized to various scales. Specific feature maps from FPN are selected for refinement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>MPSR architecture. On an input image to Faster R-CNN, the auxiliary branch extracts samples and resizes them to different scales. Each sample is fed to the FPN and specific features are selected to refine RPN and RoI heads in Faster R-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, red bounding boxes are negative samples in training while they actually contain part of objects and may even be true positive samples in other contexts (as in bottom right). These improper negative samples require sufficient contexts and clues to suppress, inhibiting being mistaken for potential objects. Such an interference is trivial when abundant annotations are available, but it is quite harmful to the sparse and biased distribution in FSOD. Moreover, with multi-scale training, a large number of extra improper negative samples are introduced, which further hurts the performance.</figDesc><table /><note>4 Multi-Scale Positive Sample Refinement4.1 Multi-Scale Positive Sample Refinement Branch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>FPN feature map selection for different object scales. For each object, two specific feature maps are activated, fed to RPN and detection (RoI) heads respectively 32 2 64 2 128 2 256 2 512 2 800 2</figDesc><table><row><cell>RPN P2 P3</cell><cell>P4</cell><cell>P5</cell><cell>P6</cell><cell>P6</cell></row><row><cell>RoI P2 P2</cell><cell>P2</cell><cell>P3</cell><cell>P4</cell><cell>P5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>to construct few-shot detection datasets. PASCAL VOC. Our networks are trained on the modified VOC 2007 trainval and VOC 2012 trainval sets. The standard VOC 2007 test set is used for evaluation. The evaluation metric is the mean Average Precision (mAP). Both the trainval sets are split by object categories, where 5 are randomly chosen as novel classes and the left 15 are base classes. Here we follow</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different methods in terms of mAP (%) of novel classes using the three splits on the VOC 2007 test set</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Class Split 1</cell><cell></cell><cell></cell><cell cols="2">Class Split 2</cell><cell></cell><cell></cell><cell cols="2">Class Split 3</cell><cell></cell></row><row><cell>Method/Shot</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell cols="13">YOLO-FS [16] 14.8 26.7 33.9 47.2 15.7 22.7 30.1 39.2 19.2 25.7 40.6 41.3</cell></row><row><cell cols="13">Meta R-CNN [41] 19.9 35.0 45.7 51.5 10.4 29.6 34.8 45.4 14.3 27.5 41.2 48.1</cell></row><row><cell>Baseline</cell><cell cols="12">24.5 40.8 44.6 47.9 16.7 34.9 37.0 40.9 27.3 36.3 41.2 45.2</cell></row><row><cell cols="13">Baseline-FPN 25.5 41.1 49.6 56.9 15.5 37.7 38.9 43.8 29.9 37.9 46.3 47.8</cell></row><row><cell cols="13">MPSR (ours) 41.7 51.4 55.2 61.8 24.4 39.2 39.9 47.8 35.6 42.3 48.0 49.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>AP (%) of each novel class on the 3-/10-shot VOC dataset of the first class split. mAP (%) of novel classes and base classes are also presented</figDesc><table><row><cell></cell><cell></cell><cell>Novel Classes</cell><cell>Mean</cell></row><row><cell>Shot</cell><cell>Method</cell><cell cols="2">bird bus cow mbike sofa Novel Base</cell></row><row><cell></cell><cell cols="3">YOLO-FS [16] 26.1 19.1 40.7 20.4 27.1 26.7 64.8</cell></row><row><cell></cell><cell cols="3">Meta R-CNN[41] 30.1 44.6 50.8 38.8 10.7 35.0 64.8</cell></row><row><cell>3</cell><cell>Baseline</cell><cell cols="2">34.9 26.9 53.3 50.8 38.2 40.8 45.2</cell></row><row><cell></cell><cell>Baseline-FPN</cell><cell cols="2">32.6 29.4 45.5 56.2 41.7 41.1 66.2</cell></row><row><cell></cell><cell cols="3">MPSR (ours) 35.1 60.6 56.6 61.5 43.4 51.4 67.8</cell></row><row><cell></cell><cell cols="3">YOLO-FS [16] 30.0 62.7 43.2 60.6 39.6 47.2 63.6</cell></row><row><cell></cell><cell cols="3">Meta R-CNN [41] 52.5 55.9 52.7 54.6 41.6 51.5 67.9</cell></row><row><cell>10</cell><cell>Baseline</cell><cell cols="2">38.6 48.6 51.6 57.2 43.4 47.9 47.8</cell></row><row><cell></cell><cell>Baseline-FPN</cell><cell cols="2">41.8 68.4 61.7 66.8 45.8 56.9 70.0</cell></row><row><cell></cell><cell>MPSR (ours)</cell><cell cols="2">48.3 73.7 68.2 70.8 48.2 61.8 71.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>AP (%) and AR (%) of 10-/30-shot scores of novel classes on COCO minival Baseline-FPN 9.5 17.3 9.4 2.7 8.4 15.9 14.8 20.6 20.6 4.7 19.3 33.1 MPSR (ours) 9.8 17.9 9.7 3.3 9.2 16.115.7 21.2 21.2 4.6 19.6 34.3 30 YOLO-FS [16] 9.1 19.0 7.6 0.8 4.9 16.8 13.2 17.7 17.8 1.5 10.4 33.5 Meta R-CNN [41] 12.4 25.3 10.8 2.8 11.6 19.0 15.0 21.4 21.7 8.6 20.0 32.1 Baseline 12.6 25.7 11.0 3.2 11.8 20.7 15.9 21.8 21.8 5.1 18.0 36.9 Baseline-FPN 13.7 25.1 13.3 3.6 12.5 23.317.8 24.7 24.7 5.4 21.6 40.5 MPSR (ours) 14.1 25.4 14.2 4.0 12.9 23.0 17.7 24.2 24.3 5.5 21.0 39.3</figDesc><table><row><cell>Shot</cell><cell>Method</cell><cell>AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL</cell></row><row><cell></cell><cell cols="2">YOLO-FS [16] 5.6 12.3 4.6 0.9 3.5 10.5 10.1 14.3 14.4 1.5 8.4 28.2</cell></row><row><cell></cell><cell cols="2">Meta R-CNN [41] 8.7 19.1 6.6 2.3 7.7 14.0 12.6 17.8 17.9 7.8 15.6 27.2</cell></row><row><cell>10</cell><cell>Baseline</cell><cell>8.8 18.7 7.1 2.9 8.1 15.0 12.9 17.2 17.2 4.1 14.2 29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>AP (%) on bus/cow class. Two 10-shot datasets are constructed on VOC split-1, where scales of instances are random or limited. Std over 5 runs are presented FPN 68.4±0.6 39.5±1.3 61.7±0.9 39.9±1.2 MPSR (ours) 73.7±1.6 54.0±1.4 68.2±1.0 52.5±1.6</figDesc><table><row><cell></cell><cell>Bus</cell><cell>Cow</cell></row><row><cell>Method</cell><cell cols="2">Random Limited Random Limited</cell></row><row><cell>Baseline-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>mAP (%) comparison of novel/base classes on VOC split-1: Baseline-FPN, SNIPER [34], Baseline-FPN with scale augmentation/image pyramids and MPSR</figDesc><table><row><cell></cell><cell></cell><cell>Novel</cell><cell></cell><cell></cell><cell>Base</cell><cell></cell></row><row><cell>Method/Shot</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell>Baseline-FPN</cell><cell cols="6">25.5 41.1 49.6 56.9 66.2 67.9</cell></row><row><cell>SNIPER [34]</cell><cell>1.4</cell><cell cols="5">21.0 39.7 67.8 74.8 76.2</cell></row><row><cell cols="7">Scale Augmentation 29.8 44.7 49.8 52.7 67.1 68.8</cell></row><row><cell>Image Pyramids</cell><cell cols="6">29.5 48.4 50.4 58.1 67.5 68.3</cell></row><row><cell>MPSR (ours)</cell><cell cols="6">41.7 51.4 55.2 59.4 67.8 68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>mAP (%) of MPSR with different settings of novel classes on VOC split-1</figDesc><table><row><cell>Baseline</cell><cell>Object</cell><cell>Manual</cell><cell>Refinement</cell><cell></cell><cell>Shot</cell><cell></cell></row><row><cell>FPN</cell><cell>Pyramids</cell><cell>Selection</cell><cell>RPN RoI</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">25.5 41.1 49.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">30.8 43.6 49.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">36.7 48.0 54.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">33.7 48.2 54.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">41.7 51.4 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>mAP (%) of novel classes on VOC split-1 applying refinement during different training stages FPN 25.5 41.1 49.6 56.9 Only Base 26.6 43.5 50.7 59.4 Only Few-shot 36.5 47.3 50.6 59.0 Both Steps 41.7 51.4 55.2 61.8</figDesc><table><row><cell>Method/Shot</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>Baseline-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>AP (%) of each novel class on the few-shot VOC datasets. mAP (%) of novel classes are also presented mbike sofa mean aero bottle cow horse sofa mean boat cat mbike sheep sofa mean</figDesc><table><row><cell></cell><cell></cell><cell>Class Split 1</cell><cell>Class Split 2</cell><cell>Class Split 3</cell></row><row><cell>Shot</cell><cell>Method</cell><cell>bird bus cow</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Note-rcnn: Noise tolerant ensemble rcnn for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) (2015) 1, 2, 4</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) (2019) 2, 4, 5, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SAN: learning relationship between convolutional features for multi-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML DeepLearning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thoracic disease identification and localization with limited supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 2, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning of object detectors from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) (2015) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SNIPER: efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">C-MIL: continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta R-CNN: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) (2019) 2, 4, 5, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Meta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baseline-Fpn 25</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>2 22.1 52.1 18.8 25.5 20.7 9.4 19.4 13.1 15.0 15.5 11.4 41.6 42.7 35.9 17.8 29.9</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Meta</surname></persName>
		</author>
		<idno>41] 17.2 34.4 43.8 31.8 0</idno>
		<imprint/>
	</monogr>
	<note>4 25.5 12.4 0.1 44.4 50.1 0.1 19.4 10.6 24.0 36.2 19.2 0.8 18.2 Baseline 36.5 10.6 39.5 55.2 26.3 33.6 31.9 9.1 45.5 18.3 22.8 25.5 6.8 49.7 52.6 35.7 22.9 33.5</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Meta</surname></persName>
		</author>
		<idno>41] 30</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Meta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Meta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
