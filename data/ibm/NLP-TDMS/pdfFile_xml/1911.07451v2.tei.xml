<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DirectPose: Direct End-to-End Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DirectPose: Direct End-to-End Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the first direct end-to-end multi-person pose estimation framework, termed DirectPose. Inspired by recent anchor-free object detectors, which directly regress the two corners of target bounding-boxes, the proposed framework directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or boundingbox detection and RoI operations in top-down ones. We also propose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the main difficulty-the feature mis-alignment between the convolutional features and predictions in this end-to-end framework. KPAlign improves the framework's performance by a large margin while still keeping the framework end-to-end trainable. With the only post-processing non-maximum suppression (NMS), our proposed framework can detect multi-person keypoints with or without bounding-boxes in a single shot. Experiments demonstrate that the end-to-end paradigm can achieve competitive or better performance than previous strong baselines of both bottom-up and top-down methods. We hope that our end-to-end approach can provide a new perspective for the human pose estimation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-person pose estimation (a.k.a. keypoint detection) is a crucial step in the understanding of human behavior in images and videos. Previous methods for the task can be roughly categorized into bottom-up <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">19,</ref><ref type="bibr" target="#b24">21,</ref><ref type="bibr" target="#b26">23]</ref> and top-down [9, <ref type="bibr" target="#b29">26,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b2">3]</ref> methods. Bottom-up methods first detect all the possible keypoints in an input image in an instance-agnostic fashion, which are followed by a grouping or assembling process to produce the final instanceaware keypoints. The grouping process is often heuristic and many tricks are involved to achieve a good performance. In contrast, top-down methods first detect each individual instance with a bounding-box and then reduce the task to single-instance keypoint detection. Although top- * The first two authors equally contributed to this work. Corresponding author: chunhua.shen@adelaide.edu.au</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Final Predictions (x 0 , y 0 ) (x 1 , y 1 ) (x k-2 , y k-2 ) (x k-1 , y k-1 )</p><p>… <ref type="figure">Figure 1</ref> -The naive direct end-to-end keypoint detection framework. As shown in the figure, the framework requires a single feature vector on the final feature maps to encode all the essential information of an instance (e.g., the precise locations of some keypoints for the instance, denoted as (x0, y0), ...(x k−1 , y k−1 )). As shown in experiments, this single feature vector faces difficulty in preserving sufficient information for challenging instance-level recognition tasks such as keypoint detection. Here we propose a keypoint alignment (KPAlign) module to overcome the issue.</p><p>down methods can avoid the heuristic grouping process, they come with the price of long computational time since they cannot fully leverage the sharing computation mechanism of convolutional neural networks (CNNs). Moreover, the running time of top-down methods depends on the number of instances in the image, making them unreliable in some instant applications such as autonomous vehicles. Importantly, both bottom-up and top-down methods are not end-to-end 1 , which is in conflict with deep learning's philosophy of learning everything together.</p><p>Recently, anchor-free object detection <ref type="bibr" target="#b31">[28,</ref><ref type="bibr" target="#b12">11]</ref> is emerging and has demonstrated superior performance than previous anchor-based object detection. These anchor-free object detectors directly regress two corners of a target bounding-box, without using pre-defined anchor boxes. <ref type="figure">Figure 2</ref> -The proposed direct end-to-end multi-person pose estimation framework. The framework shares a similar architecture with one-stage object detectors such as FCOS <ref type="bibr" target="#b31">[28]</ref> but the bounding-box branch is replaced with a keypoint branch. KPAlign: the proposed keypoint alignment module, as described in Sec. 2.2. Heatmaps: the branch for jointly heatmap-based learning and will be removed when testing. Keypoints: the branch for keypoint detection. Classification is from FCOS and used to classify the locations on the feature maps into "person" or "not person". Center-ness is also from FCOS and predicts how far the current location is from the center of its target object.</p><p>The straightforward and effective solution for object detection gives rise to a question: can keypoint detection be solved with this simple framework as well? It is easy to see that the keypoints for an instance can be considered as a special bounding-box with more than two corner points, and thus the task could be solved by attaching more output heads to the object detection networks. This solution is intriguing since 1) it is end-to-end trainable (i.e., directly mapping a raw input image to the desired instance-aware keypoints). 2) It can avoid the shortcomings of both topdown and bottom-up methods as it needs neither grouping or bounding-box detection. 3) It can unify object detection and keypoint detection in a single simple and elegant framework.</p><p>However, we show that such a naive approach performs unsatisfactorily, mainly due to the fact that these object detectors resort to a single feature vector to regress all the keypoints of interest for an instance, with the hope that the single feature vector can faithfully preserve the essential information (e.g., the precise locations of all the keypoints) in its receptive field, as shown in <ref type="figure">Fig. 1</ref>. While the single feature vector may be sufficiently good to carry information for simple bounding-box detection as shown in <ref type="bibr" target="#b31">[28]</ref>, where only two corner points are involved in a bounding-box, it has difficulties in encoding rich information for the more challenging keypoint detection. As shown in our experiments, this straightforward approach yields inferior performance.</p><p>In this work, we propose a keypoint alignment (KPAlign) mechanism to largely overcome the aforementioned problem of the solution. Instead of using a single feature vector to regress all the keypoints for an instance, the proposed KPAlign aligns the convolutional features with a target keypoint (or a group of keypoints) as possible as it can, and then predicts the location of the target keypoint(s) with the aligned features. Since the target keypoints and the used features are roughly aligned, the features are only required to encode the information in its neighborhood. It is evident that encoding the neighborhood is much easier than encoding the whole receptive field, which thus results in an improved performance. Moreover, the KPAlign module is differentiable, thus keeping the model end-to-end trainable. Additionally, it is well-known that learning a regressionbased model is difficult. However, in this work, we find the regression task can largely benefit from a heatmap-based learning. As a result, we propose to jointly learn the two tasks during training. When testing, the heatmap-based branch is disabled and thus does not impose any overheads to the framework.</p><p>To summarize, the proposed one-stage regression-based keypoint detection enjoys the followings advantages over previous top-down or bottom-up approaches.</p><p>• The proposed framework is direct, totally end-to-end trainable. To predict, it maps an input image to keypoints for each individual instance directly, relying on neither intermediate operators like RoI feature cropping, nor grouping post-processing, which sets our work apart from previous frameworks [9, 1] with multiple steps. • Our proposed framework can bypass the major shortcomings of both top-down and bottom-up methods. For example, compared to top-down methods, our framework can avoid the issue of early commitment and decouple computational complexity from the number of instances in an input image. Compared to bottom-up methods, our framework eliminates the heuristic post-processing assembling the detected keypoints into full-body instances. • Moreover, unlike previous top-down and bottom-up methods, both of which require a heatmap-based FCNs to detect keypoints and thus are with quantization error, our proposed framework directly regresses the precise coordinates of keypoints and thus decouple the output resolution of the networks and the precision of keypoint localization. It makes our framework have the potential to detect very dense keypoints (i.e., the keypoints crowd together). • Finally, the framework suggests that the keypoint detection task can also be solved with the same methodology as bounding-box detection (i.e., directly regressing all the keypoints or the corners of boundingboxes), resulting in a unifying framework for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Top-down Methods: Top-down methods <ref type="bibr" target="#b29">[26,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b27">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">30,</ref><ref type="bibr" target="#b1">2]</ref> break the multi-person pose estimation task into two sub-tasks -person detection and single-person pose estimation. The person detection predicts a boundingbox for each instance in the input image. Next, the instance is cropped from the original image and a singleperson pose estimation is applied to predict the keypoints for the cropped instance. Moreover, some approaches such as Mask R-CNN [9] crop convolutional features rather than raw images, improving the efficiency of these methods. Top-down methods often have better performance but have higher computational complexity as it needs to repeatedly run the single-person pose estimation for each instance. Moreover, it also suffers from early commitment. In other words, it is difficult for these methods to recover an instance if it is missing in detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-up Methods:</head><p>In contrast to top-down methods, which first identify individual instances by a detector, bottom-up methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b14">13]</ref> first detect all possible keypoints in an instance-agnostic fashion. Afterwards, a grouping process is employed to assemble these keypoints into full-body keypoints. Bottom-up methods can take advantage of the sharing convolutional computation, thus being faster than top-down methods. However, the grouping process is heuristic and involves many tricks and hyperparameters. Recently, a one-stage framework <ref type="bibr" target="#b23">[20]</ref> makes the grouping process simpler. Compared to this work, our end-to-end framework further reduces the design complexity of a human pose estimation framework by directly mapping an input image to the desired keypoints.</p><p>Additionally, both top-down and bottom-up methods requires multiple steps to obtain the final keypoint detection results. Some of the steps are non-differentiable and make these methods impossible to be trained in an end-to-end fashion, which is the major difference between our methods and previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head><p>Conceptually, our proposed keypoint detection framework is a simple extension of the anchor-free object detector FCOS <ref type="bibr" target="#b31">[28]</ref>, with one new output branch for keypoint detection. In this section, we first introduce FCOS detector and show how it can be extended to keypoint detection. Next, we illustrate our proposed KPAlign module, which allows the framework to leverage the feature-prediction alignment and improves the performance by a large margin. Finally, we present that how the jointly learning of the regressionbased task and a heatmap-based task can be used to further boost the precision of keypoint localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">End-to-End Multi-Person Pose Estimation</head><p>FCOS Detector: FCOS detector is a recently-proposed anchor-free object detector. Unlike previous detectors such as RetinaNet <ref type="bibr" target="#b17">[15]</ref> or Faster R-CNN <ref type="bibr" target="#b28">[25]</ref>, FCOS eliminates anchor boxes and directly regresses the target boundingboxes. It has been shown that FCOS can achieve even better performance than its anchor-based counterparts such as RetinaNet. To be specific, anchor-based detectors regard anchor boxes as training samples, which can be viewed as sliding windows on the input image. In contrast, FCOS views the pixels on the input image (or the locations on feature maps) as training samples, analogue to semantic segmentation. The pixels in a ground-truth box are viewed as positive samples and are required to regress the four offsets from the pixel (or location) to four boundaries of the ground-truth box (or equivalently the bounding-box's lefttop and right-bottom coordinates relative to the pixel). Otherwise, the pixels are negative samples. The archiecture of FCOS is shown in <ref type="figure">Fig. 2</ref> (changing the keypoint branch to a bounding-box branch and removing the heatmap prediction branch). FCOS shares a similar architecture with RetinaNet but has ∼ 9× less outputs.</p><p>Keypoint Representation: It is straightforward to extend the bounding-box representation of FCOS to a keypoint representation. Specfically, we increase the scalars that each pixel regresses from 4 to 2K, where K is the number of keypoints for each instance. Similarly, the 2K scalars denote the relative coordinates to the current pixel. In other words, we regard keypoints as a special bounding-box with K corner points. The locator is essentially a 3 × 3 convolution layer and predicts the rough locations of the keypoints. Next, the feature sampler samples feature vectors at these locations. Thus, the aligner can roughly align the features and the predicted keypoints. The predictor employs these aligned feature vectors to make the final keypoint predictions.</p><p>Our End-to-End Framework: The keypoint representation results in our naive keypoint detection archiecture. As shown in <ref type="figure">Fig. 2</ref>, it is implemented by applying a convolutional branch on all levels of the output feature maps of FPN <ref type="bibr" target="#b16">[14]</ref> (i.e., P 3 , P 4 , P 5 , P 6 and P 7 ). The downsampling ratios of these feature maps to the input image are 8, 16, 32, 64 and 128, respectively. Note that the parameters of the branch are shared between FPN levels as in the bounding-box detection branch of the FCOS detector. The output channels of the branch is 2K, where K is the number of keypoints for each instance. The original bounding-box branch can be kept for simultaneous keypoint and bounding-box detection. Moreover, for keypoint-only detection, it is worth noting that we only use keypoint annotations without bounding-boxes. However, during training, FCOS requires a bounding-box for each instance to determine a positive or negative label for each location on the FPN feature maps. Here, we employ the minimum enclosing rectangles of keypoints of the instances as pseudo-boxes for computing the training labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Keypoint Alignment (KPAlign) Module</head><p>We conduct preliminary experiments with the aforementioned naive keypoint detection framework. However, as shown in our experiments, it has inferior performance. We attribute the inferior performance to the lack of the alignment between the features and the predicted keypoints. Essentially, the naive framework makes use of a single feature vector at a location on the input feature maps to regress all the keypoints for an instance. As a result, the single feature vector is required to encode all the required information for the instance. This is difficult because many keypoints are far away from the center of the feature vector's receptive field and it has been shown in <ref type="bibr" target="#b21">[18]</ref> that the intensity of the feature's response decays quickly as the input signal deviates from the center of its receptive field. As shown in many FCN-based frameworks [9, 17], keeping the feature and prediction aligned is crucial to good performance. Thus the feature only needs to encode the information in a local patch, which is much easier.</p><p>In this work, we propose a keypoint alignment (KPAlign) module to recover the feature-prediction alignment in the framework. KPAlign is used to replace the convolutional layer for the final keypoint detection in the naive framework and take as input the same feature maps, denoted as F ∈ R H×W ×C , where C being 256 is the number of channels of the feature maps. Analogous to a convolution operation, KPAlign is densely slid through the input feature maps F. For simplicity, we take as an example a specific location (i, j) on F to illustrate how KPAlign works. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, KPAlign consists of two components -an aligner ζ and a predictor φ. The aligner consists of a locator and a feature sampler, and outputs the aligned feature vectors. The aligner can be formulated as,</p><formula xml:id="formula_0">o o o 0 , o o o 1 , ..., o o o K−1 , v v v 0 , v v v 1 , ..., v v v K−1 = ζ(F),<label>(1)</label></formula><p>where o o o t ∈ R 2 , produced by the locator in <ref type="figure" target="#fig_0">Fig. 3</ref>, is the location where the feature vector used to predict the t-th keypoint of an instance should be sampled. v v v t ∈ R C is the sampled feature vector. Note that the location o o o t is defined over R 2 and thus it can be fractional. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">9]</ref>, we make use of bilinear interpolation to compute the features at a fractional location. Additionally, the location is encoded as the coordinates relative to (i, j) and thus is translation invariant.</p><p>Next, the predictor φ takes the outputs of the aligner as inputs to predict the final coordinates of the keypoints. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, the predictor includes K convolution layers (i.e., one for each keypoint). Let us assume that we are looking for the t-th keypoint for the instance and let φ t denote the t-th convlutional layer in the predictor. φ t takes v v v t as input and predicts the coordinates of the t-th keypoint relative to the location where v v v t is sampled (i.e., o o o t ). Finally, the coordinates of the t-th keypoint, denoted as x x x t , are the sum of the two sets of coordinates. Formally,</p><formula xml:id="formula_1">x x x t = φ t (v v v t ) + o o o t , t = 0, 1, ..., K − 1.<label>(2)</label></formula><p>Note that the coordinates need to be re-scaled by the downsampling ratio of F. We omit the re-scaling operator here for simplicity. Note that all the operations in KPAlign module are differentiable and therefore the whole model can be trained in an end-to-end fashion with standard back-propagation, which sets our work apart from previous bottom-up or top-down keypoint detection frameworks such as CMU-Pose <ref type="bibr" target="#b0">[1]</ref> or Mask R-CNN <ref type="bibr">[9]</ref>. Being endto-end trainable also makes the locator be able to learn to localize the keypoints without explicit supervision, which is critically important to KPAlign.</p><p>Grouped KPAlign: The aforementioned KPAlign module is required to sample K feature vectors for K keypoints. This is actually not necessary because some keypoints (e.g., nose, eyes and ears) always populate in a local area. Therefore, we propose to group the keypoints and the keypoints in the same group will use the same feature vector, which reduces the number of sampled feature vectors from K to G and achieves a similar performance, where G is the number of groups.</p><p>Using Separate Convolutional Features: In the KPAlign described before, all of the keypoint groups use the feature maps F as the input. However, we find that the performance can be improved remarkably if we use separate feature maps for the G keypoint groups (i.e., using F t , t = 0, 1, ..., G−1).</p><p>In that way, the demand for the information encoded in a single F t can be further mitigated. In order to reduce the computational complexity, the number of channels of each F t is set as C 4 (i.e., from 256 to 64).</p><p>Where to Sample Features? For the sake of convenience, the sampler in the aforementioned aligner samples features on the input feature maps of the locator, and therefore the predictor and locator take as inputs the same feature maps. However, it is not reasonable as the locator and predictor require different levels of feature maps. The locator predicts the initial but imprecise locations for all the keypoints (or keypoint groups) of an instance and thus requires high-level features with a larger receptive field. In contrast, the predictor needs to make precise predictions but only for the keypoints in a local area because the features have been aligned by the aligner. As a result, the predictor prefers high-resolution low-level features with a smaller receptive field. To this end, we feed lower levels of feature maps into the sampler. Specifically, if a locator uses feature maps P L and P L is not the finest feature maps, the sampler will take P L−1 as the input. If P L is already the finest feature maps, the sampler will still sample on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Regularization from Heatmap Learning</head><p>It is well-known that regression-based tasks are difficult to learn <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">27]</ref> and have poor generalization. That is why almost all previous keypoint detection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b33">29,</ref><ref type="bibr" target="#b29">26,</ref><ref type="bibr" target="#b22">19]</ref> are based on heatmap prediction, which cast keypoint detection to a pixel-to-pixel prediction task. However, this reformulation makes an end-to-end training infeasible since it involves the non-differentiable transformation between the heatmap-based prediction and desired continuous keypoint coordinates. Therefore, an end-to-end keypoint detection framework has to be regression-based.</p><p>As a result, we need to seek a way that can make the regression-based task easier to learn and generalize. To this end, given the fact that heatmap-based learning is much easier, we use the heatmap-based prediction task as an auxiliary task. Thus, the heatmap-based task can serve as a hint for the regression-based task and thus can regularize the task. In our experiments, the jointly learning significantly boosts the performance of the regression-based task. Note that the heatmap-based task is only used as an auxiliary loss during training. It is removed when testing.</p><p>Heatmap Prediction: As shown in <ref type="figure">Fig. 2</ref>, the heatmap prediction task takes as input the FPN feature maps P 3 with downsampling ratio being 8. Afterwards, two 3 × 3 conv layers with channel being 128 are applied here, which are followed by another 3 × 3 conv layer with output channel being K for the final heatmap prediction, where K is the number of keypoints for each instance. Previous heatmap-based keypoint detection methods <ref type="bibr" target="#b0">[1]</ref> generate unnormalized Gaussian distribution centered at each keypoint and thus generate the heatmaps in a per-pixel regression fashion. In contrast, since our framework does not rely on the heatmap prediction when testing, we perform a per-pixel classification here for simplicity. Note that we make use of multiple binary classifiers (i.e., one-versus-all) and therefore the number of output channels is K instead of K + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth Heatmaps and Loss Function:</head><p>The ground-truth heatmaps are generated as follows. On the heatmaps, if a location is the nearest location to a keypoint with type t, the classification label for the location is set as t, where t ∈ {1, 2, ..., K}. Otherwise, the label is 0. 2 Finally, in order to overcome the imbalance between positive and negative samples, we use focal loss <ref type="bibr" target="#b17">[15]</ref> as the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Our experiments are conducted on human keypoint detection task of the large-scale benchmark COCO dataset <ref type="bibr" target="#b19">[16]</ref>. The dataset contains more than 250K person instances with 17 annotated keypoints. Following the common practice <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">9]</ref>, we use the COCO trainval35k split (57K images) for training and minival split (5K images) as validation for our ablation study. We report our main results on the test-dev split (20K images). Unless specified, we only make use of the human keypoint annotations without bounding-boxes. The performance is computed with Average Percision (AP) based on Object Keypoint Similarity (OKS).</p><p>Implementation Details: Unless specified, ResNet-50 <ref type="bibr" target="#b11">[10]</ref> is used as our backbone networks. We use two training schedules. The first is quick and used to train a fast prototype of our models in ablation experiments. Specifically, the models are trained with stochastic gradient descent (SGD) on 8 V100 GPUs for 25 epochs with a minibatch of 16 images. For the main results on test-dev split, we use a longer training schedule; the models are trained for 100 epochs with a mini-batch of 32 images. We set the initial learning rate to 0.01 and use a linear schedule base lr × (1 − iter max iter ) to decay it. Weight decay and momentum are set as 0.0001 and 0.9, respectively. We initialize our backbone networks with the weights pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. For the newly added layers, we initialize them as in <ref type="bibr" target="#b17">[15]</ref>. When training, the images are randomly resized and horizontally flipped with probability being 0.5, and the images are also randomly cropped into 800 × 800 patches. When testing, we run inference on the whole image and the testing images are resized to have its shorter side being 800 and their longer side less or equal to 1333. If bounding-box detection is available, NMS is applied to the detected bounding-boxes. Otherwise, we do NMS on the minimum enclosing rectangles of keypoints of the instances. The NMS threshold is set as 0.5 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline: the naive end-to-end framework</head><p>We first experiment with the naive end-to-end keypoint detection framework in <ref type="figure">Fig. 1</ref> by replacing the bounding-box head in FCOS with the keypoint detection head. Moreover, as described before, we use pseudo-boxes to compute the label for each location on FPN feature maps during training. As shown in <ref type="table" target="#tab_0">Table 1</ref>  <ref type="figure">Fig. 1</ref>. "w/ KPAlign † ": using the KPAlign module in the naive framework but disabling the aligner in it. "w/ KPAlign": using the full-featured KPAlign module.</p><p>obtain low performance (43.4% in AP kp ). As mentioned before, the low performance is due to the misalignment between the features and keypoint predictions. In the following experiments, we will show that our proposed KPAlign can overcome the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Keypoint alignment (KPAlign) module</head><p>In this section, we equip the above naive framework with our proposed KPAlign module. As shown in <ref type="figure">Fig. 2</ref>, KPAlign serves as the final prediction layer, which was a standard convolutional layer in the naive framework. As shown in <ref type="table" target="#tab_0">Table 1</ref>, KPAlign improves the keypoint detection performance by a large margin (more than 7 points in AP kp ). In order to demonstrate that the improvement is indeed due to the retained alignment between the features and keypoint predictions rather than other factors (e.g., slightly more network parameters), we conduct another experiment in which the aligner of KPAlign is disabled. In other words, the offsets predicted by the locator are ignored and thus all the keypoints of an instance are predicted with the same features as in the naive framework. As shown in <ref type="table" target="#tab_0">Table 1</ref>, without the aligner, the performance drops dramatically to 43.0% in AP kp , which is nearly the same as the performance of the naive framework. Therefore, it is safe to claim that the improvement is due to the retained alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Grouped KPAlign</head><p>As described before, it is not necessary to sample K (i.e., 17 on COCO) feature vectors (one feature vector per keypoint) as some keypoints are always together and thus can be predicted with the same feature vector. In this experiments, we divide the keypoints into 9 groups 3 , which reduces the number of the sampled feature vectors from 17 to 9 and makes the module faster. As shown in <ref type="table">Table 2</ref>  <ref type="table">Table 2</ref> -Ablation experiments on COCO minival for the design choices in KPAlign. "+ Grouped": using Grouped KPAlign, which has slightly better performance than the original but largely reduce the number of sampled feature vectors, thus being faster. "+ Sep. features": using separate (but slimmer) feature maps for different keypoint groups, which has similar computational complexity but improved performance. "+ Better sampling": the predictor samples features on finer feature maps (i.e., from PL to PL−1).</p><p>use the Grouped KPAlign for all the following experiments. We also attempted other ways forming the groups but they achieve a similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Using separate convolutional features</head><p>As described before, using separate feature maps for these keypoint groups can improve the performance. Here we conduct an experiment for this. As shown in <ref type="table">Table 2</ref>, using separate feature maps can boost the performance by 0.8% in AP kp (from 50.6% to 51.4%). Note that the number of channels of these separate feature maps is reduced from 256 to 64, and thus the model has similar computational complexity to the original one. As a result, the model using separate feature maps achieves a better trade-off between speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Where to sample features in KPAlign?</head><p>In this experiment, the sampler samples on finer feature maps, as described in Sec. 2.2, since the locator requires low-resolution high-level feature maps with a larger receptive field while the predictor prefers high-resolution lowlevel feature maps. As shown in <ref type="table">Table 2</ref> ("+ Better Sampling"), using the sampling strategy can improve the performance to 52.2%. Note that using the sampling strategy does not increase the computational complexity of the model. Moreover, the better sampling strategy improves the AP kp 50 and AP kp 75 by 0.1% and 1.0%, respectively, which implies that the sampling strategy can result in more accurate keypoint predictions because the improvement mainly comes from the AP kp s at higher thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Regularization from heatmap learning</head><p>As shown in <ref type="table">Table 3</ref> ("w/ 8× Heatmaps"), by jointly learning the regression-based model with a heatmap prediction task, the performance of the regression-based task can be largely improved from 52.2% to 58.0%. Note that the heatmap prediction is only used during training to provide the multi-task regularization. Moreover, we also conduct experiments with the heatmap prediction with a lower resolution (i.e., "w/ 16× Heatmaps"). As shown in <ref type="table">Table 3</ref>, even with the low-resolution heatmaps, the model can still yield a similar performance. This suggests that our method is not sensitive to the design choices for the heatmap learning and thus eliminates the heuristic tuning for the heatmap branch. This sets our method apart from previous heatmap-based bottom-up methods, whose performance highly depends on the design of the heatmap branch (e.g., the heatmaps' resolution and etc.). Moreover, we find that our method is highly under-fitting and previous methods such as <ref type="bibr" target="#b29">[26]</ref> with heatmaps learning are trained with much more epochs than ours, and therefore we increase the number of epochs from 25 to 100. As shown in <ref type="table">Table 3</ref>, this improves the performance by 5.1% in AP kp .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Combining with Bounding Box Detection</head><p>As mentioned before, by simply adding a bounding-box branch, the proposed framework can simultaneously detect bounding boxes and keypoints. Here we confirm it by the experiment. The bounding-box detection is implemented by adding the original box detection head of FCOS to the framework. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparisons with State-of-the-art Methods</head><p>In this section, we evaluate the proposed end-to-end keypoint detection framework on MS-COCO test-dev split and compare it with previous bottom-up and top-down ones. We make use of the best model in ablation experiments. As shown in <ref type="table">Table 5</ref>, without any bells and whistles (e.g., multi-scale and flipping testing, the refining in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">19]</ref>, and any other tricks), the end-to-end framework achieves 62.2% and 63.3% in AP kp on COCO test-dev split, with ResNet-50 and ResNet-101 as the backbone, respectively. With multi-scale testing, our framework can achieve 63.0% and 64.8% with ResNet-50 and ResNet-101, respectively. Qualitative results will be provided in the supplemental material.</p><p>Compared to Bottom-up Methods: The performance of our ResNet-50 based end-to-end framework is better (62.2% vs. 61.8%) than the strong baseline CMU-Pose <ref type="bibr" target="#b0">[1]</ref> that uses multi-scale testing and post-processing with CPM <ref type="bibr" target="#b33">[29]</ref>, and filters the results with an object detector. Our framework also achieves much better performance than the bottom-up method AE <ref type="bibr" target="#b22">[19]</ref> (63.3% vs. 56.6%) and is even better than the method with refining. Compared to Per-sonLab, with the same backbone ResNet-101 and singlescale testing, our proposed framework also has a competitive performance with it (63.3% vs. 65.5%). Note that our proposed framework is much simpler than these bottom-up methods, in both training and testing.</p><p>Compared to Top-down Methods: With the same backbone ResNet-50, the proposed method has a similar performance with previous strong baseline Mask R-CNN (62.2% vs. 62.7%). Our model is still behind other top-down methods. However, it is worth noting that these methods often employ a separate bounding-box detector to obtain person instances. These instances are then cropped from the original image and a single person pose estimation method is separately applied to each the cropped image to obtain the final results. As noted before, this strategy is slow as it cannot take advantage of the sharing computation mechanism in CNNs. In contrast, our proposed end-to-end framework is much simpler and faster since it directly maps the raw input images to the final instance-aware keypoint detections with a fully convolutional network.</p><p>Timing: The averaged inference time of our model on COCO minival split is 74ms and 87ms per image with ResNet-50 and ResNet-101, respectively, which is slightly faster than Mask R-CNN with the same hardware and backbones (Mask R-CNN takes 78ms per image with ResNet-50). Additionally, the running time of Mask R-CNN depends on the number of the instances while our model, similar to one-stage object detectors, has nearly constant inference time for any number of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">More Discussions and Results</head><p>Here 1) we further compare our proposed DirectPose against the recent SPM method <ref type="bibr" target="#b23">[20]</ref>. 2) We show the visualization results of the proposed KPAlign module. 3) The loss curves of training with or without the proposed heatmap learning are shown as well. 4) We show some final detection results with or without the simultaneous bounding-box detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison to SPM</head><p>Here we highlight the difference between our proposed DirectPose and SPM <ref type="bibr" target="#b23">[20]</ref>. SPM makes use of Hierarchical Structured Pose Representations (Hierarchical SPR) to avoid learning the long-range displacements between the root and the keypoints, which shares a similar motivation with the proposed KPAlign. However, SPM considers all the key nodes (including the root nodes and intermediate nodes) in the hierarchical SPR as the regression targets, and instance-agnostic heatmaps are used to predict these nodes. This is similar to OpenPose <ref type="bibr" target="#b0">[1]</ref> with the only exception that SPM predicts these nodes instead of the final keypoints, and thus the predicted nodes are also instance-agnostic. As a result, SPM still needs a grouping post-processing to assemble the detected nodes into full-body poses. In contrast, the proposed KPAlign only requires the coordinates of the final keypoints as the supervision, and aligns the features and the predicted keypoints in an unsupervised fashion. Hence, our proposed framework can directly predict the desired instance-aware keypoints, without the need for any form of grouping post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visualization of KPAlign</head><p>The visualization results of KPAlign are shown in <ref type="figure">Fig. 5</ref>. As shown in the figure, the proposed KPAlign can make use of the features near the keypoints to predict them. Thus, the feature vectors can avoid encoding the keypoints far from their spatial location, which results in improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Losses of using Heatmap Learning</head><p>In order to demonstrate the impact of the heatmap learning, we plot the loss curves of training with or without the heatmap learning in <ref type="figure" target="#fig_1">Fig. 4</ref>. As shown in the figure, the heatmap learning can greatly help the training of the model and make the model achieve a much lower loss value, thus resulting in much better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of Keypoint Detections</head><p>We show more visualization results of DirectPose in <ref type="figure">Fig. 6</ref>. As shown in the figure, the proposed Direct-Pose can directly detect all the desired instance-aware keypoints without the need for the grouping post-processing or bounding-box detection. The results of the proposed Di-rectPose with simultaneous bounding-box detection are also shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 -</head><label>3</label><figDesc>The proposed keypoint detection framework with the Keypoint Alignment (KPAlign) module. Feature pyramid networks (FPNs) are not shown here. The aligner consists a locator and a sampler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 -</head><label>4</label><figDesc>The loss curves of training with or without the heatmap learning. As shown in the figure, with the heatmap learning, the model can achieve a significantly lower loss value and thus much better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>, the naive framework can only AP kp AP kp Ablation experiments on COCO minival for the proposed KPAlign module. Baseline: the naive keypoint detection framework, as shown in</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell>Baseline</cell><cell>43.4</cell><cell>73.8</cell><cell>45.1</cell><cell>38.9</cell><cell>50.9</cell></row><row><cell>w/ KPAlign  †</cell><cell>43.0</cell><cell>74.2</cell><cell>43.9</cell><cell>39.0</cell><cell>49.6</cell></row><row><cell>w/ KPAlign</cell><cell>50.5</cell><cell>77.6</cell><cell>54.9</cell><cell>44.4</cell><cell>60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AP kp AP kp</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell>Baseline</cell><cell>52.2</cell><cell>78.3</cell><cell>56.6</cell><cell>46.3</cell><cell>61.7</cell></row><row><cell>w/ 16× Heatmap</cell><cell>57.7</cell><cell>82.8</cell><cell>63.1</cell><cell>51.8</cell><cell>66.9</cell></row><row><cell>w/ 8× Heatmap</cell><cell>58.0</cell><cell>82.5</cell><cell>63.3</cell><cell>52.7</cell><cell>66.6</cell></row><row><cell>+ Longer sched.</cell><cell>63.1</cell><cell>85.6</cell><cell>68.8</cell><cell>57.7</cell><cell>71.3</cell></row><row><cell cols="6">Table 3 -Ablation experiments on COCO minival for Direct-</cell></row><row><cell cols="6">Pose with heatmap prediction. Baseline: without the heatmap</cell></row><row><cell cols="6">learning. "16× Heatmaps": predicting the heatmaps with</cell></row><row><cell cols="6">downsampling ratio being 16. "8× Heatmaps": predicting the</cell></row><row><cell cols="6">heatmaps with downsampling ratio being 8 (i.e., using P3). "+</cell></row><row><cell cols="6">Long sched.": increasing the number of training epochs from 25</cell></row><row><cell cols="6">to 100. As shown in the table, learning with heatmap prediction</cell></row><row><cell cols="6">can largely improve the performance. Moreover, using the de-</cell></row><row><cell cols="6">sign choices of the heatmaps (e.g., the resolution) have a small</cell></row><row><cell cols="6">impact on the final performance, which is one of the advantages</cell></row><row><cell cols="5">of our framework over previous bottom-up methods.</cell><cell></cell></row><row><cell cols="2">w/ BBox AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP kp AP kp 50</cell><cell>AP kp 75</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.1</cell><cell>85.6</cell><cell>68.8</cell></row><row><cell>55.3</cell><cell>81.5</cell><cell>59.9</cell><cell>61.5</cell><cell>84.3</cell><cell>67.5</cell></row><row><cell cols="6">Table 4 -Our framework with person bounding-box detection</cell></row><row><cell cols="6">on COCO minival. The proposed framework can achieve rea-</cell></row><row><cell cols="6">sonable person detection results (55.3% in AP). As a reference,</cell></row><row><cell cols="6">the Faster R-CNN person detector in Mask R-CNN [9] achieves</cell></row><row><cell>53.7% in AP.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 ,Table 5 -</head><label>45</label><figDesc>our framework can Method AP kp AP kp The performance of our proposed end-to-end framework on COCO test-dev split. * and † respectively denote using refining and multi-scale testing. As shown in the table, the new end-to-end framework achieves competitive or better performance than previous strong baselines (e.g., Mask R-CNN and CMU-Pose). achieve a reasonable person detection performance, which is similar to the Faster R-CNN detector in Mask R-CNN (55.3% vs. 53.7%). Although Mask R-CNN can also simultaneously detect bounding-boxes and keypoints, we further unify the two tasks into the same methodology.</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>AP kp 75</cell><cell>AP kp M</cell><cell>AP kp L</cell></row><row><cell></cell><cell cols="3">Top-down Methods</cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [9]</cell><cell>62.7</cell><cell>87.0</cell><cell>68.4</cell><cell>57.4</cell><cell>71.1</cell></row><row><cell>CPN [3]</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell></row><row><cell>RMPE [6]</cell><cell>72.3</cell><cell>89.2</cell><cell>79.1</cell><cell>68.0</cell><cell>78.6</cell></row><row><cell>CFN [12]</cell><cell>72.6</cell><cell>86.1</cell><cell>69.7</cell><cell>78.3</cell><cell>64.1</cell></row><row><cell>HRNet-W48 [26]</cell><cell>75.5</cell><cell>92.5</cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell></row><row><cell></cell><cell cols="3">Bottom-up Methods</cell><cell></cell><cell></cell></row><row><cell>CMU-Pose  *  † [1]</cell><cell>61.8</cell><cell>84.9</cell><cell>67.5</cell><cell>57.1</cell><cell>68.2</cell></row><row><cell>AE [19]</cell><cell>56.6</cell><cell>81.8</cell><cell>61.8</cell><cell>49.8</cell><cell>67.0</cell></row><row><cell>AE  *</cell><cell>62.8</cell><cell>84.6</cell><cell>69.2</cell><cell>57.5</cell><cell>70.6</cell></row><row><cell>AE  *  †</cell><cell>65.5</cell><cell>86.8</cell><cell>72.3</cell><cell>60.6</cell><cell>72.6</cell></row><row><cell>PersonLab [21]</cell><cell>65.5</cell><cell>87.1</cell><cell>71.4</cell><cell>61.3</cell><cell>71.5</cell></row><row><cell>PersonLab  †</cell><cell>67.8</cell><cell>89.0</cell><cell>75.4</cell><cell>64.1</cell><cell>75.5</cell></row><row><cell cols="4">Direct End-to-end Methods</cell><cell></cell><cell></cell></row><row><cell>Ours (R-50)</cell><cell>62.2</cell><cell>86.4</cell><cell>68.2</cell><cell>56.7</cell><cell>69.8</cell></row><row><cell>Ours (R-50)  †</cell><cell>63.0</cell><cell>86.8</cell><cell>69.3</cell><cell>59.1</cell><cell>69.3</cell></row><row><cell>Ours (R-101)</cell><cell>63.3</cell><cell>86.7</cell><cell>69.4</cell><cell>57.8</cell><cell>71.2</cell></row><row><cell>Ours (R-101)  †</cell><cell>64.8</cell><cell>87.8</cell><cell>71.1</cell><cell>60.4</cell><cell>71.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here we mean 'direct end-to-end'; i.e., the model is trained end-toend with keypoint annotations solely during training, and for inference, the model is able to map an input to keypoints for each individual instance without box detection and grouping post-processing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Strictly speaking, the generated ground-truth is a set of binary labels, rather than the conventional real-valued heatmap. We slightly abuse the term here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">These groups respectively include (nose, left eye, right eye, left ear, right ear), (left shoulder, ), (left elbow, left wrist), (right shoulder, ), (right elbow, right wrist), (left hip, ), (left knee, left ankle), (right hip, ) and (right knee, right ankle).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have proposed the first direct end-to-end human pose estimation framework, termed DirectPose. Our proposed model is end-to-end trainable and can directly map a raw input image to the desired instance-aware keypoint detections within constant inference time, eliminating the need for the grouping post-processing in bottom-up methods or the bounding-box detection and RoI operations in top-down ones. We also proposed a keypoint alignment (KPAlign) module to overcome the major difficulty that is the lack of the alignment between the convolutional features and the predictions in the end-to-end model, significantly improving the keypoint detection performance. Additionally, we further improve the regression-based task's performance by jointly learning it with a heatmap-based task. Experiments demonstrate that the new end-to-end method can obtain competitive or better performance than previous bottom-up and top-down methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial PoseNet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3582" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Intell. &amp; Statistics</title>
		<meeting>Int. Conf. Artificial Intell. &amp; Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Locator&apos;s Outputs Final Detection Ground-Truth Locator&apos;s Outputs Final Detection Ground-Truth</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The orange point denotes the original location where the features will be used if KPAlign is not used. The second image shows the final keypoint detection results. As shown in the figure, the proposed KPAlign can make use of the features near the keypoints to predict them. The final image shows that the ground-truth keypoints</title>
	</analytic>
	<monogr>
		<title level="m">Figure 5 -Visualization results of KPAlign on MS-COCO minival</title>
		<imprint/>
	</monogr>
	<note>The first image in each group shows the outputs of the locator in KPAlign (i.e., the locations where the sampler samples the features used to predict the keypoints). Zoom in for a better look</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Figure 6 -Visualization results of the proposed DirectPose on MS-COCO minival. DirectPose can directly detect a wide range of poses. Note that some small-scale people do not have ground-truth keypoint annotations in the training set of MS-COCO, thus they might be missing when testing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3178" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<title level="m">FCOS: Fully convolutional one-stage object detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Figure 7 -Visualization results of the proposed DirectPose with the simultaneous bounding-box detection on MS-COCO minival</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4724" to="4732" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
