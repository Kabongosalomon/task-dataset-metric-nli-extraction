<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human pose estimation · 3D from a single image</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering multi-person 3D poses with absolute scales from a single RGB image is a challenging problem due to the inherent depth and scale ambiguity from a single view. Addressing this ambiguity requires to aggregate various cues over the entire image, such as body sizes, scene layouts, and inter-person relationships. However, most previous methods adopt a top-down scheme that first performs 2D pose detection and then regresses the 3D pose and scale for each detected person individually, ignoring global contextual cues. In this paper, we propose a novel system that first regresses a set of 2.5D representations of body parts and then reconstructs the 3D absolute poses based on these 2.5D representations with a depth-aware part association algorithm. Such a single-shot bottom-up scheme allows the system to better learn and reason about the inter-person depth relationship, improving both 3D and 2D pose estimation. The experiments demonstrate that the proposed approach achieves the state-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is applicable to in-the-wild videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed an increasing trend of research on monocular 3D human pose estimation because of its wide applications in augmented reality, human-computer interaction, and video analysis. This paper aims to address the problem of estimating absolute 3D poses of multiple people simultaneously from a single RGB image. Compared to the single-person 3D pose estimation problem that focuses on recovering the root-relative pose, i.e., the 3D locations of human-body keypoints relative to the root of the skeleton, the task addressed here additionally needs to recover the 3D translation of each person in the camera coordinate system.</p><p>While there has been remarkable progress in recovering the root-relative 3D pose of a single person from an image <ref type="bibr" target="#b41">[36,</ref><ref type="bibr" target="#b23">18,</ref><ref type="bibr" target="#b17">12,</ref><ref type="bibr" target="#b13">8]</ref>, it was not until recently that more attention was paid to the multi-person case. Most existing methods for multi-person 3D pose estimation extend the single-person approach with a separate stage to recover the absolute position of each detected person separately. They either use another neural network to regress the 3D translation of the person from the cropped image <ref type="bibr" target="#b27">[22]</ref> or compute it based on the prior about the body size <ref type="bibr" target="#b11">[6,</ref><ref type="bibr" target="#b46">41,</ref><ref type="bibr" target="#b47">42]</ref>, which ignore the global context of the whole image. Another line of work tries to recover body positions with a ground plane constraint <ref type="bibr" target="#b24">[19]</ref>, but this approach assumes that the feet are visible, which is not always true, and accurate estimation of ground plane geometry from a single image is still an open problem.</p><p>We argue that the robust estimation of global positions of human bodies requires to aggregate the depth-related cues over the whole image, such as the 2D sizes of human bodies, the occlusion between them, and the layout of the scene. Recent advances in monocular depth estimation have shown that convolutional neural networks (CNNs) are able to predict the depth map from an RGB image <ref type="bibr" target="#b21">[16,</ref><ref type="bibr" target="#b18">13]</ref>, which is particularly successful on human images <ref type="bibr" target="#b20">[15]</ref>. This observation motivates us to directly learn the depths of human bodies from the input image instead of recovering them in a post-processing stage.</p><p>To this end, we propose a novel single-shot bottom-up approach to multiperson 3D pose estimation, which predicts absolute 3D positions and poses of multiple people in a single forward pass. We regress the root depths of human bodies in the form of a novel root depth map, which only requires 3D pose annotations as supervision. We train a fully convolutional network to regress the root depth map, as well as 2D keypoint heatmaps, part affinity fields (PAFs), and part relative-depth maps that encode the relative depth between two joints of each body part. Then, the detected 2D keypoints are grouped into individuals based on PAFs using a part association algorithm, and absolute 3D poses are recovered with the root depth map and part relative-depth maps. The whole pipeline is illustrated in <ref type="figure">Fig. 2</ref>.</p><p>We also show that predicting depths of human bodies is beneficial for the part association and 2D pose estimation. We observe that many association errors occur when two human bodies overlap in the image. Knowing the depths of them allows us to reason about the occlusion between them when assigning the detected keypoints. Moreover, from the estimated depth, we can infer the spatial extent of each person in 2D and avoid linking two keypoints with an unreasonable distance. With these considerations, we propose a novel depth-aware part association algorithm and experimentally demonstrate its effectiveness.</p><p>To summarize, the contributions of this work are:</p><p>-A single-shot bottom-up framework for multi-person 3D pose estimation, which can reliably estimate absolute positions of multiple people by leveraging depth-relevant cues over the entire image. -A depth-aware part association algorithm to reason about inter-person occlusion and bone-length constraints based on predicted body depths, which also benefits 2D pose estimation. -The state-of-the-art performance on public benchmarks, with the generalization to in-the-wild images and the flexibility for both whole-body and half-body pose estimation. The code, demonstration videos and other supplementary material are available at https://zju3dv.github.io/SMAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Multi-person 2D pose. Existing methods for multi-person 2D pose estimation can be approximately divided into two classes. Top-down approaches detect human first and then estimate keypoints with a single person pose estimator <ref type="bibr" target="#b10">[5,</ref><ref type="bibr" target="#b12">7,</ref><ref type="bibr" target="#b32">27,</ref><ref type="bibr" target="#b44">39]</ref>. Bottom-up approaches localize all keypoints in the image first and then group them to individuals <ref type="bibr" target="#b8">[3,</ref><ref type="bibr" target="#b14">9,</ref><ref type="bibr" target="#b28">23,</ref><ref type="bibr" target="#b36">31,</ref><ref type="bibr" target="#b15">10,</ref><ref type="bibr" target="#b31">26,</ref><ref type="bibr" target="#b30">25]</ref>. Cao et al. <ref type="bibr" target="#b8">[3]</ref> propose Open-Pose and use part affinity fields (PAFs) to represent the connection confidence between keypoints. They solve the part association problem with a greedy strategy. Newell et al. <ref type="bibr" target="#b28">[23]</ref> propose an approach that simultaneously outputs detection and group assignments in the form of pixel-wise tags.</p><p>Single-person 3D pose. Researches on single-person 3D pose estimation from a single image have already achieved remarkable performances in recent years. One-stage approaches directly regress 3D keypoints from images and can leverage shading and occlusion information to resolve the depth ambiguity. Most of them are learning-based <ref type="bibr" target="#b6">[1,</ref><ref type="bibr" target="#b41">36,</ref><ref type="bibr" target="#b33">28,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b40">35,</ref><ref type="bibr" target="#b45">40,</ref><ref type="bibr" target="#b13">8,</ref><ref type="bibr" target="#b26">21]</ref>. Two-stage approaches estimate the 2D pose first and then lift it to the 3D pose, including learning-based <ref type="bibr" target="#b23">[18,</ref><ref type="bibr" target="#b35">30,</ref><ref type="bibr" target="#b48">43]</ref>, optimization-based <ref type="bibr" target="#b49">[44,</ref><ref type="bibr" target="#b43">38]</ref> and exemplar-based <ref type="bibr" target="#b9">[4]</ref> methods, which can benefit from the reliable result of 2D pose estimation.</p><p>Multi-person 3D pose. For the task of multi-person 3D pose estimation, topdown approaches focus on how to integrate the pose estimation task with the detection framework. They crop the image first and then regress the 3D pose with a single-person 3D pose estimator. Most of them estimate the translation of each person with an optimization strategy that minimizes the reprojection error computed over sparse keypoints <ref type="bibr" target="#b11">[6,</ref><ref type="bibr" target="#b38">33,</ref><ref type="bibr" target="#b39">34]</ref> or dense semantic correspondences <ref type="bibr" target="#b46">[41]</ref>. Moon et al. <ref type="bibr" target="#b27">[22]</ref> regard the area of 2D bounding box as a prior and adopt a neural network to learn a correction factor. In their framework, they regress the rootrelative pose and the root depth separately. Informative cues for inferring the interaction between people may lose during the cropping operation. Another work <ref type="bibr" target="#b42">[37]</ref> regresses the full depth map based on the existing depth estimation framework <ref type="bibr" target="#b21">[16]</ref>, but their 'read-out' strategy is not robust to 2D outliers. On the other hand, bottom-up approaches focus on how to represent pose annotations as several maps in a robust way <ref type="bibr" target="#b25">[20,</ref><ref type="bibr" target="#b7">2,</ref><ref type="bibr" target="#b47">42,</ref><ref type="bibr" target="#b24">19]</ref>. However, they either optimize the translation in a post-processing way or ignore the task of root localization. XNect <ref type="bibr" target="#b24">[19]</ref> extends the 2D location map in <ref type="bibr" target="#b26">[21]</ref> to 3D ones and estimates the translation with a calibrated camera and the ground plane constraint, but the feet may be invisible in crowded scenes and obtaining the extrinsic parameters of the camera is not practical in most applications. Another line of work tries to recover the SMPL model <ref type="bibr" target="#b47">[42,</ref><ref type="bibr" target="#b46">41]</ref>, and their focus lies in using scene constraints and avoiding interpenetration, which is weakly related to our task. Taking these factors into consideration, a framework that both considers recovering the translation in a single forward pass and aggregating global features over the image will be helpful to this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular depth estimation.</head><p>Depth estimation from a single view suffers from inherent ambiguity. Nevertheless, several methods make remarkable advances in recent years <ref type="bibr" target="#b21">[16,</ref><ref type="bibr" target="#b18">13]</ref>. Li et al. <ref type="bibr" target="#b20">[15]</ref> observe that Mannequin Challenge could be a good source for human depth datasets. They generate training data using multi-view stereo reconstruction and adopt a data-driven approach to recover a dense depth map, achieving good results. However, such a depth map lacks scale consistency and cannot reflect the real depth. bone-length constraint. Based on these results, the absolute 3D pose of each person can be reconstructed with a camera model. Individual modules of our system are introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intermediate representations</head><p>Given the input image, SMAP regresses the following intermediate representations, based on which 3D poses will be reconstructed: Root depth map. As the number of people in an input image is unknown, we propose to represent the absolute depths for all human bodies in the image by a novel root depth map. The root depth map has the same size as the input image. The map values at the 2D locations of root joints of skeletons equal their absolute depths. An example is shown in <ref type="figure">Fig. 2</ref>. In this way, we are able to represent the depths of multiple people without predefining the number of people. During training, we only supervise the values of root locations. The proposed root depth map can be learned together with other cues within the same network as shown in <ref type="figure">Fig. 2</ref> and only requires 3D poses (instead of full depth maps) as supervision, making our algorithm very efficient in terms of both model complexity and training data.</p><p>It is worth noting that visual perception of object scale and depth depends on the size of field of view (FoV), i.e., the ratio between the image size and the focal length. If two images are obtained with different FoVs, the same person at the same depth will occupy different proportions in these two images and seem to have different depths for the neural network, which may mislead the learning of depth. Thus, we normalize the root depth by the size of FoV as follows:</p><formula xml:id="formula_0">Z = Z w f ,<label>(1)</label></formula><p>where Z is the normalized depth, Z is the original depth, and f and w are the focal length and the image width both in pixels, respectively. So w/f is irrelevant to image resolution, but equals to the ratio between the physical size of image sensor and the focal length both in millimeters, i.e., FoV. The normalized depth values can be converted back to metric values in inference.</p><p>Keypoint heatmaps. Each keypoint heatmap indicates the probable locations of a specific type of keypoints for all people in the image. Gaussian distribution is used to model uncertainties at the corresponding location.</p><p>Part affinity fields (PAFs). PAFs proposed in <ref type="bibr" target="#b8">[3]</ref> include a set of 2D vector fields. Each vector field corresponds to a type of body part where the vector at each pixel represents the 2D orientation of the corresponding body part.</p><p>Part relative-depth map. Besides the root depth, we also need depth values of other keypoints to reconstruct a 3D pose. Instead of predicting absolute depth values, for other keypoints we only regress their relative depths compared to their parent nodes in the skeleton, which are represented by part relative-depth maps. Similar to PAFs, each part relative-depth map corresponds to a type of body part, and every pixel that belongs to a body part encodes the relative depth between two joints of the corresponding body part. This dense representation provides rich information to reconstruct a 3D skeleton even if some keypoints are invisible.</p><p>Network architecture. We use Hourglass <ref type="bibr" target="#b29">[24]</ref> as our backbone and modify it to a multi-task structure with multiple branches that simultaneously output the above representations as illustrated in <ref type="figure">Fig. 2</ref>. Suppose the number of predefined joints is J. Then, there are 4J − 2 channels in total (heatmaps and PAFs: J + 2(J − 1), root depth map: 1, part relative-depth map: J − 1). Each output branch in our network only consists of two convolutional layers. Inspired by <ref type="bibr" target="#b19">[14]</ref>, we adopt multi-scale intermediate supervision. The L 1 loss is used to supervise the root depths and L 2 losses on other outputs. The effect of the network size and the multi-scale supervision will be validated in the experiments.  <ref type="bibr" target="#b8">[3]</ref>, and our results with depth-aware part association. The example in the first row shows the effect of ordinal prior. Under this circumstance, <ref type="bibr" target="#b8">[3]</ref> assigns the pelvis to the occluded person while we give priority to the front person. The example in the second row shows the effect of the adaptive bonelength threshold, indicated by the green circle. As a noisy response occurs at the right ankle of the person it doesn't belong to, <ref type="bibr" target="#b8">[3]</ref> will induce a false connection while our algorithm will not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth-aware part association</head><p>Given 2D coordinates of keypoints from keypoint heatmaps after non-maximum suppression, we need to associate detected joints with corresponding individuals. Cao et al. <ref type="bibr" target="#b8">[3]</ref> propose to link joints greedily based on the association scores given by PAFs. Basically, we follow their method to calculate association scores. However, PAFs scores might be unreliable due to occlusion between people. <ref type="figure" target="#fig_2">Fig. 3</ref> shows two examples where the above association strategy fails. We propose to leverage the estimated depth maps to address the part association ambiguities caused by inter-person occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordinal prior.</head><p>A key insight to solve the occlusion issue is to give priority to the unoccluded person when assigning joints. The occlusion status can be inferred from the depth map. Therefore, we sort root joints from near to far according to the predicted root depth, instead of following the order of PAFs scores. Our association process starts with the root and proceeds along the skeleton tree successively.</p><p>Adaptive bone-length constraint. To avoid linking two keypoints with an unreasonable distance, Cao et al. <ref type="bibr" target="#b8">[3]</ref> constrain the association with a threshold defined as half of the image size. However, such a fixed threshold is ineffective as the 2D bone length depends on its depth. We adopt an adaptive bone-length threshold to penalize the unreasonable association, depending on predicted body depths. For each part, we compute the mean bone length in the training set in advance. Then, its maximal length in 2D is computed and used as the distance threshold for the corresponding part:</p><formula xml:id="formula_1">d cons = λ · D bone · f Z · w = λ · D bone Z ,<label>(2)</label></formula><p>where D bone is the 3D average length of a limb, Z is the normalized root depth predicted by our network, and λ is a relaxation factor. d cons is used to filter unreasonable links. From Eq. 2 we can see that the adaptive threshold is not affected by camera intrinsic parameters or image resizing and is only related to the depth value estimated by our network and the statistical bone length.</p><p>As the association can benefit from depth information, we call it depth-aware part association. <ref type="figure" target="#fig_2">Fig. 3(d)</ref> shows our qualitative results. The proposed scheme will also be validated by experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">3D pose reconstruction</head><p>Reconstruction. Following the connection relations obtained from part association, relative depths from child nodes to parent nodes can be read from the corresponding locations of part relative-depth maps. With the root depth and relative depths of body parts, we are able to compute the depth of each joint. Given 2D coordinates and joint depths, the 3D pose can be recovered through the perspective camera model:</p><formula xml:id="formula_2">X, Y, Z T = ZK −1 x, y, 1 T ,<label>(3)</label></formula><p>where [X, Y, Z] T and [x, y] T are 3D and 2D coordinates of a joint respectively. K is the camera intrinsic matrix, which is available in most applications, e.g., from device specifications. In our experiments, we use the focal lengths provided by the datasets (same as <ref type="bibr" target="#b27">[22]</ref>). For internet images with unknown focal lengths, we use a default value which equals the input image width in pixels. Note that the focal length will not affect the predicted ordinal depth relations between people.</p><p>Refinement. The above reconstruction procedure may introduce two types of errors. One is the cumulative error in the process of joint localization due to the hierarchical skeleton structure, and the other is caused by back projection when the depth is not accurate enough to calculate X and Y coordinates of 3D pose. Moreover, severe occlusion and truncation frequently occur in crowded scenes, which make some keypoints invisible. Therefore, we use an additional neural network named RefineNet to refine visible keypoints and complete invisible keypoints for each 3D pose. RefineNet consists of five fully connected layers. The inputs are 2D pose and 3D root-relative pose while the output is the refined 3D root-relative pose. The coordinates of invisible keypoints in the input are set to be zero. Note that RefineNet doesn't change the root depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed approach on two widely-used datasets and compare it to previous approaches. Besides, we provide thorough ablation analysis to validate our designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>CMU Panoptic <ref type="bibr" target="#b16">[11]</ref> is a large-scale dataset that contains various indoor social activities, captured by multiple cameras. Mutual occlusion between individuals and truncation makes it challenging to recover 3D poses. Following <ref type="bibr" target="#b46">[41]</ref>, we choose two cameras (16 and 30), 9600 images from four activities (Haggling, Mafia, Ultimatum, Pizza) as our test set, and 160k images from different sequences as our training set. <ref type="bibr" target="#b25">[20]</ref>. MuCo-3DHP is an indoor multi-person dataset for training, which is composited from single-person datasets. MuPoTS-3D is a test set consisting of indoor and outdoor scenes with various camera poses, making it a convincing benchmark to test the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuCo-3DHP and MuPoTS-3D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We adopt Adam as optimizer with 2e-4 learning rate, and train two models for 20 epochs on the CMU Panoptic and MuCo-3DHP datasets separately, mixed with COCO data <ref type="bibr" target="#b22">[17]</ref>. The batch size is 32 and 50% data in each mini-batch is from COCO (same as <ref type="bibr" target="#b25">[20,</ref><ref type="bibr" target="#b27">22]</ref>). Images are resized to a fixed size 832×512 as the input to the network. Note that resizing doesn't change FoV. Since the COCO dataset lacks 3D pose annotations, weights of 3D losses are set to zero when the COCO data is fed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>MPJPE. MPJPE measures the accuracy of the 3D root-relative pose. It calculates the Euclidean distance between the predicted and the groundtruth joint locations averaged over all joints.</p><p>RtError. Root Error (RtError) is defined as the Euclidean distance between the predicted and the groundtruth root locations.</p><p>3DPCK. 3DPCK is the percentage of correct keypoints. A keypoint is declared correct if the Euclidean distance between predicted and groundtruth coordinates is smaller than a threshold (15cm in our experiments). PCK rel measures relative pose accuracy with root alignment; PCK abs measures absolute pose accuracy without root alignment; and PCK root only measures the accuracy of root joints. AUC means the area under curve of 3DPCK over various thresholds. <ref type="table" target="#tab_1">Table 1</ref>. Results on the Panoptic dataset. For <ref type="bibr" target="#b27">[22]</ref>, we used the code provided by the authors and trained it on the Panoptic dataset. *The average of <ref type="bibr" target="#b47">[42]</ref> is recalculated following the standard practice in <ref type="bibr" target="#b46">[41]</ref>, i.e., average over activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Haggling PCOD. We propose a new metric named the percentage of correct ordinal depth (PCOD) relations between people. The insight is that predicting absolute depth from a single view is inherently ill-posed, while consistent ordinal relations between people are more meaningful and suffice many applications. For a pair of people (i, j), we compare their root depths and divide the ordinal depth relation into three classes: closer, farther, and roughly the same (within 30cm). PCOD equals the classification accuracy of predicted ordinal depth relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4</head><p>Comparison with state-of-the-art methods CMU Panoptic.   MuPoTS-3D. We follow the protocol of <ref type="bibr" target="#b27">[22]</ref>. Additionally, PCK root and PCOD are used to evaluate the 3D localization of people. In terms of the absolute pose which we are more concerned with, it can be observed from <ref type="table" target="#tab_0">Table 2</ref> that our model is superior to <ref type="bibr" target="#b27">[22]</ref> in relevant metrics including PCK abs , PCK root and PCOD by a large margin. It also demonstrates that our model has higher PCK rel compared with all bottom-up methods and most top-down methods except <ref type="bibr" target="#b27">[22]</ref>. Note that we achieve higher AUC rel compared to <ref type="bibr" target="#b27">[22]</ref> for the relative 3D pose of matched people.</p><p>Comparison with top-down methods. We provide additional analysis to compare our single-shot bottom-up method to the state-of-the-art top-down method <ref type="bibr" target="#b27">[22]</ref>. <ref type="figure" target="#fig_3">Fig. 4</ref> shows thorough comparisons in terms of PCK root and PCK rel .</p><p>For root localization, we compare to two methods: 1) regressing the scale from each cropped bounding box using a neural network as in <ref type="bibr" target="#b27">[22]</ref> and 2) estimating the 3D translation by optimizing reprojection error with the groundtruth 2D pose and the estimated relative 3D pose from <ref type="bibr" target="#b27">[22]</ref> ('FitT' in <ref type="figure" target="#fig_3">Fig. 4</ref>). We achieve better PCK root over various thresholds than both of them. Notably, we achieve roughly 100% accuracy with a threshold 1m. As for relative pose estimation, <ref type="bibr" target="#b27">[22]</ref> achieves higher PCK rel (@15cm) as it adopts a separate off-the-shelf network <ref type="bibr" target="#b41">[36]</ref> that is particularly optimized for relative 3D pose estimation. Despite that, we obtain better PCK rel when the threshold is smaller and higher AUC rel . <ref type="figure" target="#fig_4">Fig. 5</ref> shows several scenarios (various poses, occlusion, and truncation) in which the top-down method <ref type="bibr" target="#b27">[22]</ref> may fail as it predicts the scale for each detected person separately and ignore global context. Instead, the proposed bottom-up design is able to leverage features over the entire image instead of only using cropped features in individual bounding boxes.</p><p>Furthermore, our running time and memory remain almost unchanged with the number of people in the image while those of <ref type="bibr" target="#b27">[22]</ref> grow faster with the number of people due to its top-down design, as shown in the supplementary material.</p><p>Depth estimation. Apart from our method, there are two alternatives for depth estimation: 1) regressing the full depth map rather than the root depth map. 2) using the cropped image as the input to the network rather than the whole image. For the first alternative, since there is no depth map annotation in existing multi-person outdoor datasets, we use the released model of the stateof-the-art human depth estimator <ref type="bibr" target="#b20">[15]</ref>, which is particularly optimized for human depth estimation trained on a massive amount of in-the-wild 'frozen people' videos. For the second alternative, <ref type="bibr" target="#b27">[22]</ref> is the state-of-the-art method that estimates root depth from the cropped image, so we compare with it. <ref type="figure" target="#fig_5">Fig. 6</ref> demonstrates scatter plots of the groundtruth root depth versus the predicted root depth of three methods on the MuPoTS-3D dataset. Ideally, the estimated depths should be linearly correlated to the ground truth, resulting a straight line in the scatter plot. Our model shows better consistency than baselines. Note that, while the compared methods are trained on different datasets, the images in the test set MuPoTS-3D are very different from the training images for all methods. Though not rigorous, this comparison is still reasonable to indicate the performance of these methods when applied to unseen images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation analysis</head><p>Architecture. <ref type="table" target="#tab_2">Table 3</ref> shows how different designs of our framework affect the multi-person 3D pose estimation accuracy: 1) the performance of our model will degrade severely without depth normalization. As we discussed in Section 3.  show that, even with a one-stage hourglass network, our method still achieves higher PCK root and PCK abs than the top-down method <ref type="bibr" target="#b27">[22]</ref>.</p><p>Part association. To compare the proposed depth-aware part association with the 2D part association in <ref type="bibr" target="#b8">[3]</ref>, we evaluate relevant metrics on Panoptic and MuPoTS-3D datasets. Note that the threshold of 2DPCK is the half of the head size. <ref type="table" target="#tab_3">Table 4</ref> lists the results (2DPA vs. DAPA) and reveals that our depthaware part association outperforms the 2D part association in all these metrics.</p><p>Besides, <ref type="figure" target="#fig_2">Fig. 3</ref> shows some qualitative examples.</p><p>RefineNet. <ref type="table" target="#tab_1">Table 1</ref> and 3 show that RefineNet is able to improve both relative and absolute pose estimation. It is able to complete invisible keypoints and refine visible keypoints with a learned 3D pose prior. The improvement is more significant on the Panoptic dataset since the training and test images are captured by cameras with similar views.</p><p>Please refer to the supplementary material for more experimental details and results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel single-shot bottom-up framework to estimate absolute multi-person 3D poses from a single RGB image. The proposed framework uses a fully convolutional network to regress a set of 2.5D representations for multiple people, from which the absolute 3D poses can be reconstructed. Additionally, benefited from the depth estimation of human bodies, a novel depth-aware part association algorithm was proposed and proven to benefit 2D pose estimation in crowd scenes. Experiments demonstrated state-of-the-art performance as well as generalization ability of the proposed approach.</p><p>Supplementary Material: SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation</p><p>In this supplementary material, we provide more experimental details and results. Additionally, qualitative results on in-the-wild images from the Internet are shown in the supplementary video.</p><p>1 More details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Loss function</head><p>There are three output branches of the network, illustrated in <ref type="figure">Fig. 2</ref>. The first branch regresses keypoint heatmaps H J and PAFs C simultaneously, while the second branch regresses part relative-depth maps H ∆Z . L 2 loss is applied to these two branches. The third branch predicts root depth map H RZ . According to 2D location of detected root (x root , y root ), we can get the predicted root depth H RZ (x root , y root ) and compared it with the groundtruth normalized depth Z * using L 1 loss. The total loss is computed by weighted summation of all losses. Our loss functions are as follows.</p><formula xml:id="formula_3">L total = w 2D · L 2D + w ∆Z · L ∆Z + w RZ · L RZ L 2D = N i=1 p ||H J,i (p) − H * J,i (p)|| 2 2 + 2N −2 i=1 p ||C i (p) − C * i (p)|| 2 2 L ∆Z = N −1 i=1 p ||H ∆Z,i (p) − H * ∆Z,i (p)|| 2 2 L RZ = M i=1 ||H RZ (x root i , y root i ) − Z * i || 1 ,</formula><p>where N , M are the number of joints, the number of detected people (root joints) respectively, p means each pixel location and superscript * denotes the groundtruth. The default settings are: w 2D =0.1, w ∆Z =5, w RZ =10. <ref type="table" target="#tab_4">Table 5</ref> provides detailed information about running time and memory of the state-of-the-art top-down method <ref type="bibr" target="#b27">[22]</ref> and our method. Note that our method is almost not affected by the number of people in the image. 2 More results compared with SOTA Due to the limited space, only the average PCK abs is reported in the main manuscript. Here we provide more thorough experimental results. <ref type="table">Table 8</ref> presents sequence-wise PCK abs on the MuPoTS-3D dataset and demonstrates that our PCK abs is higher than the state-of-the-art top-down method <ref type="bibr" target="#b27">[22]</ref>, especially for outdoor scenarios (TS6-TS20). <ref type="table">Table 7</ref> shows that our model has higher PCK rel compared with all bottom-up methods and most top-down methods except <ref type="bibr" target="#b27">[22]</ref>. Note that we have higher AUC rel compared with <ref type="bibr" target="#b27">[22]</ref> as we state in the main manuscript. <ref type="table" target="#tab_1">Table 10</ref> shows the results on the Human3.6M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Running time and memory</head><p>3 More ablation analysis 3.1 Effect of the multi-task structure SMAP simultaneously output 2D information (keypoint heatmaps and PAFs), root depth map, and part relative-depth map. To analyze the impact of our single-shot multi-task structure on root localization, we delete some of the output branches and evaluate the performance, as indicated in <ref type="table" target="#tab_1">Table 11</ref>. One variant is only to regress the root position and its depth alone (row 2 of <ref type="table" target="#tab_1">Table 11</ref>). This variant can obtain an acceptable result, which reflects the significance of our bottom-up design for root localization. Another variant which adds the keypoint heatmaps and PAFs branches (row 3 of <ref type="table" target="#tab_1">Table 11</ref>) significantly improves the performance, indicating that 2D cues (pose, body size) are also beneficial to root depth estimation. Nevertheless, this variant is still inferior to the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Influence of camera intrinsics</head><p>Here we make three comparisons:1) full model with known camera intrinsics. 2) full model without camera intrinsics. 3) without normalization.</p><p>RtError of our full model reaches 23.3cm on the MuPoTS-3D dataset. If the intrinsic parameter is not provided (use default intrinsics), RtError increases to 67cm. Note that the ordinal depth relation remains unchanged. If the model lacks normalization, RtError is as high as 120cm. <ref type="table">Table 6</ref>. Sequence-wise PCK abs on the MuPoTS-3D dataset for matched groundtruths. S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 Moon et al. <ref type="bibr" target="#b27">[22]</ref>  .5 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg. <ref type="bibr">Moon</ref>   <ref type="table">Table 7</ref>. PCK rel on the MuPoTS-3D dataset for matched groundtruths. 'T' denotes top-down methods while 'B' denotes bottom-up methods. <ref type="table">Table 9</ref>. PCK rel on the MuPoTS-3D dataset for all groundtruths. 'T' denotes topdown methods while 'B' denotes bottom-up methods. <ref type="table" target="#tab_1">Table 10</ref>. MPJPE Results on Human3.6M dataset. Note that there is no groundtruth bounding box information in inference time.</p><p>Method MPJPE Rogez et al. <ref type="bibr" target="#b38">[33]</ref> 87.7 Mehta et al. <ref type="bibr" target="#b25">[20]</ref> 69.9 Dabral et al. <ref type="bibr" target="#b11">[6]</ref> 65.2 Mehta et al. <ref type="bibr" target="#b24">[19]</ref> 63.6 Rogez et al. <ref type="bibr" target="#b39">[34]</ref> 63.5 Moon et al. <ref type="bibr" target="#b27">[22]</ref> 54.4 Ours 54.1 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Equal contribution. arXiv:2008.11469v1 [cs.CV] 26 Aug 2020 We propose a novel framework named SMAP to estimate absolute 3D poses of multiple people from a single RGB image. The figure visualizes the result of SMAP on an in-the-wild image. The proposed single-shot and bottom-up design allows SMAP to leverage the entire image to infer the absolute locations of multiple people consistently, especially in terms of the ordinal depth relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 2 .</head><label>22</label><figDesc>presents the pipeline of our approach, which consists of a single-shot bottom-up framework named SMAP. With a single RGB image as input, SMAP outputs 2D representations including keypoint heatmaps and part affinity fields<ref type="bibr" target="#b8">[3]</ref>. Additionally, it also regresses 2.5D representations including root depth map and part relative-depth maps, which encode depth information of human bodies. Then, a depth-aware part association algorithm is proposed to assign detected 2D keypoints to individuals, depending on an ordinal prior and an adaptive … Overview of the proposed approach. Given a single image, our single-shot network SMAP regresses several intermediate representations including 2D keypoint heatmaps, part affinity fields (PAFs), root depth map, and part relative-depth maps (Red means the child joint has a larger depth than its parent joint, and blue means the opposite). With a new depth-aware part association algorithm, body parts belonging to the same person are linked. With all these intermediate representations combined, absolute 3D poses of all people can be recovered. Finally, an optional RefineNet can be used to further refine the recovered 3D poses and complete invisible keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Depth-aware part association. From left to right: candidate links, part affinity fields, pose estimation results from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparisons of root localization and relative pose. The curves are PCKroot and PCK rel over different thresholds on the MuPoTS-3D dataset. Blue: result of [22]. Green: estimating the translation by minimizing the reprojection error. Red: our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison. The results of three example images. For each example, the top row shows the input image, and the bottom row shows the results of<ref type="bibr" target="#b27">[22]</ref> (left) and the proposed method (right), respectively. The red circles highlight the difference in localization of human bodies between two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>1, normalizing depth values by the size of FoV makes depth learning easier. 2)Multi-scale supervision is beneficial. 3) To show that our performance gain in terms of the absolute 3D pose is mostly attributed to our single-shot bottom-up design rather than the network size, we test with a smaller backbone. The results Comparison with alternative depth estimation methods. The scatter plots show the consistency of root depth estimation on the MuPoTS-3D dataset. The X and Y axes are the predicted, groundtruth root depth, respectively. The dashed line means the ideal result, i.e., estimation equals ground truth. (a) 'Read-out' root depths from the full depth map estimated by<ref type="bibr" target="#b20">[15]</ref>. (b) State-of-the-art top-down approach<ref type="bibr" target="#b27">[22]</ref>. (c) Our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>59.5 45.3 51.4 46.2 53.0 27.4 23.7 26.4 39.1 23.6 Ours 42.1 41.4 46.5 16.3 53.0 26.4 47.5 18.7 36.7 73</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results on the MuPoTS-3D dataset. All numbers are average values over 20 activities.</figDesc><table><row><cell>Mafia</cell><cell>Ultim.</cell><cell>Pizza</cell><cell>Average</cell></row></table><note>rel PCK abs PCKroot AUC rel PCOD PCK rel PCK abs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>demonstrates quantitative comparison between stateof-the-art methods and our model. It indicates that our model outperforms previous methods in all metrics by a large margin. In particular, the error on the Pizza sequence decreases significantly compared with the previous work. As the Pizza sequence shares no similarity with the training set, this improvement shows our generalization ability.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of the structure design on the MuPoTS-3D dataset. The default backbone is Hourglass model with three stages, and 'Smaller Backbone' means one-stage model.</figDesc><table><row><cell>Design</cell><cell cols="5">Recall PCKroot PCK abs PCK rel PCOD</cell></row><row><cell>Full Model</cell><cell>92.3</cell><cell>45.5</cell><cell>38.7</cell><cell>80.5</cell><cell>97.0</cell></row><row><cell>No Normalization</cell><cell>92.3</cell><cell>5.7</cell><cell>8.7</cell><cell>78.9</cell><cell>95.7</cell></row><row><cell>No Multi-scale Supervision</cell><cell>92.1</cell><cell>45.2</cell><cell>36.2</cell><cell>75.4</cell><cell>93.1</cell></row><row><cell>No RefineNet</cell><cell>92.3</cell><cell>45.5</cell><cell>34.7</cell><cell>70.9</cell><cell>97.0</cell></row><row><cell>Smaller Backbone</cell><cell>91.1</cell><cell>43.8</cell><cell>35.1</cell><cell>75.7</cell><cell>96.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of the part association. '2DPA' means the 2D part association proposed by<ref type="bibr" target="#b8">[3]</ref>. 'DAPA' means the depth-aware part association we proposed. Both of them are based on the same heatmaps and PAFs results.Fig. 7. Qualitative results on in-the-wild images from the Internet.</figDesc><table><row><cell></cell><cell cols="2">Panoptic</cell><cell></cell><cell></cell><cell>MuPoTS-3D</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">Recall 2DPCK Recall PCKroot PCK abs PCK rel PCOD</cell></row><row><cell>2DPA</cell><cell>94.3</cell><cell>92.4</cell><cell>92.1</cell><cell>45.3</cell><cell>38.6</cell><cell>80.2</cell><cell>96.5</cell></row><row><cell cols="2">DAPA 96.4</cell><cell>93.1</cell><cell>92.3</cell><cell>45.5</cell><cell>38.7</cell><cell>80.5</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Running time and memory comparison.</figDesc><table><row><cell></cell><cell></cell><cell>3-people</cell><cell></cell><cell cols="2">20-people</cell></row><row><cell></cell><cell></cell><cell cols="4">Time(ms) Memory(M) Time(ms) Memory(M)</cell></row><row><cell></cell><cell>DetectNet</cell><cell>120.0</cell><cell>899</cell><cell>120.0</cell><cell>899</cell></row><row><cell>[22]</cell><cell>PoseNet</cell><cell>14.7</cell><cell>815</cell><cell>71.8</cell><cell>1491</cell></row><row><cell></cell><cell>RootNet</cell><cell>13.0</cell><cell>803</cell><cell>58.9</cell><cell>1051</cell></row><row><cell></cell><cell>SMAP</cell><cell>57.0</cell><cell>1379</cell><cell>57.0</cell><cell>1379</cell></row><row><cell>Ours</cell><cell>DAPA</cell><cell>4.5</cell><cell>-</cell><cell>8.8</cell><cell>-</cell></row><row><cell></cell><cell>RefineNet</cell><cell>0.80</cell><cell>∼0.5</cell><cell>0.83</cell><cell>∼0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>et al. [22] 18.3 14.9 38.2 29.5 36.8 23.6 14.4 20.0 18.8 25.4 31.8 Ours 46.0 22.7 24.3 38.9 47.5 34.2 35.0 20.0 38.7 64.8 38.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>Ablation study of the structure design on the MuPoTS-3D dataset. Note that our full model consists of root depth, relative depth and 2D branches.</figDesc><table><row><cell>Design</cell><cell cols="5">Recall PCKroot PCK abs PCK rel PCOD</cell></row><row><cell>Full Model</cell><cell>92.3</cell><cell>45.5</cell><cell>38.7</cell><cell>80.5</cell><cell>97.0</cell></row><row><cell>Root Depth Only</cell><cell>85.0</cell><cell>29.9</cell><cell>-</cell><cell>-</cell><cell>88.3</cell></row><row><cell>Root Depth + 2D Branches</cell><cell>92.1</cell><cell>43.6</cell><cell>-</cell><cell>-</cell><cell>96.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: The authors would like to acknowledge support from NSFC (No. 61806176), Fundamental Research Funds for the Central Universities (2019QNA5022) and ZJU-SenseTime Joint Lab of 3D Vision.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Table 8. Sequence-wise PCK abs on the MuPoTS-3D dataset. Accuracy for all groundtruths S1 S2 S3 S4 S5 S6 S7 S8 S9 S10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">S1 S2 S3 S4 S5 S6 S7 S8 S9 S10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep, robust and single shot 3d multi-person human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-person 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Singlenetwork whole-body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Panoptic studio: A massively multiview system for social interaction capture. TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular depth estimation using relative depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, partbased, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Véges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lőrincz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
