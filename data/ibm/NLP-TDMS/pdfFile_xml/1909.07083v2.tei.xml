<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controllable Text-to-Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
							<email>bowen.li@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<email>xiaojuan.qi@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<email>thomas.lukasiewicz@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Controllable Text-to-Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN. The goal of this paper is to generate images from text, and also allow the user to manipulate synthetic images using natural language descriptions, in one framework. In particular, we focus on modifying visual attributes (e.g., category, texture, and colour) of objects in the generated images by changing given text descriptions. To achieve this, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can synthesise high-quality images, and also allow the user to manipulate objects' attributes, without affecting the generation of other content.</p><p>Our ControlGAN contains three novel components. The first component is the word-level spatial and channel-wise attention-driven generator, where an attention mechanism is exploited to allow the 33rd</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating realistic images that semantically match given text descriptions is a challenging problem and has tremendous potential applications, such as image editing, video games, and computer-aided design. Recently, thanks to the success of generative adversarial networks (GANs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">15]</ref> in generating realistic images, text-to-image generation has made remarkable progress <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref> by implementing conditional GANs (cGANs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>, which are able to generate realistic images conditioned on given text descriptions.</p><p>However, current generative networks are typically uncontrollable, which means that if users change some words of a sentence, the synthetic image would be significantly different from the one generated from the original text as shown in <ref type="figure">Fig. 1</ref>. When the given text description (e.g., colour) is changed, corresponding visual attributes of the bird are modified, but other unrelated attributes (e.g., the pose and position) are changed as well. This is typically undesirable in real-world applications, when a user wants to further modify the synthetic image to satisfy her preferences. This bird has a yellow back and rump, gray outer rectrices, and a light gray breast. <ref type="table">(original text)</ref> This bird has a red back and rump, yellow outer rectrices, and a light white breast.</p><p>(modified text)</p><p>Text <ref type="bibr" target="#b27">[27]</ref> [25] Ours Original <ref type="figure">Figure 1</ref>: Examples of modifying synthetic images using a natural language description. The current state of the art methods generate realistic images, but fail to generate plausible images when we slightly change the text. In contrast, our method allows parts of the image to be manipulated in correspondence to the modified text description while preserving other unrelated content.</p><p>generator to synthesise subregions corresponding to the most relevant words. Our generator follows a multi-stage architecture <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28]</ref> that synthesises images from coarse to fine, and progressively improves the quality. The second component is a word-level discriminator, where the correlation between words and image subregions is explored to disentangle different visual attributes, which can provide the generator with fine-grained training signals related to visual attributes. The third component is the adoption of the perceptual loss <ref type="bibr" target="#b6">[7]</ref> in text-to-image generation, which can reduce the randomness involved in the generation, and enforce the generator to preserve visual appearance related to the unmodified text.</p><p>To this end, an extensive analysis is performed, which demonstrates that our method can effectively disentangle different attributes and accurately manipulate parts of the synthetic image without losing diversity. Also, experimental results on the CUB <ref type="bibr" target="#b23">[23]</ref> and COCO <ref type="bibr" target="#b10">[10]</ref> datasets show that our method outperforms existing state of the art both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text-to-image Generation. Recently, there has been a lot of work and interest in text-to-image generation. Mansimov et al. <ref type="bibr" target="#b11">[11]</ref> proposed the AlignDRAW model that used an attention mechanism over words of a caption to draw image patches in multiple stages. Nguyen et al. <ref type="bibr" target="#b13">[13]</ref> introduced an approximate Langevin approach to synthesise images from text. Reed et al. <ref type="bibr" target="#b16">[16]</ref> first applied the cGAN to generate plausible images conditioned on text descriptions. Zhang et al. <ref type="bibr" target="#b27">[27]</ref> decomposed text-to-image generation into several stages generating image from coarse to fine. However, all above approaches mainly focus on generating a new high-quality image from a given text, and cannot allow the user to manipulate the generation of specific visual attributes using natural language descriptions.</p><p>Image-to-image translation. Our work is also closely related to conditional image manipulation methods. Cheng et al. <ref type="bibr" target="#b2">[3]</ref> produced high-quality image parsing results from verbal commands. Zhu et al. <ref type="bibr" target="#b31">[31]</ref> proposed to change the colour and shape of an object by manipulating latent vectors. Brock et al. <ref type="bibr" target="#b1">[2]</ref> introduced a hybrid model using VAEs <ref type="bibr" target="#b9">[9]</ref> and GANs, which achieved an accurate reconstruction without loss of image quality. Recently, Nam et al. <ref type="bibr" target="#b12">[12]</ref> built a model for multi-modal learning on both text descriptions and input images, and proposed a text-adaptive discriminator which utilised word-level text-image matching scores as supervision. However, they adopted a global pooling layer to extract image features, which may lose important fine-grained spatial information. Moreover, the above approaches focus only on image-to-image translation instead of text-to-image generation, which is probably more challenging.</p><p>Attention. The attention mechanism has shown its efficiency in various research fields including image captioning <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30]</ref>, machine translation <ref type="bibr" target="#b0">[1]</ref>, object detection <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29]</ref>, and visual question answering <ref type="bibr" target="#b26">[26]</ref>. It can effectively capture task-relevant information and reduce the interference from less important one. Recently, Xu et al. <ref type="bibr" target="#b25">[25]</ref> built the AttnGAN model that designed a word-level spatial attention to guide the generator to focus on subregions corresponding to the most relevant word. However, spatial attention only correlates words with partial regions without taking channel </p><formula xml:id="formula_0">G 2 G 1 D 1 D 2 D 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder</head><p>Text Encoder word-level discriminator information into account. Also, different channels of features in CNNs may have different purposes, and it is crucial to avoid treating all channels without distinction, such that the most relevant channels in the visual features can be fully exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Controllable Generative Adversarial Networks</head><p>Given a sentence S, we aim to synthesise a realistic image I that semantically aligns with S (see <ref type="figure" target="#fig_1">Fig. 2</ref>), and also make this generation process controllable -if S is modified to be S m , the synthetic resultĨ should semantically match S m while preserving irrelevant content existing in I (shown in <ref type="figure">Fig. 4</ref>). To achieve this, we propose three novel components: 1) a channel-wise attention module, 2) a word-level discriminator, and 3) the adoption of the perceptual loss in the text-to-image generation. We elaborate our model as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We adopt the multi-stage AttnGAN <ref type="bibr" target="#b25">[25]</ref> as our backbone architecture (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Given a sentence S, the text encoder -a pre-trained bidirectional RNN <ref type="bibr" target="#b25">[25]</ref> -encodes the sentence S into a sentence feature s ∈ R D with dimension D describing the whole sentence, and word features w ∈ R D×L with length L (i.e., number of words) and dimension D. Following <ref type="bibr" target="#b27">[27]</ref>, we also apply conditioning augmentation (CA) to s. The augmented sentence features is further concatenated with a random vector z to serve as the input to the first stage. The overall framework generates an image from coarseto fine-scale in multiple stages, and, in each stage, the network produces a hidden visual feature v i , which is the input to the corresponding generator G i to produce a synthetic image. Spatial attention <ref type="bibr" target="#b25">[25]</ref> and our proposed channel-wise attention modules take w and v i as inputs, and output attentive word-context features. These attentive features are further concatenated with the hidden feature v i and then serve as input for the next stage.</p><p>The generator exploits the attention mechanism via incorporating a spatial attention module <ref type="bibr" target="#b25">[25]</ref> and the proposed channel-wise attention module. The spatial attention module <ref type="bibr" target="#b25">[25]</ref> can only correlate words with individual spatial locations without taking channel information into account. Thus, we introduce a channel-wise attention module (see Sec. 3.2) to exploit the connection between words and channels. We experimentally find that the channel-wise attention module highly correlates semantically meaningful parts with corresponding words, while the spatial attention focuses on colour descriptions (see <ref type="figure" target="#fig_3">Fig. 6</ref>). Therefore, our proposed channel-wise attention module, together with the spatial attention, can help the generator disentangle different visual attributes, and allow it to focus only on the most relevant subregions and channels.</p><formula xml:id="formula_1">corre L w v k f α k D × L w k : (H k * W k ) × L C × (H k * W k ) m k : C × L L × (H k * W k ) (a) channel-wise attention (b) word-level discriminator n w MatMul Softmax MatMul Repeat Mul. Cosine Similarity Sigmoid ∑ D × L b: D × L C × (H * W ) L × D m : L × (H * W ) (H * W ) × L n : D × (H * W ) γ′ : D × L b: D × L Trans.</formula><p>Trans.</p><p>Trans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MatMul</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax MatMul</head><p>Trans. Transpose</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MatMul</head><p>Hadamard Product</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summation</head><p>Mul.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeat Along Row Direction</head><p>Matrix Multiplication</p><formula xml:id="formula_2">F k F′ α k : C × L C × (H k * W k ) β : L × (H * W )</formula><p>Word-Level Self Attention <ref type="bibr" target="#b12">[12]</ref> γ : 1 × L F k , F′ Perception Layer ∑ <ref type="figure">Figure 3</ref>: The architecture of proposed channel-wise attention module and word-level discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Channel-Wise Attention</head><p>At the k th stage, the channel-wise attention module (see <ref type="figure">Fig. 3</ref> (a)) takes two inputs: the word features w and hidden visual features v k ∈ R C×(H k * W k ) , where H k and W k denote the height and width of the feature map at stage k. The word features w are first mapped into the same semantic space as the visual features v k via a perception layer</p><formula xml:id="formula_3">F k , producingw k = F k w, where F k ∈ R (H k * W k )×D .</formula><p>Then, we calculate the channel-wise attention matrix m k ∈ R C×L by multiplying the converted word featuresw k and visual features v k , denoted as m k = v kwk . Thus, m k aggregates correlation values between channels and words across all spatial locations. Next, m k is normalised by the softmax function to generate the normalised channel-wise attention matrix α k as</p><formula xml:id="formula_4">α k i,j = exp(m k i,j ) L−1 l=0 exp(m k i,l )</formula><p>.</p><p>(1)</p><p>The attention weight α k i,j represents the correlation between the i th channel in the visual features v k and the j th word in the sentence S, and higher value means larger correlation.</p><p>Equipped with the channel-wise attention matrix α k , we obtain the final channel-wise attention</p><formula xml:id="formula_5">features f α k ∈ R C×(H k * W k ) , denoted as f α k = α k (w k ) T .</formula><p>Each channel in f α k is a dynamic representation weighted by the correlation between words and corresponding channels in the visual features. Thus, channels with high correlation values are enhanced resulting in a high response to corresponding words, which can facilitate disentangling word attributes into different channels, and also reduce the influence from irrelevant channels by assigning a lower correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-Level Discriminator</head><p>To encourage the generator to modify only parts of the image according to the text, the discriminator should provide the generator with fine-grained training feedback, which can guide the generation of subregions corresponding to the most relevant words. Actually, the text-adaptive discriminator <ref type="bibr" target="#b12">[12]</ref> also explores the word-level information in the discriminator, but it adopts a global average pooling layer to output a 1D vector as image feature, and then calculates the correlation between image feature and each word. By doing this, the image feature may lose important spatial information, which provides crucial cues for disentangling different visual attributes. To address the issue, we propose a novel word-level discriminator inspired by <ref type="bibr" target="#b12">[12]</ref> to explore the correlation between image subregions and each word; see <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>Our word-level discriminator takes two inputs: 1) word features w, w encoded from the text encoder, which follows the same architecture as the one (see <ref type="figure" target="#fig_1">Fig. 2</ref> (a)) used in the generator, where w and w denote word features encoded from the original text S and a randomly sampled mismatched text, respectively, and 2) visual features n real , n fake , both encoded by a GoogleNet-based <ref type="bibr" target="#b22">[22]</ref> image encoder from the real image I and generated images I , respectively.</p><p>For simplicity, in the following, we use n ∈ R C×(H * W ) to represent visual features n real and n fake , and use w ∈ R D×L for both original and mismatched word features. The word-level discriminator contains a perception layer F that is used to align the channel dimension of visual feature n and word feature w, denoted asñ = F n, where F ∈ R D×C is a weight matrix to learn. Then, the word-context correlation matrix m ∈ R L×(H * W ) can be derived via m = w Tñ , and is further normalised by the softmax function to get a correlation matrix β:</p><formula xml:id="formula_6">β i,j = exp(m i,j ) (H * W )−1 l=0 exp(m i,l ) ,<label>(2)</label></formula><p>where β i,j represents the correlation value between the i th word and the j th subregion of the image. Then, the image subregion-aware word features b ∈ R D×L can be obtained by b =ñβ T , which aggregates all spatial information weighted by the word-context correlation matrix β.</p><p>Additionally, to further reduce the negative impact of less important words, we adopt the word-level self-attention <ref type="bibr" target="#b12">[12]</ref> to derive a 1D vector γ with length L reflecting the relative importance of each word. Then, we repeat γ by D times to produce γ , which has the same size as b. Next, b is further reweighted by γ to getb, denoted asb = b γ , where represents element-wise multiplication.</p><p>Finally, we derive the correlation between the i th word and the whole image as Eq. <ref type="formula" target="#formula_7">(3)</ref>:</p><formula xml:id="formula_7">r i = σ( (b i ) T w i ||b i || ||w i || ),<label>(3)</label></formula><p>where σ is the sigmoid function, r i evaluates the correlation between the i th word and the image, and b i and w i represent the i th column of b and w, respectively.</p><p>Therefore, the final correlation value L corre between image I and sentence S is calculated by summing all word-context correlations, denoted as L corre (I, S) = L−1 i=0 r i . By doing so, the generator can receive fine-grained feedback from the word-level discriminator for each visual attribute, which can further help supervise the generation and manipulation of each subregion independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Perceptual Loss</head><p>Without adding any constraint on text-irrelevant regions (e.g., backgrounds), the generated results can be highly random, and may also fail to be semantically consistent with other content. To mitigate this randomness, we adopt the perceptual loss <ref type="bibr" target="#b6">[7]</ref> based on a 16-layer VGG network <ref type="bibr" target="#b21">[21]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b18">[18]</ref>. The network is used to extract semantic features from both the generated image I and the real image I, and the perceptual loss is defined as</p><formula xml:id="formula_8">L per (I , I) = 1 C i H i W i φ i (I ) − φ i (I) 2 2 ,<label>(4)</label></formula><p>where φ i (I) is the activation of the i th layer of the VGG network, and H i and W i are the height and width of the feature map, respectively.</p><p>To our knowledge, we are the first to apply the perceptual loss <ref type="bibr" target="#b6">[7]</ref> in controllable text-to-image generation, which can reduce the randomness involved in the image generation by matching feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Functions</head><p>The generator and discriminator are trained alternatively by minimising both the generator loss L G and discriminator loss L D .</p><p>Generator objective. The generator loss L G as Eq. (5) contains an adversarial loss L G k , a textimage correlation loss L corre , a perceptual loss L per , and a text-image matching loss L DAMSM <ref type="bibr" target="#b25">[25]</ref>.</p><formula xml:id="formula_9">L G = K k=1 (L G k + λ 2 L per (I k , I k ) + λ 3 log(1 − L corre (I k , S))) + λ 4 L DAMSM ,<label>(5)</label></formula><p>where K is the number of stages, I k is the real image sampled from the true image distribution P data at stage k, I k is the generated image at the k th stage sampled from the model distribution P G k , λ 2 , λ 3 , λ 4 are hyper-parameters controlling different losses, L per is the perceptual loss described in Sec. 3.4, which puts constraint on the generation process to reduce the randomness, the L DAMSM <ref type="bibr" target="#b25">[25]</ref> is used to measure text-image matching score based on the cosine similarity, and L corre reflects the correlation between the generated image and the given text description considering spatial information.</p><p>The adversarial loss L G k is composed of the unconditional and conditional adversarial losses shown in Eq. <ref type="formula" target="#formula_10">(6)</ref>: the unconditional adversarial loss is applied to make the synthetic image be real, and the conditional adversarial loss is utilised to make the generated image match the given text S.</p><formula xml:id="formula_10">L G k = − 1 2 E I k ∼P G k log(D k (I k )) unconditional adversarial loss − 1 2 E I k ∼P G k log(D k (I k , S)) conditional adversarial loss .<label>(6)</label></formula><p>Discriminator objective. The final loss function for training the discriminator D is defined as:</p><formula xml:id="formula_11">L D = K k=1 (L D k + λ 1 (log(1 − L corre (I k , S)) + log L corre (I k , S ))),<label>(7)</label></formula><p>where L corre is the correlation loss determining whether word-related visual attributes exist in the image (see Sec. 3.3), S is a mismatched text description that is randomly sampled from the dataset and is irrelevant to I k , and λ 1 is a hyper-parameter controlling the importance of additional losses.</p><p>The adversarial loss L D k contains two components: the unconditional adversarial loss determines whether the image is real, and the conditional adversarial loss determines whether the given image matches the text description S:</p><formula xml:id="formula_12">L D k = − 1 2 E I k ∼Pdata [log(D k (I k ))] − 1 2 E I k ∼P G k log(1 − D k (I k )) unconditional adversarial loss − 1 2 E I k ∼Pdata [log(D k (I k , S))] − 1 2 E I k ∼P G k log(1 − D k (I k , S)) conditional adversarial loss .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the effectiveness of our approach, we conduct extensive experiments on the CUB bird <ref type="bibr" target="#b23">[23]</ref> and the MS COCO <ref type="bibr" target="#b10">[10]</ref> datasets. We compare with two state of the art GAN methods on text-to-image generation, StackGAN++ <ref type="bibr" target="#b28">[28]</ref> and AttnGAN <ref type="bibr" target="#b25">[25]</ref>. Results for the state of the art are reproduced based on the code released by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our method is evaluated on the CUB bird <ref type="bibr" target="#b23">[23]</ref> and the MS COCO <ref type="bibr" target="#b10">[10]</ref> datasets. The CUB dataset contains 8,855 training images and 2,933 test images, and each image has 10 corresponding text descriptions. As for the COCO dataset, it contains 82,783 training images and 40,504 validation images, and each image has 5 corresponding text descriptions. We preprocess these two datasets based on the methods introduced in <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>There are three stages (K = 3) in our ControlGAN generator following <ref type="bibr" target="#b25">[25]</ref>. The three scales are 64 × 64, 128 × 128, and 256 × 256, and spatial and channel-wise attentions are applied at the stages 2 and 3. The text encoder is a pre-trained bidirectional LSTM <ref type="bibr" target="#b20">[20]</ref> to encode the given text description into a sentence feature with dimension 256 and word features with length 18 and dimension 256.</p><p>In the perceptual loss, we compute the content loss at layer relu2_2 of VGG-16 <ref type="bibr" target="#b21">[21]</ref> pre-trained on the ImageNet <ref type="bibr" target="#b18">[18]</ref>. The whole network is trained using the Adam optimiser <ref type="bibr" target="#b8">[8]</ref> with the learning rate 0.0002. The hyper-parameters λ 1 , λ 2 , λ 3 , and λ 4 are set to 0.5, 1, 1, and 5 for both datasets, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State of the Art</head><p>Quantitative results. We adopt the Inception Score <ref type="bibr" target="#b19">[19]</ref> to evaluate the quality and diversity of the generated images. However, as the Inception Score cannot reflect the relevance between an image and a text description, we utilise R-precision <ref type="bibr" target="#b25">[25]</ref> to measure the correlation between a generated image and its corresponding text. We compare the top-1 text-to-image retrieval accuracy (Top-1 Acc) on the CUB and COCO datasets following <ref type="bibr" target="#b12">[12]</ref>.</p><p>Quantitative results are shown in <ref type="table" target="#tab_0">Table 1</ref>, our method achieves better IS and R-precision values on the CUB dataset compared with the state of the art, and has a competitive performance on the COCO dataset. This indicates that our method can generate higher-quality images with better diversity, which semantically align with the text descriptions.</p><p>To further evaluate whether the model can generate controllable results, we compute the L 2 reconstruction error <ref type="bibr" target="#b12">[12]</ref> between the image generated from the original text and the one from the modified text shown in <ref type="table" target="#tab_0">Table 1</ref>. Compared with other methods, ControlGAN achieves a significantly lower reconstruction error, which demonstrates that our method can better preserve content in the image generated from the original text.</p><p>Qualitative results. We show qualitative comparisons in <ref type="figure">Fig. 4</ref>. As we can see, according to modifying given text descriptions, our approach can successfully manipulate specific visual attributes accurately. Also, our method can even handle out-of-distribution queries, e.g., red zebra on a river shown in the last two columns of <ref type="figure">Fig. 4</ref>. All the above indicates that our approach can manipulate different visual attributes independently, which demonstrates the effectiveness of our approach in disentangling visual attributes for text-to-image generation. <ref type="figure">Fig. 5</ref> shows the visual comparison between ControlGAN, AttnGAN <ref type="bibr" target="#b25">[25]</ref>, and StackGAN++ <ref type="bibr" target="#b28">[28]</ref>. It can be observed that when the text is modified, the two compared approaches are more likely to generate new content, or change some visual attributes that are not relevant to the modified text. For instance, as shown in the first two columns, when we modify the colour attributes, StackGAN++ changes the pose of the bird, and AttnGAN generates new background. In contrast, our approach is able to accurately manipulate parts of the image generation corresponding to the modified text, while preserving the visual attributes related to unchanged text.</p><p>In the COCO dataset, our model again achieves much better results compared with others shown in <ref type="figure">Fig. 5</ref>. For example, as shown in the last four columns, the compared approaches cannot preserve the shape of objects and even fail to generate reasonable images. Generally speaking, the results on COCO are not as good as on the CUB dataset. We attribute this to the few text-image pairs and more abstract captions in the dataset. Although there are a lot of categories in COCO, each category only has a few number of examples, and captions focus mainly on the category of objects rather than detailed descriptions, which makes text-to-image generation more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Component Analysis</head><p>Effectiveness of channel-wise attention. Our model implements channel-wise attention in the generator, together the spatial attention, to generate realistic images. To better understand the effectiveness of attention mechanisms, we visualise the intermediate results and corresponding attention maps at different stages.</p><p>This bird is yellow with black and has a very short beak.</p><p>This bird is orange with grey and has a very short beak.</p><p>The small bird has a dark brown head and light brown body.</p><p>The small bird has a dark tan head and light grey body.</p><p>A large group of cows on a farm.</p><p>A large group of white cows on a farm.</p><p>A crowd of people fly kites on a hill.</p><p>A crowd of people fly kites on a park.</p><p>A group of zebras on a grassy field with trees in background.</p><p>A group of red zebras on a river with trees in background. <ref type="figure">Figure 4</ref>: Qualitative results on the CUB and COCO datasets. Odd-numbered columns show the original text and even-numbered ones the modified text. The last two are an out-of-distribution case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>This bird has a white neck and breast with a turquoise crown and feathers a small short beak.</p><p>This bird has a grey neck and breast with a blue crown and feathers a small short beak.</p><p>This bird has wings that are yellow and has a brown body.</p><p>This bird has wings that are black and has a red body.</p><p>A giraffe is standing on the dirt.</p><p>A giraffe is standing on the dirt in an enclosure.</p><p>A zebra stands on a pathway near grass.</p><p>A sheep stands on a pathway near grass.</p><p>StackGAN++ <ref type="bibr" target="#b28">[28]</ref> AttnGAN <ref type="bibr" target="#b25">[25]</ref> Ours <ref type="figure">Figure 5</ref>: Qualitative comparison of three methods on the CUB and COCO datasets. Odd-numbered columns show the original text and even-numbered ones the modified text.  We experimentally find that the channel-wise attention correlates closely with semantic parts of objects, while the spatial attention focuses mainly on colour descriptions. <ref type="figure" target="#fig_3">Fig. 6</ref> shows several channels of feature maps that correlate with different semantics, and our channel-wise attention module assigns large correlation values to channels that are semantically related to the word describing parts of a bird. This phenomenon is further verified by the ablation study shown in <ref type="figure">Fig. 7 (left</ref>  This yellow bird has grey and white wings and a red belly.</p><p>The bird is small and round with white belly and blue wings.</p><p>The bird is small and round with white head and blue wings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours without channel-wise attention</head><p>Ours Input This bird's wing bar is brown and yellow, and its belly is yellow.</p><p>This bird's wing bar is brown and yellow, and its belly is white.</p><p>A small bird with a brown colouring and white belly.</p><p>A small bird with a brown colouring and white head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours without</head><p>word-level discriminator Ours <ref type="figure">Figure 7</ref>: Left: ablation study of channel-wise attention; right: ablation study of the word-level discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The bird is small with a pointed bill, has black eyes, and a yellow crown.</p><p>The bird is small with a pointed bill, has black eyes, and an orange crown.</p><p>A bird with a white belly and metallic blue wings with a small beak.</p><p>A bird with a white head and metallic blue wings with a small beak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours without perceptual loss</head><p>Ours Input A songbird is white with blue feathers and black eyes.</p><p>A songbird is yellow with blue and green feathers and black eyes.</p><p>A tiny bird, with green flank, white belly, yellow crown, and a pointy bill.</p><p>A tiny bird, with green flank, grey belly, blue crown, and a pointy bill.</p><p>Ours with text-adaptive discriminator Ours <ref type="figure">Figure 8</ref>: Left: ablation study of the perceptual loss <ref type="bibr" target="#b6">[7]</ref>; right: comparison between our word-level discriminator and text-adaptive discriminator <ref type="bibr" target="#b12">[12]</ref>.</p><p>Effectiveness of word-level discriminator. To verify the effectiveness of the word-level discriminator, we first conduct an ablation study that our model is trained without word-level discriminator, shown in <ref type="figure">Fig. 7</ref> (right side), and then we construct a baseline model by replacing our discriminator with a text-adaptive discriminator <ref type="bibr" target="#b12">[12]</ref>, which also explores the correlation between image features and words. Visual comparisons are shown in <ref type="figure">Fig. 8 (right side)</ref>. We can easily observe that the compared baseline fails to manipulate the synthetic images. For example, as shown in the first two columns, the bird generated from the modified text has a totally different shape, and the background has been changed as well. This is due to the fact that the text-adaptive discriminator <ref type="bibr" target="#b12">[12]</ref> uses a global pooling layer to extract image features, which may lose important spatial information.</p><p>Effectiveness of perceptual loss. Furthermore, we conduct an ablation study that our model is trained without the perceptual loss, shown in <ref type="figure">Fig. 8</ref> (left side). Without perceptual loss, images generated from modified text are hard to preserve content that are related to unmodified text, which indicates that the perceptual loss can potentially introduce a stricter semantic constraint on the image generation and help reduce the involved randomness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a controllable generative adversarial network (ControlGAN), which can generate and manipulate the generation of images based on natural language descriptions. Our ControlGAN can successfully disentangle different visual attributes and allow parts of the synthetic image to be manipulated accurately, while preserving the generation of other content. Three novel components are introduced in our model: 1) the word-level spatial and channel-wise attention-driven generator can effectively disentangle different visual attributes, 2) the word-level discriminator provides the generator with fine-grained training signals related to each visual attribute, and 3) the adoption of perceptual loss reduces the randomness involved in the generation, and enforces the generator to reconstruct content related to unmodified text. Extensive experimental results demonstrate the effectiveness and superiority of our method on two benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our proposed ControlGAN. In (b), L corre is the correlation loss discussed in Sec. 3.3. In (c), L per is the perceptual loss discussed in Sec. 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This white and black bird with a red head, brown wings and a white belly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top: visualisation of feature channels at stage 3. The number at the top-right corner is the channel number, and the word that has the highest correlation α i,j in Eq. 1 with the channel is shown under the image. Bottom: spatial attention produced in stage 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison: Inception Score, R-precision, and L 2 reconstruction error of state of the art and ControlGAN on the CUB and COCO datasets.</figDesc><table><row><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell>COCO</cell></row><row><cell>Method</cell><cell>IS</cell><cell cols="3">Top-1 Acc(%) L2 error IS</cell><cell>Top-1 Acc(%) L2 error</cell></row><row><cell cols="3">StackGAN++ 4.04 ± .05 45.28 ± 3.72</cell><cell>0.29</cell><cell>8.30 ± .10</cell><cell>72.83 ± 3.17</cell><cell>0.32</cell></row><row><cell>AttnGAN</cell><cell cols="2">4.36 ± .03 67.82 ± 4.43</cell><cell>0.26</cell><cell cols="2">25.89 ± .47 85.47 ± 3.69</cell><cell>0.40</cell></row><row><cell>Ours</cell><cell cols="2">4.58 ± .09 69.33 ± 3.23</cell><cell>0.18</cell><cell cols="2">24.06 ± .60 82.43 ± 2.43</cell><cell>0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>side). Without channel-wise attention, our model fails to generate controllable results when we modify the text related to parts of a bird. In contrast, our model with channel-wise attention can generate better controllable results.</figDesc><table><row><cell></cell><cell>This yellow</cell></row><row><cell></cell><cell>bird has grey</cell></row><row><cell>Input</cell><cell>and white</cell></row><row><cell></cell><cell>wings and a</cell></row><row><cell></cell><cell>red head.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07093</idno>
		<title level="m">Neural photo editing with introspective adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagespirit: Verbal guided image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image synthesis via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5706" to="5714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
		<title level="m">Generating images from captions with attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text-adaptive generative adversarial networks: manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Top-down control of visual attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Caltech-Ucsd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MDNet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6428" to="6436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
