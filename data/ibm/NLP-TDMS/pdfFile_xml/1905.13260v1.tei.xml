<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<email>yuewu@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
							<email>yandong.guo@live.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to scale up to a large number of classes. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Natural learning systems are inherently incremental where new knowledge is continuously learned over time while existing knowledge is maintained <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>. Many computer vision applications in the real world require incremental learning capabilities. For example, a face recognition system should be able to add new persons without forgetting the faces already learned. However, most deep learning approaches suffer from catastrophic forgetting <ref type="bibr" target="#b14">[15]</ref> -a significant performance degradation, when the past data are not available.</p><p>The missing data for old classes introduce two challenges -(a) maintaining the classification performance on old classes, and (b) balancing between old classes and new  <ref type="figure">Figure 1</ref>. Performance degradation of incremental learning algorithms on ImageNet-100 (100 classes) and ImageNet-1000 (1000 classes). Each dataset has 10 incremental steps. The degradation is the gap between the accuracy of the final incremental step and the accuracy of a non-incremental classifier, which is trained using all data. When the scale goes up (from ImageNet-100 to ImageNet-1000), the degradation for the state-of-the-art algorithms (iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref>) increases. The degradation for our BiC method is small for both scales. Although iCaRL has similar relative degratation with our method (increase by 50% from ImageNet-100 to ImageNet-1000), it performs poorly across the scales. classes. Distillation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2]</ref> has been used to effectively address the former challenge. Recent studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref> also show that selecting a few exemplars from the old classes can alleviate the imbalance problem. These methods perform well on small datasets. However, they suffer from a significant performance degradation when the number of classes becomes large (e.g. thousands of classes). <ref type="figure">Fig. 1</ref> demonstrates the performance degradation of these state-of-the-art algorithms, using a non-incremental classifier as the reference. When the number of classes increases from 100 to 1000, both iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref> have more degradation.</p><p>Why is it more challenging to handle a large number of classes for incremental learning? We believe this is due to the coupling of two factors. First, the training data are unbalanced. Secondly, as the number of classes increases, it is more likely to have visually similar classes (e.g. multiple dog classes in ImageNet) across different incremental steps. Under the incremental constraint with data imbal- ance, the increasing number of visually similar classes is particularly challenging since the small margin around the boundary between classes is too sensitive to the data imbalance. The boundary is pushed to favor classes with more samples.</p><p>In this work, we present a method to address the data imbalance problem in large scale incremental learning. Firstly, we found a strong bias towards the new classes in the classifier layer (i.e. the last fully connected layer) of the convolution neural network (CNN). Based upon this finding, we propose a simple and effective method, called BiC (bias correction), to correct the bias. We add a bias correction layer after the last fully connected (FC) layer (shown in <ref type="figure" target="#fig_1">Fig. 2</ref>), which is a simple linear model with two parameters. The bias correction layer is learned at the second stage, after learning the convolution layers and FC layer at the first stage. The data, including exemplars from the old classes and samples from the new classes, are split into a training set for the first stage and a validation set for the second stage. The validation set is helpful to approximate the real distribution of both old and new classes in the feature space, allowing us to estimate the bias in FC layer. We found that the bias can be effectively corrected with a small validation set.</p><p>Our BiC method achieves remarkably good performance, especially on large scale datasets. The experimental results show that our method outperforms state-of-the-art algorithms (iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref>) on two large datasets (ImageNet ILSVRC 2012 and MS-Celeb-1M) by a large margin. Our BiC method gains 11.1% on ImageNet and 13.2% on MS-Celeb-1M, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Incremental learning has been a long standing problem in machine learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12]</ref>. Before the deep learning took off, people had been developing incremental learning techniques by leveraging linear classifiers, ensemble of weak classifiers, nearest neighbor classifiers, etc. Recently, thanks to the exciting progress in deep learning, there has been a lot of research on incremental learning with deep neural network models. The work can be roughly divided into three categories depending on whether they require real data or synthetic data or nothing from the old classes.</p><p>Without using old data: Methods in the first category do not require any old data. <ref type="bibr" target="#b8">[9]</ref> presented a method for domain transfer learning. They try to maintain the performance on old tasks by freezing the final layer and discouraging the change of shared weights in feature extraction layers. <ref type="bibr" target="#b9">[10]</ref> proposed a technique to remember old tasks by constraining the important weights when optimizing a new task. One limitation of this approach is that the old and new tasks may conflict on these important weights. <ref type="bibr" target="#b12">[13]</ref> presented a method that applies knowledge distillation <ref type="bibr" target="#b7">[8]</ref> to maintain the performance on old tasks. <ref type="bibr" target="#b12">[13]</ref> separated the old and new tasks in multi-task learning, which is different from learning classifier incrementally. <ref type="bibr" target="#b22">[23]</ref> applied knowledge distillation for learning object detectors incrementally. <ref type="bibr" target="#b17">[18]</ref> utilized autoencoder to retain the knowledge from old tasks. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> updated knowledge dictionary for new tasks and kept dictionary coefficients for old tasks.</p><p>Using synthetic data: Both <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b26">[27]</ref> employed GAN <ref type="bibr" target="#b3">[4]</ref> to replay synthetic data for old tasks. <ref type="bibr" target="#b21">[22]</ref> applied cross entropy loss on synthesis data with the old solver's response as the target. <ref type="bibr" target="#b26">[27]</ref> utilized a root mean-squared error for learning the response of old tasks on synthetic data. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> highly depends on the capability of generative models and struggles with complex objects and scenes.</p><p>Using exemplars from old data: Methods in the third category require part of the old data. <ref type="bibr" target="#b18">[19]</ref> proposed a method to select a small number of exemplars from each old class. <ref type="bibr" target="#b1">[2]</ref> keeps classifiers for all incremental steps and used them as distillation. It introduces balanced finetuning and temporary distillation to alleviate the imbalance between the old and new classes. <ref type="bibr" target="#b13">[14]</ref> proposed a continuous learning framework where the training samples for different tasks are used one by one during training. It constrains the cross entropy loss on softmax outputs of old tasks when the new task comes. <ref type="bibr" target="#b27">[28]</ref> proposed a training method that grows a network hierarchically as new training data are added. Similarly, <ref type="bibr" target="#b20">[21]</ref> increases the number of layers in the network to handle new coming data.</p><p>Our BiC method belongs to the third category, we keep exemplars from the old classes in the similar manner to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. However, we handle the data imbalance differently. We first locate a strong bias in the classifier layer (the last fully connected layer), and then apply a linear model to correct the bias using a small validation set. The validation set is a small subset of exemplars which is excluded from training and used for bias correction alone. Compared with the state of the art ( <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>), our BiC method is more effective on large datasets with 1000+ classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Baseline: Incremental Learning using Knowledge Distillation</head><p>In this section, we introduce a baseline solution for incremental learning using knowledge distillation <ref type="bibr" target="#b12">[13]</ref>. This is corresponding to the first stage in <ref type="figure" target="#fig_1">Fig. 2</ref>. For an incremental step with n old class and m new classes, we learn a new model to perform classification on n + m classes, by using the knowledge distillation from an old model that classifies the old n classes (illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>). The new model is learned by using a distilling loss and a classification loss.</p><p>Let us denote the samples of the new classes as respectively. The distilling loss is formulated as follows:</p><formula xml:id="formula_0">X m = {(x i , y i ), 1 ≤ i ≤ M, y i ∈ [n + 1, .., n + m]},</formula><formula xml:id="formula_1">L d = x∈X n ∪X m n k=1 −π k (x) log[π k (x)],<label>(1)</label></formula><formula xml:id="formula_2">π k (x) = eô k (x)/T n j=1 eô j (x)/T , π k (x) = e o k (x)/T n j=1 e oj (x)/T ,</formula><p>where T is the temperature scalar. The distilling loss is computed for all samples from the new classes and exemplars from the old classes (i.e.X n ∪ X m ). We use the softmax cross entropy as the classification loss, which is computed as follows:</p><formula xml:id="formula_3">L c = (x,y)∈X n ∪X m n+m k=1 −δ y=k log[p k (x)],<label>(2)</label></formula><p>where δ y=k is the indicator function and p k (x) is the output probability (i.e. softmax of logits) of the k-th class in n + m old and new classes.  The overall loss combines the distilling loss and the classification loss as follows:</p><formula xml:id="formula_4">L = λL d + (1 − λ)L c ,<label>(3)</label></formula><p>where the scalar λ is used to balance between the two terms. The scalar λ is set to n n+m , where n and m are the number of old and new classes. λ is 0 for the first batch since all classes are new. For the extreme case where n m, λ is nearly 1, indicating the importance to maintain the old classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Diagnosis: FC Layer is Biased</head><p>The baseline model has a bias towards the new classes, due to the imbalance between the number of samples from the new classes and the number of exemplars from the old classes. We have a hypothesis that the last fully connected layer is biased as the weights are not shared across classes. To validate this hypothesis, we design an experiment on CIFAR-100 dataset with five incremental batches (each has 20 classes).</p><p>First, we train a set of incremental classifiers using the baseline method. The classification accuracy quickly drops as more incremental steps arrive (shown as the bottom curve in <ref type="figure" target="#fig_5">Fig. 4-(a)</ref>). For the last incremental step (class 81-100), we observe a strong bias towards the newest 20 classes in the confusion matrix ( <ref type="figure" target="#fig_5">Fig. 4-(b)</ref>). Compared to the upper bound, i.e. the classifiers learned using all training data (the top curve in <ref type="figure" target="#fig_5">Fig. 4-(a)</ref>), the baseline model has a performance degradation.</p><p>Then, we conduct another experiment to evaluate if the fully connected layer is heavily biased. This experiment has two steps for each incremental batch: (a) applying the baseline model to learn both the feature and fully connected layers, (b) freezing the feature layers and retrain the fully connected layer alone using all training samples from both old and new classes. Compared to the baseline, the accuracy improves (the second top curve in <ref type="figure" target="#fig_5">Fig. 4-(a)</ref>). The accuracy on the final classifier on 100 classes improves by 20%. These results validate our hypothesis that the fully connected layer is heavily biased. We also observe the gap between this result and the upper bound, which reflects the bias within the feature layers. In this paper, we focus on correcting the bias in the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Bias Correction (BiC) Method</head><p>Based upon our finding that the fully connected layer is heavily biased, we propose a simple and effective bias correction method (BiC). Our method includes two stages in training (shown in <ref type="figure" target="#fig_1">Fig. 2</ref>). Firstly, we train the convolution layers and the fully connected layer by following the baseline method. At the second stage, we freeze both the convolution and the fully connected layers, and estimate two bias parameters by using a small validation set. In this section, we discuss how the validation set is generated and the details of the bias correction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Validation Set</head><p>We estimate the bias by using a small validation set. The basic idea is to exclude the validation set from training the feature representation, allowing them to reflect the unbiased distribution of both old and new classes on the feature space (shown in <ref type="figure" target="#fig_6">Fig. 5</ref>). Therefore, we split the exemplars from the old classes and the samples from the new classes into a training set and a validation set. The training set is used to learn the convolution and fully connected layers (see <ref type="figure" target="#fig_1">Fig.  2</ref>), while the validation set is used for the bias correction. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the generation of the validation set. The stored exemplars from the old classes are split into a training subset (referred to train old ) and a validation subset (referred to val old ). The samples for the new classes are also split into a training subset (referred to train new ) and a validation subset (referred to val new ). train old and train new are used to learn the convolution and FC layers (see <ref type="figure" target="#fig_1">Fig.  2</ref>). val old and val new are used to estimate the parameters in the bias correction layer. Note that val old and val new are balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Bias Correction Layer</head><p>The bias correction layer should be simple with a small number of parameters, since val old and val new have small size. Thus, we use a linear model (with two parameters) to correct the bias. This is achieved by adding a bias correction layer in the network (shown in <ref type="figure" target="#fig_1">Fig. 2)</ref>. We keep the output logits for the old classes (1, . . . , n) and apply a linear model to correct the bias on the output logits for the new classes (n + 1, . . . , n + m) as follows:</p><formula xml:id="formula_5">q k = o k 1 ≤ k ≤ n αo k + β n + 1 ≤ k ≤ n + m ,<label>(4)</label></formula><p>where α and β are the bias parameters on the new classes and o k (defined in Section 3) is the output logits for the k-th class. Note that the bias parameters (α, β) are shared by all new classes, allowing us to estimate them with a small validation set. When optimizing the bias parameters, the convolution and fully connected layers are frozen. The classification loss (softmax with cross entropy) is used to optimize the bias parameters as follows:</p><formula xml:id="formula_6">L b = − n+m k=1 δ y=k log[sof tmax(q k )].<label>(5)</label></formula><p>We found that this simple linear model is effective to correct the bias introduced in the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We compare our BiC method to the state-of-the-art methods on two large datasets (ImageNet ILSVRC 2012 <ref type="bibr" target="#b19">[20]</ref> and MS-Celeb-1M <ref type="bibr" target="#b5">[6]</ref>), and one small dataset (CIFAR-100 <ref type="bibr" target="#b10">[11]</ref>). We also perform ablation experiments to analyze different components of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>We use all data in CIFAR-100 and ImageNet ILSVRC 2012 (referred to ImageNet-1000), and randomly choose 10000 classes in MS-Celeb-1M (referred to Celeb-10000). We follow iCaRL benchmark protocol <ref type="bibr" target="#b18">[19]</ref> to select exemplars. The total number of exemplars for the old classes are fixed. The details of these three datasets are as follows: CIFAR-100: contains 60k 32 × 32 RGB images of 100 object classes. Each class has 500 training images and 100 testing images. 100 classes are split into 5, 10, 20 and 50 incremental batches. 2,000 samples are stored as exemplars. ImageNet-1000: includes 1,281,167 images for training and 50,000 images for validation. 1000 classes are split into 10 incremental batches. 20,000 samples are stored as exemplars.</p><p>Celeb-10000: a random subset of 10,000 classes are selected from MS-Celeb-1M-base <ref type="bibr" target="#b4">[5]</ref> face dataset which has 20,000 classes. MS-Celeb-1M-base is a smaller yet nearly noise-free version of MS-Celeb-1M <ref type="bibr" target="#b5">[6]</ref>, which has near 100,000 classes with a total of 1.2 million aligned face images. For the randomly selected 10,000 classes, there are 293,052 images for training and 141,984 images for validation. 10000 classes are split into 10 incremental batches (1000 classes per batch). 50,000 samples are stored as exemplars.</p><p>For our BiC method, the ratio of train/validation split on the exemplars is 9:1 for CIFAR-100 and ImageNet-1000. This ratio is obtained from the ablation study (see Section 6.6). We change the split ratio to 4:1 on Celeb-10000, allowing at least one validation image kept per person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Implementation Details</head><p>Our implementation uses TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We use an 18layer ResNet <ref type="bibr" target="#b6">[7]</ref> for ImageNet-1000 and Celeb-10000 and use a 32-layer ResNet for CIFAR-100. The ResNet implementation is from TensorFlow official models 1 . The training details for each dataset are listed as follows: ImageNet-1000 and Celeb-10000: Each incremental training has 100 epochs. The learning rate is set to 0.1 and reduces to 1/10 of the previous learning rate after 30, 60, 80 and 90 epochs. The weight decay is set to 0.0001 and the batch size is 256. Image pre-processing follows the VGG pre-processing steps <ref type="bibr" target="#b23">[24]</ref>, including random cropping, horizontal flip and aspect preserving resizing and mean subtraction. CIFAR-100: Each incremental training has 250 epochs. The learning rate starts from 0.1 initially and reduces to 0.01, 0.001 and 0.0001 after 100, 150 and 200 epochs, respectively. The weight decay is set to 0.0002 and the batch size is 128. Random cropping and horizontal flip is adapted for data augmentation following the original ResNet implementation <ref type="bibr" target="#b6">[7]</ref>.</p><p>For a fair comparison with iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref>, we use the same networks, keep the same number of exemplars and follow the same protocols of splitting classes into incremental batches. We use the identical class order generated from iCaRL implementation 2 for CIFAR-100 and ImageNet-1000. On Celeb-10000, the class order is randomly generated and identical for all comparisons. The temperature scalar T in Eq. 1 is set to 2 by following [13, 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison on Large Datasets</head><p>In this section, we compare our BiC method with the state-of-the-art methods on two large datasets (ImageNet-1000 and Celeb-10000). The state-of-the-art methods include LwF <ref type="bibr" target="#b12">[13]</ref>, iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref>. All of them 1 https://github.com/tensorflow/models/tree/ master/official/resnet utilize knowledge distillation to prevent catastrophic forgetting. iCaRL and EEIL keep exemplars for old classes, while LwF does not use any old data. The incremental learning results on ImageNet-1000 are shown in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_7">Figure 6</ref>-(a). Our BiC method outperforms both EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref> by a large margin. BiC has a small gain for the first couple of incremental batches compared with iCaRL and is worse than EEIL in the first two increments. However, the gain of BiC increases as more incremental batches arrive. Regarding the final incremental classifier on all classes, our BiC method outperforms EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref> by 18.5% and 26.5% respectively. On average over 10 incremental batches, BiC outperforms EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref> by 11.1% and 19.7% respectively.</p><p>Note that the data imbalance increases as more incremental steps arrive. The reason is that the number of exemplars per old class decreases as the incremental step increases, since the total number of exemplars is fixed (by following the fix memory protocol in EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref>). The gap between our BiC method and other methods becomes wider as the incremental step increases with more data imbalance. This demonstrates the advantage of our BiC method.</p><p>We also observe that EEIL performs better for the second batch (even higher than the first batch) on ImageNet-1000. This is mostly due to the enhanced data augmentation (EDA) in EEIL that is more effective for the first couple of incremental batches when data imbalance is mild. EDA includes random brightness shift, contrast normalization, random cropping and horizontal flipping. In contrast, BiC only applies random cropping and horizontal flipping. EEIL <ref type="bibr" target="#b1">[2]</ref> shows that EDA is effective for early incremental batches when data imbalance is not severe. Even without the enhanced data augmentation, our BiC still outperforms EEIL by a large margin on ImageNet-1000 starting from the third batch.</p><p>The incremental learning results on Celeb-10000 are shown in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_7">Figure 6-(b)</ref>. To the best of our knowl-LwF <ref type="bibr" target="#b12">[13]</ref>   edge, we have not seen any incremental learning method reporting results on 10,000 or more classes. The results for iCaRL is generated by applying its github implementation 2 on Celeb-10000 dataset. For the first couple of incremental steps, our BiC method is slightly better than (&lt; 3%) iCaRL. But since the third incremental step, the gap becomes wider. At the last incremental step, BiC outperforms iCaRL by 22.4%. The average gain over 10 incremental batches is 13.2%. These results demonstrate our BiC method is more effective and robust to deal with a large number of classes. As the number of classes increases, it is more frequent to have visually similar classes across different increment batches with unbalanced data. This introduces a strong bias towards new classes and misclassifies the old classes that are visually similar. Our BiC method is able to effectively reduce this bias and improve the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison between Different Scales</head><p>In this section, we compare our BiC method with the state-of-the-art on two different scales on ImageNet. The small scale deals with random selected 100 classes (referred to ImageNet-100), while the large scale involves all 1000 classes (referred to ImageNet-1000). Both scales have 10 incremental batches. This follows the same protocol with EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref>. The results for ImageNet-1000 is the same as in the previous section.</p><p>The incremental learning results on Imagenet-100 and ImageNet-1000 are shown in <ref type="figure" target="#fig_9">Fig. 7</ref>. Our BiC method outperforms the state-of-the-art for both scales in terms of the final incremental accuracy and the average incremental accuracy. But the gain for the large scale is bigger. We also compare the final incremental accuracy (the last step) to the upper bound, which is obtained by training a non-incremental model using all classes and their train- ing data (shown at the last step in <ref type="figure" target="#fig_9">Fig. 7)</ref>. Compared to the upper bound, our BiC method degrades 10.5% and 16.0% on ImageNet-100 and ImageNet-1000 respectively. However, EEIL [2] degrades 15.1% and 37.2% and iCaRL <ref type="bibr" target="#b18">[19]</ref> degrades 31.1% and 45.2%. Compared with EEIL <ref type="bibr" target="#b1">[2]</ref> and iCaRL <ref type="bibr" target="#b18">[19]</ref>, which have more performance degradation from the small scale to large scale, our BiC method is much more consistent. This demonstrates that BiC has better capability to handle the large scale. We are aware that BiC is behind EEIL <ref type="bibr" target="#b1">[2]</ref> for the first three incremental batches on ImageNet-100. As explained in Section 6.3, this is mostly due to enhanced data argumentation (EDA) in EEIL [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Comparison on a Small Dataset</head><p>We also compare our BiC method with the state-of-theart algorithms on a small dataset -CIFAR-100 <ref type="bibr" target="#b10">[11]</ref>. The incremental learning results with four different splits of 5, 10, 20 and 50 classes are shown in <ref type="figure" target="#fig_10">Fig. 8</ref>. Our BiC method has similar performance with iCaRL <ref type="bibr" target="#b18">[19]</ref> and EEIL <ref type="bibr" target="#b1">[2]</ref>. BiC is better on the split of 50 and 20 classes, but is slightly behind EEIL on the split of 10 and 5 classes. The margins are small for all splits. Although our method focuses on the large scale incremental learning, it is also compelling on the small scale. Note that EEIL has more data augmentation such as brightness augmentation and contrast normalization, which are not utilized in LwF, iCaRL or BiC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Ablation Study</head><p>We now analyze the components of our BiC method and demonstrate their impact. The ablation study is performed on CIFAR-100 <ref type="bibr" target="#b10">[11]</ref>, as incremental learning on large dataset is time consuming. The ablation study is performed on CIFAR-100 with an incremental of 20 classes. The size of the stored exemplars from old classes is 2,000. In the following ablation study, we analyze (a) the impact of bias correction, (b) the split of validation set, and (c) the sensitivity of exemplar selection.</p><p>The Impact of Bias Correction We compare our BiC method with two variations of baselines and the upper bound, to analyze the impact of bias correction. The baselines and the upper bound are explained as follows: baseline-1: the model is trained using the classification loss alone (Eq. 2). baseline-2: the model is trained using both the distilling loss and the classification loss (Eq. 3). Compared to the baseline-1, the distilling loss is added. BiC: the model is trained using both the distilling loss and the classification loss, with the bias correction. upper bound: the model is firstly trained using both the distilling loss and classification loss. Then, the feature layers are frozen and the classifier layer (i.e. the fully connected layer) is retrained using all training data (including the samples from the old classes that are not stored). Although it is infeasible to have all training samples from the old classes, it shows the upper bound for the bias correction in the fully connected layer.</p><p>The incremental learning results are shown in <ref type="table">Table 3</ref>. With the help of the knowledge distillation, baseline-2 is slightly better than baseline-1 since it retains the classifica-tion capability on the old classes. However, both baseline-1 and baseline-2 have low accuracy on the final step to classify all 100 classes (about 40%). This is mainly because of the data imbalance between the old and new classes. When using the bias correction, BiC improves the accuracy on all incremental steps. The classification accuracy on the final step (100 classes) is boosted from 40.34% to 56.69%. This demonstrates that the bias is a big issue and our method is effective to address it. Furthermore, our method is close to the upper bound. The small gap (4.24%) from our approach 56.69% to the upper bound 60.93% shows the superiority of our method.</p><p>The confusion matrices of these four variations are shown in <ref type="figure">Fig. 9</ref>. Clearly, baseline-1 and baseline-2 suffer from the bias towards the new classes (strong confusions on the last 20 classes). BiC reduces the bias and has similar confusion matrix to the upper bound.</p><p>These results validate our hypothesis that there exists a strong bias towards the new classes in the last fully connected layer. In addition, the results demonstrate that the proposed bias correction using a linear model on a small validation set is capable to correct the bias.</p><p>The Split of Validation Set We study the impact of different splits of the validation set (see Section 5.1). As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, our BiC splits the stored exemplars from the old classes into a training set (train old ) and a validation set (val old ). The samples from the new classes also have a train/val split (train new and val new ). train old and train new are used to learn the convolution layers and the fully connected layer, while val old and val new are used to learn the bias correction layer. Note that val old and val new are balanced, having the same number of samples per class. Since only a few exemplars (i.e. train old val old ) are stored for the old classes, it is critical to find a good split that deals with the trade-off between training the feature representation and correcting the bias in the fully connected layer. <ref type="table">Table 4</ref> shows the incremental learning results for four different splits of train old : val old . The split of 9:1 has the best classification accuracy for all four incremental steps.     <ref type="table">Table 4</ref>. Incremental learning results on CIFAR-100 with a batch of 20 classes for different training/validation split on exemplars from old classes. The training set is used to learn the feature and classifier layers, and the validation set is used to learn the bias correction layer. The best results are marked in bold.</p><p>The column 20 refers to learning a classifier for the first 20 classes, without incremental learning. As the portion for the validation set increases, the performance drops consistently due to the lack of exemplars (from the old classes) to train the feature layers. A small validation set ( 1 10 of exemplars) is good enough to estimate the bias parameters (α and β in Eq. 4). In this paper, we use split 9:1 for all other experiments except Celeb-10000. The split 4:1 is adopted in Celeb-10000, as each old class only has 5 exemplars for the last incremental step.</p><p>The Sensitivity of Exemplar Selection We also study the impact of different exemplar management strategies. We compare two strategies: (a) random selection, and (b) the exemplar management strategy proposed by iCaRL <ref type="bibr" target="#b18">[19]</ref>. iCaRL maintains the samples that closed to the class center in the feature space. Both strategies store 2,000 exemplars from old classes. The incremental learning results are shown in <ref type="table" target="#tab_4">Table 5</ref>. iCaRL exemplar management strategy performs slightly better than the random selection. The gap is about 1%. This demonstrates that our method is not sensitive to the exemplar selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we proposed a new method to address the imbalance issue in incremental learning, which is critical when the number of classes becomes large. Firstly, we validated our hypothesis that the classifier layer (the last fully connected layer) has a strong bias towards the new classes, which has substantially more training data than the old classes. Secondly, we found that this bias can be effectively corrected by applying a linear model with a small validation set. Our method has excellent results on two large datasets with 1,000+ classes (ImageNet ILSVRC 2012 and MS-Celeb-1M), outperforming the state-of-the-art by a large margin (11.1% on ImageNet ILSVRC 2012 and 13.2% on MS-Celeb-1M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgments</head><p>Part of the work was done when Yue Wu was an intern at Microsoft. This research is supported in part by the NSF IIS Award 1651902.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our BiC method. The exemplars from the old classes and the samples of the new classes are split into training and validation sets. The training set is used to train the convolution layers and FC layer (in stage 1). The validation set is used for bias correction (in stage 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>) = [o -. , o -0 , … , o -' ] '34 ( ) = [o . , o 0 , … , o ' , o '3. , … , o '34 ] Diagram of the baseline solution using distillation. It contains two losses: the distilling loss on old classes and the softmax cross-entropy loss on all old and new classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where M is the number of new samples, x i and y i are the image and the label, respectively. The selected exemplars from the old n classes are denoted asX n = {(x j ,ŷ j ), 1 ≤ j ≤ N s ,ŷ j ∈ [1, .., n]}, where N s is the number of selected old images (N s /n M/m). Let us also denote the output logits of the old and new classifiers asô n (x) = [ô 1 (x), ...,ô n (x)] and o n+m (x) = [o 1 (x), ..., o n (x), o n+1 (x), ..., o n+m (x)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>removal Our method: remove bias in the last FC layer Retrain the last FC layer using all data Train all layers using all data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Experimental results on CIFAR-100 with split of 20 classes to validate the bias in the last FC layer. (a) classification accuracy curves for baseline, our bias correction (BiC), retraining FC layer using all data, and training the whole network using all data (from bottom to top). (b) confusion matrix of the incremental classifier from 80 classes to 100 classes without bias removal. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Diagram of bias correction. Since the number of exemplars from old classes is small, they have narrow distributions on the feature space. This causes the learned classifier to prefer new classes. Validation samples, not involved in training feature representation, may better reflect the unbiased distribution of both old and new classes in the feature space. Thus, we can use the validation samples to correct the bias. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Incremental learning results (accuracy %) on (a) ImageNet-1000 and (b) Celeb-10000. Both datasets have ten incremental batches. The Upper Bound result, shown in the last step, is obtained by training a non-incremental model using all training samples from all classes. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>90.0 77.0 68.0 59.5 52.5 49.5 46.5 43.0 40.5 39.0 iCaRL [19] 90.0 83.0 77.5 70.5 63.0 57.5 53.5 50.0 48.0 44.0 EEIL [2] 95.0 95.5 86.0 77.5 71.0 68.0 62.0 59.8 55.0 52.0 BiC(Ours) 94.1 92.5 89.6 89.1 85.7 83.2 80.2 77.5 75.0 73.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2Figure 7 .</head><label>7</label><figDesc>https://github.com/srebuffi/iCaRL Incremental learning results (accuracy %) on ImageNet-100 and ImageNet-1000. Both have ten incremental batches. The Upper Bound result, shown in the last step, is obtained by training a non-incremental model using all training samples from all classes. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Incremental learning results on CIFAR-100 with split of (a) 5 classes, (b) 10 classes, (c) 20 classes and (d) 50 classes. The Upper Bound result, shown in the last step, is obtained by training a non-incremental model using all training samples for all classes. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 .Figure 9 .</head><label>39</label><figDesc>Incremental learning results on CIFAR-100 with a batch of 20 classes. baseline-1 uses the classification loss alone. baseline-2 uses both the distilling loss and the classification loss. BiC corrects the bias in FC layer of baseline-2. Upper bound retrains the last FC layer using all samples from both old and new classes after learning the model of baseline-2. The best results are marked in bold. Confusion matrices of four different variations: (a) baseline-1 (b) baseline-2, (c) BiC, (d) upper bound. Both baseline-1 and baseline-2 have strong bias towards new classes. BiC is capable to remove most of the bias and have similar confusion matrix with the upper bound. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Incremental learning results (accuracy %) on ImageNet-1000 dataset with an increment of 100 classes. LwF<ref type="bibr" target="#b12">[13]</ref> does not use any exemplars from the old classes. iCaRL<ref type="bibr" target="#b18">[19]</ref>, EEIL<ref type="bibr" target="#b1">[2]</ref> and our BiC method use the same amount of exemplars from the old classes. Note that the numbers for LwF, iCaRL and EEIL on ImageNet-1000 are estimated from the figures in the original papers. The best results are marked in bold. Incremental learning results (accuracy %) on Celeb-10000 dataset with an increment of 1000 classes. iCaRL<ref type="bibr" target="#b18">[19]</ref> and our BiC method use the same amount of exemplars from the old classes. The best results are marked in bold.</figDesc><table><row><cell></cell><cell>1000 2000 3000 4000 5000</cell><cell>6000</cell><cell>7000 8000 9000 10000</cell></row><row><cell cols="4">iCaRL [19] 94.31 94.26 91.09 86.88 81.06 77.45 75.29 71.34 68.78 65.56</cell></row><row><cell>BiC(Ours)</cell><cell cols="3">95.90 96.65 96.68 96.16 95.43 94.45 93.35 91.90 90.18 87.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>68.30 55.10 48.52 39.83 baseline-2 85.05 72.22 59.41 50.43 40.34 BiC(Ours) 84.00 74.69 67.93 61.25 56.69 upper bound 84.39 76.15 69.51 64.03 60.93</figDesc><table><row><cell>Variations</cell><cell>cls loss distilling loss bias removal FC retrain</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell>baseline-1</cell><cell></cell><cell>84.40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>74.59 66.76 60.14 55.55 iCaRL [19] 84.00 74.69 67.93 61.25 56.69 Incremental learning results on CIFAR-100 with a batch of 20 classes for different exemplar management strategies. The best results are marked in bold.</figDesc><table><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell>random</cell><cell>85.20</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incremental and decremental support vector machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="409" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One-shot face recognition by promoting underrepresented classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongwoo</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minju</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00122</idno>
		<title level="m">Less-forgetting learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From n to n+ 1: Multiclass transfer incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilja</forename><surname>Kuzborskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3358" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6470" to="6479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learn++: An incremental learning algorithm for supervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robi</forename><surname>Polikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalita</forename><surname>Upda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Upda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics, part C (applications and reviews)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="497" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal Rannen Ep</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ICCV 2017</title>
		<meeting>ICCV 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2994" to="3003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lifelong metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active lifelong learning with&quot; watchdog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A strategy for an uncompromising incremental learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00744</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Error-driven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
