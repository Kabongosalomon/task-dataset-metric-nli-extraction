<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Network Models for Human Dynamics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@eecs</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Network Models for Human Dynamics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units <ref type="bibr" target="#b30">[31]</ref> .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans have a remarkable ability to make accurate short-term predictions about the world around them conditioned on prior events <ref type="bibr" target="#b40">[41]</ref>. Predicting the movements of other humans is an important facet of these predictions. Although the number of possible movements is enormous, conditioning on visual history can reduce the range of probable outcomes to a manageable degree of variation. For example, a walking pedestrian will most likely continue walking, and will probably not begin dancing spontaneously. Short term predictions of human kinematics allows people to adjust their behavior, plan their actions, and properly direct their attention when interacting with others. Similarly, for Computer Vision algorithms, predicting human motion is important for timely human-computer interaction <ref type="bibr" target="#b16">[17]</ref>, obstacle avoidance <ref type="bibr" target="#b21">[22]</ref>, and people tracking <ref type="bibr" target="#b7">[8]</ref>. While simpler physical phenomena, such as the motion of inanimate objects, can be predicted using known physical laws, there is no simple equation that governs the conscious movements of a person. Predicting the motion of humans instead calls for a statistical approach that can model the range of variation of future behavior, and presents a tremendous challenge for machine learning algorithms.</p><p>We address this challenge by introducing Encoder-Recurrent-Decoder (ERD) networks, a type of Recurrent Neural Network (RNN) model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b23">24]</ref> that combines representation learning with learning temporal dynamics. We apply this model to generation, labeling, and forecasting of human kinematics. We consider two data domains: motion capture ("mocap") and video sequences. For mocap, conditioning on a mocap sequence so far, we learn a distribution over mocap feature vectors in the subsequent frame. At test time, by supplying mocap samples as input back to the model, long sequences are synthesized. For video, conditioning on a person bounding box sequence, we predict the body joint locations in the current frame or, for the task of body pose forecasting, at a specific point in the future. In the mocap case, the input and output domains coincide (3D body joint angles). In the video case, the input and output domains differ (raw video pixels versus body joint locations).</p><p>RNNs are network models that process sequential data using recurrent connections between their neural activations at consecutive time steps. They have been successfully applied in the language domain for text and handwriting generation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref>, image captioning <ref type="bibr" target="#b42">[43]</ref>, action recognition <ref type="bibr" target="#b5">[6]</ref>. Ranzato et al. <ref type="bibr" target="#b22">[23]</ref> applies RNNs for visual prediction by quantizing the visual signal into a vocabulary of visual words, and predicts a distribution over those words in the next frame, given the visual word sequence observed at a particular pixel location.</p><p>We advocate a visual predictive model that is "La-grangian" in nature <ref type="bibr" target="#b47">[48]</ref>: it predicts future outcomes conditioning on an object tracklet rather than on a tube fixated at a particular pixel location, as <ref type="bibr" target="#b22">[23]</ref> (the "Eulerian" approach). Such object-centric conditioning exploits more relevant visual history of the object for prediction. In contrast, a visual tube fixated at a particular pixel location encounters dramatically different content under camera or object motion. In the ERD, the encoder transforms the input data to a representation where learning of dynamics is easy. The decoder transcribes the output of the recurrent layers to the desired visual form. For mocap generation, the encoder and decoder are multilayer fully connected networks. For video pose labeling and prediction, the encoder is a Convolutional Neural Network (CNN) <ref type="bibr" target="#b3">[4]</ref> initialized by a CNN per frame body part detector and decoder is a fully connected network. ERDs simultaneously learn both the representation most suitable for recognition or prediction (input to the recurrent layer), as well as its dynamics, represented in the recurrent weights, by jointly training encoding, decoding and recurrent subnetworks. We found such joint finetuning crucial for empirical performance.</p><p>We test ERDs in kinematic tracking and forecasting in the H3.6M video pose dataset of Ionescu et al. <ref type="bibr" target="#b12">[13]</ref>. It is currently the largest video pose dataset publicly available. It contains a diverse range of activities performed by professional actors and recorded with a Vicon motion capture system. We show that ERDs effectively learn human dynamics in video and motion capture. In motion generation, ERDs synthesize mocap data across multiple activities and subjects. We demonstrate the importance of the nonlinear encoder and decoder in ERDs by comparing to previous multilayer LSTM models <ref type="bibr" target="#b8">[9]</ref>. We show that such models do not produce realistic motion beyond very short horizons. For video pose labeling, ERDs outperforms a per frame body part CNN detector, particularly in the case of left-right confusions. For future pose forecasting, ERDs forecast joint positions 400ms in the future, outperforming first order motion modeling with optical flow constancy assumptions.</p><p>Our experiments show that the proposed ERD models can simultaneously model multiple activity domains, implicitly detect the right activity scenario, and adapt their output labels or predictions accordingly. The action transitioning is transparent, in contrast to previous switching dynamical systems or switching HMMs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref> for activity modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Motion generation Generation of naturalistic human motion using probabilistic models trained on motion capture data has previous been addressed in the context of computer graphics and machine learning. Prior work has tackled synthesis of stylized human motion using bilinear spatiotemporal basis models <ref type="bibr" target="#b0">[1]</ref>, Hidden Markov Models <ref type="bibr" target="#b2">[3]</ref>, linear dynamical systems <ref type="bibr" target="#b20">[21]</ref>, and Gaussian process latent variable models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b39">40]</ref>, as well as multilinear variants thereof <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>. Unlike methods based on Gaussian processes, we use a parametric representation and a simple, scalable supervised training method that makes it practical to train on large datasets.</p><p>Dynamical models based on Restricted Boltzmann Machines (RBMs) have been proposed for synthesis and infilling of motion data <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. While such approaches have the advantage of learning probabilistic models, this also results in a substantially more complex training algorithm and, when multilayer models are used, requires sampling for approximate inference. In contrast, our RNNbased models can be trained with a simple stochastic gradient descent method, and can be evaluated very efficiently at test time with simple feedforward operations.</p><p>Video pose labeling and forecasting Temporal context has been exploited in kinematic tracking using dynamic programming over multiple per frame body pose hypotheses <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, where unary potentials encore detectors' confidence and pairwise potentials encode temporal smoothness. Optical flow has been used in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> to adjust the temporal smoothness penalty across consecutive frames. Optical flow can only estimate the motion of body joints that do not move too fast and do not get occluded or dis-occluded. Moreover, the temporal coupling is again pairwise, not long range. ERDs keep track of body parts as they become occluded and disoccluded by aggregating information in time across multiple frames, rather than the last frame.</p><p>Parametric temporal filters such as Kalman filtering <ref type="bibr" target="#b46">[47]</ref>, HMMs or Gaussian processes for activity specific dynamics <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> generally use simple, linear dynamics models for prediction. Such simple dynamics are only valid within very short temporal horizons, making it difficult to incorporate long range temporal information. Switching dynamic systems or HMMs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref> detect activity transitioning explicitly. In contrast, in ERD, action transitioning is transparent to the engineer, and also more effective. Moreover, HMM capacity increases linearly with increasing numbers of hidden states, but its parameter count increases quadratically. This makes it difficult to scale such models to large and diverse datasets. ERDs scale better than previous parametric methods in capturing human dynamics. RNNs use distributed representations: each world "state" is represented with the ensemble of hidden activations in the recurrent layer, rather than a single one. Thus, adding a neural unit quadratically increases the number parameters yet doubles the representation power -assuming binary units.</p><p>Standard temporal smoothers or filters are disconnected from the pose detector and operate on its output, such as the space of body joint locations. This representation discards context information that may be present in the original <ref type="figure">Figure 1</ref>. ERDs for human dynamics in video and motion capture. Given a mocap sequence till time t, the ERD for mocap generation predicts the mocap vector at time instance t + 1. Given a person tracklet till time t, ERD for video forecasting predicts body joint heat maps of the next frame t + 1. ERD for video labeling predicts heat maps of the current frame instead.</p><formula xml:id="formula_0">fc3(54) relu2 fc2 (100) relu1 fc1 (500) fc2 (500) relu1 fc1 (500) noise lstm2 (1000) lstm1 (1000) Mo7on capture genera7on Encoder--Recurrent-- Decoder (ERD) relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) Video pose forecas7ng Video pose labeling relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) fc3(54) relu2 fc2 (100) relu1 fc1 (500) fc2 (500) relu1 fc1 (500) noise lstm2 (1000) lstm1 (1000) fc3(54) relu2 fc2 (1000) relu1 fc1 (5000) fc2 (5000) relu1 fc1 (1000) noise lstm2 (1000) lstm1 (1000) Motion capture generation Encoder-Recurrent- Decoder (ERD) +1 −1 −1 −1 +1 relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) Video pose forecasting Video pose labeling relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) relu5 conv5 relu4 conv4 relu3 conv3 pool2 relu2 conv2 pool1 relu1 conv1 lstm1 (1000) fc2 (648) relu1 fc1 (500) fc3(54) relu2 fc2 (1000) relu1 fc1 (5000) fc2 (5000) relu1 fc1 (1000) noise lstm2 (1000) lstm1 (1000)</formula><p>video. In contrast, ERDs learn the representation suitable for temporal reasoning and can take advantage of visual appearance and context. <ref type="figure">Figure 1</ref> illustrates ERD models for recurrent kinematic tracking and forecasting. At each time step t, vector x t of a sequence x = (x 1 , · · · , x T ) passes through the encoder, the recurrent layers, and the decoder network, producing the output y t . In general, we are interested in estimating some function f (x) of the input x at the current time step, or at some time in the future. For example, in the case of motion capture, we are interested in estimating the mocap vector at the next frame. Since both the input and output consists of mocap vectors, f is the identity transformation, and the desired output at step t is f (x t+1 ). In case of video pose labeling and forecasting, f (x) denotes body joint locations corresponding to the image in the current bounding box x. At step t, we are interested in estimating either f (x t ) in the case of labeling, or f (x t+H ) in the case of forecasting, where H is the forecast horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ERDs for recurrent kinematic tracking and forecasting</head><p>The units in each recurrent layer implement the Long Short Term Memory functions <ref type="bibr" target="#b10">[11]</ref>, where writing, resetting, and reading a value from each recurrent hidden unit is explicitly controlled via gating units, as described by Graves <ref type="bibr" target="#b8">[9]</ref>. Although LSTMs have four times more parameters than regular RNNs, they facilitate long term storage of task-relevant data. In Computer Vision, LSTMs have been used so far for image captioning <ref type="bibr" target="#b42">[43]</ref> and action classifica-tion in videos <ref type="bibr" target="#b5">[6]</ref>.</p><p>ERD architecture extends prior work on LSTMs by augmenting the model with encoder and decoder networks. Omitting the encoder and decoder networks and instead using linear mappings between the input, recurrent state, and output caused underfitting on all three of our tasks. This can be explained by the complexity of the mocap and video input in comparison to the words or pen stroke 2D locations considered in prior work <ref type="bibr" target="#b8">[9]</ref>. For example, word embeddings were not crucial for RNNs to do well in text generation or machine translation, and the standard one hot encoding vocabulary representation also showed excellent results <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating Motion Capture</head><p>Our goal is to predict the mocap vector in the next frame, given a mocap sequence so far. Since the output y t has the same format as the input x t+1 , if we can predict x t+1 , we can "play" the motion forward in time to generate a novel mocap sequence by feeding the output at the preceding time step as the input to the current one.</p><p>Each mocap vector consists of a set of 3D body joint angles in a kinematic tree representation. We represent the orientation of each joint by an exponential map in the coordinate frame of its parent, corresponding to 3 degrees of freedom per joint. The global position of the body in the x-y plane and the global orientation about the vertical z axis are predicted relative to the previous frame, since each clip has an arbitrary global position. This is similar to the approach taken in previous work <ref type="bibr" target="#b33">[34]</ref>. We standardize our input by mean subtraction and division by the standard deviation along each dimension.</p><p>We consider both deterministic and probabilistic predictions. In the deterministic case, the decoder's output y t is a single mocap vector. In this case, we train our model by minimizing the Euclidean loss between target and predicted body joint angles. In the probabilistic case, y t parametrizes a Gaussian Mixture Model (GMM) over mocap vectors in the next frame. We then minimize the GMM negative loglikelihood during training:</p><formula xml:id="formula_1">L(x) = − T t=1 logPr(x t+1 |y t )<label>(1)</label></formula><p>We use five mixture components and diagonal covariances.</p><p>The variances are outputs of exponential layers to ensure positivity, and the mixture component probabilities are outputs of a softmax layer, similar to <ref type="bibr" target="#b8">[9]</ref>. During training, we pad the variances in each iteration by a fixed amount to ensure they do not collapse around the mixture means.</p><p>Weights are initialized randomly. We experimented with initializing the encoder and decoder networks of the mocap ERD from the (first two layers of) encoder and (last two layers of) decoder of a) a ten layer autoencoder trained for dimensionality reduction of mocap vectors <ref type="bibr" target="#b9">[10]</ref>, b) a "skip" autoencoder trained to reconstruct the mocap vector in few frames in the future given the current one. In both cases, we did not observe improvement over random weight initialization. We train our ERD model with stochastic gradient descent and backpropagation through time <ref type="bibr" target="#b49">[50]</ref> with momentum and gradient clipping at 25, using the publicly available Caffe package <ref type="bibr" target="#b14">[15]</ref> and the LSTM layer implementation from <ref type="bibr" target="#b5">[6]</ref>. We regularize our mocap ERD with denoising: we provide mocap vectors corrupted with zero mean Gaussian noise <ref type="bibr" target="#b41">[42]</ref> and have the model predict the correct, uncorrupted mocap vector in the next frame. We found it valuable to progressively increase the noise standard deviation, learning from non-corrupted examples first. This corresponds to a type of curriculum learning. At test time, we run the model forward by feeding the predictions as input to the model in the following time step. Without denoising, this kind of forward unrolling suffers from accumulation of small prediction mistakes at each frame, and the model quickly falls into unnatural regions of the state space. Denoising ensures that corrupted mocap data are shown to the network during training so that it learns to correct small amounts of drift and stay close to the manifold of natural poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Labeling and forecasting video pose</head><p>In the previous section, we described how the ERD model can be used to synthesize naturalistic human motion by training on motion capture datasets. In this section, we extend this model to identify human poses directly from pixels in a video. We consider a pose labeling task and a pose forecasting task. In the labeling task, given a bounding box sequence depicting a person, we want to estimate body joint locations for the current frame, given the sequence so far. In the forecasting task, we want to estimate body joint locations for a specific future time instance instead.</p><p>We represent K body joint locations as a set of K N ×N heat maps over the person's bounding box, that represent likelihood for each joint to appear in each of the N 2 grid locations, similar to <ref type="bibr" target="#b37">[38]</ref>. Predicting heat maps naturally incorporates uncertainty over body joint locations, as opposed to predicting body joint pixel coordinates. <ref type="figure">Figure 1right</ref> illustrates our ERD architecture for video pose labeling and forecasting. The encoder is a five layer convolutional network with architecture similar to Krizhevsky et al. <ref type="bibr" target="#b17">[18]</ref>. Our decoder is a two layer network with fully connected layers interleaved with rectified linear unit layers. The output of the decoder is body joint heat maps over the person bounding box in the current frame for the labeling task, or body joint heat maps at a specified future time instance for the forecasting task.</p><p>We train both our pose labeler and forecaster ERDs under a Euclidean loss between estimated and target heat maps. We initialize the weights of the encoder from a six layer convolutional network trained for per frame body part detection, in which the final CONV6 layer corresponds to the body joint heat maps.</p><p>Empirically, we found it valuable to input to the recurrent layer not the per frame estimated heat maps (CONV6), but rather the preceding CONV5 feature maps. These feature maps capture rich appearance information, rather than merely body joint likelihood. Rich appearance information assists the network in discriminating between different actions and pose dynamics without explicit switching across activity domains, as previous switching dynamical linear systems or HMMs <ref type="bibr" target="#b20">[21]</ref>.</p><p>We use two networks on different image scales for our per frame pose detector and ERD: one where the output layer resolution is 6×6 and one that works on double image size and has output resolution of 12×12. The heat maps of the coarser scale are upsampled and added to the finer scale to provide the final combined 12×12 heat maps. Multiple scales have shown to be beneficial for static pose estimation in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We test our method on the H3.6M dataset of Ionescu et al. <ref type="bibr" target="#b12">[13]</ref>, which is currently the largest video pose dataset. It consists of 15 activity scenarios, performed by seven different professional actors and recorded from four static cameras. For each activity scenario, subject, and camera viewpoint, there are two video sequences, each be-tween 3000 and 5000 frames. Each activity scenario features rich gestures, pose variations and interesting subactions performed by the actors. For example, the walking activity includes holding hands, carrying a heavy load, putting hands in the pockets, looking around etc. The activities are recorded using a Vicon motion capture system that tracks markers on actors' body joints and provides high quality 3D body joint locations. 2D body joints locations are obtained by projecting the 3D positions onto the image plane using the known camera calibration and viewpoint. For all our experiments, we treat subject 5 as the test subject and all others as our training subjects.</p><p>Motion capture generation We compare our ERD mocap generator with a) an LSTM recurrent neural network with linear encoder and decoders that has 3 LSTM layers of 1000 units each (architecture found through experimentation to work well), b) Conditional Restricted Boltzmann Machines (CRBMs) of Taylor et al. <ref type="bibr" target="#b33">[34]</ref>, c) Gaussian Process Dynamic Model (GPDM) of Wang et al. <ref type="bibr" target="#b43">[44]</ref>, and d) a nearest neighbor N-gram model (NGRAM). For CRBM and GPDM, we used the code made publicly available by the authors. For the nearest neighbor N-gram model, we used a frame window of length N = 6 and Euclidean distance on 3D angles between the conditioning prefix and our training set, and copy past the subsequent frames of the best matching training subsequence. We applied denoising during training to regularize both the ERD and the LSTM-3LR. For all models, the mocap frame sequences were subsampled by two. ERD, LSTM-3LR and CRBM are trained on multiple activity scenarios (Walking, Eating and Smoking). GPDM is trained on Walking activity only, because its cubic complexity prohibits its training on a large number of sequences. Our comparison focuses on motion forecasting (prediction) and synthesis, conditioning on motion prefixes of our test subject. Mocap in-filling and denoising are nontrivial with our current model but developing this functionality is an interesting avenue for future work.</p><p>We show qualitative motion synthesis results in <ref type="figure">Figure  2</ref> and quantitative motion prediction errors in <ref type="table" target="#tab_0">Table 1</ref>. In <ref type="figure">Figure 2</ref>, the conditioning motion prefix from our test subject is shown in green and the generated motion is shown in blue. In <ref type="table" target="#tab_0">Table 1</ref>, we show Euclidean norm between the synthesized motion and ground-truth motion for our test subject for different temporal horizons past the conditioning motion prefix, the largest being 560msecs, averaged across 8 different prefixes. The stochasticity of human motion prevents a metric evaluation for longer temporal horizons, thus all comparisons in previous literature are qualitative. LSTM-3LR dominates the short-term motion generation, yet soon converges to the mean pose, as shown in <ref type="figure">Figure 2</ref>. CRBM also provides smooth short term motion completions, yet quickly drifts to unrealistic motions. ERD provides slightly Ground--truth ERD CRBM N--gram GPDM t LSTM--3LR <ref type="figure">Figure 2</ref>. Motion synthesis. LSTM-3LR and CRBMs <ref type="bibr" target="#b33">[34]</ref> provide smooth short-term motion completions (for up to 600msecs), mimicking well novel styles of motion, (e.g., here, walking with upright back). However, ERD generates realistic motion for longer periods of time while LSTM-3LR soon converges to the mean pose and CRBM diverges to implausible motion. NGRAM has a non-smooth transition from conditioning to generation. Per frame mocap vectors predicted by GPDM <ref type="bibr" target="#b43">[44]</ref> look plausible, but their temporal evolution is far from realistic. You can watch the corresponding video results at https://sites.google.com/site/motionlstm/ . less smooth completions, yet can generate realistic motion for long periods of time. For ERD, the smallest error was always produced by the most probable GMM sample, which was similar to the output of an ERD trained under a standard Euclidean loss. N-gram model exhibits a sudden change of style during transitioning from the conditioning prefix to the first generated frame, and cannot generate anything outside of the training set. Due to low-dimensional embedding, GPDM cannot adequately handle the breadth of styles in the training data, and produces unrealistic temporal evolution. The quantitative and qualitative motion generation results of ERD and LSTM-3LR suggest an interesting tradeoff between smoothness of motion completion (interesting motion extrapolations) and stable long-term motion generation. Generating short-term motion that mimics the style of the test subject is possible with LSTM-3LR, yet, since the network has not encountered similar examples during training, it is unable to correctly generate motion for longer periods of time. In contrast, ERD gears the generated motion towards similarly moving training examples. ERD though cannot really extrapolate, but rather interpolate among the training subjects. It does provides much smoother motion completions than the N-gram baseline. Both setups are interesting and useful in different applications, and in between architectures potentially lie somewhere in between the two ends of that spectrum. Finally, it is surprising that LSTM-3LR outperforms CRBMs given its simplicity during training and testing, not requiring inference over latent variables.  Pretraining. Initialization of the CNN encoder with the weights of a body pose detector leads to a much better solution than random weight initialization. For motion generation, we did not observe this performance gap between pertaining and random initialization, potentially due to much shallower encoder and low dimensionality of the mocap data.</p><p>Video pose labeling Given a person bounding box sequence, we want to label 2D pixel locations of the person's body joint locations. Both occluded and non-occluded body joints are required to be detected correctly: the occluder's appearance often times contains useful information regarding the location of an occluded body joint <ref type="bibr" target="#b4">[5]</ref>. Further, for transcribing 2D to 3D pose, all body joints are required <ref type="bibr" target="#b31">[32]</ref>.</p><p>We compare our ERD video labeler against two baselines: a per frame CNN pose detector (PF) used as the encoder part of our ERD model, and a dynamic programming approach over multiple body pose hypotheses per frame (VITERBI) similar in spirit to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>. For our VITERBI baseline, we consider for each body joint in each frame all possible grid locations and encode temporal smoothness as the negative exponential of the Euclidean distance between the locations of the same body joint across consecutive frames. The intuition behind VITERBI is that temporal smoothness will help rule out isolated, bad pose estimates, by promoting ones that have lower per frame scores, yet are more temporally coherent. We evaluate our model and baselines by recording the highest scoring pixel location for each frame and body joint. We compute the percentage of detected joints within a tolerance radius of a circle centered at the ground-truth body joint locations, for various tolerance thresholds. We normalize the tolerance radii with the distance between left hip and right shoulder. This is the standard evaluation metric for static image pose labeling <ref type="bibr" target="#b24">[25]</ref>. We show pose labeling performance curves in <ref type="figure" target="#fig_1">Figure 4</ref>. For a video comparison between ERD and the per frame CNN detector, please see the video at https://sites.google.com/site/motionlstm/.</p><p>discriminatively learning to integrate temporal information for body joint tracking, instead of employing generic motion smoothness priors. ERD's performance boost stems from correcting left and right confusions of the per frame part detector, as <ref type="figure" target="#fig_2">Figure 5</ref> qualitatively illustrates. Left and right confusion is a major challenge for per frame part detectors, to the extent that certain works measure their performance in image centric coordinates, rather than object centric <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b24">25]</ref>. Last, VITERBI is marginally better than the per frame CNN detector. While motion coherence proved important when combined with shallow and inaccurate per frame body pose detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, it does not improve much upon stronger multilayer CNNs. <ref type="figure" target="#fig_0">Figure 3</ref> compares ERD training and test losses during finetuning the encoder from (the first five layers of) our per frame CNN pose detector, versus training the encoder from scratch (random weights). CNN encoder's initialization is crucial to reach a good solution.</p><p>We further compare our video labeler in a subset of 200 video sequences of around 50 frames each from the Flic-Motion dataset of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> that we annotated densely in time with person bounding boxes. We used 170 video sequences for training and 30 for testing. We show performance curves for the upper body joints in <ref type="figure" target="#fig_3">Figure 7</ref>. VITERBI has simi-  lar performance as in H3.6M, marginally exceeding the per frame CNN detector. However ERD does much worse since the training set is too small to learn effectively. Finetuning from the model learnt from H3.6M did not help since H3.6M concerns full body motion while FlicMotion captures upper body only. We did not change the architecture in comparison to the ERD used in H3.6M. It is probable that a smaller recurrent layer and decoder would improve performance preventing overfitting. Large training sets such as in H3.6M allow high capacity discriminative temporal smoothers as our video labelled ERD to outperform generic motion smoothness priors for human dynamics. Video pose forecasting We predict 2D body joint locations 400ms ahead of the current frame. <ref type="figure" target="#fig_4">Figure 6</ref> shows pose forecasting performance curves for our ERD model, a model that assumes zero object and camera motion (NoMotion-NM), and a model that assumes constant optical flow within the prediction horizon (OF). ERD carries out more accurate predictions than the zero order and first order motion baselines, as also shown qualitatively in <ref type="figure">Figure  8</ref>. Optical flow based motion models cannot make reasonable predictions for occluded body joints, since their frame to frame displacements are not observed. Further, standard motion models suffer from separation of the observation model (part detector) and temporal aggregation, which ERD combines into a single network.</p><p>Discussion Currently, the mocap ERD performs better on periodic activities (walking, smoking etc) in comparison to non periodic ones (sitting etc.). Interesting directions for future research is predicting 3D angle differences from frame to frame as opposed to angles directly. Such transformation prediction may generalize better to new subjects, focusing more on motion rather than appearance of the skeleton. We are also investigating using large frame volumes as input to our video prediction ERDs with spatio-temporal convolutions in CONV1 as opposed to a single frame LSTM, in order to exploit short temporal horizon more effectively.  <ref type="figure">Figure 8</ref>. Video pose forecasting 400ms in the future. Left: the prediction of the body part detector 400ms before superimosed on the frame to predict pose for (zero motion model). MiddleLeft: Predictions of the ERD. The body joints have been moved towards their correct location. MiddleRight: The current and 400ms ahead frame superimposed. Right: Ground-truth body joint location (discretized in a N × N heat map grid). In all cases we show the highest scoring heat map grid location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented end-to-end discriminatively trained encoder-recurrent-decoder models for modeling human kinematics in videos and motion capture. ERDs learn the representation for recurrent prediction or labeling, as well as its dynamics, by jointly training encoder recurrent and decoder networks. Such expressive models of human dynamics come at a cost of increased need for training examples. In future work, we plan to explore semi-supervised models in this direction, as well learning human dynamics in multi-person interaction scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Pretraining. Initialization of the CNN encoder with the weights of a body pose detector leads to a much better solution than random weight initialization. For motion generation, we did not observe this performance gap between pertaining and random initialization, potentially due to much shallower encoder and low dimensionality of the mocap data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Video pose labeling in H3.6M. Quantitative comparison of a per frame CNN body part detector of [38] (PF), dynamic programming for temporal coherence of the body pose sequence in the spirit of [20, 2] (VITERBI), and ERD video pose labeler. ERD outperforms the per frame detector as well as the dynamic programming baseline. Oracle curve shows the performance upper-bound imposed by our grid resolution of 12x12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Left-right disambiguation. ERD corrects left-right confusions of the per frame CNN detector by aggregating appearance features (CONV5) across long temporal horizons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Video pose labeling in FlicMotion. ERD does not succeed in learning effectively from the small set of 170 videos of about 50 frames each. Large training sets, such as those provided in H3.6M, are necessary for ERD video labeler to outperform generic motion smoothness priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Video pose forecasting. Quantitative comparison between the ERD model, a zero motion (NM), and constant velocity (OF) models. ERD outperforms the baselines for the lower body limbs, which are frequently occluded and thus their per frame motion is not frequently observed using optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Motion prediction error during 80, 160, 240, 320, 400, 480 and 560 msecs past the conditioning prefix for our test subject during Walking activity. Quantitative evaluation for longer temporal horizons is not possible due to stochasticity of human motion.</figDesc><table><row><cell></cell><cell>80</cell><cell>160 240 320 400 480 560</cell></row><row><cell>ERD</cell><cell cols="2">0.89 1.39 1.93 2.38 2.76 3.09 3.41</cell></row><row><cell cols="3">LSTM-3LR 0.41 0.67 1.15 1.50 1.78 2.02 2.26</cell></row><row><cell cols="3">CRBM [34] 0.68 1.13 1.55 2.00 2.45 2.90 3.34</cell></row><row><cell>6GRAM</cell><cell cols="2">1.67 2.36 2.94 3.43 3.83 4.19 4.53</cell></row><row><cell cols="3">GPDM [44] 1.76 2.5</cell><cell>3.04 3.52 3.92 4.28 4.61</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jeff Donahue and Philipp Krähenbühl for useful discussions. We gratefully acknowledge NVIDIA corporation for the donation of K40 GPUs for this research. This research was funded by ONR MURI N000014-10-1-0933.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bilinear spatiotemporal basis models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diverse &quot;m&quot;-best solutions in markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<title level="m">Style machines. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric methods for learning Markov switching processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multihypothesis motion planning for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Likhachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Style translation for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1082" to="1089" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language modeling in meeting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anticipating human activities for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2071</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>NIPS. 2012. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unified spatio-temporal articulated model for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno>abs/1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A recurrent neural network that learns to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modec:multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing human motion with stretchable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Looselimbed people: Estimating 3D human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative density propagation for 3D human motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">80</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factored conditional restricted boltzmann machines for modeling motion style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamical binary latent variable models for 3d human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4659</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Topologically-constrained latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A roadmap for cognitive development in humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vernon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hofsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fadiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4555</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multifactor gaussian process models for style-content separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gaussian process dynamical models for human motion. TPAMI, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video object tracking using adaptive kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Comun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Lagrangian and eulerian specification of the flow field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<idno>1989. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gradient-based learning algorithms for recurrent networks and their computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
