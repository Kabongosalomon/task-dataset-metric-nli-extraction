<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><surname>Emara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossam</forename><forename type="middle">E</forename><surname>Abd</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Munim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><forename type="middle">M</forename><surname>Abbas</surname></persName>
							<email>hazem.abbas@eng.asu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Ain Shams University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted, DICTA 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic image segmentation plays a pivotal role in many vision applications including autonomous driving and medical image analysis. Most of the former approaches move towards enhancing the performance in terms of accuracy with a little awareness of computational efficiency. In this paper, we introduce LiteSeg, a lightweight architecture for semantic image segmentation. In this work, we explore a new deeper version of Atrous Spatial Pyramid Pooling module (ASPP) and apply short and long residual connections, and depthwise separable convolution, resulting in a faster and efficient model. LiteSeg architecture is introduced and tested with multiple backbone networks as Darknet19, MobileNet, and ShuffleNet to provide multiple trade-offs between accuracy and computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with 640 Ã— 360 resolution on the Cityscapes dataset.</p><p>Index Terms-semantic image segmentation, atrous spatial pyramid pooling, encoder decoder, and depthwise separable convolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic image segmentation is defined as the assigning of every pixel in a given image to a specific categorical label. Semantic segmentation <ref type="bibr" target="#b1">[1]</ref>- <ref type="bibr" target="#b4">[4]</ref>, and similar to image classification <ref type="bibr" target="#b5">[5]</ref>- <ref type="bibr" target="#b7">[7]</ref> and object detection <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, has seen considerable progress due to the employment of deep learning architectures, especially convolutional neural networks (CNN). This progress has resulted in a much better quality of real-world applications, such as autonomous driving, medical diagnosis <ref type="bibr" target="#b10">[10]</ref>, and aerial image segmentation <ref type="bibr" target="#b11">[11]</ref>.</p><p>Despite the high accuracy achieved by recent proposed architectures <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b3">[3]</ref> for semantic segmentation, they are not computationally efficient especially for the applications that are needed to be run on edge devices, such as autonomous driving cars, robots, or augmented reality kits. Numerous attempts have been investigated in providing lightweight semantic segmentation architectures, such as ERFNet <ref type="bibr" target="#b12">[12]</ref>, ESP-Net <ref type="bibr" target="#b13">[13]</ref>, Enet <ref type="bibr" target="#b14">[14]</ref>, CCC <ref type="bibr" target="#b15">[15]</ref>, and DSNet <ref type="bibr" target="#b16">[16]</ref>. Some of these lightweight architectures attempts aimed at obtaining realtime performance with a considerable reduction in network parameters which significantly causes a loss in accuracy measures <ref type="bibr" target="#b13">[13]</ref>- <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Other methods paid more attention to both accuracy and real-time performance which leads to gain a better real-time performance when compared to complex networks and a better accuracy than the first group <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b16">[16]</ref>. Semantic segmentation. The Fully Convolutional Network (FCN) <ref type="bibr" target="#b1">[1]</ref> is a pivotal approach which paves the way to employ deep learning methods into the semantic segmentation problem. In FCN, a classification model such as, GoogleNet <ref type="bibr" target="#b7">[7]</ref> or VGG <ref type="bibr" target="#b18">[18]</ref> was used as an encoder to extract features from tested images and then these feature maps were upsampled to pixelwise dense predictions by cascaded layers of unpooling and deconvolution operations. Accuracy of this architecture was improved by using skip architecture, in which semantic information from deep layer and spatial information from the earlier layers were combined to get better results. Despite the breakthrough of FCN architecture, it has suffered from low resolution prediction. Many variant of FCN are proposed to solve this problem. For example, the work in <ref type="bibr" target="#b19">[19]</ref> proposed a multi-scale network which employed a different three scale to generate a fine, high resolution predictions. Another solution was proposed by <ref type="bibr" target="#b20">[20]</ref>, in which a more complex deconvolution network was used to produce high resolution predictions instead of the used one in <ref type="bibr" target="#b1">[1]</ref> which used a single bilinear interpolation layer. A different approach <ref type="bibr" target="#b21">[21]</ref> employed dilated convolution to increase the receptive field without any increase the in number of parameters and computational cost, followed by bilinear interpolation layers to scale up the feature maps to the input image size. Then a conditional random field (CRF) <ref type="bibr" target="#b22">[22]</ref> was used as a post processing to refine the result image. PSPNet <ref type="bibr" target="#b3">[3]</ref>, Deeplabv3 <ref type="bibr" target="#b4">[4]</ref>, Deeplabv3+ <ref type="bibr" target="#b2">[2]</ref> capture information at multiple scales by either applying pooling operations with different kernel size and they called it pyramid pooling module (PPM) or employing dilated convolution with different rate and that was called Atrous Spatial Pyramid Pooling (ASPP).</p><p>Real-Time Segmentation. Most of the mentioned approaches are not efficient for real-time applications as they employed large backbone networks such as GoogleNet <ref type="bibr" target="#b7">[7]</ref>, Xcepetion <ref type="bibr" target="#b18">[18]</ref>, or ResNet <ref type="bibr" target="#b6">[6]</ref>, or employed a large CNN architectures for both the encoder or decoder sides. This has lead to having a large number of parameters to be tuned and a large floating point operations (FLOPS), even though they are efficient form accuracy perspective. Many approaches have been proposed to deal with this problem, e.g., ERFNet <ref type="bibr" target="#b12">[12]</ref> employed a residual connection and depthwise separable convolution to increase receptive field to achieve high accuracy with a reasonable performance. Alternatively, ESPNet <ref type="bibr" target="#b13">[13]</ref> proposed an efficient module called efficient spatial pyramid (ESP), which uses point wise convolution and spatial pyramid of dilated convolution. ESPnet along with Enet provide a lightweight architectures but with a degradation in accuracy. RTSeg <ref type="bibr" target="#b17">[17]</ref> provided a decoupled encoder-decoder architecture which allows to plug any encoder (i.e., VGG16 <ref type="bibr" target="#b18">[18]</ref>, MobileNet <ref type="bibr" target="#b23">[23]</ref>, ShuffleNet <ref type="bibr" target="#b24">[24]</ref>, ResNet18 <ref type="bibr" target="#b6">[6]</ref>) or decoder (i.e., UNet <ref type="bibr" target="#b10">[10]</ref>, Dilation <ref type="bibr" target="#b25">[25]</ref>, SkipNet <ref type="bibr" target="#b1">[1]</ref>) architectures independently. They have found out that using SkipNet architecture along with MobileNet and ShuffleNet provided the best tradeoff between accuracy and performance. Motivated by the encoder-decoder architecture, Atours Spatial Pyramid Pooling (ASPP), dilated convolution, and depthwise separable convolution, we design a novel architecture called LiteSeg which is capable of adapting any backbone network. This capability would allow a variety in trade-offs between computational cost and accuracy to fit multiple needs by choosing different backbone networks. In summary, our main contributions are:</p><p>â€¢ LiteSeg, a real time competitive architecture is presented and tested with three different backbone networks, Darknet19 <ref type="bibr" target="#b8">[8]</ref>, MobileNetV2 <ref type="bibr" target="#b23">[23]</ref>, and ShuffleNet <ref type="bibr" target="#b24">[24]</ref>, achieving performance 70.75%, 67.81%, and 65.17%, respectively on Cityscapes dataset. â€¢ A new deeper version of ASPP module is adapted to improve the results along with using long and short residual connection. The rest of the paper is organized as follows. Section 2 describes the proposed architecture, LiteSeg, in details. In Section 3, both the accuracy and the computational efficiency of the proposed model is evaluated and the paper is finally concluded in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>Here, we will describe our architecture LiteSeg and its new deeper version of ASPP module called Deeper atrous Spatial Pyramid Pooling (DASPP) module ( <ref type="figure" target="#fig_0">Figure 1</ref>). In addition, the atrous convolution, depthwise separable convolution and long and short residual connection are briefly introduced. Then, Deeplabv3+ <ref type="bibr" target="#b2">[2]</ref> which is used as the decoder module will be reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Atrous Convolution</head><p>In convolutional architecture, decreasing the receptive field size will result in a spatial information loss that can be attributed to the strided convolution and pooling layers. To overcome this problem, the dilated convolution was used in <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b25">[25]</ref> to increase the receptive field without any reduction in the feature map resolution and an increase in trainable parameters. This allows network to learn global context features across the entire image for refining full-resolution predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depthwise Separable Convolution</head><p>Standard convolution is computationally an expensive operation due to the large number of parameters to be tuned and thus the needed FLOPS. To tackle this problem, depthwise separable convolution is a suggested solution to replace standard convolution without compromising the accuracy. The main idea of depthwise separable convolution is to split both the input and the kernel into channels -they share the same number of channels-, and each input channel will be convolved with the corresponding kernel channel. Then, the pointwise convolution is performed using a 1 Ã— 1 kernel to project the output of the depthwise convolution into new channel space. Employing depthwise separable convolution <ref type="bibr" target="#b26">[26]</ref> was empirically proven to reduce the computational cost with similar or better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Long and short residual connection</head><p>He et al. <ref type="bibr" target="#b6">[6]</ref> proposed a residual learning framework to allow training of very deep networks. Unlike the traditional feedforward neural network, ResNets introduce an identity shortcut connection. Let X is the input feature map, F (X) is the residual, and H(X) is the output of residual block, the residual learning takes the form H(X) = F (X)+X such that, if there is no residual it will work as identity mapping, that means it can eliminate the effect of the DASPP module if it turns out to be an unnecessary. The resulting learned residual assures that the proposed network would not perform worse than without it. Fusion and reusing of low-level features -which include color blobs or edges-from bottom layers and high-level features from top layers have been proven to be helpful for high resolution segmentation <ref type="bibr" target="#b27">[27]</ref>. This fusion can be done between feature maps from close layers by short residual connection (SRC) and far layers by long residual connection(LRC). These connections act as memory units <ref type="bibr" target="#b28">[28]</ref> in the network as they allow preserving the information from the bottom layers to the top layers. There are two approaches to carry out residual connections, one by element-wise addition <ref type="bibr" target="#b6">[6]</ref> and the other by concatenating the feature maps <ref type="bibr" target="#b28">[28]</ref>. Here, we employed the concatenation approach as an element-wise addition that require the residual output and the input have the same dimension width, height, and depth instead of the conventional concatenation which requires the same dimension of width and height only. The mismatch in width and height can be maintained by upsampling and optionally a 1 Ã— 1 convolution can be used to reduce the depth of the features for computational efficiency. It was found out that long skip connection helps to make clearer semantic boundaries and short skip connections with DASPP help in fine tuning the semantic segments and thus providing richer geometrical information <ref type="figure" target="#fig_1">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proposed Encoder</head><p>The proposed encoder contains a backbone network architecture which acts as an image classification architecture for feature extraction. These architectures were chosen to meet our performance criteria, so we tested the architecture with different three lightweight models MobileNet, ShuffleNet, and Darknet19. Not only is the type of the backbone network controls the performance, but also the output stride <ref type="bibr" target="#b4">[4]</ref> which is defined as the ratio between the input image size and the last feature map of the encoder. Let height H, width W , and depth C be the input image dimension and the outputs of the backbone network are h, w, and c, so the output stride is defined as os = H Ã— W/h Ã— w. Decreasing the output stride leads to having high resolution feature map and also better results <ref type="bibr" target="#b4">[4]</ref> as more spatial information throughout the network is preserved but it comes with computational cost. The output stride of backbone networks is controlled by removing max pool layers and modifying the stride for the last convolution layers. Deeplabv3+ <ref type="bibr" target="#b2">[2]</ref> with output stride equal to 16 is the best trade-off between accuracy and computational efficiency. Moreover, they found out that the accuracy can be greatly improved using output stride equal to 8 but with huge computational cost and the computational efficiency can be improved by increasing the output stride to 32 with a compromise in accuracy. Therefore, the proposed backbone network is configured with output stride of 32 for MobileNetV2 <ref type="bibr" target="#b23">[23]</ref> and ShuffleNet <ref type="bibr" target="#b24">[24]</ref> and output stride of 16 for Darknet19 to achieve different trade-offs between accuracy and speed. DeepLabv3 <ref type="bibr" target="#b4">[4]</ref> employs Atrous Spatial Pyramid Pooling (ASPP) module with different dilation rates to capture multi-scale information, following the presented approach in ParseNet <ref type="bibr" target="#b29">[29]</ref>. Here, a new deeper version of ASPP module is proposed (called Deeper Atrous Spatial Pyramid Pooling (DASPP)), by adding standard 3 Ã— 3 convolution after 3 Ã— 3 atrous convolutions to refine the features and also fusing the input and the output of the DASPP module via short residual connection. Also, the number of convolution filters of ASPP is reduced from 255 to 96 to gain computational performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Deeplabv3+ as a Decoder</head><p>Deeplabv3+ <ref type="bibr" target="#b2">[2]</ref> presented a simplified decoder that is composed of standard 3 Ã— 3 convolution and upsampling layers. Here, we added another 3 Ã— 3 convolution layer and reduced the number of filters in all 3 Ã— 3 convolution from 256 to 96 for computational performance gain. Additional, the output of the encoder is augmented with low level features from earlier layers of the backbone network via long residual connection. These low level features might have large number of feature maps, and in order to resolve this problem, a 1Ã—1 convolution is utilized to reduce the number of channels of low level feature. Otherwise, with some light backbone networks, there will be no need to apply the 1 Ã— 1 convolution on low level features because of the low number of channels (e.g., 24 in case of using MobileNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS AND VALIDATION</head><p>In our evaluation of the proposed method, the effectiveness of LiteSeg with different backbone networks is empirically tested and the results are compared with the lightweight state-of-the-art architectures on Cityscapes <ref type="bibr" target="#b30">[30]</ref> dataset. The performance of the proposed model is measured in terms of mean intersection over union (mIOU), giga floating point operations (GFLOPs), and the number of parameters (Params) in millions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Computing Environment</head><p>The Cityscapes dataset is a large-scale dataset for semantic understanding of urban scenes. It contains 5000 images with fine annotations divided into 2975 images for training, 500 images for validation, and 1525 images for testing. It also contains about 20000 images with coarse annotations that can be used as extra data for fine-tuning the models.</p><p>The experiments were carried out on a computer with Intel Core i7-8700 @ 3.2GHZ, 16GB memory, and NVIDIA GTX1080Ti GPU card. This computer runs Ubuntu 18.04 and PyTorch <ref type="bibr" target="#b31">[31]</ref> version 0.4.1 with CUDA 9.0 and cudnn 7.0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Protocol</head><p>Stochastic Gradient Descent with Nesterov <ref type="bibr" target="#b32">[32]</ref> was used with a momentum value of 0.9 and an initial learning rate of 10 âˆ’7 for ShuffleNet and MobileNetV2 backbone networks and 10 âˆ’8 for Darknet19 backbone network, and a weight decay 4 Ã— 10 âˆ’5 . We applied multiple learning rate policies where the learning rate changes after every five epochs such that the learning rate of the current epoch is calculated by initial learning rateÃ—(1âˆ’epoch/max epochs) power with power 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Encoder Options</head><p>Baseline Model. First our experiments are conducted with a baseline architecture which employs ASPP and decoder modules from the Deeplabv3+ <ref type="bibr" target="#b2">[2]</ref>. This baseline was tested with three different backbone networks, MobileNetV2 <ref type="bibr" target="#b23">[23]</ref>, ShuffleNet <ref type="bibr" target="#b24">[24]</ref>, and Darknet19 <ref type="bibr" target="#b8">[8]</ref> with output stride of 32 during both training and testing phases. As shown in the first row of <ref type="table" target="#tab_0">Table I</ref>, employing Darknet19 as the backbone network for LiteSeg produces an appreciable improvement in the accuracy when compared to MobileNetV2 and ShuffleNet as it is a more efficient classification model <ref type="bibr" target="#b8">[8]</ref>. This can be attributed to the fact that the generated features for the decoders make the architecture more efficient as a classifier.</p><p>Employing DASPP Module. As shown in the second row of <ref type="table" target="#tab_0">Table I</ref>, employing DASPP module along with decreasing output stride from 32 to 16, considerably increases the accuracy of the network by 2.37% when using Darknet19 as a backbone network. It also shows that employing DASPP module along with keeping output stride at 32 for MobileNetV2 and ShuffleNet increases the accuracy of the network by 0.1% and 0.9%, respectively. For DASPP module, we employed dilation rates <ref type="bibr" target="#b3">(3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b9">9)</ref> for the three 3 Ã— 3 convolutions in the first layer, and used standard 3 Ã— 3 convolution in the second layer of convolutions.</p><p>Pre-Training on The Coarse Dataset. Due to the lack of finely annotated data for semantic segmentation models, several works <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> found that object-level and imagelevel labels can improve the result of semantic segmentation models. LiteSeg is trained on the coarse data for 20 epochs and then the trained model is used for training the fine data. The third row of <ref type="table" target="#tab_0">Table I</ref> shows that using a trained network on coarsely annotated data improves the accuracy of the network by 0.7%, 1.6%, and 1.3% when using Darknet19, MobileNetV2, and ShuffleNet, respectively.</p><p>Multi-Scale Input. Learning network with multi-scale images forces the network to well predict across multiple sizes of input images <ref type="bibr" target="#b8">[8]</ref>. Following this strategy, we augmented the dataset with multi-scale input images as our network is a fully convolutional network which makes it accept different dimensions of images. This makes the proposed models efficient for predicting various sizes of input images, as stated in the fourth row of <ref type="table" target="#tab_0">Table I</ref>.</p><p>Employing Depthwise Separable Convolution. Not only does the use of depthwise separable convolution instead of using standard convolution in our network reduce the FLOPS as stated in <ref type="table" target="#tab_0">Table II</ref>, but it also improves the accuracy of the network by 0.5%, 0.7%, and 0.7% when using Darknet19, MobileNetV2, and ShuffleNet, respectively when evaluating images of size 1024 Ã— 2048, as stated in the fifth rows of <ref type="table" target="#tab_0">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Performance Evaluation</head><p>The computational efficiency of the proposed models is assessed here. Both the inference time, which reflects the realtime performance, and number of parameters, which reflects   These results clearly show the ability of LiteSeg to generate different lightweight models to manipulate the accuracy and computational efficiency by using different backbone network. For example, using 640 Ã— 360 input resolution, LiteSeg with MobileNetV2 <ref type="bibr" target="#b23">[23]</ref> as a backbone network achieved a speed of 161 FPS which exceeds the speed of ESPNet <ref type="bibr" target="#b13">[13]</ref> by 17 FPS on the same machine, while providing an improved accuracy by 7.51%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cityscapes Benchmark Results</head><p>The models with the best result on the validation set are selected and compared with the results of the proposed model when experimented in the test set. The results are then uploaded to the official benchmark of Cityscapes dataset. As shown in <ref type="table" target="#tab_0">Table IV</ref>, we compare our result on the test set with other state-of-the-art real-time models for semantic image segmentation. Although the LiteSeg-DarkNet19 has a high GFLOPS compared with ERFNet, it has improved the accuracy of ERFNet and DSNet by 2.75% and 1.45%, respectively, just with a sacrifice of 7 FPS for ERFNet and 2.5 FPS for DSNet <ref type="table" target="#tab_0">(Table III)</ref>. In <ref type="table">Table V</ref>, the mIOU of the main categories of Cityscapes test set are listed and one can easily observe that the most common categories in the dataset have the highest mIOU score. The results of LiteSeg are displayed in <ref type="figure">Figure 3</ref> for qualitative analysis against ESPNet <ref type="bibr" target="#b13">[13]</ref> and ERFNet <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we proposed LiteSeg, a novel lightweight architecture for semantic image segmentation. The ability of LiteSeg to adapt multiple backbone networks allows for providing multiple trade-offs to fit embedded devices and deep learning workstations. We introduced a new module named DASPP to improve semantic boundaries of captured features from backbone network. The proposed network, LiteSeg, was evaluated with ShuffleNet as a backbone network on the Cityscapes test dataset showing that it is able to achieve 65.17% mIoU at 31 FPS for full image resolution 1024x2048 on a single Nvidia GTX 1080TI GPU. <ref type="figure">Fig. 3</ref>. Visualization results of multiple models on Cityscapes validation set <ref type="bibr" target="#b30">[30]</ref>. From top to down 1-Input RGB images; 2-Ground truths; 3-LiteSeg-Darknet predictions; 4-LiteSeg-MobileNet predictions; 5-LiteSeg-ShuffleNet predictions; 6-ERFNet predictions; 7-ESPNet predictions; 8-Color map for Cityscapes classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>General LiteSeg diagram including the encoder module with its components backbone network and DASPP module, and the decoder module. Encoder module takes an input image and generates a high dimensional feature vector. The decoder module restores the spatial information from this feature vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of the output after encoder module, to show the effectiveness of short residual connection (SRC), long residual connection (LRC), and DASPP module on our model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>RESULTS IN MIOU ON THE CITYSCAPES VALIDATION SET USING LiteSeg WITH AN INPUT IMAGE SIZE 512 Ã— 1024 USING DIFFERENT BACKBONE NETWORKS. BASELINE NETWORK IS MINIMAL VERSION OF DEEPLABV3+. FT: USING COARSE DATASET. MS: MULTI-SCALE TRAINING STRATEGY. DW: EMPLOYING DEPTHWISE SEPARABLE CONVOLUTION. RESULTS WITH '*' WERE EVALUATED ON IMAGES WITH SIZES 512 Ã— 1024 AND 1024 Ã— 2048 AND LISTED AS 512 Ã— 1024 ACCURACY/1024X2048 ACCURACY.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EFFECT</head><label>II</label><figDesc>OF EMPLOYING DEPTHWISE SEPARABLE CONVOLUTION TO REDUCE THE NUMBER OF FLOATING POINT OPERATIONS, INSTEAD OF STANDARD CONVOLUTION. THE UNIT OF ALL LISTED NUMBER IS GIGA FLOATING POINT OPERATIONS (GFLOPS). THEY ARE MEASURED ON IMAGE SIZE 1024 Ã— 512.</figDesc><table><row><cell>Convolution type</cell><cell>LiteSeg-Darknet</cell><cell>LiteSeg-MobileNet</cell><cell>LiteSeg-ShuffleNet</cell></row><row><cell>Standard Convolution</cell><cell>123.26</cell><cell>18.86</cell><cell>9.36</cell></row><row><cell>Depthwise Separable convolution</cell><cell>103.09</cell><cell>4.9</cell><cell>2.75</cell></row><row><cell cols="4">the memory footprint, are measured. A set 200 images for</cell></row><row><cell cols="4">the burn-in process and 200 images for evaluation are used</cell></row><row><cell cols="4">in the process. Table III compares the proposed models to</cell></row><row><cell cols="4">current state-of-the-art real-time segmentation networks using</cell></row><row><cell cols="2">the same computing environment.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III INFERENCE</head><label>III</label><figDesc>TIME ANALYSIS ON IMAGES WITH RESOLUTION 360X640 AND FULL RESOLUTION 1024X2048 USING OUR MACHINE. DSNET RESULT WAS TAKEN FROM THEIR PAPER, THEY USED NVIDIA GTX 1080TI ON THEIR EXPERIMENTS.</figDesc><table><row><cell>Network</cell><cell cols="3">FPS (360x640) FPS (1024x2048) Params(in millions)</cell></row><row><cell>ErfNet [12]</cell><cell>105</cell><cell>15</cell><cell>2.07</cell></row><row><cell>DSNet [16]</cell><cell>100.5</cell><cell>-</cell><cell>0.91</cell></row><row><cell>LiteSeg-Darknet (ours)</cell><cell>98</cell><cell>15</cell><cell>20.55</cell></row><row><cell>ESPNET [13]</cell><cell>144</cell><cell>25</cell><cell>0.364</cell></row><row><cell>LiteSeg-MobileNet (ours)</cell><cell>161</cell><cell>22</cell><cell>4.38</cell></row><row><cell>LiteSeg-ShuffleNet (ours)</cell><cell>133</cell><cell>31</cell><cell>3.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF OUR PROPOSE LITESEG AND SIMILAR ARCHITECTURES ON CITYSCAPES TEST SET. FOR RESULTS WITH '*', GFLOPS IS COMPUTED ON IMAGE RESOLUTION 640X360. CATEGORY RESULTS OF OUR LITESEG MODELS ON CITYSCAPES TEST SET. ALL NUMBER REPRESENT THE MIOU.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="5">GFLOPS Class mIOU Category mIOU</cell></row><row><cell>SegNet* [35]</cell><cell></cell><cell></cell><cell cols="2">286.03</cell><cell>56.1%</cell><cell>79.1%</cell></row><row><cell>ESPNet [13]</cell><cell></cell><cell></cell><cell cols="2">9.67</cell><cell>60.3%</cell><cell>82.2%</cell></row><row><cell>ENet [14]</cell><cell></cell><cell></cell><cell cols="2">8.52</cell><cell>58.3%</cell><cell>80.4%</cell></row><row><cell>ERFNet [12]</cell><cell></cell><cell></cell><cell cols="2">53.48</cell><cell>68.0%</cell><cell>86.5%</cell></row><row><cell cols="2">SkipNet-ShuffleNet [17]</cell><cell></cell><cell cols="2">4.63</cell><cell>58.3%</cell><cell>80.2%</cell></row><row><cell cols="5">SkipNet-MobilenetNet [17] 13.8</cell><cell>61.5%</cell><cell>82.0%</cell></row><row><cell>CCC2 [15]</cell><cell></cell><cell></cell><cell cols="2">6.29</cell><cell>61.96%</cell><cell>nan</cell></row><row><cell>DSNet [16]</cell><cell></cell><cell></cell><cell>nan</cell><cell></cell><cell>69.3%</cell><cell>86.0%</cell></row><row><cell cols="3">LightSeg-MobileNet (ours)</cell><cell>4.9</cell><cell></cell><cell>67.81%</cell><cell>86.79%</cell></row><row><cell cols="3">LightSeg-ShuffleNet (ours)</cell><cell cols="2">2.75</cell><cell>65.17%</cell><cell>85.39%</cell></row><row><cell cols="3">LightSeg-DarkNet19 (ours)</cell><cell cols="2">103.09</cell><cell>70.75%</cell><cell>88.29%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE V</cell><cell></cell></row><row><cell>Model</cell><cell>Flat</cell><cell cols="2">Nature</cell><cell>Object</cell><cell>Sky</cell><cell cols="2">Construction Human</cell><cell>Vehicle</cell></row><row><cell>LightSeg-MobileNet</cell><cell cols="6">97.90% 91.70% 62.75% 94.62% 90.44%</cell><cell>79.26%</cell><cell>90.88%</cell></row><row><cell>LightSeg-ShuffleNet</cell><cell cols="6">97.88% 91.22% 57.43% 93.99% 89.69%</cell><cell>77.27%</cell><cell>90.28%</cell></row><row><cell cols="7">LightSeg-DarkNet19 98.44% 98.44% 65.94% 94.99% 91.67%</cell><cell>81.73%</cell><cell>92.95%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daspp Ft Ms Dw</forename><surname>Baseline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liteseg</surname></persName>
		</author>
		<title level="m">Darknet LiteSeg-MobileNet LiteSeg-ShuffleNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic segmentation of aerial images with an ensemble of cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ISPRS Annals of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1789" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Concentratedcomprehensive convolutions for lightweight semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dsnet for real-time driving scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rtseg: Real-time semantic segmentation comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1603" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Light-weight refinenet for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
