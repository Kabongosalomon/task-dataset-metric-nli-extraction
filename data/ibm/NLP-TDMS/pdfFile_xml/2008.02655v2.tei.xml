<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivansh</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression recognition</term>
					<term>student-teacher network</term>
					<term>semi- supervised learning</term>
					<term>multi-level attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial expression recognition from videos in the wild is a challenging task due to the lack of abundant labelled training data. Large DNN (deep neural network) architectures and ensemble methods have resulted in better performance, but soon reach saturation at some point due to data inadequacy. In this paper, we use a self-training method that utilizes a combination of a labelled dataset and an unlabelled dataset (Body Language Dataset -BoLD). Experimental analysis shows that training a noisy student network iteratively helps in achieving significantly better results. Additionally, our model isolates different regions of the face and processes them independently using a multi-level attention mechanism which further boosts the performance. Our results show that the proposed method achieves state-of-the-art performance on benchmark datasets CK+ and AFEW 8.0 when compared to other single models. Code available at github.com/vkumar1997/Emotion-BEEU</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic facial expression recognition from images/videos has many applications such as human-computer interaction (HCI), bodily expressed emotions, human behaviour understanding, and has thus gained a lot of attention in academia and industry. Although there has been extensive research on this subject, facial expression recognition in the wild remains a challenging problem because of several factors such as occlusion, illumination, motion blur, subject-specific facial variations, along with the lack of extensive labelled training datasets. Following a similar line of research, our task aims to classify a given video in the wild to one of the seven broad categorical emotions. We propose an efficient model that addresses the challenges posed by videos in the wild while tackling the issue of labelled data inadequacy. The input data used for facial expression recognition can be multi-modal, i.e. it may have visual information as well as audio information. However, the scope of this paper is limited to emotion classification using only visual information. Most of the recent research on the publicly-available AFEW 8.0 (Acted Facial Expressions in the Wild) <ref type="bibr" target="#b0">[1]</ref> dataset has focused on improving accuracy without regard to computational complexity, architectural complexity, energy &amp; policy considerations, generality, and training efficiency. Several state-of-the-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> on this dataset have originated from the EmotiW <ref type="bibr" target="#b4">[5]</ref> challenge with no clear computational-cost analysis. Fan et al. <ref type="bibr" target="#b1">[2]</ref> achieved the highest validation accuracy based on visual cues, but they used a fusion of five different architectures with more than 300 million parameters. In contrast, our proposed method uses a single model with approximately 25 million parameters and comparable performance.</p><p>While previous work focused on improving performance by increasing model capacity, our method focuses on better pre-processing, feature selection, and adequate training. Prior research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> uses simple aggregation or averaging operation on features from multiple frames to form a fixed-dimensional feature vector. However, such methods do not account for the fact that a few principal frames in a video can be used to identify the target emotion, while the rest of the frames have a negligible contribution. Frame-attention has been used <ref type="bibr" target="#b9">[10]</ref> for selectively processing frames in a video, but it can further be coupled with spatial-attention which could identify the most discriminative regions in a particular frame. We use a three-level attention mechanism in our model: a) spatial-attention block that helps to selectively process feature maps of a frame, b) channel-attention block that focuses on the face regions at a local and a global level, i.e. eyes region (upper face), mouth region (lower face) and whole face, and c) frame-attention block that helps to identify the most important frames in a video. AFEW 8.0 <ref type="bibr" target="#b0">[1]</ref> has several limitations (Sec. 2) that restricts the generalization capabilities of deep learning models. To overcome these limitations, we use an unlabelled subset of the BoLD dataset <ref type="bibr" target="#b10">[11]</ref> for semi-supervised learning. Inspired by Xie et al. <ref type="bibr" target="#b11">[12]</ref>, we use a teacher-student learning method where the training process is iterated by using the same student again as the teacher. During the training of the student, noise is injected into the student model to force it to generalize better than the teacher. Results show that the student performs better with each iteration, hence improving the overall accuracy on the validation set.</p><p>The rest of the paper is organized as follows. Sec. 3 explains the datasets (AFEW 8.0 <ref type="bibr" target="#b0">[1]</ref>, CK+ <ref type="bibr" target="#b12">[13]</ref> and BoLD <ref type="bibr" target="#b10">[11]</ref>) that are used for training our model along with the pre-processing pipeline used for face detection, alignment and illumination correction. Sec. 4.1 explains the backbone network and covers the three types of attention and its importance in detail. Sec. 4.2 covers the use of the BoLD dataset for iterative training and the experimental results of semisupervised learning. Sec. 5.3 compares the results of our methods to other stateof-the-art methods on the AFEW 8.0 dataset. Additionally, we use another benchmark dataset CK+ <ref type="bibr" target="#b12">[13]</ref> (posed conditions) as well as perform ablation studies (Sec. 5.4) to prove the validity of our model and training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Facial Expression Recognition: A number of methods have been proposed on the AFEW 8.0 dataset <ref type="bibr" target="#b0">[1]</ref> since the first EmotiW <ref type="bibr" target="#b4">[5]</ref> challenge in 2013. Earlier approaches include non-deep learning methods such as multiple kernel learning <ref type="bibr" target="#b13">[14]</ref>, least-square regression on grassmanian manifold <ref type="bibr" target="#b14">[15]</ref>, and feature fusion with kernel learning <ref type="bibr" target="#b15">[16]</ref>, whereas recent approaches include deep-learning methods such as frame-attention networks <ref type="bibr" target="#b9">[10]</ref>, multiple spatial-temporal learning <ref type="bibr" target="#b2">[3]</ref>, and deeply supervised emotion recognition <ref type="bibr" target="#b1">[2]</ref>. Although several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref> have achieved impressive results on the AFEW 8.0 dataset, many have used ensemble (fusion) based methods and considered multiple modalities without commenting on the resources and time required to train such models. Spatial-temporal methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> aim to model motion information or temporal coherency in the videos using 3D Convolution <ref type="bibr" target="#b18">[19]</ref> or LSTM (Long short-term memory) <ref type="bibr" target="#b19">[20]</ref>. However, owing to computational efficiency and the ability to treat sequential information with a global context, several studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref> related to facial expression recognition have successfully implemented attention-based methods by assigning a weight to each timestep in the video. Similarly, spatial self-attention has been used <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> as a means to guide the process of feature extraction and find the importance of each local image feature. Our model builds upon the spatial self-attention mechanism and additionally uses a channelattention mechanism to exploit the differential effects of facial feedback signals from the upper-face and lower-face regions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Training Datasets: Despite being a long-established dataset, AFEW 8.0 <ref type="bibr" target="#b0">[1]</ref> has several shortcomings. Firstly, the dataset contains significantly fewer training examples for fear, surprise and disgust categories which makes the dataset imbalanced. Secondly, the videos are extracted from mainstream cinema, and scenes depicting fear are often shot in the dark, which again makes the model biased towards other categories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. Such limitations warrant the use of additional datasets for better generalization. However, not many in-the-wild labelled video datasets are publicly available for facial expression recognition. Several related datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> are captured in posed conditions and are restricted to a certain country or community. Aff-Wild2 <ref type="bibr" target="#b28">[29]</ref> is another popular dataset, but it contains per-frame annotations, and thus cannot be used in our work which performs video-level classification based on facial expressions. We use an unlabelled portion of the BoLD dataset <ref type="bibr" target="#b10">[11]</ref> since the videos are of the desired length and are captured from movies similar to our labelled dataset. Semi-Supervised Learning: The semi-supervised approach is effective in classification problems when the labelled training data is not sufficient. We use noisy student training <ref type="bibr" target="#b11">[12]</ref> for semi-supervised learning, in which the trick involves the student to be deliberately noised when it trains on the combined labelled and unlabelled dataset. Input noise is added to the student model in the form of data augmentations, which ensures that different alterations of the same video should have the same emotion, hence making the student model more robust. Additionally, model noise is added in the form of dropout, which forces the student (single model) to match the performance of an ensemble model. Other tech- niques for semi-supervised learning include self-training <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, data-distillation <ref type="bibr" target="#b31">[32]</ref> and consistency training <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Self-training is similar to noisy student training, but it does not use or justify the role of noise in training a powerful student. Data-distillation uses the approach of strengthening the teacher using ensembles instead of weakening the student; however, a smaller student makes it difficult to mimic the teacher. Consistency training adds regularization parameters to the teacher model during training to induce invariance to input and model noise, resulting in confident pseudo-labels. However, such constraints lead to lower accuracy and a less powerful teacher <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>In this section, we first describe the datasets that we use in our experiments, followed by the pre-processing pipeline. [13] contains 327 video sequences (5878 frames) divided into seven categories, i.e anger (45 samples), disgust (59 samples), fear (25 samples), happy (69 samples), sad (28 samples), surprise (83 samples), and contempt (18 samples). The motivation behind testing our method on a posed dataset is to establish the robustness of our model and semi-supervised learning method irrespective of the data source. Since CK+ does not have a testing set, we report the average accuracy obtained using 10-fold cross-validation as seen in other studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Unlabelled Set: BoLD (Body Language Dataset) <ref type="bibr" target="#b10">[11]</ref> contains videos selected from the publicly available AVA dataset <ref type="bibr" target="#b40">[41]</ref>, which contains a list of YouTube movie IDs. While the gathered videos are annotated based on body language, the videos having a close shot of the face instead of the whole or partially-occluded body are unlabelled. To create an AFEW-like subset from the BoLD dataset, we impose two conditions to automatically validate a video. Firstly, a video should have f (≥ 30) such consecutive frames where only one actor's face is detected by MTCNN (Multi-task Cascaded Convolutional Networks) <ref type="bibr" target="#b34">[35]</ref>. Secondly, the bounding box of the face detected using MTCNN should exceed an occupied area threshold for the majority of those f frames. If the video satisfies the above two conditions, a smaller video with those f frames is added to the unlabelled dataset. Using this procedure, we create a subset of 3450 videos (224,258 frames) from the original BoLD dataset. Some of the examples gathered are shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>Pre-Processing: Previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> have used CNN-based detector provided by dlib <ref type="bibr" target="#b41">[42]</ref> for face alignment. However, the alignment of faces is highly dependent on accurate detection of facial landmarks and CNN-based detector provided by dlib is not reliable for 'in-the-wild' conditions (especially non-frontal faces). We use MTCNN <ref type="bibr" target="#b34">[35]</ref> for face detection and alignment. If MTCNN detects multiple faces in a frame, the face with the largest bounding box is selected. After obtaining the facial landmarks, its alignment is corrected using the angle between the line connecting the landmark points of the eyes and the horizontal line. After detection and alignment, the cropped face is resized to 224*224 pixels, which is the input size of our model.</p><p>We use the landmarks given by MTCNN to isolate the mouth (lower face) and eyes (upper face) region. The upper face is isolated using the eyes landmarks with the desired left eye normalized co-ordinates being (0.2, 0.6) and right-eye co-ordinates being (0.8, 0.6) in the new frame, which is enough to occlude the lower-half of the face in almost all frames ( <ref type="figure" target="#fig_1">Fig. 1)</ref>. A similar procedure is used for occluding the upper-half of the face and isolating the mouth region using leftmouth and right-mouth landmarks. All landmark-based crops are again resized to 224*224 pixels.</p><p>As addressed earlier, some of the categories of emotions are often captured in the dark in movies, which requires an illumination correction step. Several methods have been suggested for illumination normalization such as gamma correction <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, Difference of Gaussians (DoG) <ref type="bibr" target="#b44">[45]</ref> and histogram equaliza- tion <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> which are effective for facial expression recognition. However, these methods tend to amplify noise, tone distortion, and other artefacts. Hence, we use a state-of-the-art pre-trained deep learning model, i.e. Enlighten-GAN <ref type="bibr" target="#b35">[36]</ref> (U-Net <ref type="bibr" target="#b47">[48]</ref> as generator) which provides appropriate results ( <ref type="figure" target="#fig_1">Fig. 1</ref>) with uniform illumination and suppressed noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Our proposed methodology is divided into two phases, i.e. a) architecture implementation that defines the backbone network with the three-level attention mechanism, and b) semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>Backbone Network: We use ResNet-18 <ref type="bibr" target="#b48">[49]</ref> architecture as our backbone network, with minor modifications to increase its computational efficiency. Features from each residual block are combined to form the final feature vector (see <ref type="figure" target="#fig_3">Fig.  2</ref>). Hence, the final vector has a "multi-level knowledge" from all the residual blocks, ensuring more diverse and robust features. The model is first pre-trained on the FERPlus dataset <ref type="bibr" target="#b49">[50]</ref>. Our input at frame-level is an image with nine channels (RGB channels from the face, eyes, and mouth region). To process them independently, the model uses group convolution <ref type="bibr" target="#b50">[51]</ref> (groups = 3), i.e. it uses a different set of filters for each of the three regions to get the final output feature maps. Group convolution results in a lower computational cost since each kernel filter does not have to convolve on all the feature maps of the previous layer. Simultaneously, it allows data parallelism where each filter group is learning a unique representation and forms a global (face region) or local (eyes and mouth region) context vector from each frame of a video. To allow more filters per group, we increase the number of filters in each residual block, as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p><p>Spatial-Attention: A common approach in previous methods is a simple aggregation or average pooling of feature maps to form a fixed dimensional feature vector. However, we use spatial-attention <ref type="bibr" target="#b22">[23]</ref> that concatenates the feature maps based on the attention weight it has been assigned. Let us assume the output from a residual block is of shape C = H × W × D where H and W are the output height and width, and D is the number of output filters. This 3D tensor C is reshaped to a 2D matrix L of shape R × D where R = H * W . The spatialattention mechanism takes the input matrix L and outputs a weight matrix M of shape h × R (h = 2, h is for multiple hops of attention). Each row of the output matrix represents a different hop of attention, and each column has normalized weights due to softmax (see <ref type="bibr">Equation 1</ref>). The objective is to find the weighted average of R frame descriptors to obtain a vector v of length D (or h * D with multiple hops).</p><formula xml:id="formula_0">M = sof tmax(W s2 tanh(W s1 L T )) (1) v = f latten(M * L)<label>(2)</label></formula><p>Equation 1 represents multi-head spatial-attention where W s1 is of shape U × D and W s2 is of shape h × U (U can be set arbitrarily). From this, we obtain flattened vector v using Equation 2. The spatial-attention module is applied on each residual block (see <ref type="figure" target="#fig_3">Fig. 2</ref>) and the output vectors are aggregated to obtain a final vector of length l = 960 each for face (f 1 ), eyes (f 2 ) and mouth (f 3 ) regions. The advantages of spatial attention can be seen in <ref type="figure" target="#fig_4">Fig. 3</ref>. While the feature vector from the face is encoded with a global context, the feature maps from the eyes and mouth region have additional information regarding the minute expressions such as furrowed brow or flared nostrils.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel-Attention:</head><p>Let f 1 , f 2 , and f 3 be the feature vectors obtained from the face, the eyes, and the mouth region respectively. We model the cross-channel interactions using a lightweight attention module. We use two fully-connected layers to obtain a weight α (Equation 3) for each channel group using which we obtain a weighted averagef v (Equation 4) of the three feature vectors. ReLU (Rectified Linear Unit) activation is used after the first layer to capture nonlinear interactions among the channels.</p><formula xml:id="formula_1">α i = σ(w T (ReLU (W T f i ))) (3) f v = 3 i=1 α i * f i 3 i=1 α i<label>(4)</label></formula><p>where σ is the sigmoid activation function, w is a vector of length r (set arbitrarily), and W is a matrix of shape l × r. In <ref type="figure" target="#fig_4">Fig. 3</ref>, we see that the model assigns more weight to the mouth region instead of the eyes region for an expression depicting happiness which is consistent with our findings that mouth region is more prominent for the happy category ( <ref type="figure" target="#fig_5">Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-Attention:</head><p>For a video having n frames, we obtain vectorf i of lengtĥ l from each frame after the channel-attention module. Finally, we use frameattention to assign the most discriminative frames a higher weight. Following a similar intuition as in channel-attention, we use two fully-connected layers to obtain a weightα (Equation 5) for each frame using which we find a weighted average f v (Equation 6) of the frame features.</p><formula xml:id="formula_2">α i = σ(ŵ T (ReLU (Ŵ Tf i ))) (5) f v = n i=1α i * f i n i=1α i<label>(6)</label></formula><p>whereŵ is a vector of lengthr (set arbitrarily), andŴ is a matrix of shapê l ×r. <ref type="figure" target="#fig_4">Fig. 3</ref> shows how the model assigns a higher weight to the frames which distinctively contains expression depicting happiness. The feature vector f v is passed through a fully-connected layer to obtain the final 7-dimensional output.</p><p>Implementation Details: We use weighted cross-entropy as our loss function where class weights are assigned based on number of training samples to alleviate the problem of unbalanced data. Additionally, M (Equation 1) is regularized by adding the frobenius norm of matrix M M T −I to the loss function which enforces multi spatial-attention to focus on different regions <ref type="bibr" target="#b22">[23]</ref>. We use Adam optimizer with an initial learning rate of 1e-5 (reduced by 40% after every 30 epochs) and the model is trained for 100 epochs. The training takes around 8 minutes for 1 epoch for AFEW 8.0 training dataset with two NVIDIA Tesla K80 cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Noisy Student Training [12]</head><p>Once the model is trained on the labelled set and the best possible model is obtained, we use it as a teacher model to create pseudo-labels on the subset of BoLD dataset that we collected. After generating the pseudo-labels, a student model (same size or larger than teacher) is trained on the combination of labelled and unlabelled dataset. While training the student model, we deliberately add noise in the form of random data augmentations and dropout (with 0.5 probability at the final hidden layer). Random data augmentations (using RandAugment <ref type="bibr" target="#b51">[52]</ref>) include transformations such as brightness change, contrast change, translation, sharpness change and flips. RandAugment automatically applies n <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> random operations with a random magnitude m [0 <ref type="bibr">, 9]</ref>. After the noisy student is trained on the combined data, the trained student becomes the new teacher that generates new pseudo-labels for the unlabelled dataset. The iterative training continues until we observe a saturation in performance. From <ref type="figure" target="#fig_6">Fig. 4</ref>, we see how noisy training helps the student become more robust with the addition of noise. While the teacher may give different predictions for different alterations of the same video, the student is more accurate and stable with its predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we show the results obtained with and without iterative selftraining, followed by comparison with state-of-the-art methods and ablation studies.   regions have different feedback signals that dominate different categories of emotions, and b) if isolating the regions and processing them independently leads to an increase of accuracy. As seen in the confusion matrix ( <ref type="figure" target="#fig_5">Fig. 5)</ref>, the eyes region is better than the mouth region in the prediction of sadness and disgust categories. Intuitively, the squinted eyes expression in disgust and the droopy eyelids or furrowed eyebrows expression in sadness makes the eyes region pronounced. On the other hand, the mouth region is comparatively better with categories that require lip movements like happiness, anger, and surprise. Overall, 52.50% accuracy is achieved using the proposed model, which is slightly better than the model that only uses faces. Furthermore, we see a significant increase in the macro f1 score when we include the eyes and mouth region along with faces indicating that the predictions are comparatively more unbiased for the seven categories (an advantage for noisy student training). The proposed model is still biased against fear, surprise, and disgust categories, but performs better than several existing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref> where the reported accuracies for these categories are close to 0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Without Student Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">With Iterative Training</head><p>Using noisy student training, we report our experimental results for four iterations on the AFEW 8.0 dataset and two iterations on the CK+ dataset. Data Balancing: Since the model is biased, the number of pseudo-labels in the unlabelled dataset for some categories is smaller than in other categories. We try to match the distribution of the training set by duplicating images of fear, disgust, and surprise categories. Additionally, images of angry, happy, and neutral classes are filtered out based on confidence scores. <ref type="figure" target="#fig_8">Fig. 6</ref> shows that balancing the pseudo-labels leads to better accuracy in each iteration compared to the student model without data balancing. The same trend is not observed for the CK+ dataset since the pseudo-labels roughly have the same distribution as the training set. <ref type="table">Table 1</ref>. We compare our results to the top-performing single models evaluated on the AFEW 8.0 dataset and state-of-the-art models evaluated on the CK+ dataset.</p><formula xml:id="formula_3">AFEW 8.0 CK+ Models</formula><p>Acc. Models Acc. CNN-RNN (2016) <ref type="bibr" target="#b17">[18]</ref> 45.43% Lomo (2016) <ref type="bibr" target="#b39">[40]</ref> 92.00% DSN-HoloNet (2017) <ref type="bibr" target="#b53">[54]</ref> 46.47% CNN + Island Loss (2018) <ref type="bibr" target="#b38">[39]</ref> 94.35% DSN-VGGFace (2018) <ref type="bibr" target="#b1">[2]</ref> 48.04% FAN (2019) (Fusion) <ref type="bibr" target="#b9">[10]</ref> 94.80% VGG-Face + LSTM (2017) <ref type="bibr" target="#b3">[4]</ref> 48.60%</p><p>Hierarchial DNN (2019) <ref type="bibr" target="#b54">[55]</ref> 96.46% VGG-Face (2019) <ref type="bibr" target="#b20">[21]</ref> 49.00% DTAGN (2015) <ref type="bibr" target="#b37">[38]</ref> 97.25% ResNet-18 (2018) <ref type="bibr" target="#b55">[56]</ref> 49.70% MDSTFN (2019) <ref type="bibr" target="#b56">[57]</ref> 98.38% FAN (2019) <ref type="bibr" target="#b9">[10]</ref> 51.18% Compact CNN (2018) <ref type="bibr" target="#b57">[58]</ref> 98.47% DenseNet-161 (2018) <ref type="bibr" target="#b16">[17]</ref> 51.44% ST Network (2017) <ref type="bibr">[</ref> Unlabelled Dataset Size: As stated in the original paper <ref type="bibr" target="#b11">[12]</ref>, using a large amount of unlabelled data leads to better accuracy. After data balancing, we use a fraction of the BoLD dataset and report the accuracy after several iterations of training until the performance saturates (see <ref type="figure" target="#fig_8">Fig. 6</ref>). For both CK+ and AFEW 8.0 dataset, we observe that using the whole unlabelled training set is better as opposed to using just a fraction of the dataset. <ref type="figure" target="#fig_8">Fig. 6</ref> shows a steady increase in all categories and overall accuracy with an increase in data size after four iterations of training on the AFEW 8.0 dataset. Importance of Noise: Noise helps the student to be more robust than the teacher, as addressed in Sec. 2. The accuracy only reaches 53.5% on the AFEW 8.0 dataset without noise in student training, and no improvement is seen on the CK+ dataset. However, we achieve an accuracy of 55.17% after noisy training, which shows that input and model perturbations are vital while training the student. Additionally, <ref type="figure" target="#fig_8">Fig. 6</ref> shows that it is better when the pseudo-labels are generated without noise, i.e. the teacher remains as powerful as possible.</p><p>Batch Size Ratio: When training on combined data, a batch of labelled images and a batch of unlabelled images are concatenated for each training step. If the batch sizes of labelled and unlabelled sets are equal, the model will complete several epochs of training on labelled data before completing one epoch of training on the BoLD dataset due to its larger size. To balance the number of epochs of training on both datasets, the batch size of the unlabelled set is kept higher than the labelled set. <ref type="figure" target="#fig_6">Fig. 4</ref> shows that a batch size ratio of 2:1 or 3:1 is ideal for training when AFEW 8.0 is used as the labelled training set. Similarly, a batch size ratio of 5:1 is ideal for the CK+ dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with other methods</head><p>We evaluate our model on the labelled datasets and show a comparison with the existing state-of-the art-methods <ref type="table">(Table 1)</ref>. On the AFEW 8.0 dataset, we achieve an accuracy of 52.5% without iterative training and 55.17% with iterative training. When comparing to existing best single models, our proposed method improves upon the current baseline [3] by 1.6%. Compared to static-based CNN methods that aim to combine frame scores for video-level recognition, we achieve a significant improvement of 3.73% over the previous baseline <ref type="bibr" target="#b16">[17]</ref>. We conduct a comparison of performance and speed of the existing state-of-the-art models including fusion methods (only visual modality) with our proposed model. Several methods that show higher validation accuracy have significantly higher computational demand which may be impractical for real-time world applications. For instance, <ref type="bibr" target="#b55">[56]</ref> uses an ensemble of 50 models with the same architecture and yet attains a 52.2% validation accuracy. Similarly, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> use a combination of multiple deep learning models where each model has a higher computational cost than ours. We measure the computational complexity of state-of-the-art methods using FLOPS (Floating point operations) and results show that our method is the most optimal based on performance and speed <ref type="figure" target="#fig_9">(Fig. 7)</ref>.</p><p>On the CK+ dataset, our method achieves an on par 10-fold cross-validation accuracy when compared to other state-of-the-art methods. While our model achieves an accuracy of only 98.77% without iterative learning, the accuracy improves by 0.92% when training data of each fold is combined with the unlabelled dataset for two iterations. This confirms our premise that self-training using noisy student is a robust procedure and can be used to increase the performance of a model on several other labelled data sources. Additionally, our results show that one can achieve better performance on a posed dataset when trained with an unlabelled in-the-wild dataset in a semi-supervised manner, which can be an effective alternative to labour-intensive tasks like gathering additional posed samples or labelling data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>Our baseline model is ResNet-18 where the video-level feature vector is an unweighted average of all the frame-level feature vectors. Without sophisticated pre-processing, the baseline achieves an accuracy of 47.5%. To better understand the significance of each component, we record our results after every change to the baseline model ( <ref type="table" target="#tab_1">Table 2</ref>). Significant improvements are observed when features are concatenated from multiple residual blocks using spatial-attention, and when frame features are combined from multiple regions using group convolution and channel-attention. Additionally, <ref type="table" target="#tab_1">Table 2</ref> shows the increase in validation accuracy with each loop of iterative learning. As suggested by <ref type="bibr" target="#b11">[12]</ref>, noisy student learning may perform better if the student is larger in size than the teacher. Since ResNet-34 <ref type="bibr" target="#b48">[49]</ref> has a comparatively larger capacity, we report its results besides ResNet-18 as the student model for each iteration. As seen in <ref type="table" target="#tab_1">Table 2</ref>, our results do not show improvement when ResNet-18 in our student model is replaced with a larger backbone. A possible explanation is that the unlabelled dataset used by <ref type="bibr" target="#b11">[12]</ref> is a hundred times larger than the labelled dataset and using a student with higher capacity may have resulted in better performance. On the contrary, our unlabelled dataset is only four times larger than the labelled dataset. Gathering additional unlabelled samples and using a larger student may result in a further increase in accuracy on the AFEW 8.0 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a multi-level attention model for video-based facial expression recognition, which is trained using a semi-supervised approach. Our contribution is a cost-effective single model that achieves on par performance with state-of-the-art models using two strategies. Firstly, we use attention with multiple sources of information to capture spatially and temporally important features, which is a computationally economical alternative to the fusion of multiple learning models. Secondly, we use self-training to overcome the lack of labelled video datasets for facial expression recognition. The proposed training scheme can be extended to other related tasks in the field of affective computing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>equal contribution arXiv:2008.02655v2 [cs.CV] 24 Feb 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The pre-processing steps mainly include face detection and alignment (MTCNN<ref type="bibr" target="#b34">[35]</ref>), illumination correction (Enlighten-GAN<ref type="bibr" target="#b35">[36]</ref>) and landmark-based cropping. Examples from labelled dataset (AFEW 8.0) and unlabelled dataset (BoLD dataset) are shown. As seen in the figure, only videos with a close shot of the face are selected from the BoLD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Labelled Sets: AFEW 8.0 (Acted Facial Expression in the Wild) [1] contains videos with seven emotion labels, i.e. anger (197 samples), neutral (207 samples), sad (179 samples), fear (127 samples), surprise (120 samples), happiness (212 samples), and disgust (114 samples) from different movies. The train set consists of 773 video samples (46,080 frames), and the validation set consists of 383 video samples (21,157 frames). The results are reported on the validation set since the test set labels are only available to EmotiW challenge [5] participants. Some of the example frames are shown in Fig. 1. CK+ (Cohn Kanade Extended)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Figure showsthe backbone network (ResNet-18) and the three-level attention mechanism. Inputs are first processed via Spatial-Attention, followed by Channel-Attention and finally by Frame-Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>This figure shows how multi-level attention works in the proposed method. Spatial-attention (from last residual block) chooses the dominant feature maps from each region. Channel-attention picks the most important region that most clearly shows the target emotion. Frame-attention assigns the salient frames a higher weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>shows the results of processing individual regions (without group convolution and channel attention) on the AFEW 8.0 dataset, along with the proposed methodology. Our objective is to explore a) if upper face region and lower face</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Semi-supervised algorithm is presented in the flow-chart. We also show an example video from AFEW 8.0 dataset where the frames underwent different augmentations. Predictions without iterative training are shown in red and predictions after iterative training are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>This figure shows the confusion matrices, the accuracies, and the macro f1 scores achieved on the AFEW 8.0 dataset using different regions of the face. The proposed model (Face + Eyes + Mouth) achieves the highest accuracy. An=Angry, Sa=Sad, Ne=Neutral, Ha=Happy, Su=Surprise, Fe=Fear, Di=Disgust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>This figure shows the experimental results of noisy student training for four iterations using AFEW 8.0 and BoLD datset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of performance (in accuracy) vs computational cost (in FLOPS -Floating point operations per second) of state-of-the-art models evaluated on AFEW 8.0 dataset. FLOPS for the models are estimated values based on the backbone network unless explicitly specified by the authors. Most optimal models will be closer to the top-left corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>This table showsthe ablation studies conducted with AFEW 8.0 dataset. Component Importance shows the increase in accuracy with the addition of each component separately. Noisy Student Training shows the increase in accuracy with each loop of iterative learning and the effect of using a larger student.</figDesc><table><row><cell>Component Importance</cell><cell></cell><cell cols="3">Noisy Student Training</cell></row><row><cell>Component</cell><cell cols="4">Acc. Iteration Student Acc.</cell></row><row><cell>ResNet-18 (Baseline)</cell><cell>47.5%</cell><cell>0</cell><cell>-</cell><cell>52.5%</cell></row><row><cell cols="2">+ MTCNN, Enlighten-GAN (Sec. 3) 48.3% + Features from all blocks (Sec. 4.1) 49.3%</cell><cell>1</cell><cell cols="2">ResNet-18 53.5% ResNet-34 53.5%</cell></row><row><cell>+ Spatial-Attention (Sec. 4.1) + Multiple Regions (Sec. 4.1)</cell><cell>50.3% 51.2%</cell><cell>2</cell><cell cols="2">ResNet-18 54.6% ResNet-34 54.5%</cell></row><row><cell>+ Channel-Attention (Sec. 4.1) + Frame-Attention (Sec. 4.1)</cell><cell>51.7% 52.5%</cell><cell>3</cell><cell cols="2">ResNet-18 54.9% ResNet-34 54.8%</cell></row><row><cell cols="2">+ Iteration 1 -Self-training (Sec. 4.2) 53.5% + Iteration 2 -Self-training (Sec. 4.2) 54.6%</cell><cell>4</cell><cell cols="2">ResNet-18 55.2% ResNet-34 55.2%</cell></row><row><cell cols="2">+ Iteration 3 -Self-training (Sec. 4.2) 54.9% + Iteration 4 -Self-training (Sec. 4.2) 55.2%</cell><cell>5</cell><cell cols="2">ResNet-18 55.2% ResNet-34 55.2%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>The authors acknowledge the support of Professor James Wang for providing the opportunity to work on this project during his course on Artificial Emotion Intelligence at the Pennsylvania State University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using deeplysupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="584" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple spatiotemporal feature learning for video-based emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal multimodal fusion for video emotion classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotiw 2019: Automatic emotion, engagement and cohesion prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="546" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression extracted automatically from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="80" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuharenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04598</idno>
		<title level="m">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<title level="m">Deep learning using linear support vector machines</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3866" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbee: Towards automated recognition of bodily expression of emotion in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 ieee computer society conference on computer vision and pattern recognition-workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple kernel learning for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathyanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Partial least squares regression on grassmannian manifold for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild with feature fusion and multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-feature based emotion recognition for video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion recognition with spatial attention and temporal softmax pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aminbeidokhti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cardinal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self multi-head attention-based convolutional neural networks for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A false trail to follow: differential effects of the facial feedback signals from the upper and lower face on the recognition of micro-expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2015</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Covariance pooling for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Induced disgust, happiness and surprise: an addition to the mmi facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect</title>
		<meeting>3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The japanese female facial expression (jaffe) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of third international conference on automatic face and gesture recognition</title>
		<meeting>third international conference on automatic face and gesture recognition</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Aff-wild2: Extending the aff-wild database for affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatically generating extraction patterns from untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on artificial intelligence</title>
		<meeting>the national conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1044" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06972</idno>
		<title level="m">Enlightengan: Deep light enhancement without paired supervision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on deep evolutional spatial-temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4193" to="4203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Island loss for learning discriminative features in facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lomo: Latent ordinal model for facial analysis in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5580" to="5589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Preprocessing technique for face recognition applications under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Devarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Journal of Computer Science and Technology</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial expression recognition with fusion features extracted from salient facial areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An improved difference of gaussian filter in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved facial expression recognition based on dwt feature for deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Bendjillali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beladgham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Merit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taleb-Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">324</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Development of a personified face emotion recognition technique using fitness function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karthigayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R M</forename><surname>Juhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugisaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yaacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mamat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Desa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life and Robotics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="203" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training deep networks for facial expression recognition with crowd-sourced label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="279" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Randaugment: Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-cue fusion for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning supervised scoring ensemble for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on multimodal interaction</title>
		<meeting>the 19th ACM international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient facial expression recognition algorithm based on hierarchical deep neural network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="41273" to="41285" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An occam&apos;s razor view on learning audiovisual emotion recognition with small training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="589" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep spatial-temporal feature fusion for facial expression recognition in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A compact deep learning model for robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
