<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 LARGE BATCH OPTIMIZATION FOR DEEP LEARNING: TRAINING BERT IN 76 MINUTES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
							<email>youyang@cs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
							<email>jingli@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
							<email>sashank@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
							<email>jhseu@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
							<email>sanjivk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
							<email>bsrinadh@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
							<email>xiaodansong@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
							<email>demmel@cs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@cs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 LARGE BATCH OPTIMIZATION FOR DEEP LEARNING: TRAINING BERT IN 76 MINUTES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Yang You was a student researcher at Google Brain. This project was done when he was at Google Brain.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains RESNET on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and RESNET-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes <ref type="table">(Table 1)</ref>. The LAMB implementation is available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of large scale datasets, training large deep neural networks, even using computationally efficient optimization methods like Stochastic gradient descent (SGD), has become particularly challenging. For instance, training state-of-the-art deep learning models like BERT and ResNet-50 takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b13">He et al., 2016)</ref>. Thus, there is a growing interest to develop optimization solutions to tackle this critical issue. The goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks, mostly focusing on approaches based on variants of SGD.</p><p>Methods based on SGD iteratively update the parameters of the model by moving them in a scaled (negative) direction of the gradient calculated on a minibatch. However, SGD's scalability is limited by its inherent sequential nature. Owing to this limitation, traditional approaches to improve SGD training time in the context of deep learning largely resort to distributed asynchronous setup <ref type="bibr" target="#b5">(Dean et al., 2012;</ref><ref type="bibr" target="#b24">Recht et al., 2011)</ref>. However, the implicit staleness introduced due to the asynchrony limits the parallelization of the approach, often leading to degraded performance. The feasibility of computing gradient on large minibatches in parallel due to recent hardware advances has seen the resurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous SGD. However, naïvely increasing the batch size typically results in degradation of generalization performance and reduces computational benefits <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</p><p>Synchronous SGD on large minibatches benefits from reduced variance of the stochastic gradients used in SGD. This allows one to use much larger learning rates in SGD, typically of the order square root of the minibatch size. Surprisingly, recent works have demonstrated that up to certain minibatch sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the Published as a conference paper at ICLR 2020 training <ref type="bibr" target="#b12">Goyal et al. (2017)</ref>. These works also elucidate two interesting aspects to enable the use of linear scaling in large batch synchronous SGD: (i) linear scaling of learning rate is harmful during the initial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be used initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size. Using these tricks, <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> was able to drastically reduce the training time of ResNet-50 model from 29 hours to 1 hour using a batch size of 8192. While these works demonstrate the feasibility of this strategy for reducing the wall time for training large deep neural networks, they also highlight the need for an adaptive learning rate mechanism for large batch learning.</p><p>Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this problem. The most successful in this line of research is the LARS algorithm <ref type="bibr" target="#b30">(You et al., 2017)</ref>, which was initially proposed for training RESNET. Using LARS, ResNet-50 can be trained on ImageNet in just a few minutes! However, it has been observed that its performance gains are not consistent across tasks. For instance, LARS performs poorly for attention models like BERT. Furthermore, theoretical understanding of the adaptation employed in LARS is largely missing. To this end, we study and develop new approaches specially catered to the large batch setting of our interest.</p><p>Contributions. More specifically, we make the following main contributions in this paper.</p><p>• Inspired by LARS, we investigate a general adaptation strategy specially catered to large batch learning and provide intuition for the strategy.</p><p>• Based on the adaptation strategy, we develop a new optimization algorithm (LAMB) for achieving adaptivity of learning rate in SGD. Furthermore, we provide convergence analysis for both LARS and LAMB to achieve a stationary point in nonconvex settings. We highlight the benefits of using these methods for large batch settings.</p><p>• We demonstrate the strong empirical performance of LAMB across several challenging tasks. Using LAMB we scale the batch size in training BERT to more than 32k without degrading the performance; thereby, cutting the time down from 3 days to 76 minutes. Ours is the first work to reduce BERT training wall time to less than couple of hours.</p><p>• We also demonstrate the efficiency of LAMB for training state-of-the-art image classification models like RESNET. To the best of our knowledge, ours is first adaptive solver that can achieve state-of-the-art accuracy for RESNET-50 as adaptive solvers like Adam fail to obtain the accuracy of SGD with momentum for these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">RELATED WORK</head><p>The literature on optimization for machine learning is vast and hence, we restrict our attention to the most relevant works here. Earlier works on large batch optimization for machine learning mostly focused on convex models, benefiting by a factor of square root of batch size using appropriately large learning rate. Similar results can be shown for nonconvex settings wherein using larger minibatches improves the convergence to stationary points; albeit at the cost of extra computation. However, several important concerns were raised with respect to generalization and computational performance in large batch nonconvex settings. It was observed that training with extremely large batch was difficult <ref type="bibr" target="#b17">(Keskar et al., 2016;</ref><ref type="bibr" target="#b14">Hoffer et al., 2017)</ref>. Thus, several prior works carefully hand-tune training hyper-parameters, like learning rate and momentum, to avoid degradation of generalization performance <ref type="bibr" target="#b12">(Goyal et al., 2017;</ref><ref type="bibr" target="#b19">Li, 2017;</ref><ref type="bibr" target="#b31">You et al., 2018;</ref><ref type="bibr" target="#b25">Shallue et al., 2018)</ref>. <ref type="bibr" target="#b18">(Krizhevsky, 2014)</ref> empirically found that simply scaling the learning rate linearly with respect to batch size works better up to certain batch sizes. To avoid optimization instability due to linear scaling of learning rate, <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> proposed a highly hand-tuned learning rate which involves a warm-up strategy that gradually increases the LR to a larger value and then switching to the regular LR policy (e.g. exponential or polynomial decay). Using LR warm-up and linear scaling, <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> managed to train RESNET-50 with batch size 8192 without loss in generalization performance. However, empirical study <ref type="bibr" target="#b25">(Shallue et al., 2018)</ref> shows that learning rate scaling heuristics with the batch size do not hold across all problems or across all batch sizes.</p><p>More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch training garnered significant interests. Several recent works successfully scaled the batch size to large values using adaptive learning rates without degrading the performance, thereby, finishing RESNET-50 training on ImageNet in a few minutes <ref type="bibr" target="#b31">(You et al., 2018;</ref><ref type="bibr" target="#b15">Iandola et al., 2016;</ref><ref type="bibr" target="#b3">Codreanu et al., 2017;</ref><ref type="bibr" target="#b0">Akiba et al., 2017;</ref><ref type="bibr" target="#b16">Jia et al., 2018;</ref><ref type="bibr" target="#b26">Smith et al., 2017;</ref><ref type="bibr" target="#b20">Martens &amp; Grosse, 2015;</ref><ref type="bibr" target="#b6">Devarakonda et al., 2017;</ref><ref type="bibr" target="#b21">Mikami et al., 2018;</ref><ref type="bibr" target="#b23">Osawa et al., 2018;</ref><ref type="bibr" target="#b32">You et al., 2019;</ref><ref type="bibr" target="#b28">Yamazaki et al., 2019)</ref>. To the best of our knowledge, the fastest training result for RESNET-50 on ImageNet is due to <ref type="bibr" target="#b29">Ying et al. (2018)</ref>, who achieve 76+% top-1 accuracy. By using the LARS optimizer and scaling the batch size to 32K on a TPUv3 Pod, <ref type="bibr" target="#b29">Ying et al. (2018)</ref> was able to train RESNET-50 on ImageNet in 2.2 minutes. However, it was empirically observed that none of these performance gains hold in other tasks such as BERT training (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Notation. For any vector x t ∈ R d , either x t,j or [x t ] j are used to denote its j th coordinate where j ∈ [d]. Let I be the d × d identity matrix, and let I = [I 1 , I 2 , ..., I h ] be its decomposition into column submatrices</p><formula xml:id="formula_0">I i = d × d h . For x ∈ R d , let x (i)</formula><p>be the block of variables corresponding to the columns of I i i.e.,</p><formula xml:id="formula_1">x (i) = I i x ∈ R di for i = {1, 2, · · · , h}. For any function f : R d → R, we use ∇ i f (x)</formula><p>to denote the gradient with respect to x (i) . For any vectors u, v ∈ R d , we use u 2 and u/v to denote elementwise square and division operators respectively. We use . and . 1 to denote l 2 -norm and l 1 -norm of a vector respectively.</p><p>We start our discussion by formally stating the problem setup. In this paper, we study nonconvex stochastic optimization problems of the form</p><formula xml:id="formula_2">min x∈R d f (x) := E s∼P [ (x, s)] + λ 2 x 2 ,<label>(1)</label></formula><p>where is a smooth (possibly nonconvex) function and P is a probability distribution on the domain S ⊂ R k . Here, x corresponds to model parameters, is the loss function and P is an unknown data distribution.</p><p>We assume function (x) is L i -smooth with respect to x (i) , i.e., there exists a constant L i such that</p><formula xml:id="formula_3">∇ i (x, s) − ∇ i (y, s) ≤ L i x (i) − y (i) , ∀ x, y ∈ R d , and s ∈ S,<label>(2)</label></formula><p>for all i ∈ [h]. We use L = (L 1 , · · · , L h ) to denote the h-dimensional vector of Lipschitz constants. We use L ∞ and L avg to denote max i L i and i Li h respectively. We assume the following bound on the variance in stochastic gradients:</p><formula xml:id="formula_4">E ∇ i (x, s) − ∇ i f (x) 2 ≤ σ 2 i for all x ∈ R d and i ∈ [h]. Furthermore, we also assume E [∇ (x, s)] i − [∇f (x)] i 2 ≤σ 2 i for all x ∈ R d and i ∈ [d].</formula><p>We use σ = (σ 1 , · · · , σ h ) andσ = (σ 1 , · · · ,σ d ) to denote the vectors of standard deviations of stochastic gradient per layer and per dimension respectively. Finally, we assume that the gradients are bounded i.e., [∇l(x, s)] j ≤ G for all i ∈ [d], x ∈ R d and s ∈ S. Note that such assumptions are typical in the analysis of stochastic first-order methods (cf. <ref type="bibr" target="#b9">(Ghadimi &amp; Lan, 2013a;</ref><ref type="bibr" target="#b11">Ghadimi et al., 2014)</ref>).</p><p>Stochastic gradient descent (SGD) is one of the simplest first-order algorithms for solving problem in Equation 1. The update at the t th iteration of SGD is of the following form:</p><formula xml:id="formula_5">x t+1 = x t − η t 1 |S t | st∈St ∇ (x t , s t ) + λx t ,<label>(SGD)</label></formula><p>where S t is set of b random samples drawn from the distribution P. For very large batch settings, the following is a well-known result for SGD. Theorem 1 ( <ref type="bibr" target="#b10">(Ghadimi &amp; Lan, 2013b)</ref>). With large batch b = T and using appropriate learning rate, we have the following for the iterates of SGD:</p><formula xml:id="formula_6">E ∇f (x a ) 2 ≤ O (f (x 1 ) − f (x * ))L ∞ T + σ 2 T .</formula><p>where x * is an optimal solution to the problem in equation 1 and x a is an iterate uniformly randomly chosen from {x 1 , · · · , x T }.</p><p>However, tuning the learning rate η t in SGD, especially in large batch settings, is difficult in practice. Furthermore, the dependence on L ∞ (the maximum of smoothness across dimension) can lead to significantly slow convergence. In the next section, we discuss algorithms to circumvent this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHMS</head><p>In this section, we first discuss a general strategy to adapt the learning rate in large batch settings. Using this strategy, we discuss two specific algorithms in the later part of the section. Since our primary focus is on deep learning, our discussion is centered around training a h-layer neural network.</p><p>General Strategy. Suppose we use an iterative base algorithm A (e.g. SGD or ADAM) in the small batch setting with the following layerwise update rule:</p><formula xml:id="formula_7">x t+1 = x t + η t u t ,</formula><p>where u t is the update made by A at time step t. We propose the following two changes to the update for large batch settings:</p><p>1. The update is normalized to unit l 2 -norm. This is ensured by modifying the update to the form u t / u t . Throughout this paper, such a normalization is done layerwise i.e., the update for each layer is ensured to be unit l 2 -norm. 2. The learning rate is scaled by φ( x t ) for some function φ : R + → R + . Similar to the normalization, such a scaling is done layerwise.</p><p>Suppose the base algorithm A is SGD, then the modification results in the following update rule:</p><formula xml:id="formula_8">x (i) t+1 = x (i) t − η t φ( x (i) t ) g (i) t g (i) t ,<label>(3)</label></formula><p>for all layers i ∈ [h] and where x (i) t and g (i)</p><p>t are the parameters and the gradients of the i th layer at time step t. The normalization modification is similar to one typically used in normalized gradient descent except that it is done layerwise. Note that the modification leads to a biased gradient update; however, in large-batch settings, it can be shown that this bias is small. It is intuitive that such a normalization provides robustness to exploding gradients (where the gradient can be arbitrarily large) and plateaus (where the gradient can be arbitrarily small). Normalization of this form essentially ignores the size of the gradient and is particularly useful in large batch settings where the direction of the gradient is largely preserved.</p><p>The scaling term involving φ ensures that the norm of the update is of the same order as that of the parameter. We found that this typically ensures faster convergence in deep neural networks. In practice, we observed that a simple function of φ(z) = min{max{z, γ l }, γ u } works well. It is instructive to consider the case where φ(z) = z. In this scenario, the overall change in the learning rate is</p><formula xml:id="formula_9">x (i) t g (i) t</formula><p>, which can also be interpreted as an estimate on the inverse of Lipschitz constant of the gradient (see equation 2). We now discuss different instantiations of the strategy discussed above. In particular, we focus on two algorithms: LARS (3.1) and the proposed method, LAMB (3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LARS ALGORITHM</head><p>The first instantiation of the general strategy is LARS algorithm <ref type="bibr" target="#b30">(You et al., 2017)</ref>, which is obtained by using momentum optimizer as the base algorithm A in the framework. LARS was earlier proposed for large batch learning for RESNET on ImageNet. In general, it is observed that the using (heavy-ball) momentum, one can reduce the variance in the stochastic gradients at the cost of little bias. The pseudocode for LARS is provide in Algorithm 1.</p><p>We now provide convergence analysis for LARS in general nonconvex setting stated in this paper. For the sake of simplicity, we analyze the case where β 1 = 0 and λ = 0 in Algorithm 1. However, our analysis should extend to the general case as well. We will defer all discussions about the convergence rate to the end of the section.</p><formula xml:id="formula_10">Theorem 2. Let η t = η = 2(f (x1)−f (x * )) α 2 u L 1T for all t ∈ [T ], b = T , α l ≤ φ(v) ≤ α u for all v &gt; 0 where α l , α u &gt; 0.</formula><p>Then for x t generated using LARS (Algorithm 1), we have the following bound</p><formula xml:id="formula_11">E 1 √ h h i=1 ∇ i f (x a ) 2 ≤ O (f (x 1 ) − f (x * ))L avg T + σ 2 1 T h ,</formula><p>where x * is an optimal solution to the problem in equation 1 and x a is an iterate uniformly randomly chosen from {x 1 , · · · , x T }.</p><formula xml:id="formula_12">Algorithm 1 LARS Input: x1 ∈ R d , learning rate {ηt} T t=1 , parameter 0 &lt; β1 &lt; 1, scaling function φ, &gt; 0 Set m0 = 0 for t = 1 to T do Draw b samples St from P Compute gt = 1 |S t | s t ∈S t ∇ (xt, st) mt = β1mt−1 + (1 − β1)(gt + λxt) x (i) t+1 = x (i) t − ηt φ( x (i) t ) m (i) t m (i) t for all i ∈ [h] end for Algorithm 2 LAMB Input: x1 ∈ R d , learning rate {ηt} T t=1 , parameters 0 &lt; β1, β2 &lt; 1, scaling function φ, &gt; 0 Set m0 = 0, v0 = 0 for t = 1 to T do Draw b samples St from P. Compute gt = 1 |S t | s t ∈S t ∇ (xt, st). mt = β1mt−1 + (1 − β1)gt vt = β2vt−1 + (1 − β2)g 2 t mt = mt/(1 − β t 1 ) vt = vt/(1 − β t 2 ) Compute ratio rt = m t √ v t + x (i) t+1 = x (i) t − ηt φ( x (i) t ) r (i) t +λx (i) t (r (i) t + λx (i) t ) end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LAMB ALGORITHM</head><p>The second instantiation of the general strategy is obtained by using ADAM as the base algorithm A. ADAM optimizer is popular in deep learning community and has shown to have good performance for training state-of-the-art language models like BERT. Unlike LARS, the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in ADAM and (ii) layerwise normalization obtained due to layerwise adaptivity. The pseudocode for LAMB is provided in Algorithm 2. When β 1 = 0 and β 2 = 0, the algorithm reduces to be Sign SGD where the learning rate is scaled by square root of the layer dimension <ref type="bibr" target="#b2">(Bernstein et al., 2018)</ref>.</p><p>The following result provides convergence rate for LAMB in general nonconvex settings. Similar to the previous case, we focus on the setting where β 1 = 0 and λ = 0. As before, our analysis extends to the general case; however, the calculations become messy.</p><formula xml:id="formula_13">Theorem 3. Let η t = η = 2(f (x1)−f (x * )) α 2 u L 1T for all t ∈ [T ], b = T , d i = d/h for all i ∈ [h], and α l ≤ φ(v) ≤ α u for all v &gt; 0 where α l , α u &gt; 0.</formula><p>Then for x t generated using LAMB (Algorithm 2), we have the following bounds:</p><p>1. When β 2 = 0, we have</p><formula xml:id="formula_14">E 1 √ d ∇f (x a ) 1 2 ≤ O (f (x 1 ) − f (x * ))L avg T + σ 2 1 T h , 2. When β 2 &gt; 0, we have E[ ∇f (x a ) 2 ] ≤ O G 2 d h(1 − β 2 ) × 2(f (x 1 ) − f (x * )) L 1 T + σ 1 √ T ,</formula><p>where x * is an optimal solution to the problem in equation 1 and x a is an iterate uniformly randomly chosen from {x 1 , · · · , x T }.</p><p>Discussion on convergence rates. We first start our discussion with the comparison of convergence rate of LARS with that of SGD (Theorem 1). The convergence rates of LARS and SGD differ in two ways: (1) the convergence criterion is</p><formula xml:id="formula_15">(E[ h i=1 ∇ i f ]) 2 as opposed to E[ ∇f 2</formula><p>] in SGD and (2) the dependence on L and σ in the convergence rate. Briefly, the convergence rate of LARS is better than SGD when the gradient is denser than curvature and stochasticity. This convergence rate comparison is similar in spirit to the one obtained in <ref type="bibr" target="#b2">(Bernstein et al., 2018)</ref>. Assuming that the convergence criterion in Theorem 1 and Theorem 2 is of similar order (which happens when gradients are fairly dense), convergence rate of LARS and LAMB depend on L avg instead of L ∞ and are thus, significantly better than that of SGD. A more quantitative comparison is provided in Section C of the Appendix. The comparison of LAMB (with β 2 = 0) with SGD is along similar lines. We obtain slightly worse rates for the case where β 2 &gt; 0; although, we believe that its behavior should be better than the case β 2 = 0. We leave this investigation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We now present empirical results comparing LAMB with existing optimizers on two important large batch training tasks: BERT and RESNET-50 training. We also compare LAMB with existing optimizers for small batch size (&lt; 1K) and small dataset (e.g. CIFAR, MNIST) (see Appendix).</p><p>Experimental Setup. To demonstrate its robustness, we use very minimal hyperparameter tuning for the LAMB optimizer. Thus, it is possible to achieve better results by further tuning the hyperparameters. The parameters β 1 and β 2 in Algorithm 2 are set to 0.9 and 0.999 respectively in all our experiments; we only tune the learning rate. We use a polynomially decaying learning rate of η t = η 0 ×(1−t/T ) in Algorithm 2), which is the same as in BERT baseline. This setting also works for all other applications in this paper. Furthermore, for BERT and RESNET-50 training, we did not tune the hyperparameters of LAMB while increasing the batch size. We use the square root of LR scaling rule to automatically adjust learning rate and linear-epoch warmup scheduling. We use TPUv3 in all the experiments. A TPUv3 Pod has 1024 chips and can provide more than 100 petaflops performance for mixed precision computing. To make sure we are comparing with solid baselines, we use grid search to tune the hyper-parameters for ADAM, ADAGRAD, ADAMW (ADAM with weight decay), and LARS. We also tune weight decay for ADAMW. All the hyperparameter tuning settings are reported in the Appendix. Due to space constraints, several experimental details are relegated to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BERT TRAINING</head><p>We first discuss empirical results for speeding up BERT training. For this experiment, we use the same dataset as <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>, which is a concatenation of Wikipedia and BooksCorpus with 2.5B and 800M words respectively. We specifically focus on the SQuAD task 2 in this paper. The F1 score on SQuAD-v1 is used as the accuracy metric in our experiments. All our comparisons are with respect to the baseline BERT model by <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>. To train BERT, <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> first train the model for 900k iterations using a sequence length of 128 and then switch to a sequence length of 512 for the last 100k iterations. This results in a training time of around 3 days on 16 TPUv3 chips. The baseline BERT model 3 achieves a F1 score of 90.395. To ensure a fair comparison, we follow the same SQuAD fine-tune procedure of <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> without modifying any configuration (including number of epochs and hyperparameters). As noted earlier, we could get even better results by changing the fine-tune configuration. For instance, by just slightly changing the learning rate in the fine-tune stage, we can obtain a higher F1 score of 91.688 for the batch size of 16K using LAMB. We report a F1 score of 91.345 in <ref type="table">Table 1</ref>, which is the score obtained for the untuned version. Below we describe two different training choices for training BERT and discuss the corresponding speedups.</p><p>For the first choice, we maintain the same training procedure as the baseline except for changing the training optimizer to LAMB. We run with the same number of epochs as the baseline but with batch size scaled from 512 to 32K. The choice of 32K batch size (with sequence length 512) is mainly due to memory limits of TPU Pod. Our results are shown in <ref type="table">Table 1</ref>. By using the LAMB optimizer, we are able to achieve a F1 score of 91.460 in 15625 iterations for a batch size of 32768 (14063 iterations for sequence length 128 and 1562 iterations for sequence length 512). With 32K batch size, we reduce BERT training time from 3 days to around 100 minutes. We achieved 49.1 times speedup by 64 times computational resources (76.7% efficiency). We consider the speedup is great because we use the synchronous data-parallelism. There is a communication overhead coming from transferring of the gradients over the interconnect. For RESNET-50, researchers are able to achieve 90% scaling efficiency because RESNET-50 has much fewer parameters (# parameters is equal to #gradients) than BERT (25 million versus 300 million).</p><p>To obtain further improvements, we use the Mixed-Batch Training procedure with LAMB. Recall that BERT training involves two stages: the first 9/10 of the total epochs use a sequence length of 128, while the last 1/10 of the total epochs use a sequence length of 512. For the second stage training, which involves a longer sequence length, due to memory limits, a maximum batch size of only 32768 can be used on a TPUv3 Pod. However, we can potentially use a larger batch size for the first stage because of a shorter sequence length. In particular, the batch size can be increased to 131072 for the first stage. However, we did not observe any speedup by increasing the batch size from 65536 to 131072 for the first stage, thus, we restrict the batch size to 65536 for this stage. By using this strategy, we are able to make full utilization of the hardware resources throughout the training <ref type="table">Table 1</ref>: We use the F1 score on SQuAD-v1 as the accuracy metric. The baseline F1 score is the score obtained by the pre-trained model (BERT-Large) provided on BERT's public repository (as of February 1st, 2019). We use TPUv3s in our experiments. We use the same setting as the baseline: the first 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used a sequence length of 512. All the experiments run the same number of epochs. Dev set means the test data. It is worth noting that we can achieve better results by manually tuning the hyperparameters. The data in this procedure. Increasing the batch size is able to warm-up and stabilize the optimization process <ref type="bibr" target="#b26">(Smith et al., 2017)</ref>, but decreasing the batch size brings chaos to the optimization process and can cause divergence. In our experiments, we found a technique that is useful to stabilize the second stage optimization. Because we switched to a different optimization problem, it is necessary to re-warm-up the optimization. Instead of decaying the learning rate at the second stage, we ramp up the learning rate from zero again in the second stage (re-warm-up). As with the first stage, we decay the learning rate after the re-warm-up phase. With this method, we only need 8599 iterations and finish BERT training in 76 minutes (100.2% efficiency).</p><p>Comparison with ADAMW and LARS. To ensure that our approach is compared to a solid baseline for the BERT training, we tried three different strategies for tuning ADAMW: (1) ADAMW with default hyperparameters (see <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>) (2) ADAMW with the same hyperparameters as LAMB, and (3) ADAMW with tuned hyperparameters. ADAMW stops scaling at the batch size of 16K because it is not able to achieve the target F1 score (88.1 vs 90.4). The tuning information of ADAMW is shown in the Appendix. For 64K/32K mixed-batch training, even after extensive tuning of the hyperparameters, we fail to get any reasonable result with ADAMW optimizer. We conclude that ADAMW does not work well in large-batch BERT training or is at least hard to tune. We also observe that LAMB performs better than LARS for all batch sizes (see <ref type="table" target="#tab_1">Table 2</ref>). ImageNet training with ResNet-50 is an industry standard metric that is being used in MLPerf 4 . The baseline can get 76.3% top-1 accuracy in 90 epochs <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>. All the successful implementations are based on momentum SGD <ref type="bibr" target="#b13">(He et al., 2016;</ref><ref type="bibr" target="#b12">Goyal et al., 2017)</ref> or LARS optimizer <ref type="bibr" target="#b29">(Ying et al., 2018;</ref><ref type="bibr" target="#b16">Jia et al., 2018;</ref><ref type="bibr" target="#b21">Mikami et al., 2018;</ref><ref type="bibr" target="#b31">You et al., 2018;</ref><ref type="bibr" target="#b28">Yamazaki et al., 2019)</ref>. Before our study, we did not find any paper reporting a state-of-the-art accuracy achieved by ADAM, ADAGRAD, or ADAMW optimizer. In our experiments, even with comprehensive hyper-parameter tuning, ADAGRAD/ADAM/ADAMW (with batch size 16K) only achieves 55.38%/66.04%/67.27% top-1 accuracy. After adding learning rate scheme of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref>, the top-1 accuracy of ADAGRAD/ADAM/ADAMW was improved to 72.0%/73.48%/73.07%. However, they are still much lower than 76.3%. The details of the tuning information are in the Appendix. <ref type="table" target="#tab_2">Table 3</ref> shows that LAMB can achieve the target accuracy. Beyond a batch size of 8K, LAMB's accuracy is higher than the momentum. LAMB's accuracy is also slightly better than LARS. At a batch size of 32K, LAMB achieves 76.4% top-1 accuracy while LARS achieves 76.3%. At a batch size of 2K, LAMB is able to achieve 77.11% top-1 accuracy while LARS achieves 76.6%.  <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>. + means adding the learning rate scheme of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> to the optimizer: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>. All the adaptive solvers were comprehensively tuned.    </p><formula xml:id="formula_16">5 2 3.0 ×10 3 5 2 2.5 ×10 3 5 2 2.0 ×10 3 5 2 1.5 ×10 3 5 2 1.0 ×10 3 5 2 0.5 ×10 3 5 2 0.0 ×10 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warmup Ratio</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Large batch techniques are critical to speeding up deep neural network training. In this paper, we propose the LAMB optimizer, which supports adaptive elementwise updating and layerwise learning rates. Furthermore, LAMB is a general purpose optimizer that works for both small and large batches. We also provided theoretical analysis for the LAMB optimizer, highlighting the cases where it performs better than standard SGD. LAMB achieves a better performance than existing optimizers for a wide range of applications. By using LAMB, we are able to scale the batch size of BERT pre-training to 64K without losing accuracy, thereby, reducing the BERT training time from 3 days to around 76 minutes. LAMB is also the first large batch adaptive solver that can achieve state-of-the-art accuracy on ImageNet training with RESNET-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>We want to thank the comments from George Dahl and Jeff Dean. We want to thank Michael Banfield, Dehao Chen, Youlong Cheng, Sameer Kumar, and Zak Stone for TPU Pod support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF THEOREM 2</head><p>Proof. We analyze the convergence of LARS for general minibatch size here. Recall that the update of LARS is the following</p><formula xml:id="formula_17">x (i) t+1 = x (i) t − η t φ( x (i) t ) g (i) t g (i) t ,</formula><p>for all i ∈ [h]. For simplicity of notation, we reason the</p><p>Since the function f is L-smooth, we have the following:</p><formula xml:id="formula_18">f (x t+1 ) ≤ f (x t ) + ∇ i f (x t ), x (i) t+1 − x (i) t + h i=1 L i 2 x (i) t+1 − x (i) t 2 = f (x t ) − η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j g (i) t + h i=1 L i η 2 t φ 2 ( x (i) t ) 2 ≤ f (x t ) − η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j g (i) t − [∇ i f (x t )] j ∇ i f (x t ) + [∇ i f (x t )] j ∇ i f (x t ) + η 2 t α 2 u 2 L 1 = f (x t ) − η t h i=1 φ( x (i) t ) × ∇ i f (x t ) − η t h i=1 di j=1 [∇ i f (x t )] j × g (i) t,j g (i) t − [∇ i f (x t )] j ∇ i f (x t ) + η 2 t α 2 u 2 L 1<label>(4)</label></formula><p>The first inequality follows from the lipschitz continuous nature of the gradient. Let ∆</p><formula xml:id="formula_19">(i) t = g (i) t − ∇ i f (x t ).</formula><p>Then the above inequality can be rewritten in the following manner:</p><formula xml:id="formula_20">f (x t+1 ) ≤ f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) − η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × (∆ (i) t,j + [∇ i f (x t )] j ) ∆ (i) t + ∇ i f (x t ) − [∇ i f (x t )] j ∇ i f (x t ) + η 2 t α 2 u 2 L 1 = f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) − η t h i=1 φ( x (i) t ) × ∆ (i) t + ∇ i f (x t ), ∇ i f (x t ) ∆ (i) t + ∇ i f (x t ) − ∇ i f (x t ) + η 2 t α 2 u 2 L 1 = f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) + η t h i=1 φ( x (i) t ) × ∇ i f (x t ) ∆ (i) t + ∇ i f (x t ) − ∆ (i) t + ∇ i f (x t ), ∇ i f (x t ) ∆ (i) t + ∇ i f (x t ) + η 2 t α 2 u 2 L 1 = f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) + η 2 t α 2 u 2 L 1 + η t h i=1 φ( x (i) t ) × ∇ i f (x t ) ∆ (i) t + ∇ i f (x t ) − ∆ (i) t + ∇ i f (x t ) 2 + ∆ (i) t , ∆ (i) t + ∇ i f (x t ) ∆ (i) t + ∇ i f (x t ) .<label>(5)</label></formula><p>Using Cauchy-Schwarz inequality in the above inequality, we have:</p><formula xml:id="formula_21">f (x t+1 ) ≤ f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) + η t h i=1 φ( x (i) t ) × ∇ i f (x t ) − ∆ (i) t + ∇ i f (x t ) + ∆ (i) t + η 2 t α 2 u 2 L 1 ≤ f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) + 2η t h i=1 φ( x (i) t ) × ∆ (i) t + η 2 t α 2 u 2 L 1</formula><p>Taking expectation, we obtain the following:</p><formula xml:id="formula_22">E[f (x t+1 )] ≤ f (x t ) − η t h i=1 φ( x (i) t ) ∇ i f (x t ) + 2η t h i=1 φ( x (i) t ) × E[ ∆ (i) t ] + η 2 t α 2 u 2 L 1 ≤ f (x t ) − η t α l h i=1 ∇ i f (x t ) + 2η t α u σ 1 √ b + η 2 t α 2 u 2 L 1 .<label>(6)</label></formula><p>Summing the above inequality for t = 1 to T and using telescoping sum, we have the following inequality:</p><formula xml:id="formula_23">E[f (x T +1 )] ≤ f (x 1 ) − ηα l T t=1 h i=1 E[ ∇ i f (x t ) ] + 2ηT α u σ 1 √ b + η 2 α 2 u T 2 L 1 .</formula><p>Rearranging the terms of the above inequality, and dividing by ηT α l , we have:</p><formula xml:id="formula_24">1 T T t=1 h i=1 E[ ∇ i f (x t ) ] ≤ f (x 1 ) − E[f (x T +1 )] T ηα l + 2α u σ 1 √ bα l + ηα 2 u 2α l L 1 ≤ f (x 1 ) − f (x * ) T ηα l + 2α u σ 1 α l √ b + ηα 2 u 2α l L 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF THEOREM 3</head><p>Proof. We analyze the convergence of LAMB for general minibatch size here. Recall that the update of LAMB is the following</p><formula xml:id="formula_25">x (i) t+1 = x (i) t − η t φ( x (i) t ) r (i) t r (i) t ,</formula><p>for all i ∈ [h]. For simplicity of notation, we reason the</p><p>Since the function f is L-smooth, we have the following:</p><formula xml:id="formula_26">f (x t+1 ) ≤ f (x t ) + ∇ i f (x t ), x (i) t+1 − x (i) t + h i=1 L i 2 x (i) t+1 − x (i) t 2 = f (x t ) −η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × r (i) t,j r (i) t T1 + h i=1 L i α 2 u η 2 t 2<label>(7)</label></formula><p>The above inequality simply follows from the lipschitz continuous nature of the gradient. We bound term T 1 in the following manner:</p><formula xml:id="formula_27">T 1 ≤ −η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × r (i) t,j r (i) t ≤ −η t h i=1 di j=1 1 − β 2 G 2 d i φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j − η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × r (i) t,j r (i) t 1(sign(∇ i f (x t )] j ) = sign(r (i) t,j ))<label>(8)</label></formula><p>This follows from the fact that r</p><formula xml:id="formula_28">(i) t ≤ di 1−β2 and √ v t ≤ G.</formula><p>If β 2 = 0, then T 1 can be bounded as follows:</p><formula xml:id="formula_29">T 1 ≤ −η t h i=1 di j=1 1 d i φ( x (i) t ) × |[∇ i f (x t )] j | − η t h i=1 di j=1 φ( x (i) t ) × [∇ i f (x t )] j × r (i) t,j r (i) t 1(sign(∇ i f (x t )] j ) = sign(r (i) t,j ))</formula><p>The rest of the proof for β 2 = 0 is similar to argument for the case β 2 &gt; 0, which is shown below.</p><p>Taking expectation, we have the following:</p><formula xml:id="formula_30">E[T 1 ] ≤ −η t h i=1 di j=1 1 − β 2 G 2 d i E φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j − η t h i=1 di j=1 E φ( x (i) t ) × [∇ i f (x t )] j × r (i) t,j r (i) t 1(sign(∇ i f (x t )] j ) = sign(g (i) t,j )) ≤ −η t h i=1 di j=1 1 − β 2 G 2 d i E φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j + η t h i=1 di j=1 E α u |[∇ i f (x t )] j |1(sign(∇ i f (x t )] j ) = sign(g (i) t,j )) ≤ −η t h i=1 di j=1 1 − β 2 G 2 d i E φ( x (i) t ) × [∇ i f (x t )] j × g (i) t,j − η t h i=1 di j=1 α u |[∇ i f (x t )] j |P(sign(∇ i f (x t )] j ) = sign(g (i) t,j ))<label>(9)</label></formula><p>Using the bound on the probability that the signs differ, we get:</p><formula xml:id="formula_31">E[T 1 ] ≤ −η t α l h(1 − β 2 ) G 2 d ∇f (x t ) 2 + η t α u h i=1 di j=1 σ i,j √ b .</formula><p>Substituting the above bound on T 1 in equation 7, we have the following bound:</p><formula xml:id="formula_32">E[f (x t+1 )] ≤ f (x t ) − η t α l h(1 − β 2 ) G 2 d ∇f (x t ) 2 + η t α u σ 1 √ b + η 2 t α 2 u L 1 2 (10) Input: x1 ∈ R d , learning rate {ηt} T t=1 , parame- ters 0 &lt; β1, β2 &lt; 1, scaling function φ, &gt; 0, parameters 0 &lt; {β t 1 } T t=1 &lt; 1 Set m0 = 0, v0 = 0 for t = 1 to T do Draw b samples St from P. Compute gt = 1 |S t | s t ∈S t ∇ (xt, st). mt = β1mt−1 + (1 − β1)gt m = β t+1 1 m t 1−Π t+1 i=1 β i 1 + (1−β t 1 )g t 1−Π t i=1 β i 1 vt = β2vt−1 + (1 − β2)g 2 t v = β 2 v t 1−β t 2 Compute ratio rt =m √v + x (i) t+1 = x (i) t − ηt φ( x (i) t ) r (i) t +λx (i) t (r (i) t + λxt) end for Algorithm 4 NN-LAMB Input: x1 ∈ R d , learning rate {ηt} T t=1 , parameters 0 &lt; β1, β2 &lt; 1, scaling function φ, &gt; 0, parame- ters 0 &lt; {β t 1 } T t=1 &lt; 1 Set m0 = 0, v0 = 0 for t = 1 to T do Draw b samples St from P. Compute gt = 1 |S t | s t ∈S t ∇ (xt, st). mt = β1mt−1 + (1 − β1)gt m = β t+1 1 m t 1−Π t+1 i=1 β i 1 + (1−β t 1 )g t 1−Π t i=1 β i 1 vt = β2vt−1 + (1 − β2)g 2 t v = β t+1 2 v t 1−Π t+1 i=1 β i 2 + (1−β t 2 )g 2 t 1−Π t i=1 β i 2 Compute ratio rt =m √v + x (i) t+1 = x (i) t − ηt φ( x (i) t ) r (i) t +λx (i) t (r (i) t + λxt) end for</formula><p>Summing the above inequality for t = 1 to T and using telescoping sum, we have the following inequality:</p><formula xml:id="formula_33">E[f (x T +1 )] ≤ f (x 1 ) − η t α l h(1 − β 2 ) G 2 d T t=1 E[ ∇f (x t ) 2 ] + ηT α u σ 1 √ b + η 2 α 2 u T 2 L 1 .</formula><p>Rearranging the terms of the above inequality, and dividing by ηT α l , we have:</p><formula xml:id="formula_34">h(1 − β 2 ) G 2 d 1 T T t=1 E[ ∇f (x t ) 2 ] ≤ f (x 1 ) − E[f (x T +1 )] T ηα l + α u σ 1 α l √ b + η 2 L 1 ≤ f (x 1 ) − f (x * ) T ηα l + α u σ 1 α l √ b + ηα 2 u 2α l L 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISON OF CONVERGENCE RATES OF LARS AND SGD</head><p>Inspired by the comparison used by <ref type="bibr" target="#b2">(Bernstein et al., 2018)</ref> for comparing SIGN SGD with SGD, we define the following quantities:</p><formula xml:id="formula_35">h i=1 ∇ i f (x t ) 2 = ψ(∇f (x t ))d ∇f (x t ) 2 h ≥ ψ g d ∇f (x t ) 2 h L 2 1 ≤ ψ L d 2 L 2 ∞ h 2 σ 2 1 = ψ σ d σ 2 h .</formula><p>Then LARS convergence rate can be written in the following manner:</p><formula xml:id="formula_36">(E[ ∇f (x a ) ) 2 ≤ O (f (x 1 ) − f (x * ))L ∞ T ψ L ψ 2 g + σ 2 T ψ 2 σ ψ 2 g .</formula><p>If ψ L ψ 2 g and ψ σ ψ 2 g then LARS (i.e., gradient is more denser than curvature or stochasticity), we gain over SGD. Otherwise, SGD's upper bound on convergence rate is better.</p><p>Figure 1: This figure shows N-LAMB and NN-LAMB can achieve a comparable accuracy compared to LAMB optimizer. Their performances are much better than momentum solver. The result of momentum optimizer was reported by <ref type="bibr" target="#b12">Goyal et al. (2017)</ref>. For Nadam, we use the learning rate recipe of <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>. We also tuned the learning rate of Nadam in {1e-4, 2e-4, ..., 9e-4, 1e-3, 2e-3, ..., 9e-3, 1e-2}. D N-LAMB: NESTEROV MOMENTUM FOR LAMB <ref type="bibr" target="#b27">Sutskever et al. (2013)</ref> report that Nesterov's accelerated gradient (NAG) proposed by Nesterov <ref type="formula" target="#formula_2">(1983)</ref> is conceptually and empirically better than the regular momentum method for convex, non-stochastic objectives. <ref type="bibr" target="#b8">Dozat (2016)</ref> incorporated Nesterov's momentum into Adam optimizer and proposed the Nadam optimizer. Specifically, only the first moment of Adam was modified and the second moment of Adam was unchanged. The results on several applications (Word2Vec, Image Recognition, and LSTM Language Model) showed that Nadam optimizer improves the speed of convergence and the quality of the learned models. We also tried using Nesterov's momentum to replace the regular momentum of LAMB optimizer's first moment. In this way, we got a new algorithm named as N-LAMB (Nesterov LAMB). The complete algorithm is in Algorithm 3. We can also Nesterov's momentum to replace the regular momentum of LAMB optimizer's second moment. We refer to this algorithm as NN-LAMB (Nesterov's momentum for both the first moment and the second moment). The details of NN-LAMB were shown in Algorithm 4. <ref type="bibr" target="#b8">Dozat (2016)</ref> suggested the best performance of Nadam was achieved by β 1 = 0.975, β 2 = 0.999, and = 1e-8. We used the same settings for N-LAMB and NN-LAMB. We scaled the batch size to 32K for ImageNet training with ResNet-50. Our experimental results show that N-LAMB and NN-LAMB can achieve a comparable accuracy compared to LAMB optimizer. Their performances are much better than momentum solver <ref type="figure">(Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E LAMB WITH LEARNING RATE CORRECTION</head><p>There are two operations at each iteration in original Adam optimizer (let us call it adam-correction):</p><formula xml:id="formula_37">m t = m t /(1 − β t 1 ) v t = v t /(1 − β t 2 )</formula><p>It has an impact on the learning rate by η t := η t * (1 − β t 2 )/(1 − β t 1 ). According to our experimental results, adam-correction essentially has the same effect as learning rate warmup (see <ref type="figure">Figure 2</ref>). The warmup function often was implemented in the modern deep learning system. Thus, we can remove adam-correction from the LAMB optimizer. We did not observe any drop in the test or validation accuracy for BERT and ImageNet training.</p><p>Figure 2: The figure shows that adam-correction has the same effect as learning rate warmup. We removed adam-correction from the LAMB optimizer. We did not observe any drop in the test or validation accuracy for BERT and ImageNet training. <ref type="figure">Figure 3</ref>: We tried different norms in LAMB optimizer. However, we did not observe a significant difference in the validation accuracy of ImageNet training with ResNet-50. We use L2 norm as the default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F LAMB WITH DIFFERENT NORMS</head><p>We need to compute the matrix/tensor norm for each layer when we do the parameter updating in the LAMB optimizer. We tried different norms in LAMB optimizer. However, we did not observe a significant difference in the validation accuracy of ImageNet training with ResNet-50. In our experiments, the difference in validation accuracy is less than 0.1 percent <ref type="figure">(Figure 3)</ref>. We use L2 norm as the default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G REGULAR BATCH SIZES FOR SMALL DATASETS: MNIST AND CIFAR-10.</head><p>According to DAWNBench, DavidNet (a custom 9-layer Residual ConvNet) is the fastest model for CIFAR-10 dataset (as of April 1st, 2019) 5 . The baseline uses the momentum SGD optimizer. <ref type="table">Table 6</ref> and <ref type="figure">Figure 4</ref> show the test accuracy of CIFAR-10 training with DavidNet. The PyTorch implementation (momentum SGD optimizer) on GPUs was reported on Standford DAWNBench's website, which achieves 94.06% in 24 epochs. The Tensorflow implementation (momentum SGD optimizer) on TPU achieves a 93.72% accuracy in 24 epochs 6 . We use the implementation of TensorFlow on TPUs. LAMB optimizer is able to achieve 94.08% test accuracy in 24 epochs, which is better than other adaptive optimizers and momentum SGD. Even on the smaller tasks like MNIST training with LeNet, LAMB is able to achieve a better accuracy than existing solvers <ref type="table" target="#tab_8">(Table 7)</ref>. <ref type="figure">Figure 4</ref>: LAMB is better than the existing solvers (batch size = 512). We make sure all the solvers are carefully tuned. The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, <ref type="bibr">0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}</ref>. The momentum optimizer was tuned by the baseline implementer. The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01, 0.1, 1.0}. <ref type="table">Table 6</ref>: CIFAR-10 training with DavidNet (batch size = 512). All of them run 24 epochs and finish the training under one minute on one cloud TPU. We make sure all the solvers are carefully tuned. The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, <ref type="bibr">0.1, 0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H IMPLEMENTATION DETAILS AND ADDITIONAL RESULTS</head><p>There are several hyper-parameters in LAMB optimizer. Although users do not need to tune them, we explain them to help users to have a better understanding. β 1 is used for decaying the running average of the gradient. β 2 is used for decaying the running average of the square of gradient. The default setting for other parameters: weight decay rate λ=0.01, β 1 =0.9, β 2 =0.999, =1e-6. We did not tune β 1 and β 2 . However, our experiments show that tuning them may get a higher accuracy.</p><p>Based on our experience, learning rate is the most important hyper-parameter that affects the learning efficiency and final accuracy. <ref type="bibr" target="#b1">Bengio (2012)</ref> suggests that it is often the single most important hyper-parameter and that it always should be tuned. Thus, to make sure we have a solid baseline, we carefully tune the learning rate of ADAM, ADAMW, ADAGRAD, and momentum SGD</p><p>In our experiments, we found that the validation loss is not reliable for large-batch training. A lower validation loss does not necessarily lead to a higher validation accuracy ( <ref type="figure">Figure 5</ref>). Thus, we use the test/val accuracy or F1 score on dev set to evaluate the optimizers.</p><p>H.0.1 BERT <ref type="table" target="#tab_9">Table 8</ref> shows some of the tuning information from BERT training with ADAMW optimizer. ADAMW stops scaling at the batch size of 16K. The target F1 score is 90.5. LAMB achieves a F1 score of 91.345. The table shows the tuning information of ADAMW. In <ref type="table" target="#tab_9">Table 8</ref>, we report the best F1 score we observed from our experiments.</p><p>The loss curves of BERT training by LAMB for different batch sizes are shown in <ref type="figure" target="#fig_0">Figure 6</ref>. We observe that the loss curves are almost identical to each other, which means our optimizer scales well with the batch size.</p><p>The training loss curve of BERT mixed-batch pre-training with LAMB is shown in <ref type="figure" target="#fig_1">Figure 7</ref>. This figure shows that LAMB can make the training converge smoothly at the batch size of 64K. <ref type="figure" target="#fig_2">Figure 8</ref> shows that we can achieve 76.8% scaling efficiency by scaling the batch size (49.1 times speedup by 64 times computational resources) and 101.8% scaling efficiency with mixed-batch (65.2 times speedup by 64 times computational resources) If you are not interested in the baseline tuning details, please skip this section. <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> suggested a proper learning rate warmup and decay scheme may help improve the ImageNet classification accuracy. We included these techniques in Adam/AdamW/AdaGrad tuning. Specifically, we use the learning rate recipe of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 76.3% <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>. There techniques help to improve the accuracy of Adam/AdamW/AdaGrad to around 73%. However, even with these techniques, Adam/AdamW/AdaGrad stil can not achieve the target validation accuracy.</p><p>To make sure our baseline is solid, we carefully tuned the hyper-parameters. <ref type="table" target="#tab_10">Table 9</ref> shows the tuning information of standard Adagrad. <ref type="table" target="#tab_11">Table 10</ref> shows the tuning information of adding the learning rate scheme of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> to standard Adagrad. <ref type="table" target="#tab_13">Table 11</ref> shows the tuning information of standard Adam. <ref type="table">Table shows</ref> the tuning information of adding the learning rate scheme of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> to standard Adam. It is tricky to tune the AdamW optimizer since both the L2 regularization and weight decay have the effect on the performance. Thus we have four tuning sets.</p><p>The first tuning set is based on AdamW with default L2 regularization. We tune the learning rate and weight decay. The tuning information is in <ref type="figure" target="#fig_0">Figures 13, 14, 15, and 16</ref>.</p><p>The second tuning set is based on AdamW with disabled L2 regularization. We tune the learning rate and weight decay. The tuning information is in <ref type="figure" target="#fig_1">Figures 17, 18, 19</ref>, and 20.  Then we add the learning rate scheme of <ref type="bibr" target="#b12">Goyal et al. (2017)</ref> to AdamW and refer to it as AdamW+.</p><p>The third tuning set is based on AdamW+ with default L2 regularization. We tune the learning rate and weight decay. The tuning information is <ref type="figure">Figure 21</ref> and 22.</p><p>The fourth tuning set is based on AdamW+ with disabled L2 regularization. We tune the learning rate and weight decay. The tuning information is in <ref type="figure">Figures 23, 24</ref>, 25.</p><p>Based on our comprehensive tuning results, we conclude the existing adaptive solvers do not perform well on ImageNet training or at least it is hard to tune them.       Published as a conference paper at ICLR 2020 <ref type="figure">Figure 14</ref>: The LAMB trust ratio.   <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 <ref type="bibr" target="#b12">(Goyal et al., 2017</ref>    <ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 <ref type="bibr" target="#b12">(Goyal et al., 2017</ref> Published as a conference paper at ICLR 2020    Published as a conference paper at ICLR 2020         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 :</head><label>6</label><figDesc>This figure shows the training loss curve of LAMB optimizer. We just want to use this figure to show that LAMB can make the training converge smoothly. Even if we scale the batch size to the extremely large cases, the loss curves are almost identical to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>This figure shows the training loss curve of LAMB optimizer. This figure shows that LAMB can make the training converge smoothly at the extremely large batch size (e.g. 64K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>We achieve 76.8% scaling efficiency (49 times speedup by 64 times computational resources) and 101.8% scaling efficiency with a mixed, scaled batch size (65.2 times speedup by 64 times computational resources). 1024-mixed means the mixed-batch training on 1024 TPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>The LAMB trust ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>The LAMB trust ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>The LAMB trust ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>The LAMB trust ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>The LAMB trust ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table is collected from the untuned version.</figDesc><table><row><cell>Solver</cell><cell cols="4">batch size steps F1 score on dev set TPUs</cell><cell>Time</cell></row><row><cell>Baseline</cell><cell>512</cell><cell>1000k</cell><cell>90.395</cell><cell>16</cell><cell>81.4h</cell></row><row><cell>LAMB</cell><cell>512</cell><cell>1000k</cell><cell>91.752</cell><cell>16</cell><cell>82.8h</cell></row><row><cell>LAMB</cell><cell>1k</cell><cell>500k</cell><cell>91.761</cell><cell>32</cell><cell>43.2h</cell></row><row><cell>LAMB</cell><cell>2k</cell><cell>250k</cell><cell>91.946</cell><cell>64</cell><cell>21.4h</cell></row><row><cell>LAMB</cell><cell>4k</cell><cell>125k</cell><cell>91.137</cell><cell>128</cell><cell>693.6m</cell></row><row><cell>LAMB</cell><cell>8k</cell><cell>62500</cell><cell>91.263</cell><cell>256</cell><cell>390.5m</cell></row><row><cell>LAMB</cell><cell>16k</cell><cell>31250</cell><cell>91.345</cell><cell>512</cell><cell>200.0m</cell></row><row><cell>LAMB</cell><cell>32k</cell><cell>15625</cell><cell>91.475</cell><cell cols="2">1024 101.2m</cell></row><row><cell>LAMB</cell><cell>64k/32k</cell><cell>8599</cell><cell>90.584</cell><cell cols="2">1024 76.19m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LAMB achieves a higher performance (F1 score) than LARS for all the batch sizes. The baseline achieves a F1 score of 90.390. Thus, LARS stops scaling at the batch size of 16K.</figDesc><table><row><cell>Batch Size</cell><cell>512</cell><cell>1K</cell><cell>2K</cell><cell>4K</cell><cell>8K</cell><cell>16K</cell><cell>32K</cell></row><row><cell>LARS</cell><cell cols="7">90.717 90.369 90.748 90.537 90.548 89.589 diverge</cell></row><row><cell>LAMB</cell><cell cols="7">91.752 91.761 91.946 91.137 91.263 91.345 91.475</cell></row><row><cell cols="4">4.2 IMAGENET TRAINING WITH RESNET-50.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-1 validation accuracy of ImageNet/RESNET-50 training at the batch size of 16K (90 epochs). The performance of momentum was reported by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>HYPERPARAMETERS FOR SCALING THE BATCH SIZEFor BERT and ImageNet training, we did not tune the hyperparameters of LAMB optimizer when increasing the batch size. We use the square root LR scaling rule and linear-epoch warmup scheduling to automatically adjust learning rate. The details can be found inTables 4 and 5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The tuning</cell></row><row><cell cols="2">information was in the Appendix.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">optimizer adagrad/adagrad+</cell><cell>adam/adam+</cell><cell cols="2">adamw/adamw+ momentum</cell><cell>lamb</cell></row><row><cell>Accuracy</cell><cell>0.5538/0.7201</cell><cell>0.6604/0.7348</cell><cell>0.6727/0.7307</cell><cell>0.7520</cell><cell>0.7666</cell></row><row><cell>4.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Untuned LAMB for BERT training across different batch sizes (fixed #epochs). We use square root LR scaling and linear-epoch warmup. For example, batch size 32K needs to finish 15625 iterations. It uses 0.2×15625 = 3125 iterations for learning rate warmup. BERT's baseline achieved a F1 score of 90.395. We can achieve an even higher F1 score if we manually tune the hyperparameters.</figDesc><table><row><cell>Batch Size</cell><cell>512</cell><cell>1K</cell><cell>2K</cell><cell>4K</cell><cell>8K</cell><cell>16K</cell><cell>32K</cell></row><row><cell>Learning Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Untuned LAMB for ImageNet training with RESNET-50 for different batch sizes (90 epochs). We use square root LR scaling and linear-epoch warmup. The baseline Goyal et al. (2017) gets 76.3% top-1 accuracy in 90 epochs. Stanford DAWN Bench<ref type="bibr" target="#b4">(Coleman et al., 2017)</ref> baseline achieves 93% top-5 accuracy. LAMB achieves both of them. LAMB can achieve an even higher accuracy if we manually tune the hyperparameters.</figDesc><table><row><cell>Batch Size</cell><cell>512</cell><cell>1K</cell><cell>2K</cell><cell>4K</cell><cell>8K</cell><cell>16K</cell><cell>32K</cell></row><row><cell>Learning Rate</cell><cell>4 2 3.0 ×100</cell><cell>4 2 2.5 ×100</cell><cell>4 2 2.0 ×100</cell><cell>4 2 1.5 ×100</cell><cell>4 2 1.0 ×100</cell><cell>4 2 0.5 ×100</cell><cell>4 2 0.0 ×100</cell></row><row><cell>Warmup Epochs</cell><cell>0.3125</cell><cell>0.625</cell><cell>1.25</cell><cell>2.5</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Top-5 Accuracy</cell><cell>0.9335</cell><cell>0.9349</cell><cell>0.9353</cell><cell>0.9332</cell><cell>0.9331</cell><cell>0.9322</cell><cell>0.9308</cell></row><row><cell>Top-1 Accuracy</cell><cell>0.7696</cell><cell>0.7706</cell><cell>0.7711</cell><cell>0.7692</cell><cell>0.7689</cell><cell>0.7666</cell><cell>0.7642</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, 50}. The momentum optimizer was tuned by the baseline implementer. The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01, 0.1, 1.0}.</figDesc><table><row><cell>Optimizer</cell><cell cols="5">ADAGRAD ADAM ADAMW momentum LAMB</cell></row><row><cell>Test Accuracy</cell><cell>0.9074</cell><cell>0.9225</cell><cell>0.9271</cell><cell>0.9372</cell><cell>0.9408</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size = 1024). The tuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}. We use the same learning rate warmup and decay schedule for all of them. Our experiments show that even the validation loss is not reliable in the large-scale training. A lower validation loss may lead to a worse accuracy. Thus, we use the test/val accuracy or F1 score on dev set to evaluate the optimizers. 14 show the LAMB trust ratio at different iterations for ImageNet training with ResNet-50. From these figures we can see that these ratios are very different from each other for different layers. LAMB uses the trust ratio to help the slow learners to train faster.</figDesc><table><row><cell>Optimizer</cell><cell cols="5">Momentum Addgrad ADAM ADAMW LAMB</cell></row><row><cell>Average accuracy over 5 runs</cell><cell>0.9933</cell><cell>0.9928</cell><cell>0.9936</cell><cell>0.9941</cell><cell>0.9945</cell></row></table><note>H.1 BASELINE TUNING DETAILS FOR IMAGENET TRAINING WITH RESNET-50</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>ADAMW stops scaling at the batch size of 16K. The target F1 score is 90.5. LAMB achieves a F1 score of 91.345. The table shows the tuning information of ADAMW. In this table, we report the best F1 score we observed from our experiments.</figDesc><table><row><cell>Solver</cell><cell cols="2">batch size warmup steps</cell><cell>LR</cell><cell>last step infomation</cell><cell>F1 score on dev set</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.05×31250 0.0001 loss=8.04471, step=28126</cell><cell>diverged</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.05×31250 0.0002 loss=7.89673, step=28126</cell><cell>diverged</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.05×31250 0.0003 loss=8.35102, step=28126</cell><cell>diverged</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.10×31250 0.0001 loss=2.01419, step=31250</cell><cell>86.034</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.10×31250 0.0002 loss=1.04689, step=31250</cell><cell>88.540</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.10×31250 0.0003 loss=8.05845, step=20000</cell><cell>diverged</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.20×31250 0.0001 loss=1.53706, step=31250</cell><cell>85.231</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.20×31250 0.0002 loss=1.15500, step=31250</cell><cell>88.110</cell></row><row><cell>ADAMW</cell><cell>16K</cell><cell cols="3">0.20×31250 0.0003 loss=1.48798, step=31250</cell><cell>85.653</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="2">: The accuracy information of tuning default AdaGrad optimizer for ImageNet training with</cell></row><row><cell cols="2">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations).</cell></row><row><cell cols="2">Learning Rate Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.0026855469</cell></row><row><cell>0.001</cell><cell>0.015563965</cell></row><row><cell>0.002</cell><cell>0.022684732</cell></row><row><cell>0.004</cell><cell>0.030924479</cell></row><row><cell>0.008</cell><cell>0.04486084</cell></row><row><cell>0.010</cell><cell>0.054158527</cell></row><row><cell>0.020</cell><cell>0.0758667</cell></row><row><cell>0.040</cell><cell>0.1262614</cell></row><row><cell>0.080</cell><cell>0.24037679</cell></row><row><cell>0.100</cell><cell>0.27357993</cell></row><row><cell>0.200</cell><cell>0.458313</cell></row><row><cell>0.400</cell><cell>0.553833</cell></row><row><cell>0.800</cell><cell>0.54103595</cell></row><row><cell>1.000</cell><cell>0.5489095</cell></row><row><cell>2.000</cell><cell>0.47680664</cell></row><row><cell>4.000</cell><cell>0.5295207</cell></row><row><cell>6.000</cell><cell>0.36950684</cell></row><row><cell>8.000</cell><cell>0.31081137</cell></row><row><cell>10.00</cell><cell>0.30670166</cell></row><row><cell>12.00</cell><cell>0.3091024</cell></row><row><cell>14.00</cell><cell>0.3227946</cell></row><row><cell>16.00</cell><cell>0.0063680015</cell></row><row><cell>18.00</cell><cell>0.11287435</cell></row><row><cell>20.00</cell><cell>0.21602376</cell></row><row><cell>30.00</cell><cell>0.08315023</cell></row><row><cell>40.00</cell><cell>0.0132039385</cell></row><row><cell>50.00</cell><cell>0.0009969076</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>The accuracy information of tuning default Adam optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</figDesc><table><row><cell cols="2">Learning Rate Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.5521</cell></row><row><cell>0.0002</cell><cell>0.6089</cell></row><row><cell>0.0004</cell><cell>0.6432</cell></row><row><cell>0.0006</cell><cell>0.6465</cell></row><row><cell>0.0008</cell><cell>0.6479</cell></row><row><cell>0.001</cell><cell>0.6604</cell></row><row><cell>0.002</cell><cell>0.6408</cell></row><row><cell>0.004</cell><cell>0.5687</cell></row><row><cell>0.006</cell><cell>0.5165</cell></row><row><cell>0.008</cell><cell>0.4812</cell></row><row><cell>0.010</cell><cell>0.3673</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.53312176</cell></row><row><cell>0.0002</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.5542806</cell></row><row><cell>0.0004</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.48769125</cell></row><row><cell>0.0006</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.46317545</cell></row><row><cell>0.0008</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.40903726</cell></row><row><cell>0.001</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.42401123</cell></row><row><cell>0.002</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.33870444</cell></row><row><cell>0.004</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.12339274</cell></row><row><cell>0.006</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.122924805</cell></row><row><cell>0.008</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.08099365</cell></row><row><cell>0.010</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.016764322</cell></row><row><cell>0.012</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.032714844</cell></row><row><cell>0.014</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.018147787</cell></row><row><cell>0.016</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.0066731772</cell></row><row><cell>0.018</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.010294597</cell></row><row><cell>0.020</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.008260091</cell></row><row><cell>0.025</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.008870442</cell></row><row><cell>0.030</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.0064493814</cell></row><row><cell>0.040</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.0018107096</cell></row><row><cell>0.050</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.003540039</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.55489093</cell></row><row><cell>0.0002</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.56514484</cell></row><row><cell>0.0004</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.4986979</cell></row><row><cell>0.0006</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.47595215</cell></row><row><cell>0.0008</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.44685873</cell></row><row><cell>0.001</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.41029868</cell></row><row><cell>0.002</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.2808024</cell></row><row><cell>0.004</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.08111572</cell></row><row><cell>0.006</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.068115234</cell></row><row><cell>0.008</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.057922363</cell></row><row><cell>0.010</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.05222575</cell></row><row><cell>0.012</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.017313639</cell></row><row><cell>0.014</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.029785156</cell></row><row><cell>0.016</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.016540527</cell></row><row><cell>0.018</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.00575765</cell></row><row><cell>0.020</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.0102335615</cell></row><row><cell>0.025</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.0060831704</cell></row><row><cell>0.030</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.0036417644</cell></row><row><cell>0.040</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.0010782877</cell></row><row><cell>0.050</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.0037638347</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15</head><label>15</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.21142578</cell></row><row><cell>0.0002</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.4289144</cell></row><row><cell>0.0004</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.13537598</cell></row><row><cell>0.0006</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.33803305</cell></row><row><cell>0.0008</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.32611084</cell></row><row><cell>0.001</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.22194417</cell></row><row><cell>0.002</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.1833903</cell></row><row><cell>0.004</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.08256022</cell></row><row><cell>0.006</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.020507812</cell></row><row><cell>0.008</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.018269857</cell></row><row><cell>0.010</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.007507324</cell></row><row><cell>0.012</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.020080566</cell></row><row><cell>0.014</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.010762532</cell></row><row><cell>0.016</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0021362305</cell></row><row><cell>0.018</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.007954915</cell></row><row><cell>0.020</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.005859375</cell></row><row><cell>0.025</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.009724935</cell></row><row><cell>0.030</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0019124349</cell></row><row><cell>0.040</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.00390625</cell></row><row><cell>0.050</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16</head><label>16</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009765625</cell></row><row><cell>0.0002</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.0004</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.0006</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009358724</cell></row><row><cell>0.0008</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0022379558</cell></row><row><cell>0.001</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.001566569</cell></row><row><cell>0.002</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.009480794</cell></row><row><cell>0.004</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0033569336</cell></row><row><cell>0.006</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0029907227</cell></row><row><cell>0.008</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0018513998</cell></row><row><cell>0.010</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.009134929</cell></row><row><cell>0.012</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0022176106</cell></row><row><cell>0.014</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0040690103</cell></row><row><cell>0.016</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0017293295</cell></row><row><cell>0.018</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.00061035156</cell></row><row><cell>0.020</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0022379558</cell></row><row><cell>0.025</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0017089844</cell></row><row><cell>0.030</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0014241537</cell></row><row><cell>0.040</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0020345051</cell></row><row><cell>0.050</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0012817383</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17</head><label>17</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.00001</cell><cell>disable</cell><cell>0.48917642</cell></row><row><cell>0.0002</cell><cell>0.00001</cell><cell>disable</cell><cell>0.58152264</cell></row><row><cell>0.0004</cell><cell>0.00001</cell><cell>disable</cell><cell>0.63460284</cell></row><row><cell>0.0006</cell><cell>0.00001</cell><cell>disable</cell><cell>0.64849854</cell></row><row><cell>0.0008</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6598918</cell></row><row><cell>0.001</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6662801</cell></row><row><cell>0.002</cell><cell>0.00001</cell><cell>disable</cell><cell>0.67266846</cell></row><row><cell>0.004</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6692708</cell></row><row><cell>0.006</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6573079</cell></row><row><cell>0.008</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6639404</cell></row><row><cell>0.010</cell><cell>0.00001</cell><cell>disable</cell><cell>0.65230304</cell></row><row><cell>0.012</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6505534</cell></row><row><cell>0.014</cell><cell>0.00001</cell><cell>disable</cell><cell>0.64990234</cell></row><row><cell>0.016</cell><cell>0.00001</cell><cell>disable</cell><cell>0.65323895</cell></row><row><cell>0.018</cell><cell>0.00001</cell><cell>disable</cell><cell>0.67026776</cell></row><row><cell>0.020</cell><cell>0.00001</cell><cell>disable</cell><cell>0.66086835</cell></row><row><cell>0.025</cell><cell>0.00001</cell><cell>disable</cell><cell>0.65425617</cell></row><row><cell>0.030</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6476237</cell></row><row><cell>0.040</cell><cell>0.00001</cell><cell>disable</cell><cell>0.55478925</cell></row><row><cell>0.050</cell><cell>0.00001</cell><cell>disable</cell><cell>0.61869305</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18</head><label>18</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.0001</cell><cell>disable</cell><cell>0.5033366</cell></row><row><cell>0.0002</cell><cell>0.0001</cell><cell>disable</cell><cell>0.5949707</cell></row><row><cell>0.0004</cell><cell>0.0001</cell><cell>disable</cell><cell>0.62561035</cell></row><row><cell>0.0006</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6545207</cell></row><row><cell>0.0008</cell><cell>0.0001</cell><cell>disable</cell><cell>0.66326904</cell></row><row><cell>0.001</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6677043</cell></row><row><cell>0.002</cell><cell>0.0001</cell><cell>disable</cell><cell>0.67244464</cell></row><row><cell>0.004</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6702881</cell></row><row><cell>0.006</cell><cell>0.0001</cell><cell>disable</cell><cell>0.66033936</cell></row><row><cell>0.008</cell><cell>0.0001</cell><cell>disable</cell><cell>0.66426593</cell></row><row><cell>0.010</cell><cell>0.0001</cell><cell>disable</cell><cell>0.66151935</cell></row><row><cell>0.012</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6545817</cell></row><row><cell>0.014</cell><cell>0.0001</cell><cell>disable</cell><cell>0.65509033</cell></row><row><cell>0.016</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6529338</cell></row><row><cell>0.018</cell><cell>0.0001</cell><cell>disable</cell><cell>0.65651447</cell></row><row><cell>0.020</cell><cell>0.0001</cell><cell>disable</cell><cell>0.65334064</cell></row><row><cell>0.025</cell><cell>0.0001</cell><cell>disable</cell><cell>0.655009</cell></row><row><cell>0.030</cell><cell>0.0001</cell><cell>disable</cell><cell>0.64552814</cell></row><row><cell>0.040</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6425374</cell></row><row><cell>0.050</cell><cell>0.0001</cell><cell>disable</cell><cell>0.5988159</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19</head><label>19</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.001</cell><cell>disable</cell><cell>0.4611206</cell></row><row><cell>0.0002</cell><cell>0.001</cell><cell>disable</cell><cell>0.0076293945</cell></row><row><cell>0.0004</cell><cell>0.001</cell><cell>disable</cell><cell>0.29233804</cell></row><row><cell>0.0006</cell><cell>0.001</cell><cell>disable</cell><cell>0.57295734</cell></row><row><cell>0.0008</cell><cell>0.001</cell><cell>disable</cell><cell>0.5574748</cell></row><row><cell>0.001</cell><cell>0.001</cell><cell>disable</cell><cell>0.5988566</cell></row><row><cell>0.002</cell><cell>0.001</cell><cell>disable</cell><cell>0.586263</cell></row><row><cell>0.004</cell><cell>0.001</cell><cell>disable</cell><cell>0.62076825</cell></row><row><cell>0.006</cell><cell>0.001</cell><cell>disable</cell><cell>0.61503094</cell></row><row><cell>0.008</cell><cell>0.001</cell><cell>disable</cell><cell>0.4697876</cell></row><row><cell>0.010</cell><cell>0.001</cell><cell>disable</cell><cell>0.619751</cell></row><row><cell>0.012</cell><cell>0.001</cell><cell>disable</cell><cell>0.54243976</cell></row><row><cell>0.014</cell><cell>0.001</cell><cell>disable</cell><cell>0.5429077</cell></row><row><cell>0.016</cell><cell>0.001</cell><cell>disable</cell><cell>0.55281574</cell></row><row><cell>0.018</cell><cell>0.001</cell><cell>disable</cell><cell>0.5819295</cell></row><row><cell>0.020</cell><cell>0.001</cell><cell>disable</cell><cell>0.5938924</cell></row><row><cell>0.025</cell><cell>0.001</cell><cell>disable</cell><cell>0.541097</cell></row><row><cell>0.030</cell><cell>0.001</cell><cell>disable</cell><cell>0.45890298</cell></row><row><cell>0.040</cell><cell>0.001</cell><cell>disable</cell><cell>0.56193036</cell></row><row><cell>0.050</cell><cell>0.001</cell><cell>disable</cell><cell>0.5279134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20</head><label>20</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning default AdamW optimizer for ImageNet training with</cell></row><row><cell cols="4">ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763</cell></row><row><cell>(Goyal et al., 2017).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009969076</cell></row><row><cell>0.0002</cell><cell>0.01</cell><cell>disable</cell><cell>0.0008951823</cell></row><row><cell>0.0004</cell><cell>0.01</cell><cell>disable</cell><cell>0.00095621747</cell></row><row><cell>0.0006</cell><cell>0.01</cell><cell>disable</cell><cell>0.0012817383</cell></row><row><cell>0.0008</cell><cell>0.01</cell><cell>disable</cell><cell>0.016886393</cell></row><row><cell>0.001</cell><cell>0.01</cell><cell>disable</cell><cell>0.038146973</cell></row><row><cell>0.002</cell><cell>0.01</cell><cell>disable</cell><cell>0.0015258789</cell></row><row><cell>0.004</cell><cell>0.01</cell><cell>disable</cell><cell>0.0014241537</cell></row><row><cell>0.006</cell><cell>0.01</cell><cell>disable</cell><cell>0.081441246</cell></row><row><cell>0.008</cell><cell>0.01</cell><cell>disable</cell><cell>0.028116861</cell></row><row><cell>0.010</cell><cell>0.01</cell><cell>disable</cell><cell>0.011820476</cell></row><row><cell>0.012</cell><cell>0.01</cell><cell>disable</cell><cell>0.08138021</cell></row><row><cell>0.014</cell><cell>0.01</cell><cell>disable</cell><cell>0.010111491</cell></row><row><cell>0.016</cell><cell>0.01</cell><cell>disable</cell><cell>0.0041910806</cell></row><row><cell>0.018</cell><cell>0.01</cell><cell>disable</cell><cell>0.0038248699</cell></row><row><cell>0.020</cell><cell>0.01</cell><cell>disable</cell><cell>0.002746582</cell></row><row><cell>0.025</cell><cell>0.01</cell><cell>disable</cell><cell>0.011555989</cell></row><row><cell>0.030</cell><cell>0.01</cell><cell>disable</cell><cell>0.0065104165</cell></row><row><cell>0.040</cell><cell>0.01</cell><cell>disable</cell><cell>0.016438803</cell></row><row><cell>0.050</cell><cell>0.01</cell><cell>disable</cell><cell>0.007710775</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 21</head><label>21</label><figDesc></figDesc><table><row><cell cols="4">: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-</cell></row><row><cell cols="4">50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,</cell></row><row><cell cols="4">2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at</cell></row><row><cell cols="4">30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).</cell></row><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.0002</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.0004</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.0006</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009358724</cell></row><row><cell>0.0008</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.001</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0009765625</cell></row><row><cell>0.002</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.004</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.006</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.008</cell><cell>0.01</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.0001</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.0002</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.0004</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.0006</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0009969076</cell></row><row><cell>0.0008</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.001</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.002</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0010172526</cell></row><row><cell>0.004</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0038452148</cell></row><row><cell>0.006</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.011881511</cell></row><row><cell>0.008</cell><cell>0.001</cell><cell>default (0.01)</cell><cell>0.0061442056</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 22 :</head><label>22</label><figDesc>The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</figDesc><table><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.3665975</cell></row><row><cell>0.0002</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.5315755</cell></row><row><cell>0.0004</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.6369222</cell></row><row><cell>0.0006</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.6760457</cell></row><row><cell>0.0008</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.69557697</cell></row><row><cell>0.001</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.7076009</cell></row><row><cell>0.002</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.73065186</cell></row><row><cell>0.004</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.72806805</cell></row><row><cell>0.006</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.72161865</cell></row><row><cell>0.008</cell><cell>0.0001</cell><cell>default (0.01)</cell><cell>0.71816</cell></row><row><cell>0.0001</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.49804688</cell></row><row><cell>0.0002</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.6287028</cell></row><row><cell>0.0004</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.6773885</cell></row><row><cell>0.0006</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.67348224</cell></row><row><cell>0.0008</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.6622111</cell></row><row><cell>0.001</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.6468709</cell></row><row><cell>0.002</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.5846761</cell></row><row><cell>0.004</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.4868978</cell></row><row><cell>0.006</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.34969077</cell></row><row><cell>0.008</cell><cell>0.00001</cell><cell>default (0.01)</cell><cell>0.31193033</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 23 :</head><label>23</label><figDesc>The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</figDesc><table><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.01</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0002</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009765625</cell></row><row><cell>0.0004</cell><cell>0.01</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0006</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009969076</cell></row><row><cell>0.0008</cell><cell>0.01</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.001</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009765625</cell></row><row><cell>0.002</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009969076</cell></row><row><cell>0.004</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009969076</cell></row><row><cell>0.006</cell><cell>0.01</cell><cell>disable</cell><cell>0.0009765625</cell></row><row><cell>0.008</cell><cell>0.01</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0001</cell><cell>0.001</cell><cell>disable</cell><cell>0.0009765625</cell></row><row><cell>0.0002</cell><cell>0.001</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0004</cell><cell>0.001</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0006</cell><cell>0.001</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.0008</cell><cell>0.001</cell><cell>disable</cell><cell>0.0010172526</cell></row><row><cell>0.001</cell><cell>0.001</cell><cell>disable</cell><cell>0.0009969076</cell></row><row><cell>0.002</cell><cell>0.001</cell><cell>disable</cell><cell>0.0010579427</cell></row><row><cell>0.004</cell><cell>0.001</cell><cell>disable</cell><cell>0.0016886393</cell></row><row><cell>0.006</cell><cell>0.001</cell><cell>disable</cell><cell>0.019714355</cell></row><row><cell>0.008</cell><cell>0.001</cell><cell>disable</cell><cell>0.1329956</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 24 :</head><label>24</label><figDesc>The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</figDesc><table><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.0001</cell><cell>disable</cell><cell>0.28515625</cell></row><row><cell>0.0002</cell><cell>0.0001</cell><cell>disable</cell><cell>0.44055176</cell></row><row><cell>0.0004</cell><cell>0.0001</cell><cell>disable</cell><cell>0.56815594</cell></row><row><cell>0.0006</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6234741</cell></row><row><cell>0.0008</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6530762</cell></row><row><cell>0.001</cell><cell>0.0001</cell><cell>disable</cell><cell>0.6695964</cell></row><row><cell>0.002</cell><cell>0.0001</cell><cell>disable</cell><cell>0.70048016</cell></row><row><cell>0.004</cell><cell>0.0001</cell><cell>disable</cell><cell>0.71698</cell></row><row><cell>0.006</cell><cell>0.0001</cell><cell>disable</cell><cell>0.72021484</cell></row><row><cell>0.008</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7223918</cell></row><row><cell>0.010</cell><cell>0.0001</cell><cell>disable</cell><cell>0.72017413</cell></row><row><cell>0.012</cell><cell>0.0001</cell><cell>disable</cell><cell>0.72058105</cell></row><row><cell>0.014</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7188924</cell></row><row><cell>0.016</cell><cell>0.0001</cell><cell>disable</cell><cell>0.71695966</cell></row><row><cell>0.018</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7154134</cell></row><row><cell>0.020</cell><cell>0.0001</cell><cell>disable</cell><cell>0.71358234</cell></row><row><cell>0.025</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7145386</cell></row><row><cell>0.030</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7114258</cell></row><row><cell>0.040</cell><cell>0.0001</cell><cell>disable</cell><cell>0.7066447</cell></row><row><cell>0.050</cell><cell>0.0001</cell><cell>disable</cell><cell>0.70284015</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 25 :</head><label>25</label><figDesc>The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>: (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763<ref type="bibr" target="#b12">(Goyal et al., 2017)</ref>.</figDesc><table><row><cell cols="4">learning rate weight decay L2 regularization Top-1 Validation Accuracy</cell></row><row><cell>0.0001</cell><cell>0.00001</cell><cell>disable</cell><cell>0.31247965</cell></row><row><cell>0.0002</cell><cell>0.00001</cell><cell>disable</cell><cell>0.4534912</cell></row><row><cell>0.0004</cell><cell>0.00001</cell><cell>disable</cell><cell>0.57765704</cell></row><row><cell>0.0006</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6277669</cell></row><row><cell>0.0008</cell><cell>0.00001</cell><cell>disable</cell><cell>0.65321857</cell></row><row><cell>0.001</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6682129</cell></row><row><cell>0.002</cell><cell>0.00001</cell><cell>disable</cell><cell>0.69938153</cell></row><row><cell>0.004</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7095947</cell></row><row><cell>0.006</cell><cell>0.00001</cell><cell>disable</cell><cell>0.710612</cell></row><row><cell>0.008</cell><cell>0.00001</cell><cell>disable</cell><cell>0.70857745</cell></row><row><cell>0.010</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7094116</cell></row><row><cell>0.012</cell><cell>0.00001</cell><cell>disable</cell><cell>0.70717365</cell></row><row><cell>0.014</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7109375</cell></row><row><cell>0.016</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7058309</cell></row><row><cell>0.018</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7052409</cell></row><row><cell>0.020</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7064412</cell></row><row><cell>0.025</cell><cell>0.00001</cell><cell>disable</cell><cell>0.7035319</cell></row><row><cell>0.030</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6994629</cell></row><row><cell>0.040</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6972656</cell></row><row><cell>0.050</cell><cell>0.00001</cell><cell>disable</cell><cell>0.6971232</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/addons/blob/master/tensorflow_addons/ optimizers/lamb.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://rajpurkar.github.io/SQuAD-explorer/ 3 Pre-trained BERT model can be downloaded from https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://mlperf.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html 6 https://github.com/fenwickslab/dl_tutorials/blob/master/tutorial3_cifar10_davidnet_fix.ipynb</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Fukuda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04325</idno>
		<title level="m">Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">signsgd: compressed optimisation for non-convex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>abs/1802.04434</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale out for large minibatch sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeriu</forename><surname>Codreanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Podareanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Saletore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04291</idno>
	</analytic>
	<monogr>
		<title level="m">Residual network training on imagenet-1k with improved accuracy and reduced time to train</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dawnbench: An end-to-end deep learning benchmark and competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">101</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Devarakonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02029</idno>
		<title level="m">Adabatch: Adaptive batch sizes for training deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<idno>doi: 10.1137/ 120880811</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="267" to="305" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Firecaffe: near-linear acceleration of deep neural network training on compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2592" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<title level="m">Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling Distributed Machine Learning with System and Algorithm Co-design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Imagenet/resnet-50 training in 224 seconds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Mikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisahiro</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Kageyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05233</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Second-order optimization method for large mini-batch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12019</idno>
	</analytic>
	<monogr>
		<title level="m">Training resnet-50 on imagenet in 35 epochs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600</idno>
		<title level="m">Measuring the effects of data parallelism on neural network training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Kasagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tabuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuguchika</forename><surname>Tabaru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12650</idno>
		<title level="m">Atsushi Ike, and Kohta Nakashima. Yet another accelerated sgd: Resnet-50 training on imagenet in 74.7 seconds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Image classification at supercomputer scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06992</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet training in minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Conference on Parallel Processing</title>
		<meeting>the 47th International Conference on Parallel Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08256</idno>
		<title level="m">Large-batch training for lstm and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
