<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaptIS: Adaptive Instance Selection Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
							<email>k.sofiiuk@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
							<email>o.barinova@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff2">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaptIS: Adaptive Instance Selection Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Adaptive Instance Selection network architecture for class-agnostic instance segmentation. Given an input image and a point (x, y), it generates a mask for the object located at (x, y). The network adapts to the input point with a help of AdaIN layers [13], thus producing different masks for different objects on the same image. Adap-tIS generates pixel-accurate object masks, therefore it accurately segments objects of complex shape or severely occluded ones. AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. To illustrate the idea, we perform experiments on a challenging toy problem with difficult occlusions. Then we extensively evaluate the method on panoptic segmentation benchmarks. We obtain state-of-the-art results on Cityscapes and Mapillary even without pretraining on COCO, and show competitive results on a challenging COCO dataset. The source code of the method and the trained models are available at https://github.com/saicvul/adaptis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While humans can easily segment objects in images, this task is quite difficult to formalize. One of the formulations is semantic segmentation which aims at assigning a class label to each image pixel. Another formulation is instance segmentation, which aims to detect and segment each object instance in the image. While semantic segmentation does not imply separating objects of the same class, instance segmentation focuses only on segmenting things (i.e. countable objects such as people, animals, tools) and does not account for so-called stuff (i.e. amorphous regions of similar texture or material such as grass, sky, road). Recently, panoptic segmentation task that combines semantic and instance segmentation together, has been introduced <ref type="bibr" target="#b14">[15]</ref>. The goal of panoptic segmentation is to map each pixel of an image to a pair (l i , z i ) ∈ L × N , where l i represents the semantic class of pixel i and z i represents its instance id.</p><p>In this work we focus on instance and panoptic segmen- tation. We introduce AdaptIS, a fully differentiable end-toend network for class-agnostic instance segmentation. The network takes an image and a point proposal (x, y) as input and generates a mask of an object located at position (x, y). <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of the masks produced by AdaptIS for different input point proposals.</p><p>We cast the problem of generating an object mask as binary segmentatiIn contrast to the mainstream detection-first methods for instance segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> AdaptIS does not rely on bounding box proposals. Instead, it directly optimizes target segmentation accuracy. In contrast to instance embedding-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref> which map the pixels of an image into embedding space and then perform clustering, AdaptIS does not require heuristic post-processing steps. Instead, it directly generates an object mask for a given point proposal. on. For a given image I and a fixed point proposal (x, y) we simply optimize just one target loss function. We use a pixel-wise loss comparing AdaptIS prediction to the mask of the target object located at position (x, y) on the image I. At train time for each object we sample different point proposals. Thus, the network learns to <ref type="bibr">Figure 2</ref>. Outputs of AdaptIS for different point proposals. For most points that belong to the same object AdaptIS by design produces very similar masks. Note that the point on the border of two objects (two persons walking hand in hand) results in a merged mask of the two objects. generate the same object mask given different points on the same object (see <ref type="figure">Figure 2</ref>). Interestingly, AdaptIS does not use semantic labels during training, which allows it to segment objects of any class.</p><p>Proposed approach has many practical advantages over existing methods. Compared to detection-first methods, AdaptIS is much better suited for the cases of severe occlusions. Since it provides pixel-accurate segmentation and can better handle objects of complex shape. Besides, Adap-tIS can segment objects of different size using only singlescale features. Due to its simplicity AdaptIS has very few train-time hyperparameters and therefore can be trained on a new dataset almost without fine-tuning. AdaptIS can be easily paired with standard semantic segmentation pipeline to provide panoptic segmentation. Examples of panoptic segmentation results for Cityscapes are shown in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>In the next sections we discuss related work and explain proposed architecture for instance segmentation. Then we describe an extension of the model to provides semantic labels and propose a greedy algorithm for panoptic segmentation using AdaptIS. We perform experiments on a toy problem with difficult occlusions which is extremely challenging for the detection-first segmentation methods like Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>. In the end, we evaluate the method on Cityscapes, Mapillary and COCO benchmarks. We obtain state-of-the-art accuracy on Cityscapes and Mapillary and demonstrate competitive performance on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Detection-first instance segmentation methods. In recent years the approach to instance segmentation based on standard object detection pipeline <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23</ref>] became a mainstream, achieving state-of-the-art results on various benchmarks. However this detection-first approach has a few severe limitations. First, strongly overlapping objects may have very similar bounding boxes or even share the same bounding box. In this case, the mask branch of the network has no information about which object to segment inside a bounding box. Second, the process of mask inference is based on ROI pooling operation. ROI pooling reduces dimensionality of object features, which leads to loss of information and causes inaccurate masks for objects of complex shape. Instance embedding methods. Another interesting approach to instance segmentation is based on instance embedding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. In this approach a network maps each pixel of an input image into an embedding space. The network learns to map the pixels of the same object into close points of the embedding space, while pixels that belong to different objects are mapped into distant points of the embedding space. This property of the network is enforced by introducing several losses that directly control proximity of the embeddings. After computing the embeddings one needs to obtain instance segmentation map using some clustering method. In contrast to detection-first methods this approach allows for producing pixel-accurate object masks. However, it has it's own limitations. Usually, some heuristic clustering procedures are applied to the resulting embeddings, e.g. mean shift <ref type="bibr" target="#b4">[5]</ref> or region growing <ref type="bibr" target="#b7">[8]</ref>. In many cases this leads to sub-optimal results, as the network is not <ref type="figure">Figure 4</ref>. Architecture of AdaptIS for class-agnostic instance segmentation. The network is built on top of a (pretrained) backbone for feature extraction. It introduces 1) a lightweight instance selection network for inferring object masks that adapts to a point proposal using AdaIN mechanism <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>; 2) a controller network that fetches the feature at position (x, y), processes it in a series of fully connected layers, and provides an input for AdaIN layers in instance selection network; 3) Relative CoordConv block that helps to disambiguate similar objects located at different positions in the image. optimizing segmentation accuracy. Currently, the methods of this group demonstrate inferior performance on standard benchmarks compared to detection-first methods.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>Panoptic segmentation methods. The formulation of panoptic segmentation problem was first introduced in <ref type="bibr" target="#b14">[15]</ref>. In this work the authors presented a strong baseline that combines two independent networks for semantic <ref type="bibr" target="#b29">[30]</ref> and instance segmentation <ref type="bibr" target="#b10">[11]</ref> followed by heuristic post-processing and late fusion of the two outputs. Later the same authors proposed a single model based on feature pyramid network that showed higher accuracy than the combination of two networks <ref type="bibr" target="#b13">[14]</ref>. The other methods for panoptic segmentation base on standard instance or semantic segmentation pipelines and usually introduce additional losses that enforce some heuristic constraints on segmentation consistency <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture overview</head><p>Schematic structure of the AdaptIS is shown in <ref type="figure">Figure  4</ref>. The network takes an image and a point proposal (x, y) as input and outputs a mask of an object located at position (x, y). Below we explain the main components of the architecture.</p><p>Instance selection network. Let us have a closer look at the generation of an object mask. Suppose that we are given a vector that characterizes some object allowing us to distinguish it from other objects in the image.</p><p>But how can we generate an object mask based on this "characteristic" vector? Recently an elegant solution for similar problems was proposed in the literature on style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> and image generation <ref type="bibr" target="#b12">[13]</ref>. Using Adaptive Instance Normalization (AdaIN) layers <ref type="bibr" target="#b11">[12]</ref>, one can parameterize a network, i.e. vary the network output for the same input by providing different parameters to AdaIN.</p><p>In this work we propose a lightweight Instance Selection Network, which is parameterized using AdaIN. To the best of our knowledge, adaptive network architectures have not been used for object segmentation previously. "Characteristic" vector thus contains parameters for AdaIN layers of the instance selection network. Instance selection network generates an object mask using features extracted by a backbone and a "characteristic" vector.</p><p>Point proposals and controller network. The next question is: where can we get a good "characteristic" vector for an object?</p><p>Let us recall the idea behind the instance embedding methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. In these methods, a network maps image pixels into embedding space, pushing embeddings closer for the pixels of the same object and pulling them apart for the pixels of different objects. Thus, an embedding Q(x, y) of a pixel at position (x, y) can be viewed as unique features of an object located at (x, y). We exploit this idea in AdaptIS architecture and generate a "characteristic" vector from the embedding Q(x, y).</p><p>The output of a backbone Q may have lower spatial resolution compared to the input image. Thus, for each point (x, y) we obtain corresponding embedding Q(x, y) with the use of bilinear interpolation. The resulting embedding Q(x, y) is then processed in a controller network, that consists of a few fully connected layers. Controller network outputs a "characteristic" vector, which is then passed to AdaIN layers in instance selection network. Using this mechanism the instance selection network adapts to the selected object.</p><p>Note, that in contrast to instance embedding methods we do not optimize the distances between embeddings. Instead, the controller network connects backbone and instance selection network in a feedback loop, thus enforcing backbone to produce rich features characterizing each particular object. As a result, AdaptIS directly optimizes accuracy of an object mask without a need for auxiliary losses.</p><p>Relative CoordConv. <ref type="bibr" target="#b24">[25]</ref> showed that one necessarily needs to provide pixel coordinates to the network in order to disambiguate different object instances. <ref type="bibr" target="#b21">[22]</ref> proposed a simple CoordConv layer that feeds pixel coordinates to the network. It creates a tensor of same size as input that contains pixel coordinates normalized to [−1, 1]. This tensor is then concatenated to the input features and passed to the next layers of the network.</p><p>However, in AdaptIS we also need to provide the coordinates of an object to the network. Moreover, we would prefer not to use CoordConv in backbone, as standard backbones are pre-trained without it. Instead, we propose to use ... a Relative CoordConv block that depends on the point proposal and is used only in the later layers of AdaptIS. Similarly to original CoordConv, Relative CoordConv produces two maps, one for x-coordinates and one for y-coordinates. The values of each map vary from −1 to +1, where −1 corresponds to x − R (or y − R) and +1 corresponds to x + R (or y +R). R is a radius of Relative CoordConv that is a hyperparameter of the algorithm (it roughly sets the maximum size of an object). One may view the Relative CoordConv block as a prior on the location of an object. The output of Relative CoordConv is concatenated with the features produced by a backbone, and together they are passed to an instance selection network. Similar blocks have been used in the literature, e.g. in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Training. At train time for each image in a batch we sample K points (x i , y i ), i = 1 . . . K using object-level stratification. We pick a random object and then sample a random point at that object.</p><p>Having sampled the features Q(x i , y i ), i = 1 . . . K, we train a network to predict corresponding object masks M i , i = 1 . . . K from an input image I. In order to do that that we optimize a pixel-wise loss function that compares network output for a pair I, Q(x i , y i ) to the ground truth object mask M i . Generally, binary cross entropy is used for this type of problems. However, in our experiments we used a modification of Focal Loss <ref type="bibr" target="#b19">[20]</ref> that demonstrated slightly better results (see appendix for more details). The gradient from the loss propagates to all components of AdaptIS and to the backbone.</p><p>Random sampling of the point proposals during training automatically enforces AdaptIS to produce similar output for point proposals that belong to the same object at test time. In contrast to instance embedding methods that directly optimize the distances between the embeddings we directly optimize the resulting object masks and provide only soft constraints on the embeddings by the similarity of the masks. Examples of the learned masks for different point proposals are shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Class-agnostic instance segmentation</head><p>At test time AdaptIS outputs a pixelwise mask for a single object. However, the desired result is to segment all objects in the image rather than just one of them. For this purpose we provide different point proposals and obtain masks for multiple objects one by one. Since the objects are processed independently, their masks can overlap. To resolve these ambiguities, we use a greedy algorithm described below.</p><p>Aggregating the results. Let S denote a map of segmented objects. In the beginning all elements in S are initialized as "unknown". The algorithm proceeds in iterations. At each iteration we sample a point proposal (x i , y i ), compute a real-valued output of AdaptIS C i , and threshold it to obtain an object mask M i = C i &gt; T . At the first iteration we simply add an object on the map, i.e. mark pixels in S corresponding to M 1 as "segmented". At the next iterations we compute the intersection of M i with "segmented" area in S. In case the intersection is lower than 50%, we add the new object on the map. Otherwise we simply ignore the result and move on to the next point proposal.</p><p>The algorithm terminates either when all pixels in S are marked as "segmented" or when we run out of point proposals. In the end, each pixel is assigned an instance id that has the highest confidence among all segmented objects: R(x, y) = arg max i=1,...,N C i (x, y). <ref type="figure" target="#fig_2">Figure 5</ref> illustrates iterations of the method.</p><p>It should be noted that in the described method only a light-weight AdaptIS head needs to be run several times per image, while the backbone can be run only once. This significantly reduces the amount of computations and makes the described method quite applicable in practice.</p><p>Proposal generation. One can notice that if a point proposal (x i , y i ) is marked as "segmented" in S, then most likely it will produce an object mask M i : (x i , y i ) ∈ M i overlapping with an already segmented object. This means that all the points marked as "segmented" in S would rather not be selected as point proposals. Therefore, at each iteration of the method described above we sample a new point proposal from the set of "unknown" points.</p><p>We explore different strategies for prioritizing the choice of a point proposal at each iteration. For the toy problem in Section 6 we experiment with simple random sampling, and for panoptic segmentation in Section 5 we introduce a specialized network branch for prioritizing point proposals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Panoptic segmentation</head><p>Semantic segmentation branch. AdaptIS does not use semantic labels of the objects for training. However in practice we need to predict object class along with an object mask. For this we pair AdaptIS with standard semantic segmentation pipeline. In this work we train a Semantic Segmentation Branch jointly with AdaptIS on a shared backbone (see <ref type="figure" target="#fig_4">Figure 7</ref>). Below we explain how we use the results of semantic segmentation at test time.</p><p>Point proposal branch. We train a special Point Proposal Branch, that predicts the priority of sampling different point proposals. We formulate this task as binary segmentation: the network predicts whether a point (x, y) would make a good or a bad proposal. Point proposal branch has exactly the same structure as the semantic segmentation branch. We train it after the others with a frozen backbone.</p><p>During training we provide ground truth for the point proposal branch as follows. We pick a random object and sample several random point proposals in this object. Then we run AdaptIS with these proposals and compare the results to the ground truth object mask. After that we sort the point proposals by IoU with ground truth mask. The points that fall into top 20% are marked as positive examples and the others are marked as negative examples. We train the point proposal branch with a simple binary cross-entropy loss function.</p><p>Obtaining panoptic segmentation result. Panoptic segmentation aims at assigning class label to each image pixel, and an instance id to each pixel classified as a "thing" class <ref type="bibr" target="#b14">[15]</ref>.</p><p>Notice that all "stuff" labels can be inferred in one pass of the semantic segmentation branch. Thus, we first run the semantic segmentation branch and obtain the map of all "stuff" classes. Then we proceed with the method for instance segmentation described in Section 4. The only difference is that instead of starting with an empty map S we mark all the "stuff" pixels in S as "segmented".</p><p>We use the output of the Point Proposal Branch as follows. We are interested in point proposals that get a high score according to the point proposal branch. Point proposal branch provides a dense output of the same size as input image. To reduce the number of proposals for evaluation we first find local maximums in the result of point proposal branch (see <ref type="figure" target="#fig_6">Figure 9</ref>). For that we use a standard method based on breadth-first search. In most cases, it finds about a hundred local maximums per image, which we use as point proposals. At each iteration of the method from Section 4 we remove the point proposals that have been marked as "segmented" at the previous steps.  After assigning instance ids, we need to obtain semantic labels for each instance. Each pixel is assigned a class label l ∈ {1, ..., K} that has the highest average confidence among all labels for a mask M : l(M ) = arg max l=1,...,K (x,y)∈M p l x,y , where p l x,y is the softmax output of the semantic segmentation branch for the l-th class at pixel (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments on toy problem</head><p>Toy data. To showcase the limitations of detection-first methods we have designed a challenging toy problem. We want to generate many overlapping objects that have very similar bounding boxes. For that we generate images of size 96x96 pixels containing from 8 to 22 elongated objects that resemble bacteria or biological cells. All of those objects are of the same color (blue with red boundary) and slightly varied sizes. The position and orientation of those objects are sampled at random. To make the problem more difficult we added random blur and random high frequency noise to the images. Train set contains 10000 images, and 2000 are left for testing.</p><p>Evaluation protocol and results. In this experiment we compared AdaptIS with U-Net backbone to Mask R-CNN with ResNet-50 ImageNet pre-trained backbone. For Mask R-CNN we upsample an image by a factor of 4, i.e. to 384x384. AdaptIS is trained and runs on original images. Mask R-CNN was trained for 55 epoch starting with SGD with learning rate of 0.001 and dropping it by factor of 10 at 35 and 45 epochs. We train AdaptIS for 140 epochs from scratch with Adam with learning rate of 0.0005, β 1 = 0.9, β 2 = 0.999. For the method described in Section 4 we used simple random sampling of point proposals. At each iteration of the method we sampled 7 random points and ran AdaptIS for each of them. Then among the 7 results we chose the one with the highest average confidence of the object mask.</p><p>We measure average precision (AP) at different IoU thresholds. <ref type="table" target="#tab_0">Table 1</ref> shows results of AdaptIS and Mask R-CNN. The main source of errors of Mask R-CNN is heavy overlap of bounding boxes. In our toy dataset objects often have very similar bounding boxes, hence Mask R-CNN has no way to distinguish these objects. At the same time AdaptIS can deal with severe occlusions because it is provided with query points belonging to these objects, which allows it to segment them. We notice that AP drastically drops for Mask R-CNN at high IoU thresholds that indicates inaccurate masks shapes due to reduction in feature dimensionality in ROI Pooling. See examples of the images and results of AdaptIS and Mask R-CNN in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments on standard benchmarks</head><p>We evaluate our panoptic segmentation pipeline on the standard panoptic segmentation benchmarks: Cityscapes <ref type="bibr" target="#b3">[4]</ref>, Mapillary <ref type="bibr" target="#b23">[24]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref>. We present results on validation sets for all of those benchmarks and compare to state-of-the-art. We compute standard metrics for panoptic segmentation (namely, PQ -panoptic quality, PQ st -PQ stuff, PQ th -PQ things). For Cityscapes following the other works we additionally report the mIoU and AP, which are standard for this dataset. We use DeepLabV3+ <ref type="bibr" target="#b1">[2]</ref> architecture as backbone for all experiments, see more details in <ref type="figure" target="#fig_5">Figure 8</ref>.</p><p>Cityscapes dataset has 5000 images of ego-centric driv-   Cityscapes. It consists of a wide variety of geographic settings, camera types, weather conditions, image aspect ratios, and object frequencies. The average resolution of the images is approximately 9 megapixels, varying from 1024 × 768 to higher than 4000 × 6000. It contains 65 semantic classes, including 37 thing classes and 28 stuff classes. We train on the train part of the dataset containing 18000 training images and evaluate the method on val part containing 2000 validation images.</p><p>COCO dataset for panoptic segmentation task consists of 80 and 53 classes for thing and stuff respectively. We train on approximately 118k images from train2017 and present results on 5k images from val2017.</p><p>Cityscapes training. We train AdaptIS and semantic segmentation branches for 260 epochs and dropping learning rate by factor of 10 at 240 and 255 epochs. For ResNet-50 training we use 2 GPUs with batch size 8, for ResNet-101 and ResNeXt-101 <ref type="bibr" target="#b25">[26]</ref> we use 4 GPUs with batch size 8. We sample 6 point proposals per image during training. We train networks on random crops of size of 400×800. We use scale jitter augmentation with scale factor varying from 0.2 to 1.2. Also we use random contrast, brightness and color augmentation. Point proposal network was trained for 10 epochs.</p><p>Mapillary training. We train only ResNeXt-101 model for this dataset on 8 GPUs with batch size 8 for 65 epochs and dropping learning rate by factor of 10 at 50 and 60 epochs. We scale input images by largest side varied from 500 to 2200 pixels and then perform random crop of size 400 × 800. The dataset is large enough, therefore we use only horizontal flip augmentation. We sample 6 point proposals per image during training.</p><p>COCO training. We train only ResNeXt-101 model for this dataset on 8 GPUs with batch size 16 for 20 epochs and dropping learning rate by factor of 10 at 15 and 19 epochs. We scale input images by shortest side varied from 350 to 700 pixels and then perform random crop of size 544 × 544. We use horizontal flip augmentation. We sample 8 point proposals per image during training.</p><p>Inference details. For Cityscapes we use images of original resolution. For Mapillary we downscale all images to 2000 pixels by longest side. For COCO we rescale images to 600 pixels by shortest side. The greedy algorithm from Section 4 is applied with threshold T = 0.40 for Cityscapes and T = 0.35 for COCO and Mapillary. We use only horizontal flip augmentation at test time and do not use multiscale testing for all datasets.</p><p>Implementation details. We use MXNet Gluon <ref type="bibr" target="#b2">[3]</ref> with GluonCV <ref type="bibr" target="#b28">[29]</ref> framework for training and inference of our models. In all our experiments we use Adam optimizer with the starting learning rate 5 × 10 −5 for pre-trained backbone and 10 −4 for all other parameters, β 1 = 0.9, β 2 = 0.999. Semantic segmentation and AdaptIS branches are trained jointly with the backbone. The point proposal network was trained afterwards with frozen backbone, semantic segmentation and AdaptIS branches. We sample 48 point proposals per object during training for all datasets. For all experiments we set the radius of Relative CoordConv to 280 pixels.</p><p>Results and discussion. Cityscapes dataset. We also add Mask R-CNN for comparison as it is the basis of many works on panoptic segmentation. Interestingly, AdaptIS shows state-of-the-art accuracy in terms of PQ even without pre-training on COCO and multiscale testing. Though we did not adapt the method for Mapillary and COCO datasets, we improve upon state-ofthe-art by more than 3% in terms of PQ on Mapillary dataset (see <ref type="table">Table 3</ref>) and show metrics close to the results of very recent UPSNet (see <ref type="table">Table 5</ref>). The results on test-dev part of the Mapillary and COCO datasets are shown in <ref type="table">Table 6</ref> and <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and future work</head><p>We introduce a novel network architecture for instance and panoptic segmentation that is conceptually different from mainstream detection-first instance segmentation methods. Presented architecture in many ways overcomes the limitations of existing methods and shows good performance on standard panoptic segmentation benchmarks. We hope that this work could serve as a strong foundation for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Normalized Focal Loss</head><p>Given an image and a point (x, y) AdaptIS network produces a mask of an object located at (x, y). We cast this problem as a binary semantic segmentation. The whole network is end-to-end trainable and optimizes just one loss function, that compares network output to the ground truth object mask.</p><p>Generally, binary cross entropy (BCE) loss is used for similar problems <ref type="bibr" target="#b27">[28]</ref>. However, <ref type="bibr" target="#b19">[20]</ref> showed that BCE pays more attention to pixels that are already correctly clas- sified. <ref type="bibr" target="#b19">[20]</ref> proposed Focal Loss to overcome this problem. The idea of Focal Loss is as follows. Let M denote the ground truth mask of an object, andM denote the output of the network. Let us denote the confidence of the network for the correct output at the point (i, j) by p i,j = p M (i, j) = M (i, j) . The original Focal Loss at the point (i, j) is defined as</p><formula xml:id="formula_1">FL(i, j) = −(1 − p i,j ) γ log p i,j .</formula><p>Consider the total weight of the values of loss function for all pixels in the image P (M ) = i,j (1 − p i,j ) γ . One can see that P (M ) decreases when accuracy of the network improves. This means that the total gradient of the Focal Loss fades over time. As a consequence the training with Focal Loss is slowed down over training iterations.</p><p>In our experiments we use a modification of Focal Loss that we call Normalized Focal Loss (NFL). Given an output of the networkM we define NFL at each pixel (i, j) of the image as follows: NFL(i, j,M ) = − 1 P (M ) (1 − p i,j ) γ log p i,j .</p><p>One can notice that the total gradient of the NFL is equal to the total gradient of BCE. Like original Focal Loss, NFL concentrates on the pixels that are misclassified by the network, but it leads to faster convergence and better accuracy (see <ref type="table" target="#tab_3">Table 7</ref> for ablation studies on Cityscapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Ablation studies</head><p>We perform ablation studies to measure the influence of the loss function and the use of Relative CoordConv on the resulting accuracy. We perform experiments on Citiscapes dataset with ResNet-50 backbone. We try the following modifications to the AdaptIS architecture: 1) excluding Relative CoordConv block leads to approximately 5% drop in P Q; 2) using simple Focal loss instead of Normalized Focal Loss leads to approximately 3% drop in P Q; 3) replacing NFL with cross-entropy leads to minor degradation of PQ mainly due to P Q things . <ref type="table" target="#tab_3">Table 7</ref> contains the results of theses experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Analysis of results on COCO</head><p>For COCO and Mapillary datasets we used the same architecture as for Cityscapes and did not fine-tune any of the parameters. Actually, in the paper we present the results for the first models that we trained. Surprisingly, despite this, we have achieved state-of-the art accuracy on Mapillary val and competitive results on COCO val. We believe that the results on COCO can be substantially improved by adding more conv layers to the AdaptIS network and fine-tuning the parameters.</p><p>We also notice that most image in COCO contain just one instance of the same object. For the detection-first methods like Mask R-CNN this is not a problem because they learn to segment each object independently. But for AdaptIS one rather needs to provides examples of multiple instances on the same image. Perhaps sampling such images more frequently during training could help. See examples of failure cases on COCO in <ref type="figure" target="#fig_0">Figure 12</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>AdaptIS takes an image and a point proposal (x, y) as input and generates a mask for an object located at position (x, y). AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. The picture shows sampled point proposals, corresponding generated masks, and the final result of panoptic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Results of class-agnostic instance segmentation for the toy data: (a) -validation images, (b) -results of Mask R-CNN, (c) -results of AdaptIS. It should be noted that Mask R-CNN often fails in cases when objects significantly overlap. (d) -larger image (top) and result of instance segmentation with AdaptIS (bottom). Though AdaptIS was trained on smaller images like shown in column (a), it works quite well on larger images like the one shown in (d). In this example it has correctly segmented 234 of 250 objects, missed 16 objects and made just 3 false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Instance segmentation with AdaptIS. Point proposals at each iteration of the method are shown in light-green. Different objects are marked with different colors. "Unknown" areas of the images are shown in grey. At each iteration a new point proposal is sampled and a corresponding object is segmented. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Examples of panoptic segmentation results with AdaptIS on Cityscapes dataset. (a) -notice, how AdaptIS is able to accurately segment objects of complex shape (heatmaps show the output of AdaptIS for the car and the bicycle objects); (b) -see how AdaptIS can handle severe occlusions in crowded environments; (c) -though the proposed architecture is built on top of single-scale features, it can segment objects of different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Panoptic segmentation with AdaptIS. The AdaptIS branch for class-agnostic instance segmentation and a standard semantic segmentation branch (e.g. PSPNet or DeepLab) are trained jointly with the backbone. The Point Proposal branch uses the freezed backbone for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Architecture of AdaptIS for panoptic segmentation that we used in our experiments for ResNet-50 backbone. Each conv layer is followed by ReLU activation and BatchNorm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Point proposals generation: (a) -input image; (b)output of the point proposal branch with detected local maximums.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Examples of panoptic segmentation results on COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Examples of panoptic segmentation results on Mapillary dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">AP@.5 AP@.7 AP@.8 AP@.9</cell></row><row><cell>Mask R-CNN</cell><cell>74.6%</cell><cell>59.4%</cell><cell>43.8%</cell><cell>5.8%</cell></row><row><cell>AdaptIS</cell><cell>99.6%</cell><cell>99.2%</cell><cell>98.8%</cell><cell>97.5%</cell></row></table><note>Comparison of AdaptIS with Mask R-CNN on toy prob- lem. Toy images contain multiple overlapping objects with coin- ciding bounding boxes, and hence are difficult for detection-first methods. At the same time, AdaptIS can segment those objects almost perfectly well.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Evaluation results on Cityscapes val dataset. Evaluation results on Mapillary val dataset.ing scenarios in urban settings which are split into 2975, 500 and 1525 images for training, validation and testing respectively. It consists of 8 and 11 classes for thing and stuff. Evaluation results on Mapillary test-dev dataset. We show the first 3 rows in the leaderboard.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="2">Pre-training</cell><cell>Backbone</cell><cell>m/s test</cell><cell>PQ</cell><cell cols="2">PQ st PQ th mIoU</cell><cell>AP</cell></row><row><cell cols="2">Mask R-CNN [11]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-50</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.5</cell></row><row><cell cols="2">Mask R-CNN [11]</cell><cell></cell><cell cols="2">ImageNet+COCO</cell><cell>ResNet-50</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell></row><row><cell cols="2">TASCNet [17]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-50</cell><cell></cell><cell>55.9</cell><cell>59.8</cell><cell>50.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UPSNet [27]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-50</cell><cell></cell><cell>59.3</cell><cell>62.7</cell><cell>54.6</cell><cell>75.2</cell><cell>33.3</cell></row><row><cell cols="2">CRF + PSPNet [18]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-101</cell><cell></cell><cell>53.8</cell><cell>62.1</cell><cell>42.5</cell><cell>71.6</cell><cell>-</cell></row><row><cell cols="2">Panoptic FPN [14]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-101</cell><cell></cell><cell>58.1</cell><cell>62.5</cell><cell>52.0</cell><cell>75.7</cell><cell>33.0</cell></row><row><cell cols="2">DeeperLab [28]</cell><cell></cell><cell cols="2">ImageNet</cell><cell>Xception-71</cell><cell></cell><cell>56.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TASCNet [17]</cell><cell></cell><cell cols="2">ImageNet+COCO</cell><cell>ResNet-50</cell><cell></cell><cell>59.2</cell><cell>61.5</cell><cell>56.0</cell><cell>77.8</cell><cell>37.6</cell></row><row><cell cols="5">TASCNet-multiscale [17] ImageNet+COCO</cell><cell>ResNet-50</cell><cell>+</cell><cell>60.4</cell><cell>63.3</cell><cell>56.1</cell><cell>78.0</cell><cell>39.0</cell></row><row><cell cols="3">MRCNN + PSPNet [15]</cell><cell cols="2">ImageNet+COCO</cell><cell>ResNet-101</cell><cell></cell><cell>61.2</cell><cell>66.4</cell><cell>54</cell><cell>-</cell><cell>36.4</cell></row><row><cell cols="2">UPSNet [27]</cell><cell></cell><cell cols="2">ImageNet+COCO</cell><cell>ResNet-101</cell><cell></cell><cell>60.5</cell><cell>63.0</cell><cell>57.0</cell><cell>77.8</cell><cell>37.8</cell></row><row><cell cols="2">UPSNet-multiscale [27]</cell><cell></cell><cell cols="2">ImageNet+COCO</cell><cell>ResNet-101</cell><cell>+</cell><cell>61.8</cell><cell>64.8</cell><cell>57.6</cell><cell>79.2</cell><cell>39.0</cell></row><row><cell cols="2">AdaptIS (ours)</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-50</cell><cell></cell><cell>59.0</cell><cell>61.3</cell><cell>55.8</cell><cell>75.3</cell><cell>32.3</cell></row><row><cell cols="2">AdaptIS (ours)</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNet-101</cell><cell></cell><cell>60.6</cell><cell>62.9</cell><cell>57.5</cell><cell>77.2</cell><cell>33.9</cell></row><row><cell cols="2">AdaptIS (ours)</cell><cell></cell><cell cols="2">ImageNet</cell><cell>ResNeXt-101</cell><cell></cell><cell>62.0</cell><cell>64.4</cell><cell>58.7</cell><cell>79.2</cell><cell>36.3</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>PQ</cell><cell cols="2">PQ st PQ th</cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>PQ</cell><cell>PQ st PQ th</cell></row><row><cell>[6]</cell><cell>ResNet-50</cell><cell>17.6</cell><cell>27.5</cell><cell>10.0</cell><cell></cell><cell></cell><cell cols="2">liuxu</cell><cell>41.1</cell><cell>45.6</cell><cell>37.8</cell></row><row><cell>[28]</cell><cell>Xception-71</cell><cell>31.9</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>jie.li</cell><cell></cell><cell>38.6</cell><cell>38.2</cell><cell>38.9</cell></row><row><cell>[17]</cell><cell>ResNet-50</cell><cell>30.7</cell><cell>33.4</cell><cell>28.6</cell><cell></cell><cell></cell><cell cols="2">Ours</cell><cell>36.8</cell><cell>41.4</cell><cell>33.3</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>32.0</cell><cell>39.1</cell><cell>26.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>ResNet-101</cell><cell>33.4</cell><cell>40.2</cell><cell>28.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="2">ResNeXt-101 35.9</cell><cell>41.9</cell><cell>31.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>In all our experiments the panoptic segmentation network was trained on train part and evaluated on val part. All im- ages have the same resolution 1024 × 2048. Mapillary dataset is a street scene dataset like</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 Table 5 .Table 6 .</head><label>256</label><figDesc>Evaluation results on COCO val dataset. Evaluation results on COCO test-dev dataset.</figDesc><table><row><cell>presents comparison</cell></row><row><cell>of AdaptIS to recent works on panoptic segmentation on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Ablation studies of panoptic segmentation with AdaptIS on Cityscapes dataset. See text for more details.</figDesc><table><row><cell>Loss</cell><cell>Relative CoordConv</cell><cell cols="3">PQ, % PQ st , % PQ th , %</cell></row><row><cell>NFL</cell><cell>+</cell><cell>59.0</cell><cell>61.3</cell><cell>55.8</cell></row><row><cell>NFL</cell><cell>-</cell><cell>53.9</cell><cell>61.7</cell><cell>43.1</cell></row><row><cell>FL</cell><cell>+</cell><cell>55.9</cell><cell>60.1</cell><cell>50.1</cell></row><row><cell>BCE</cell><cell>+</cell><cell>58.7</cell><cell>61.5</cell><cell>54.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Daan De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepwarp: Photorealistic image resynthesis for gaze manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="311" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06830</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02446</idno>
	</analytic>
	<monogr>
		<title level="m">Panoptic feature pyramid networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weaklyand semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03904</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9628" to="9639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03784</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bag of freebies for training object detection neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
