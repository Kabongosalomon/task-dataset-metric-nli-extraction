<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-Fine Networks for Temporal Activity Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumara</forename><surname>Kahatapitiya</surname></persName>
							<email>kkahatapitiy@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-Fine Networks for Temporal Activity Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce Coarse-Fine Networks, a twostream architecture which benefits from different abstractions of temporal resolution to learn better video representations for long-term motion. Traditional Video models process inputs at one (or few) fixed temporal resolution without any dynamic frame selection. However, we argue that, processing multiple temporal resolutions of the input and doing so dynamically by learning to estimate the importance of each frame can largely improve video representations, specially in the domain of temporal activity localization. To this end, we propose (1) 'Grid Pool', a learned temporal downsampling layer to extract coarse features, and, (2) 'Multi-stage Fusion', a spatio-temporal attention mechanism to fuse a finegrained context with the coarse features. We show that our method outperforms the state-of-the-arts for action detection in public datasets including Charades with a significantly reduced compute and memory footprint. The code is available at https://github.com/kkahatapitiya/Coarse-Fine-Networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to represent videos is important. It requires embedding both spatial and temporal information in a sequence of frames, often implemented with 3D convolutions. Leaning to build good video representations is crucial for various vision tasks including action classification, video object segmentation, and complex human activity recognition as well as temporal localization of such activities.</p><p>One of the main challenges in video representation learning is in capturing long-term motion from a continuous video. In order for a convolutional neural network to abstract longterm motion information across many frames, a large number of (spatio-)temporal conv. layers (or such layers with large kernels) are necessary, requiring many parameters. This, combined with the difficulty in obtaining large-scale annotated videos and increased computation time, makes the learning of the video representation very challenging for non-atomic activities. This is even more challenging for temporal activity detection (i.e., localization), as the activities may very often temporally overlap. A mechanism to reliably and efficiently capture various motion in videos is necessary.</p><p>Use of frame striding or temporal pooling (i.e., lowering the frame rate) has been a successful strategy to cover a  <ref type="figure">Figure 1</ref>. Coarse-Fine Networks process information at two different temporal resolutions. The Coarse stream learns to sample the most informative frame locations through a learnable downsampling operation: Grid Pool, whereas the Fine stream process the entire temporal duration of the input to extract a fine-grained context. The connections in-between the two streams: Multi-stage Fusion, provide multiple abstraction-levels of the fine-grained context, calibrated to the temporal locations of the coarse frames. For Charades dataset <ref type="bibr" target="#b28">[29]</ref>, we configure our network to use T = 64, T = 128 and Î± = 1/4. larger time interval without increasing the number of model parameters. Since such striding loses fine details of frame changes, it was often paired with another CNN tower taking an input with a higher frame rate, forming a two-stream (or multi-stream) CNN architecture as was done in SlowFast <ref type="bibr" target="#b5">[6]</ref> and AssembleNet <ref type="bibr" target="#b24">[25]</ref>. These models confirmed the benefits of frame striding as well as multi-stream architectures to combine representations with multiple temporal resolutions. However, although using temporal striding (with a multistream multi-resolution architecture) allows the model to more easily process long-term motion, they are limited as it ignores 'importance' of each frame. Informativeness of each frame is different. It is often unnecessary and redundant to feed almost identical frames as an input to the model when there is no/little motion in video frames. On the other hand, if a human in the video is displaying a rapid motion, taking all such frames into consideration is desired. Uniform temporal striding or pooling is incapable of such dynamic frame selection.</p><p>In this paper, we propose (1) a new approach that allows a learnable dynamic selection of temporal frames within the model, as well as (2) a method to fuse such sampled (i.e., temporally 'coarse') representations with conventional,  <ref type="figure">Figure 2</ref>. Performance/complexity trade-off of state-of-the-art methods for activity 'localization' in Charades <ref type="bibr" target="#b28">[29]</ref>. Our Coarse-Fine Networks achieve superior performance than the previous best-performing method in literature, with more than one order of magnitude reduction in compute. Moreover, we do not use any additional modalities such as optical flow or object detections. more temporally 'fine' representations. We introduce the Coarse-Fine Networks. A new component named temporal Grid Pooling is presented to obtain better Coarse representations, and the Multi-stage Fusion is introduced to best combine such Coarse representations with the conventional Fine representations. Unlike <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, our Grid Pooling learns to dynamically select informative frames. <ref type="figure">Fig. 1</ref> illustrates the overview of the model, and <ref type="figure">Fig. 2</ref> shows the benefits of the model, which we discuss more in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs learning 3D spatio-temporal representations for human activity recognition have been very successful <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Two-stream approaches were often designed to combine RGB and optical flow <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, particularly focusing on video classification. SlowFast network <ref type="bibr" target="#b5">[6]</ref> showed the potential that combining representations of different temporal resolutions (i.e., frame rates) could also benefit action recognition. More recently, AssembleNet <ref type="bibr" target="#b24">[25]</ref> showed the effectiveness of multi-stream models with neural architecture search, and X3D <ref type="bibr" target="#b4">[5]</ref> studied computationally more efficient 3D conv. modules. There also are approaches focusing on the modeling of temporal structure in videos, often particularly designed to handle longer videos (with long-term motion) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>. Another group of approaches took advantage of graph representations to model human/object dynamics in the videos, often paired with sequential models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Approaches to explicitly take advantage of objects in videos have been studied as well <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Action localization: There are also a line of work focusing on the temporal action localization task. In the localization task (e.g., Charades localization <ref type="bibr" target="#b28">[29]</ref>), the objective is not about making a classification decision per segmented video but about annotating every frame with multiple ongoing activities. Use of sequential models such as LSTMs have been popular <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, and fully convolutional approaches also showed promising results <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Dynamic sampling: Selective processing of information has been of interest to the computer vision community. From Deformable convolutions <ref type="bibr" target="#b2">[3]</ref> to Graphical networks [?, <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>, various core components of neural networks are based on this idea. Multiple recent works also try to address dynamic sampling of inputs, either spatially <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, temporally <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> or spatio-temporally <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Coarse-Fine Networks</head><p>Coarse-Fine Networks explore how video architectures can benefit from different abstractions of temporal resolution and long-term temporal information. As shown in <ref type="figure">Fig. 1</ref>, we do this by processing the information at two different temporal resolutions: coarse and fine, in a two-stream architecture. The Coarse stream learns to (differentiably) select the most informative frame locations, essentially performing a learned temporal downsampling to abstract a lower temporal resolution. In contrast, the Fine stream processes the input at the original temporal resolution and provide a fine-grained context to the Coarse stream through a fusion mechanism. To abstract this context information, the Fine stream always looks at the full temporal duration of the input clip (which later gets pooled with Gaussians), whereas the Coarse stream can either look at a shorter clip or the entire clip depending on the inference interval.</p><p>In Coarse-Fine Networks, we address two key challenges: (i) how to abstract the information at a lower temporal resolution meaningfully, and, (ii) how to utilize the fine-grained context information effectively. First, to abstract coarse information, we propose Grid Pool (Sec. 3.1), a learnable temporal downsampling operation which adaptively samples the most informative frame locations with a differentiable process. Secondly, to effectively use the fine-grained context provided by the Fine stream, we introduce Multi-stage Fusion (Sec. 3.2), a set of lateral connections between the Coarse and Fine streams, which looks at multiple abstraction levels of fine-grained information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Grid Pool</head><p>Our temporal Grid Pool operation learns the most informative frame locations from a given input clip, and samples the representations corresponding to the locations based on interpolation. In fact, it can be viewed as a learnable temporal downsampling layer with a small compute overhead, which can replace the conventional temporal pooling operations. However, in contrast to these pooling operations, our Grid Pool samples by interpolating on a non-uniform grid with learnable (and adaptive) grid locations as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. First, a lightweight head (h) projects the input feature (X C ) of temporal length T to a set of confidence values {p i } i=1,Â·Â·Â· ,Î±T , where Î± &lt; 1 and Î±T is an integer (e.g.,  Î±T number of points are differentiablly sampled from an input feature of length T by learning their importance. We interpret pi as the importance of each frame location. Since we want to sample with a lower sampling duration (i.e., a higher frame rate) where we have a higher importance, we construct cdf(1 â pi) for sampling.</p><p>(e.g., 4 frames if Î± = 1/4) is, and is modeled as a function of the input representation X C :</p><formula xml:id="formula_0">{p i } i=1,Â·Â·Â· ,Î±T = h(X C ) .<label>(1)</label></formula><p>The intuition here is to sample frames at a higher frame rate where the confidence (i.e., informativeness) is high and at a lower frame rate where it is low. In other words, the stride between the interpolated frame locations should be small where the confidence is high, and vice-versa. We normalize these confidence values p i since we need the relative (not absolute) confidence to capture the relative importance of frames. To get a set of Î±T grid locations based on confidence values, we consider the Cumulative Distribution Function {cdf(1 â p i )} i=1,Â·Â·Â· ,Î±T , which is a non-uniform and monotonically-increasing function. The input of the Grid Pool layer X C is sampled/interpolated based on these grid locations to get the outputX C , while making it fully differentiable for backpropagation. This process can be represented as,</p><formula xml:id="formula_1">q t = T Â· cdf(1 â p t ) = T Â· t i=1 (1 â p i ) Î±T i=1 (1 â p i ) , X C = I X C , {q t } t=1,Â·Â·Â· ,Î±T ,<label>(2)</label></formula><p>where q t represents the grid location t, and I(Â·) represent the temporal sampling function. Here, when a grid location is non-integer, the corresponding sampled frame is a temporal interpolation between the adjacent frames. We do not perform any spatial sampling in the Grid Pool layer.</p><p>Grid Unpooling: A temporal interpolation based on a nonuniform grid as such can affect the temporal structure of the propagated features. Before feberating the final output, the frame-wise predictions of the network should be re-aligned properly for activity detection tasks. To do this, we introduce a Grid Unpool operation, which is coupled with the grid locations learned by the Grid Pool layer. This does not have any learnable parameters and simply performs the inverse operation of the former. First, Î±T re-sample grid locations are calculated based on the inverse mapping of the cdf, based on which, the logits are re-sampled to acquire the original temporal structure. The idea is to re-sample with a low frame-rate in the regions where we used a high framerate in Grid Pool, and vice-versa. Any non-integer frame location is temporally interpolated similar to Eq. 2. Finally, these logits are uniformly upsampled through interpolation to fit the input temporal resolution. For classification tasks, the Grid Unpool operation may not be necessary as a global pooling of the logits is considered as the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-stage Fusion</head><p>We introduce Multi-stage Fusion, a set of lateral connections between the two streams as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, to fuse the context from the Fine stream with the Coarse stream. We consider three main design choices here: (i) it should be capable of filtering out which fine-grained information should be passed down to the Coarse stream, (ii) it should have a calibration step to properly align the fine features with the coarse features based on their relative temporal locations, and (iii) it should be able to learn and benefit from multiple abstraction-levels of fine-grained context at each fuse-location in the Coarse stream. Our design tries to address these aspects.</p><p>Filtering fine-grained information: First, to decide which fine-grained context should be passed through to the fusion process, the fine feature X F li at abstraction-level l i is multiplied with a self-attention mask. This mask is calculated by processing the fine feature through a lightweight head (g) consisting of point-wise convolutional layers followed by a sigmoid non-linearity.</p><formula xml:id="formula_2">X F li = X F li g(X F li )</formula><p>Fine-to-Coarse correspondence: The attention-weighted fine featureX F li still needs to be calibrated for the temporal location of each coarse feature. Since the Coarse and Fine streams does not necessarily process the same, properly aligned temporal length because of our non-uniform Grid Pooling, we need to explicitly compute the frame correspondence. To make this calibration, we use a set of temporal Gaussian distributions centered at each coarse frame location {Âµ C j } j=1,Â·Â·Â· ,Î±T which abstract a locationdependent weighted average of the fine feature. We use Î±T such Coarse-centric Gaussians, each having a temporal length of T and a standard deviation Ï which is a fraction of this length. We found that considering the center and scale of these Gaussians to be hyperparameters rather than making them learnable, gives a better performance, possibly due to relatively simpler training. This calibration step can be viewed as,</p><formula xml:id="formula_3">G C j = 1 â 2ÏÏ 2 exp (t â Âµ j ) 2 2Ï 2 j=1,Â·Â·Â· ,Î±T , X F li =X F li Â· G C , where G C is the stacked Coarse-centric Gaussians and t â [0, T â 1].</formula><p>Multiple abstraction-levels: The featureX F li still corresponds to a single abstraction-level l i of fine features, where we have Multi-stage Fusion connections in multiple abstraction-levels, i.e., depths of the network. Therefore, we allow each fusion connection to look at the features from all abstraction levels by concatenating them channel-wise (after adjusting the spatial resolution through max pooling), and performing point-wise (i.e. 1 Ã 1 Ã 1) convolutions to get the final scale (A li ) and shift (B li ) features at each fusion location. This can be represented as,</p><formula xml:id="formula_4">X F = â n i=1X F li , A li = f A li (X F ) , B li = f B li (X F ) , X C li = A liX C li + B li .</formula><p>where â is the channel-wise concatenation of features from n abstraction-levels and, f A li and f B li represent projection heads to calculate the scale and shift features at each fusion location l i , respectively. This design enables Multi-stage Fusion to process multiple abstraction-levels of fine-grained context through filtering and temporal calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Details</head><p>The network architecture used as the backbone in Coarse-Fine Networks is adopted from X3D <ref type="bibr" target="#b4">[5]</ref>, which follow a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head><p>Filters <ref type="table">Table 1</ref>. Coarse-Fine Network Architecture is adopted from X3D <ref type="bibr" target="#b4">[5]</ref>, more specifically from the version X3D-M. Both streams have the same design and parameters, except for the addition of Grid Pool layer and Grid Unpool operation in the Coarse stream. The Fine stream process the entire temporal length of the input T to provide a fine-grained context, whereas the Coarse stream can look at a segmented clip of length T , for which the frame-wise predictions are required. Here, Î± &lt; 1 and Î±T is an integer. The kernel shapes follow the standard notation {T Ã S 2 , C}.</p><formula xml:id="formula_5">Output size T ÃS 2 Coarse Fine Coarse Fine input stride 10, 1 2 T Ã224 2 T Ã224 2 conv 1 1Ã3 2 , 3Ã1, 24 T Ã112 2 T Ã112 2 res 2 ï£® ï£° 1Ã1 2 , 54 3Ã3 2 , 54 1Ã1 2 , 24 ï£¹ ï£» Ã3 T Ã56 2 T Ã56 2 grid pool stride 1/Î±, 1 2 - Î±T Ã56 2 T Ã56 2 res 3 ï£® ï£° 1Ã1 2 , 108 3Ã3 2 , 108 1Ã1 2 , 48 ï£¹ ï£» Ã5 Î±T Ã28 2 T Ã28 2 res 4 ï£® ï£° 1Ã1 2 , 216 3Ã3 2 , 216 1Ã1 2 , 96 ï£¹ ï£» Ã11 Î±T Ã14 2 T Ã14 2 res 5 ï£® ï£° 1Ã1 2 , 432 3Ã3 2 , 432 1Ã1 2 , 192 ï£¹ ï£» Ã7 Î±T Ã7 2 T Ã7 2 conv 5 1Ã1 2 , 432 Î±T Ã7 2 T Ã7 2 pool 5 NoneÃ7 2 Î±T Ã1 2 T Ã1 2 fc 1 1Ã1 2 , 2048 fc 2 1Ã1 2 , #classes grid unpool stride Î±, 1 2 - T Ã1 2 T Ã1 2</formula><p>ResNet <ref type="bibr" target="#b10">[11]</ref> structure, but designed for efficiency in video models. Both Coarse and Fine streams are initialized with separate sets of parameters, but have the same number of layers and filters as shown in <ref type="table">Table 1</ref>, except for the addition of Grid Pool in the Coarse stream. Since the X3D architecture perform no temporal downsampling or pooling, it follows aggressive striding at the input level to be computationally efficient, which is set be a stride of 10 in our case. This allows the input of the Coarse stream to cover a large temporal region, compared to what common backbones such as I3D <ref type="bibr" target="#b1">[2]</ref> cover during training. This is beneficial, particularly in datasets with longer temporal duration. The Coarse stream takes in segmented clips of T = 64 frames to follow the standard X3D architecture after the Grid Pool layer (with Î± = 1/4) during training, while processing the input fully convolutionally at inference (i.e., T = 128 frames during testing). In contrast, the Fine stream always process the entire input clip, which is capped at a maximum of T = 128 frames. This limit should be adjusted based on the dataset to include the entire duration of input clips. We found T = 128 frames with a stride of 10 is adequate to cover the entire temporal length of more than 90% of the Charades <ref type="bibr" target="#b28">[29]</ref> videos. The main difference between the Coarse stream and the Fine stream is the Grid Pool layer and the corresponding Grid Unpool operation. We want to perform this learned temporal downsampling as early as possibly in the network to reduce the compute, but at the same time, having good enough features to learn the grid locations. Thus, we place the Grid Pool layer after the first residual block res 2 . We find that downsampling by a factor of 4 works well in practice, to have a good compute/performance trade-off <ref type="table">(Table 3e</ref>). To calculate the confidence values (p) in the Grid Pool layer, we use a lightweight head (h) of 3 strided convolutions with a total temporal stride of 4 and a spatial stride of 8, followed by spatial average pooling and a sigmoid non-linearity. The Grid Unpool operation has no learnable parameters. It is coupled with the grid locations predicted by the Grid Pool layer to perform the inverse operation of the former to recover the original temporal structure at the logits level.</p><p>We try to follow a lightweight design in Multi-stage Fusion as well. The self-attention maskX F li is calculated through a head (g) of 2 point-wise (i.e. 1 Ã 1 Ã 1) convolutions followed by a sigmoid non-linearity. The Coarsecentric Gaussians (G C ) have no learnable parameters, and the peak magnitude of each mask is normalized to 1. The standard deviation Ï is set to be T /8, empirically. The two heads f A l i and f A l i at each fusion location which project the concatenated multi-stage features (X F ) to scale (A li ) and shift (B li ) features contain a single point-wise convolution each. The scale features go through an additional sigmoid non-linearity. We further discuss the complexity (compute and parameters) of these operations in our ablations (subsection 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Coarse-Fine Networks on two large-scale benchmarks for activity detection: Charades <ref type="bibr" target="#b28">[29]</ref> and Multi-THUMOS <ref type="bibr" target="#b38">[39]</ref>. Note that we focus on the temporal detection (i.e., localization) task of generating multi-label activity annotations at every time step, which is more challenging than video classification. Activities may temporally overlap (e.g., sitting and eating), and the model must be trained to annotate all of them at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Charades</head><p>Dataset: Charades <ref type="bibr" target="#b28">[29]</ref> is a large scale dataset consisting of â¼9.8k continuous videos with frame-wise annotations of 157 common household activities. The dataset is split as â¼7.9k training and â¼1.8k validation videos. Each video contains an average of 6.8 activity instances, often with multiple activity classes per frame, and has longer clips averaging a duration of â¼30 seconds. Such a long duration makes it a suitable dataset to test Coarse-Fine Networks.</p><p>Training: We initialize both Coarse and Fine streams of our network with the X3D backbone pretrained on Kinetics-400 <ref type="bibr" target="#b15">[16]</ref>. For the actual training of the Coarse-Fine network as well as baselines, we follow a two-stage training process: first, training the two streams separately, followed by finetuning the combined streams.</p><p>In the first stage, the Coarse stream considers an input of 64 frames sampled at a stride of 10, whereas the Fine stream considers 16 frames sampled at the same stride. This allows both streams to process same-sized features after Grid Pool layer. We use Î± = 1/4 in our experiments. Each stream is trained for 100 epochs with a batch size of 16 at a learning rate of 0.02 at the start, which is decreased by a factor of 10 at 60 and 80 epochs.</p><p>In the second stage, the two streams are trained together as Coarse-Fine Networks, with Multi-stage Fusion parameters initialized from scratch. We train this for another 100 epochs with the same schedule and batch size, but use 10Ã increased learning rate for the newly-initialized parameters of the fusion layers. Here, the Fine stream process the entire duration of the input, which is capped at 128 frames (sampled at a stride of 10) for Charades <ref type="bibr" target="#b28">[29]</ref>. During both stages, each input is randomly sampled in [256,320] pixels, spatially cropped to 224Ã224 and applied a random horizontal flip. We use a dropout rate of 0.5 before the logits layer. The logits go through a sigmoid to make multi-label predictions for each frame. We use an average of classification and localization loss for training, similar to previous methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Inference: At inference, we make predictions for every frame. Even though our input is sampled at a stride of 10, we consider the labels for all frames (at a stride of 1) and interpolate the logits to fit the original temporal length. In other words, we evaluate our models so that the predictions are more fine-grained at original temporal resolution. However, all state-of-the-art methods in <ref type="table">Table 2</ref> report the performance for 25 equally-sampled frames per each input, which is the original Charades localization evaluation <ref type="bibr" target="#b28">[29]</ref> setting. Therefore, to make a fair comparison, we evaluate our models in the same setting, only making predictions for 25 equally-sampled frames per input. The evaluation script from the Charades challenge scales the Average Precision for each class with a corresponding class weights. At inference, the inputs are center-cropped to 224Ã224. We report the performance as mean Average Precision (mAP) and measure the compute requirement to process an input clip of 128Ã10 frames, for which our network processes only 128 frames due to input striding. The compute is reported as GFLOPs (floating-point operations Ã10 9 ) and the number of learnable parameters in millions (M), i.e., Ã10 6 . We do not take advantage of any multi-crop inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We compare the performance of the Coarse-Fine Networks with the state-of-the-art methods on Charades <ref type="bibr" target="#b28">[29]</ref> localization task (i.e., temporal activity detection) in <ref type="table">Table 2</ref>. For this evaluation, we use the standard test setting (i.e., the official 'Charades_v1_localize' evaluation) where we make predictions for equally-sampled 25 frames in the validation set. This is the same procedure followed in previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. We report the performance (mAP), compute model flow obj. mAP (%) GFLOPs Param (M) I3D (Inception) <ref type="bibr" target="#b1">[2]</ref> 15.63 2223.03 12.45 Two-stream I3D <ref type="bibr" target="#b1">[2]</ref> 17.22 4446.10 24.90 3D ResNet-50 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> 18.60 3187.63 46.52 X3D <ref type="bibr" target="#b4">[5]</ref> 18.87 37.96 3.29 X3D-L <ref type="bibr" target="#b4">[5]</ref> 20.03 147.04 5.78 I3D + super-events <ref type="bibr" target="#b21">[22]</ref> 19  <ref type="table">Table 2</ref>. Comparison with the state-of-the-art methods for activity detection on Charades <ref type="bibr" target="#b28">[29]</ref>. We report the performance (mAP), compute requirement to process a clip of 128 Ã 10 frames (GFLOPs), and the number of parameters (M). These results correspond to the original Charades localization evaluation settings. Coarse-Fine Networks significantly outperform the previous stateof-the-art with +1.4%mAP relative improvement, while reducing the compute requirement by more than one order of magnitude. It is worth noting that we do not use additional input modalities, i.e., optical-flow or object detections as any of the previous methods. The source of <ref type="bibr" target="#b19">[20]</ref> is not available to us to calculate its exact complexity values.</p><p>requirement to process a clip of 128 Ã 10 frames (GFLOPs) and the number of parameters (M).</p><p>We are able to confirm that our Coarse-Fine Network performs better than all previous methods, establishing the new state-of-the-art of 25.10% mAP on Charades localization. The Coarse-Fine Network, which only uses RGB, is not only superior to the previous RGB models but also is better than the methods using additional inputs modalities (i.e. opticalflow and object detection). It shows a relative improvement of +1.4% mAP compared to the previous best performing method <ref type="bibr" target="#b19">[20]</ref>, which benefits from additional training data (for its object detector training) and additional input modality (objects).</p><p>We also note that the Coarse-Fine Network is extremely computationally efficient. Compare to the previous models, it often requires only â¼1/75 of computations (e.g., 73 vs. 4446 GFLOPS). Further, this computation is without considering the overhead for optical flow computation or object detection in prior works. The significant computation efficiency of the Coarse-Fine Networks is due to the better utilization of the RGB features, which discards the need for processing additional modalities, as well as an efficient usage of X3D modules with our temporal Grid Pooling and Multi-stage Fusion, which we confirm further with our ablations in the next section.</p><p>We further report a version of our method: Fine-Fine Networks, in which the Grid Pool layer is removed from the Coarse stream, to highlight the importance of the Coarse features. Fine-Fine Networks still benefit from our Multi-stage Fusion. The Grid Pool operation dynamically sample important frames to generating a coarse temporal resolution, which gives the Coarse-Fine Networks an relative performance gain of +0.67% mAP and 23% computation reduction. We also evaluated the baseline extension of X3D as a two-stream network (with different temporal resolutions) in a form similar to <ref type="bibr" target="#b5">[6]</ref>, which we name SlowFast det . This does not have our Grid Pool layer or the Multi-stage Fusion mechanism. The result shows the benefits of the components, giving a relative improvement of 2.79%mAP. A larger version of X3D (i.e., X3D-L) shows that the performance improvement of Coarse-Fine compared to X3D is not merely due to the increased compute.</p><p>It is important to also note that all previous methods work on pre-extracted features from a frozen backbone, essentially making them late-modeling techniques, either using graphbased methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> or abstracting long-term temporal information <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. In contrast, our method allows end-toend training of feature fusions at intermediate locations of the network, enabling it to learn better representations using only RGB information. <ref type="figure">Fig. 2</ref> further highlights the benefit of Coarse-Fine Networks compared to previous state-of-the-arts. We show the performance/complexity trade here, with the x-axis (complexity in GFLOPs) in log-scale. Our method shows comparable performance with the previous best performing method which outperforms all previous state-of-the art methods, while being extremely efficient in design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>Here, we discuss multiple ablation experiments validating our design decisions, specifically on our Grid Pool layer and Multi-stage Fusion. We use the Charades dataset (with the localization setting as above).</p><p>In our ablation experiments, we take advantage of more robust evaluation metric to compare our approach and the baselines. We make the model to generate a multi-class activity annotation at every time step, and compare it with the ground truth to measure the mAP. This is very similar to the official 'Charades_v1_localize setting' used above, except that (i) it is evaluated for Ã25 more frames for the completeness and that (ii) we measure mAP per activity class and average them. Fusion location: First, we explore which locations in our two stream architecture would be ideal to implement the fusion connections. We consider the late fusion as a baseline, in which, the only fusion happens just before the logits layer. This is similar to the previous methods in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Next, We extend this fusion to multiple intermediate levels, specifically, after each res block, in which we fuse the two streams at only equivalent abstraction levels, i.e., at the same depth. This is similar to the fusion in SlowFast <ref type="bibr" target="#b5">[6]</ref>. Finally, we consider multiple abstraction-levels of Fine features for the fusion, which gives our Multi-stage Fusion. The results of this ablation is given in <ref type="table">Table 3a</ref>. Note that here we evaluate our fusion in a Fine-Fine Network to decouple the 73.37 (f) Importance of Grid Pool and Multi-stage Fusion combined: SlowFast det is a baseline w/o Grid Pool and Multi-stage Fusion. It samples input at different frame-rates (Fast is Ã4 faster) and uses fusion connections similar to that of Slow-Fast <ref type="bibr" target="#b5">[6]</ref>. (Coarse-Fine) <ref type="table">Table 3</ref>. Ablations on Charades <ref type="bibr" target="#b28">[29]</ref> localization comparing the design choices of Grid Pool and Multi-stage Fusion. We show the performance in mean Average Precision (mAP), and measure the compute requirement for a temporal clip of T = 128 at inference in GFLOPs (floating-point operations Ã10 9 ). In these tables, we report the performance for fine-grained predictions, making decisions for every frame. effect of Grid Pool from our fusion. Multi-stage Fusion shows a +0.81% mAP improvement compared to only using a late fusion. The improvement of considering multiple abstraction-levels is marginal, at +0.15% mAP, suggesting that features at the same abstraction level can provide the most complementary information. Fusion dimensions: We experiment on the significance of different dimensions in the fusion features. Either having only channel dimension (C) similar to <ref type="bibr" target="#b21">[22]</ref>, channel-spatial (CHW) dimensions or all channel-temporal-spatial (CHWT) similar to <ref type="bibr" target="#b5">[6]</ref> is considered here. The results of this experiment is reported in <ref type="table">Table 3b</ref>. Note that the dimensions which are not available in any of the above scenarios are average pooled before the fusion. We see that having all CHWT dimensions in the fusion feature has a large improvement compared to the baseline, specifically +4.54% mAP. Introduction of the temporal dimension (T) shows the most improvement, which is +2.79% mAP. This is in fact mainly due to the temporal Gaussians in our Fusion that calibrate the features based on the location, without which, we can not see such improvement (i.e., +0.61% mAP over a single stream, when naively selecting corresponding temporal locations in the two streams for fusion w/o Gaussians). Fusion mask: Here, we evaluate how important it is to filter the Fine features at the input of fusion, results of which, is shown in <ref type="table">Table 3c</ref>. In the Multi-stage Fusion setting, having a self-attention mask to adaptively weight each Feature point gives an improvement of +1.23% mAP compared to feeding the Fine feature directly. Pooling type: Next, we explore the performance gain caused by the proposed (temporal) Grid Pool layer. We compare against conventional temporal pooling operations such as max pooling, average pooling and even simple temporal striding. Here, we report the numbers for a Coarse-only network to decouple the effect of Grid Pool from that of Multi-stage Fusion. In these experiments, we set Î± = 1/4, which essentially means a Ã4 temporal downsampling, and perform the downsampling after the res 2 block. Max pooling, average pooling use a similar setting of kernel size of 4 and a stride of 4. Grid pooling gives a consistent improvement over other methods, specifically +1.91% mAP and +1.48% mAP over commonly used max pooling and average pooling, respectively. We also note that a simple temporal striding can outperform max pooling and average pooling by +1.28% mAP and +0.85% mAP, respectively. We suspect that the inferior performance of max/average pooling is due to the aggressive temporal striding at the input of X3D, which is a stride of 10 by default (i.e., after the pooling, the stride is 40). In such a large window, pooling tends to blur most of the temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Pool configuration:</head><p>We try different configurations of Grid Pooling to evaluate its performance and compute requirement. Similar to above, we use the Coarse-only network. We consider an input of temporal length T = 128 at a stride of 10 or T = 256 at a stride of 5, to cover entire duration of Charades videos <ref type="bibr" target="#b28">[29]</ref>. We try temporally downsampling each of these with Î± = 1/4 (Ã4 downsampling) or Î± = 1/8 (Ã8 downsampling). The performance of these configurations is given in <ref type="table">Table 3e</ref>. Ã8 downsampling shows a significantly lower performance indicating that it looses too much information with such an aggressive stride, i.e., more frames are important and need to be sampled by the Grid Pool layer. Moreover, increasing the number of input frames does not necessarily improve the performance (only +0.02% mAP) with Î± = 1/4. Among these configurations, T = 128 with Î± = 1/4 shows the best performance/compute trade-off.</p><p>Grid Pool and Multi-stage Fusion combined: Finally, we evaluate the combined performance of Grid Pool and Multi-Stage Fusion. To do this, we consider a two-stream baseline without these components, which we call SlowFast det . This performs Ã4 temporal downsampling in the Coarse stream based on striding, and use direct frame correspondences between Fine and Coarse streams for fusion, similar to Slow-Fast <ref type="bibr" target="#b5">[6]</ref> while still using X3D modules like ours. The results of this study is given in <ref type="table">Table 3f</ref>. When compared with this baseline, introduction of either Grid Pool or Multi-stage Fusion provides consistent improvements of +0.21% mAP and +2.18% mAP respectively. Our Coarse-Fine Networks outperform this baseline by a margin of +2.63% mAP.</p><p>Trade-off with X3D: Coarse-Fine network is designed to use a similar amount of computation as a two-stream version of X3D-M. Another way to use the extra compute is by increasing the number of layers. To understand if the increased compute is meaningful, we test X3D-L, a larger version of X3D <ref type="table">(Table 2</ref>). X3D-L shows of 20.03% mAP with a compute of 147.04 GFLOPS. Coarse-Fine Networks outperform this in both accuracy and speed with 25.10% mAP at 73.37 GFLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MultiTHUMOS</head><p>Dataset: MultiTHUMOS <ref type="bibr" target="#b38">[39]</ref> is an extension of the THU-MOS <ref type="bibr" target="#b13">[14]</ref> dataset with the untrimmed videos densely annotated for 65 different action classes. It provides frame-level action annotations for 30 hours of video across 413 videos, split as 200 for training and 213 for validation. On average, it contains 1.5 labels per frame and 10.5 action classes per video. It contains significantly smaller number of videos compared to Charades <ref type="bibr" target="#b28">[29]</ref> and each video has a larger temporal length, which make the training difficult. We created a segmented version of this data, where each clip is limited to a maximum of 1280 frames, which gives a similar evaluation setting to Charades. For the computational efficiency, we use the temporal striding of 10.</p><p>Training: We follow a training process similar to what we did for Charades. We follow two stages of training with our Coarse and Fine streams pretrained on Kinetics-400 <ref type="bibr" target="#b15">[16]</ref>, i.e., separately and combined. We use the same hyperparameter settings and training schedules as in Charades (refer subsection 4.1). We use a dropout rate of 0.5 before the logits layer. The logits go through a sigmoid to make multi-label predictions for each frame. We use an average of classification and localization loss for training.</p><p>Inference: At inference, we make predictions for every frame. Even though our input is sampled at a stride of 10, we consider the labels for all frames (at a stride of 1) and interpolate the logits to fit the original temporal length. Each input is center-cropped to 224Ã224. We report the performance (mAP), compute requirement to process an input clip of 1024Ã10 frames as TFLOPs (Ã10 9 ) and the number of learnable parameters(M). The length of 1024Ã10 frames is only considered as a reference for reporting the complexity values, and there are longer clips in the dataset with up to Ã5 frames. We process these frames fully convolutionally.</p><p>Results: We show the performance (mAP) of Coarse-Fine Networks on MultiTHUMOS <ref type="bibr" target="#b38">[39]</ref> activity detection with the corresponding compute requirement (TFLOPs, i.e., Ã10 12 ) in <ref type="figure" target="#fig_6">Fig. 5</ref>. We observe an improvement of +4.63% from X3D <ref type="bibr" target="#b4">[5]</ref> to Coarse-Fine. While our Coarse-Fine network is almost 75 times faster than the previous model (0.49 TFLOPS (Coarse-Fine) vs. <ref type="bibr" target="#b34">35</ref>.57 TFLOPS (I3D + TGM)), it still achieves comparable performance to the previous state-of-the-art. Models using the X3D backbone including ours lose motion details due to the aggressive 1/10 striding compared to I3D [2] that doesn't do striding, making them less effective when combined with other temporal modeling methods (e.g., TGM <ref type="bibr" target="#b22">[23]</ref>). Still, our Corase-Fine Networks were able to overcome such limitation and perform comparably. Coarse-Fine /w TGM shows a further improvement of +2.21%mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented Coarse-Fine Networks, which is a twostream architecture to combine temporally Coarse representations with Fine representations. We introduced the approach of temporal Grid Pooling that learns to differentiably select informative frames while discarding the other, to obtain Coarse representations. We also introduced the Multi-stage Fusion to best combine the Coarse stream with the Fine stream. We confirmed that the Coarse-Fine networks obtain the best known performance on Charades localization, while spending much less computation time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Î± = 1/4 and T = 128). These confidence values represent how informative each temporal interval with a size of 1/Î±</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Grid Pool layer learns a temporal downsampling operation based on non-uniform grid locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Multi-stage Fusion feeds multiple abstraction-levels of fine-grained context to the Coarse stream. First, Fine stream features get filtered through a self-attention mask. Then, these features get calibrated for each coarse frame, based on Gaussian weights centered at the corresponding coarse frame location. Finally, such calibrated features from multiple abstraction-levels get point-wise convolved to calculate the scale and shift features which provide an affine transformation to the coarse features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The network configuration used in each experiment is shown within the braces of each caption (Fine-Fine, Coarse only or Coarse-Fine). Fine-Fine Networks are used in Fusion experiments to decouple the effect of Grid Pool, and similarly, Coarse only Networks are used in Grid Pool experiments decouple the effect of Multi-stage Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Performance/complexity trade-off of state-of-the-art methods for activity detection on MultiTHUMOS<ref type="bibr" target="#b38">[39]</ref>. Our Coarse-Fine Networks /w TGM show a comparable performance with the state-of-the-arts with â¼75x speed, without using additional input modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fusion mask: The effect of using a selfattention mask to filter the fine-grained context (referFig. 4), with different fusion connections.</figDesc><table><row><cell cols="2">Fusion location</cell><cell cols="3">mAP (%) GFLOPs</cell><cell cols="4">Fusion dimensions mAP (%) GFLOPs</cell><cell>Fusion loc. Fusion mask mAP (%) GFLOPs</cell></row><row><cell cols="4">late only late+intermid one-to-one 22.50 21.84</cell><cell>77.15 81.80</cell><cell cols="2">C CHW</cell><cell>18.11 19.86</cell><cell>76.45 93.12</cell><cell>late</cell><cell>none self-attention 21.84 20.59</cell><cell>75.98 77.15</cell></row><row><cell cols="4">late+intermid multi-stage 22.65</cell><cell>94.80</cell><cell cols="2">CTHW</cell><cell>22.65</cell><cell>94.80</cell><cell>multi-stage</cell><cell>none self-attention 22.65 21.42</cell><cell>92.69 94.80</cell></row><row><cell cols="5">(a) Fusion location: Using the fusion connec-tions only before the logits, in-between each res block w/ or w/o considering multiple abstraction-levels at each fusion location. (Fine-Fine)</cell><cell cols="4">(b) Fusion dimensions: The Dimensions of Multi-stage Fusion features. When tem-poral dimension (T) is available, we use Coarse-centric Gaussians for location cali-</cell><cell>(c) (Fine-Fine)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">bration. (Fine-Fine)</cell><cell></cell><cell></cell></row><row><cell cols="3">Pooling type mAP (%) GFLOPs</cell><cell></cell><cell cols="2">Grid Pool input</cell><cell cols="2">Î± mAP (%) GFLOPs</cell><cell></cell><cell>Two-stream Network</cell><cell>mAP (%) GFLOPs</cell></row><row><cell>Max Average</cell><cell>16.21 16.64</cell><cell>15.42 15.42</cell><cell></cell><cell cols="2">T=128, stride =10</cell><cell>1/4 18.12 1/8 11.88</cell><cell>16.53 10.43</cell><cell cols="2">SlowFast det SlowFast det (w/ Grid Pool)</cell><cell>20.61 20.82</cell><cell>54.31 55.42</cell></row><row><cell>Striding Grid Pool</cell><cell>17.49 18.12</cell><cell>15.42 16.53</cell><cell></cell><cell cols="2">T=256, stride = 5</cell><cell>1/4 18.16 1/8 15.56</cell><cell>32.82 20.63</cell><cell cols="2">SlowFast det (w/ Fusion) Coarse-Fine (w/ Grid Pool w/ Fusion) 23.24 22.79</cell><cell>72.16</cell></row><row><cell cols="3">(d) Pooling type: Different types of</cell><cell cols="5">(e) Grid Pool configuration: Variations of the</cell><cell></cell></row><row><cell cols="3">temporal pooling operations used af-</cell><cell cols="5">sampling rate Î± with different temporal sizes of</cell><cell></cell></row><row><cell cols="3">ter res 2 block. A temporal stride</cell><cell cols="5">the input at the Grid Pool layer. (Coarse-only)</cell><cell></cell></row><row><cell cols="3">of 4 (equivalent to Î± = 1/4) used</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">here. (Coarse-only)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported by the National Science Foundation (IIS-2104404 and CNS-2104416). The authors thank AJ Piergiovanni for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond fixed grid: Learning geometric image representation with a deformable grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchen</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked spatio-temporal graph convolutional networks for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallabi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action genome: Actions as compositions of spatiotemporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10236" to="10247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attend and interact: Higherorder object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning on visual-symbolic graphs for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effrosyni</forename><surname>Mavroudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BÃ©jar</forename><surname>BenjamÃ­n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RenÃ©</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning latent superevents to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliencybased sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AssembleNet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GÃ¼l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GÃ¼l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Liteeval: A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7780" to="7789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6578" to="6587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
