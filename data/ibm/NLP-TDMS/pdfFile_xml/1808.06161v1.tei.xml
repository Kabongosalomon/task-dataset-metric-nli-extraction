<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since 1665, over 50 million scholarly research articles have been published <ref type="bibr" target="#b11">(Jinha, 2010)</ref>, with approximately 2.5 million new scientific papers coming out each year <ref type="bibr" target="#b31">(Ware and Mabe, 2015)</ref>. While this enormous corpus provides us with the ability to conclusively accept or reject hypotheses and yields insight into promising research directions, it is getting harder and harder to extract useful information from the literature in an efficient and timely manner due to its sheer amount. Therefore, an automatic and intelligent tool to help users locate the information of interest quickly and comprehensively is highly desired.</p><p>When searching for relevant literature for a certain field, investigators first check the abstracts of scientific papers to see whether they match the criterion of interest. This process can be expedited if the abstracts are structured; that is, if the rhetorical structural elements of scientific abstracts such as purpose, methods, results, and conclusions (American National Standards <ref type="bibr">Institute, 1979</ref>) are explicitly stated. However, even today, a significant portion of scientific abstracts is still unstructured, which causes great difficulty in information retrieval. In this paper, we develop a machine-learning based approach to automatically categorize sentences in scientific abstracts into rhetorical sections so that the desired information can be efficiently retrieved.</p><p>In a scientific abstract, each sentence can be assigned to a rhetorical structural element sequentially. This rhetorical structure profiling process can be formulated as a sequential sentence classification task, as the element assignment of any single sentence is greatly associated with the assignments of the surrounding sentences. This is in contrast to the general sentence classification problem, where each sentence is classified individually and no contextual information can be used. Previous state-of-the-art methods relied on Conditional Random Fields (CRFs) to take into account the inter-dependence between subsequent labels, which improved joint sentence classification performance by considering the label sequence information. In this work, we add a bi-directional long short-term memory (bi-LSTM) layer over the representations of individual sentences so that it can encode the contextual content and semantics from preceding and succeeding sentences for better categorical inference of the current one.</p><p>In this work, we present a hierarchical neural network model for the sequential sentence classification task, which we call a hierarchical sequential labeling network (HSLN). Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings, then uses another bi-LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation, subsequently uses a single-hidden-layer feed-forward network to transform the sentence representation to the probability vector, and finally optimizes the predicted label sequence jointly via a CRF layer. We evaluate our model on two benchmarking datasets, PubMed RCT (Dernoncourt and <ref type="bibr" target="#b5">Lee, 2017)</ref> and NICTA-PIBOSO <ref type="bibr" target="#b13">(Kim et al., 2011)</ref>, which were both generated from the PubMed database 1 . Our key contributions are summarized as follows:</p><p>1. Based on the previous best performing architecture for sequential sentence classification , we add one more layer to extract contextual information from surrounding sentences for more accurate prediction of the current one. Together with the CRF algorithm, this allows us to make use of not only the preceding labels' information but also the content and semantics of adjacent sentences to infer the label of the target sentence.</p><p>2. We remove the need for a character-based word embedding component without sacrificing performance. For individual sentence encoding, we propose the use of a CNN module as an alternative to RNN for small datasets, suffering less from over-fitting as evidenced by our experiments. Moreover, we incorporate attention-based pooling in both RNN and CNN models to further improve the performance.</p><p>3. We adopt dropout with expectation-linear regularization instead of the standard one to reduce the performance gap between training and test phases.</p><p>4. We obtain state-of-the-art results on two datasets for sequential sentence classification in medical abstracts, outperforming the previous best models by at least 2% in terms of F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous systems for sequential sentence classification concentrate on the rhetorical structure analysis of biomedical abstracts. They are mainly based on naive Bayes <ref type="bibr" target="#b29">(Ruch et al., 2007)</ref>, support vector machine (SVM) <ref type="bibr" target="#b25">(McKnight and Srinivasan, 2003;</ref><ref type="bibr" target="#b33">Yamamoto and Takagi, 2005;</ref><ref type="bibr" target="#b21">Liu et al., 2013)</ref>, Hidden Markov Model (HMM) (Lin  <ref type="bibr">, 2006)</ref>, and CRF <ref type="bibr" target="#b13">(Kim et al., 2011;</ref><ref type="bibr" target="#b8">Hassanzadeh et al., 2014;</ref><ref type="bibr" target="#b9">Hirohata et al., 2008;</ref><ref type="bibr" target="#b3">Chung, 2009</ref>). All these methods heavily rely on numerous carefully hand-engineered features such as lexical (bag-of-words (BOW)), semantic (hypernyms, synonyms), structural (part of speech (POS) tags, lemmas, orthographic shapes, headings), statistical (statistical distributions of token types) and sequential (sentence position, surrounding features, predicted labels) features. In contrast, current emerging artificial neural network (ANN) based models have removed the need for manually selected features; instead, features are self-learned from the token and/or character embeddings. These deep learning models have revolutionized the natural language processing (NLP) field with state-of-the-art results achieved in various tasks, including the most relevant text classification task <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b34">Zhang et al., 2016;</ref><ref type="bibr" target="#b4">Conneau et al., 2017;</ref><ref type="bibr" target="#b16">Lai et al., 2015;</ref><ref type="bibr" target="#b23">Ma et al., 2015)</ref>. Most of these models are built upon deep CNNs or RNNs as well as combinations of them, where CNN is good at extracting local n-gram features while RNN is suitable for sequence modeling.</p><p>The above-mentioned works for short-text classification do not consider any context of sentence semantics in the models, making them underperform in the sequential sentence classification scenario, where surrounding sentences can play a big role in inferring the label of the current sentence. Recent works that apply deep neural networks to the sequential sentence classification problem include the system proposed by , where the preceding utterances were used to help classify the current utterance in a dialog into the corresponding dialogue act. Most recent work from Dernoncourt et al.  used a CRF layer to optimize the predicted label sequence, where the preceding labels have influence on determining the current label. This model outperformed the state-of-the-art results on two datasets PubMed RCT and NICTA-PIBOSO for sentence classification in medical abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>Notation We denote scalars in italic lowercase (e.g., k), vectors in bold italic lowercase (e.g., s) and matrices in italic uppercase (e.g., W ). Colon notations x i:j and s i:j are used to denote the se-quence of scalars (x i , x i+1 , ..., x j ) and vectors (s i , s i+1 , ..., s j ).</p><p>Our model is composed of four components: the word embedding layer, the sentence encoding layer, the context enriching layer, and the label sequence optimization layer. In the following sections they will be discussed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embedding Layer</head><p>Given a sentence w = w 1 w 2 · · · w N comprising N words, this layer maps each word to a real-valued vector as its lexical-semantic representation. Word representations are encoded by the column vector in the embedding matrix W word ∈ R d w ×|V | , where d w is the dimension of the word vector and V is the vocabulary of the dataset. Each column W word i ∈ R d w is the word embedding vector for the i th word in the vocabulary. The word embeddings W word can be pre-trained on large unlabeled datasets using unsupervised algorithms such as word2vec <ref type="bibr" target="#b26">(Mikolov et al., 2013)</ref>, GloVe <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> and fastText .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Encoding Layer</head><p>This layer takes as input the embedding vector of each token in a sentence from the word embedding layer and produces a vector s to encode this sentence. The sequence of embedding vectors is first processed by a bi-directional RNN (bi-RNN) or CNN layer, similar to the ones used in the text classification before <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b20">Liu et al., 2016)</ref>. This layer outputs a sequence of hidden states h 1:N (h ∈ R d hs ) for a sentence of N words with each hidden state corresponding to a word. To form the final representation vector s of this sentence, attention-based pooling is used, which can be described using the following equations:</p><formula xml:id="formula_0">A = softmax(U s tanh(W s H + b s )), (1) S = AH T ,<label>(2)</label></formula><p>where H = h 1 h 2 · · · h N ∈ R d hs ×N , W s ∈ R d a ×d hs is the transformation matrix for soft alignment, b s ∈ R d a is the bias vector, U s ∈ R r×d a is the token level context matrix used to measure the relevance or importance of each token with respect to the whole sentence, softmax is performed along the second dimension of its input matrix, and A ∈ R r×N is the attention matrix.</p><p>Here each row of U s is a context vector u s ∈ R d a and it is expected to reflect an aspect or component of the semantics of a sentence. To represent the overall semantics of the sentence, we use multiple context vectors to focus on different parts of this sentence.</p><p>Finally, the sentence encoding vector s ∈ R rd hs is obtained by reshaping the matrix S into a vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Context Enriching Layer</head><p>This layer takes as input the sequence of individual sentence encoding vectors in a given abstract of n sentences obtained from the last sentence encoding layer, with each vector corresponding to a sentence. It outputs a new sequence of contextualized sentence encoding vectors, which are enriched with the contextual information from surrounding sentences. Specifically, the sequence of individual sentence encoding vectors is input into a bi-LSTM layer, which produces a sequence of hidden state vectors h 1:n (h ∈ R d hd ) with each corresponding to a sentence. Each of these vectors is subsequently input to a feed-forward neural network with only one hidden layer to get the corresponding probability vector r ∈ R l , which represents the probability that this sentence belongs to each label, where l is the number of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Label Sequence Optimization Layer</head><p>Within the abstract, the sequence of sentence categories implicitly follows some patterns. For example, the category Results is always followed by Conclusion, and the category Methods is certainly after the Background. Making use of such patterns can boost the classification performance via the CRF algorithm <ref type="bibr" target="#b17">(Lample et al., 2016)</ref>. Given the sequence of probability vectors r 1:n from the last context enriching layer for an abstract of n sentences, this layer outputs a sequence of labels y 1:n , where y i represents the predicted label assigned to the i th sentence.</p><p>In the CRF algorithm, in order to model dependencies between subsequent labels, we incorporate a matrix T that contains the transition probabilities between two subsequent labels; we define T [i, j] as the probability that a token with label i is followed by a token with the label j. The score of a label sequence y 1:n is defined as the sum of the probabilities of individual labels and the transition probabilities:</p><formula xml:id="formula_1">s(y 1:n ) = n i=1 r i (y i ) + n i=2 T [y i−1 , y i ].</formula><p>(3)</p><p>The score in the above equation can be transformed into the probability of a certain label sequence by taking a softmax operation over all possible label sequences:</p><formula xml:id="formula_2">p(y 1:n ) = e s(y 1:n ) ŷ 1:n ∈Y e s(ŷ 1:n ) ,<label>(4)</label></formula><p>where Y denotes the set of all possible label sequences. During the training phase, the objective is to maximize the probability of the gold label sequence. In the testing phase, given an input sequence, the corresponding sequence of predicted labels is chosen as the one that maximizes the score, computed via the Viterbi algorithm <ref type="bibr" target="#b7">(Forney, 1973)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our model on two sources of benchmarking datasets on medical scientific abstracts, where each sentence of the abstract is annotated with one label that is associated with the rhetorical structure. <ref type="table" target="#tab_2">Table 1</ref> summarizes the statistics of the two datasets.</p><p>NICTA-PIBOSO This dataset 2 was shared from the ALTA 2012 Shared Task <ref type="bibr" target="#b1">(Amini et al., 2012)</ref>, the goal of which is to build automatic sentence classifiers that can map the sentences from biomedical abstracts into a set of pre-defined categories for Evidence-Based Medicine (EBM).</p><p>PubMed RCT This new dataset was curated by <ref type="bibr" target="#b5">(Dernoncourt and Lee, 2017)</ref> 3 and is currently the largest dataset for sequential sentence classification. It is based on the PubMed database of biomedical literature and each sentence of each abstract is labeled with its role in the abstract using one of the following classes: background, objective, method, result, and conclusion. <ref type="table" target="#tab_3">Table  2</ref> presents an example abstract comprising structured sentences with their annotated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Settings</head><p>For both datasets, test performance is assessed on the training epoch with best validation performance and F1 scores (weighted average by support (the number of true instances for each label)) are reported as the results. The token embeddings were pre-trained on a large corpus combining Wikipedia, PubMed, and PMC texts (Moen and Ananiadou, 2013) using the word2vec tool 4 (denoted as "Word2vec-wiki+P.M."). They are fixed during the training phase to avoid over-fitting.</p><p>We also tried other types of word embeddings, such as the word2vec embeddings pre-trained on the Google News dataset 5 (denoted as "Word2vec-News"), word2vec embeddings pre-trained on the Wikipedia corpus 6 (denoted as "Word2vecwiki"), GloVe embeddings pre-trained on the cor-  .] Yet, attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition, but not in the sad mood condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>The current findings show that self-reported emotional eating (based on the DEBQ) might not validly predict who overeats when sad, at least not in a laboratory setting with healthy women. [...] pus of Wikipedia 2014 + Gigaword 5 7 (denoted as "Glove-wiki"), fastText embeddings pre-trained on Wikipedia 8 (denoted as "FastText-wiki"), and fastText embeddings initialized with the standard GloVe Common Crawl embeddings and then finetuned on PubMed abstracts plus MIMIC-III notes (denoted as "FastText-P.M.+MIMIC"). The comparison results are summarized in the next section. The model is trained using the Adam optimization method (Kingma and Ba, 2014). The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch. For regularization, dropout (Srivastava et al., 2014) is applied to each layer. For the version of dropout used in practice (e.g., the dropout function implemented in the TensorFlow and Pytorch libraries), the model ensemble generated by dropout in the training phase is approximated by a single model with scaled weights in the inference phase, resulting in a gap between training and inference. To reduce this gap, we adopted the dropout with expectation-linear regularization introduced by <ref type="bibr" target="#b24">Ma et al. (2016)</ref> to explicitly control the inference gap and thus improve the generaliza-tion performance.</p><p>Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in <ref type="table" target="#tab_5">Table 3</ref>. The window sizes of the CNN encoder in the sentence encoding layer are 2, 3, 4 and 5. The RNN encoder in the sentence encoding layer is set as LSTM for the PubMed datasets and gated recurrent unit (GRU) for the NICTA-PIBOSO dataset. Code for this work is available online 9 . <ref type="table" target="#tab_4">Table 4</ref> compares our model against the best performing models in the literature <ref type="bibr" target="#b21">Liu et al., 2013)</ref>. There are two variants of our model in terms of different implementations of the sentence encoding layer: the model that uses bi-RNN to encode the sentence is called HSLN-RNN; while the model that uses the CNN module is named HSLN-CNN. We have evaluated both model variants on all datasets. And as evidenced by    <ref type="table">Table 5</ref> presents the ablation analysis of our model (on the PubMed 20k dataset), where we remove one component at a time and quantify the performance drop (reported on F1 scores). As can be seen from <ref type="table">Table 5</ref>, our HSLN-CNN model uni-formly suffers a little more from the component removal than the HSLN-RNN model, indicating that the HSLN-RNN model is more robust. When the context enriching layer is removed, both models experience the most significant performance drop and can only be on par with the previous stateof-the-art results, strongly demonstrating that this proposed component is the key to the performance improvement of our model. Furthermore, even without the label sequence optimization layer, our model still significantly outperforms the best published methods that are empowered by this layer, indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences. Last but not the least, the dropout regularization and attention-based pooling components we add to our system can help further improve the model in a limited extent.  <ref type="table">Table 5</ref>: Ablation analysis. F1 scores are reported. "− context" is our model without the context enriching layer. "− seq. opt." is our model without the label sequence optimization layer. "− dropout reg.' is our model using the standard dropout strategy without the expectation-linearization regularization. "− attention" refers to the model without attention-based pooling, i.e., in the sentence encoding layer, the final hidden state is used for the HSLN-RNN model while maxpooling is used for the HSLN-CNN model. <ref type="table" target="#tab_9">Table 6</ref> and 7 detail the results of classification for each label in terms of performance scores (precision, recall and F1) and confusion matrix, respectively (for our HSLN-RNN model trained on the PubMed 20k dataset). These show that the classifier is very good at predicting the labels Methods, Results and Conclusions, whereas the greatest difficulty the classifier has is in distinguishing Background sections from Objectives sections. One fifth of Background sentences are incorrectly classified as Objectives, while around one forth of Objectives sentences are wrongly assigned to the label of Background. We conjecture this difficulty mainly comes from the fact that the difference between Background and Ob-jectives sentences in terms of writing style is less obvious compared with the other sections of the abstract. Moreover, our model has some difficulty in telling Methods sentences apart from Results sentences.    <ref type="table">Table 8</ref> presents a few examples of prediction errors that are produced by our HSLN-RNN model trained on the PubMed 20k dataset. This error analysis suggests that one of the biggest model error sources could be from the debatable gold standard labels of the dataset. For example, the sentence "Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14% in the general population." is indeed introducing the background of the problem (depressive disorders) on which this article is going to focus; however, the gold label classifies it into the Objective category. For another instance, the sentence "A post hoc analysis was conducted with the use of data from the evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness (escape)." belongs to the Result label according to the gold standard, but it makes more sense that it should be classified as a Method label. <ref type="figure" target="#fig_1">Figure 2</ref> presents an example of the transition matrix after the HSLN-RNN model has been trained on the PubMed 20k dataset, which encodes the transition probability between two subsequent labels. It effectively reflects what label is the most likely one that follows the current one. For example, by comparing the transition scores in the Result row in <ref type="figure" target="#fig_1">Figure 2</ref>, we can conclude that a sentence pertaining to the Result is typically followed by a sentence pertaining to the Conclusion and is unlikely to be followed by a sentence in the Background category (transition scores of 2.48 vs -5.46), which makes sense. From this transition matrix, we can figure out the most probable label sequence: Background → Objective → M ethod → Result → Conclusion, which is also consistent with our expectations. In order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as our locally curated word embeddings, to initialize our model. <ref type="table" target="#tab_11">Table 9</ref> gives the performance of six different word embeddings for our HSLN-RNN model trained on the PubMed 20k dataset. According to  <ref type="table">Table 8</ref>: Examples of prediction errors of our HSLN-RNN model trained on the PubMed 20k dataset. Each sentence is followed by the PMID of the abstract that this sentence belongs to, which is enclosed in middle brackets. The "Predicted" column indicates the label predicted by our model for a given sentence. The "Gold" column indicates the gold label of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>are using for evaluation is also from PubMed abstracts, using only the PubMed abstracts together with MIMIC notes without the Wikipedia corpus does not guarantee better result (see the "FastText-P.M.+MIMIC" embeddings in <ref type="table" target="#tab_11">Table 9</ref>), which may be because the corpus size of PubMed abstracts plus MIMIC notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the Wikipedia.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have presented an ANN based hierarchical sequential labeling network to classify sentences that appear sequentially in text. We demonstrate that incorporating the contextual information from surrounding sentences to help classify the current one by using an LSTM layer to sequentially process the encoded sentence representations can improve the overall quality of predictions. Our model outperforms the state-of-theart results by 2%-3% on two datasets for sequential sentence classification in medical abstracts. We expect that our proposed model can be generalized to any problem that is related to sequential sentence classification, such as the paragraph-level sequential sentence categorization in full-text articles for better text mining and document retrieval <ref type="bibr" target="#b32">(Westergaard et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>Although the whole PubMed database contains over 2 million abstracts with part of them accompanied by full-text articles, only a small fraction of them are structured and contain the label information utilized in this work. We plan to make use of the rest unannotated abstracts or full texts to pre-train our model and then fine tune it to the target annotated datasets inspired by the work from <ref type="bibr" target="#b10">(Howard and Ruder, 2018)</ref> so that the performance can be further boosted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architecture. w: original word; e: word embedding vector; h: sentence-level hidden state output by the bi-RNN or CNN layer; s: sentence representation vector; h : abstract-level hidden state output by the bi-LSTM layer; r: sentence label probability vector; y: predicted sentence label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Transition matrix of label sequence after the HSLN-RNN model has been trained on the PubMed 20k dataset. The rows represent the label of the previous sentence, while the columns represent the label of the current sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics. |C| denotes the number of labels, |V | represents the vocabulary size. For the train, validation, and test sets, we indicate the number of abstracts followed by the number of sentences in parentheses.</figDesc><table><row><cell>Category</cell><cell>Sentences</cell></row><row><cell>BACKGROUND</cell><cell>Emotional eating is associated with overeating and the development of obesity. [...]</cell></row><row><cell></cell><cell>The aim of this study was to test if attention bias for food moderates the effect</cell></row><row><cell>OBJECTIVES</cell><cell>of self-reported emotional eating during sad mood (vs neutral mood) on actual</cell></row><row><cell></cell><cell>food intake. [...]</cell></row><row><cell>METHODS</cell><cell>Participants (N = 85) were randomly assigned to one of the two experimental mood induction conditions (sad/neutral). [...]</cell></row><row><cell></cell><cell>[..</cell></row><row><cell>RESULTS</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>A typical abstract example with structured sentences and their corresponding annotated labels. The PMID of this abstract is 24854809.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>, our best model can improve the F1 scores by 2%-3% in absolute number compared with the previous best published results for all</figDesc><table><row><cell>Parameter</cell><cell cols="4">PubMed RNN CNN RNN CNN NICTA</cell></row><row><cell>d hs</cell><cell>200</cell><cell>-</cell><cell>200</cell><cell>-</cell></row><row><cell>d hd</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>300</cell></row><row><cell>d a</cell><cell>200</cell><cell>100</cell><cell>250</cell><cell>75</cell></row><row><cell>d c</cell><cell>-</cell><cell>200</cell><cell>-</cell><cell>150</cell></row><row><cell>r</cell><cell>15</cell><cell>1</cell><cell>5</cell><cell>4</cell></row><row><cell>β</cell><cell cols="4">0.01 0.001 0.01 0.01</cell></row><row><cell>dr</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6</cell><cell>0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameter settings. d hs : hidden size of the sentence-level RNN layer (single direction); d hd : hidden size of the abstract-level bi-LSTM layer (single direction); d a : dimension of the context vector u s ; r: number of context vectors; β: coefficient of the dropout regularization added to the total loss; dr: dropout.</figDesc><table><row><cell cols="3">datasets. For the PubMed 20k and 200k datasets,</cell></row><row><cell cols="3">our HSLN-RNN model achieves better results;</cell></row><row><cell cols="3">however, for the NICTA dataset, the HSLN-CNN</cell></row><row><cell cols="3">model performs better. This makes sense because</cell></row><row><cell cols="3">the CNN sentence encoder has fewer parameters</cell></row><row><cell cols="3">to be optimized, thus the HSLN-CNN model is</cell></row><row><cell cols="3">less likely to over-fit in a smaller dataset such</cell></row><row><cell cols="3">as NICTA. With sufficient data, however, the in-</cell></row><row><cell cols="3">creased capacity of the HSLN-RNN model offers</cell></row><row><cell cols="3">performance benefits. To be noted, this perfor-</cell></row><row><cell cols="3">mance gap between RNN and CNN sentence en-</cell></row><row><cell cols="3">coder gets larger as the dataset size increases from</cell></row><row><cell cols="2">20k to 200k for the PubMed dataset.</cell><cell></cell></row><row><cell>Model</cell><cell>PubMed 20k 200k</cell><cell>NICTA</cell></row><row><cell>Best Published</cell><cell></cell><cell></cell></row><row><cell>Marco Lui (Lui, 2012) bi-ANN (Dernoncourt et al., 2016)</cell><cell cols="2">-90.0 91.6 82.7 -82.0</cell></row><row><cell>Our Models</cell><cell></cell><cell></cell></row><row><cell>HSLN-CNN</cell><cell cols="2">92.2 92.8 84.7</cell></row><row><cell>HSLN-RNN</cell><cell cols="2">92.6 93.9 84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of F1 scores (weighted average by support (the number of true instances for each label)) between our model and the best published methods. The presented results of our model are evaluated on the test set of the run with the highest F1 score on the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Results (presented in percentage) in terms of</cell></row><row><cell cols="6">precision (P), recall (R) and F-measure (F1) on the test</cell></row><row><cell cols="6">set for each label obtained by our HSLN-RNN model</cell></row><row><cell cols="4">on the PubMed 20k dataset.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>B</cell><cell>C</cell><cell>M</cell><cell>O</cell><cell>R</cell></row><row><cell cols="2">B 2460</cell><cell>4</cell><cell>69</cell><cell>537</cell><cell>7</cell></row><row><cell>C</cell><cell>4</cell><cell>4413</cell><cell>11</cell><cell>1</cell><cell>142</cell></row><row><cell>M</cell><cell>37</cell><cell>11</cell><cell>9657</cell><cell>27</cell><cell>152</cell></row><row><cell>O</cell><cell>632</cell><cell>0</cell><cell>68</cell><cell>1630</cell><cell>3</cell></row><row><cell>R</cell><cell>2</cell><cell>95</cell><cell>362</cell><cell>1</cell><cell>9253</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Confusion matrix obtained by our model on</cell></row><row><cell>the PubMed 20k dataset. Rows correspond to predicted</cell></row><row><cell>labels, and columns correspond to true labels. B rep-</cell></row><row><cell>resents background, O represents objectives, M repre-</cell></row><row><cell>sents methods, R represents results, and C represents</cell></row><row><cell>conclusions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does. The combination of Wikipedia and PubMed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the Wikipedia corpus or the PubMed abstracts performs much worse. Although the dataset weSentence PredictedGold Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14% in the general population.[25829103]    Background Objective This study assessed whether diets with different fat quality and supplementation with coenzyme Q10 (CoQ) affect the metabolomic profile in urine.[24986061]    </figDesc><table><row><cell>Objective Background</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison of performance with different choices of word embeddings for our HSLN-RNN model trained on the PubMed 20k dataset (reported on F1-scores on the test set). "P.M." means PubMed.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://nlp.stanford.edu/data/glove.6B.zip 8 https://github.com/facebookresearch/fastText/blob/master/ pretrained-vectors.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/jind11/HSLN-Joint-Sentence-Classification</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by funding grant U54-HG007963 from National Human Genome Research Institute (NHGRI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">American national standard for writing abstracts (ansi z39</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="14" to="1979" />
		</imprint>
	</monogr>
	<note>American National Standards Institute</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Overview of the alta 2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Molla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sentence retrieval for abstracts of randomized controlled trials. BMC medical informatics and decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural networks for joint sentence classification in medical paper abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05251</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Viterbi algorithm. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying scientific artefacts in biomedical literature: The evidence based medicine use case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Groza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="159" to="170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying sections in scientific abstracts using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hirohata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Article 50 million: an estimate of the number of scholarly articles in existence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learned Publishing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="258" to="263" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic classification of sentences to support evidence based medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yencken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03827</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative content models for structural analysis of medical abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damianos</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis</title>
		<meeting>the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstract sentence classification for scientific papers based on transductive svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer and Information Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature stacking for sentence classification in evidence-based medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="134" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01839</idno>
		<title level="m">Dependency-based convolutional neural networks for sentence embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingkai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08017</idno>
		<title level="m">Dropout with expectation-linear regularization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Categorization of sentence types in medical abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">440</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Languages in Biology and Medicine</title>
		<editor>SPFGH Moen and Tapio Salakoski2 Sophia Ananiadou</editor>
		<meeting>the 5th International Symposium on Languages in Biology and Medicine<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using argumentation to extract key sentences from biomedical abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celia</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Chichester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imad</forename><surname>Tbahriti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Geissbühler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Violaine</forename><surname>Pillet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lovis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of medical informatics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The STM report: An overview of scientific and scholarly journal publishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comprehensive and quantitative comparison of text-mining in 15 million full-text articles versus their corresponding abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westergaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Henrik</forename><surname>Staerfeldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tønsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Brunak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1005962</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sentence classification system for multi biomedical literature summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Engineering Workshops, 2005. 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1163" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dependency sensitive convolutional neural networks for modeling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02361</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
