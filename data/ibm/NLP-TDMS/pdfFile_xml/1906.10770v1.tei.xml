<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Modular Co-Attention Networks for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="department">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Modular Co-Attention Networks for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual Question Answering (VQA) requires a finegrained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effective 'co-attention' model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness.</p><p>Experimental results demonstrate that MCAN significantly outperforms the previous state-ofthe-art. Our best single model delivers 70.63% overall accuracy on the test-dev set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimodal learning to bridge vision and language has gained broad interest from both the computer vision and natural language processing communities. Significant progress has been made in many vision-language tasks, including image-text matching <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref>, visual captioning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1]</ref>, visual grounding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> and visual question answering (VQA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>. Compared to other multimodal learning tasks, VQA is a more challenging task * Jun Yu is the corresponding author <ref type="figure">Figure 1</ref>: Accuracies vs. co-attention depth on VQA-v2 val split. We list most of the state-of-the-art approaches with (deep) co-attention models. Except for DCN <ref type="bibr" target="#b23">[24]</ref> which uses the convolutional visual features and thus leads to inferior performance, all the compared methods (i.e., MCAN, BAN <ref type="bibr" target="#b13">[14]</ref> and MFH <ref type="bibr" target="#b32">[33]</ref>) use the same bottom-up attention visual features to represent images <ref type="bibr" target="#b0">[1]</ref>. that requires fine-grained semantic understanding of both the image and the question, together with visual reasoning to predict an accurate answer.</p><p>The attention mechanism is a recent advance in deep neural networks, that has successfully been applied to the unimodal tasks (e.g., vision <ref type="bibr" target="#b21">[22]</ref>, language <ref type="bibr" target="#b3">[4]</ref>, and speech <ref type="bibr" target="#b7">[8]</ref>), as well as the aforementioned multimodal tasks. The idea of learning visual attention on image regions from the input question in VQA was first proposed by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, and it becomes a de-facto component of almost all VQA approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref>. Along with visual attention, learning textual attention on the question key words is also very important. Recent works have shown that simultaneously learning co-attention for the visual and textual modalities can benefit the fine-grained representation of the image and question, leading to more accurate prediction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>. However, these co-attention models learn the coarse interactions of multimodal instances, and the learned coattention cannot infer the correlation between each image region and each question word. This results in a significant limitation of these co-attention models.</p><p>To overcome the problem of insufficient multimodal interactions, two dense co-attention models BAN <ref type="bibr" target="#b13">[14]</ref> and DCN <ref type="bibr" target="#b23">[24]</ref> have been proposed to model dense interactions between any image region and any question word. The dense co-attention mechanism facilitates the understanding of image-question relationship to correctly answer questions. Interestingly, both of these dense co-attention models can be cascaded in depth, form deep co-attention models that support more complex visual reasoning, thereby potentially improving VQA performance. However, these deep models shows little improvement over their corresponding shallow counterparts or the coarse co-attention model MFH <ref type="bibr" target="#b32">[33]</ref> (see <ref type="figure">Figure 1</ref>). We think the bottleneck in these deep co-attention models is a deficiency of simultaneously modeling dense self-attention within each modality (i.e., word-to-word relationship for questions, and region-toregion relationship for images).</p><p>Inspired by the Transformer model in machine translation <ref type="bibr" target="#b28">[29]</ref>, here we design two general attention units: a self-attention (SA) unit that can model the dense intramodal interactions (word-to-word or region-to-region); and a guided-attention (GA) unit to model the dense intermodal interactions (word-to-region). After that, by modular composition of the SA and GA units, we obtain different Modular Co-Attention (MCA) layers that can be cascaded in depth. Finally, we propose a deep Modular Co-Attention Network (MCAN) which consists of cascaded MCA layers. Results in <ref type="figure">Figure 1</ref> shows that a deep MCAN model significantly outperforms existing state-of-the-art co-attention models on the benchmark VQA-v2 dataset <ref type="bibr" target="#b10">[11]</ref>, which verifies the synergy of self-attention and guided-attention in co-attention learning, and also highlights the potential of deep reasoning. Furthermore, we find that modeling selfattention for image regions can greatly improve the object counting performance, which is challenging for VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review previous studies on VQA, especially those studies that introduce co-attention models. Visual Question Answering (VQA). VQA has been of increasing interest over the last few years. The multimodal fusion of global features are the most straightforward VQA solutions. The image and question are first represented as global features and then fused by a multimodal fusion model to predict the answer <ref type="bibr" target="#b36">[37]</ref>. Some approaches introduce a more complex model to learn better question representations with LSTM networks <ref type="bibr" target="#b1">[2]</ref>, or a better multimodal fusion model with residual networks <ref type="bibr" target="#b14">[15]</ref>.</p><p>One limitation of the aforementioned multimodal fusion models is that the global feature representation of an image may lose critical information to correctly answer the questions about local image regions (e.g., "what is in the woman's left hand"). Therefore, recent approaches have introduced the visual attention mechanism into VQA by adaptively learning the attended image features for a given question, and then performing multimodal feature fusion to obtain the accurate prediction. Chen et al. proposed a question-guided attention map that projected the question embeddings into the visual space and formulated a configurable convolutional kernel to search the image attention region <ref type="bibr" target="#b6">[7]</ref>. Yang et al. proposed a stacked attention network to learn the attention iteratively <ref type="bibr" target="#b30">[31]</ref>. Fukui et al. <ref type="bibr" target="#b9">[10]</ref>, Kim et al. <ref type="bibr" target="#b15">[16]</ref>, Yu et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and Ben et al. <ref type="bibr" target="#b5">[6]</ref> exploited different multimodal bilinear pooling methods to integrate the visual features from the image's spatial grids with the textual features from the questions to predict the attention. Anderson et al. introduced a bottom-up and top-down attention mechanism to learn the attention on candidate objects rather than spatial grids <ref type="bibr" target="#b0">[1]</ref>. Co-Attention Models. Beyond understanding the visual contents of the image, VQA also requires to fully understand the semantics of the natural language question. Therefore, it is necessary to learn the textual attention for the question and the visual attention for the image simultaneously. Lu et al. proposed a co-attention learning framework to alternately learn the image attention and question attention <ref type="bibr" target="#b19">[20]</ref>. Yu et al. reduced the co-attention method into two steps, self-attention for a question embedding and the question-conditioned attention for a visual embedding <ref type="bibr" target="#b32">[33]</ref>. Nam et al. proposed a multi-stage coattention learning model to refine the attentions based on memory of previous attentions <ref type="bibr" target="#b22">[23]</ref>. However, these coattention models learn separate attention distributions for each modality (image or question), and neglect the dense interaction between each question word and each image region. This become a bottleneck for understanding finegrained relationships of multimodal features. To address this issue, dense co-attention models have been proposed, which establish the complete interaction between each question word and each image region <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref>. Compared to the previous co-attention models with coarse interactions, the dense co-attention models deliver significantly better VQA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modular Co-Attention Layer</head><p>Before presenting the Modular Co-Attention Network, we first introduce its basic component, the Modular Co-Attention (MCA) layer. The MCA layer is a modular composition of the two basic attention units, i.e., the selfattention (SA) unit and the guided-attention (GA) unit, inspired by the scaled dot-product attention proposed in  <ref type="figure">Figure 2</ref>: Two basic attention units with multi-head attention for different types of inputs. SA takes one group of input features X and output the attended features Z for X; GA takes two groups of input features X and Y and output the attended features Z for X guided by Y . <ref type="bibr" target="#b28">[29]</ref>. Using different combinations, we obtain three MCA variants with different motivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Attention and Guided-Attention Units</head><p>The input of scaled dot-product attention consists of queries and keys of dimension d key , and values of dimension d value . For simplicity, d key and d value are usually set to the same number d. We calculate the dot products of the query with all keys, divide each by √ d and apply a softmax function to obtain the attention weights on the values. Given a query q ∈ R 1×d , n key-value pairs (packed into a key matrix K ∈ R n×d and a value matrix V ∈ R n×d ), the attended feature f ∈ R 1×d is obtained by weighted summation over all values V with respect to the attention learned from q and K:</p><formula xml:id="formula_0">f = A(q, K, V ) = softmax( qK T √ d )V<label>(1)</label></formula><p>To further improve the representation capacity of the attended features, multi-head attention is introduced in <ref type="bibr" target="#b28">[29]</ref>, which consists of h paralleled 'heads'. Each head corresponds to an independent scaled dot-product attention function. The attended output features f is given by:</p><formula xml:id="formula_1">f = M A(q, K, V ) = [head 1 , head 2 , ..., head h ]W o (2) head j = A(qW Q j , KW K j , V W V j )<label>(3)</label></formula><p>where W Q j , W K j , W V j ∈ R d×d h are the projection matrices for the j-th head, and W o ∈ R h * d h ×d . d h is the dimensionality of the output features from each head. To prevent the multi-head attention model from becoming too large, we usually have d h = d/h. In practice, we can compute the attention function on a set of m queries Q = [q 1 ; q 2 ; ...; q m ] ∈ R m×d seamlessly by replacing q with Q in Eq. <ref type="formula">(2)</ref>, to obtain the attended output features F ∈ R m×d .</p><formula xml:id="formula_2">GA (Y) (X) (a) ID(Y)-GA(X,Y) SA GA (Y) (X) (b) SA(Y)-GA(X,Y) GA (Y) (X) SA SA (c) SA(Y)-SGA(X,Y)</formula><p>We build two attention units on top of the multihead attention to handle the multimodal input features for VQA, namely the self-attention (SA) unit and the guided-attention (GA) unit. The SA unit (see <ref type="figure">Figure 2a</ref>) is composed of a multi-head attention layer and a pointwise feed-forward layer. Taking one group of input features X = [x 1 ; ...; x m ] ∈ R m×dx , the multi-head attention learns the pairwise relationship between the paired sample &lt; x i , x j &gt; within X and outputs the attended output features Z ∈ R m×d by weighted summation of all the instances in X. The feed-forward layer takes the output features of the multi-head attention layer, and further transforms them through two fully-connected layers with ReLU activation and dropout (FC(4d)-ReLU-Dropout(0.1)-FC(d)). Moreover, residual connection <ref type="bibr" target="#b11">[12]</ref> followed by layer normalization <ref type="bibr" target="#b2">[3]</ref> is applied to the outputs of the two layers to facilitate optimization. The GA unit (see <ref type="figure">Figure  2b</ref>) takes two groups of input features X ∈ R m×dx and Y = [y 1 ; ...; y n ] ∈ R n×dy , where Y guides the attention learning for X. Note that the shapes of X and Y are flexible, so they can be used to represent the features for different modalities (e.g., questions and images). The GA unit models the pairwise relationship between the each paired sample &lt; x i , y j &gt; from X and Y , respectively.</p><p>Interpretation: Since the multi-head attention in Eq.(2) plays a key role in the two attention units, we take a closer look at it to see how it works with respect to different types of inputs. For a SA unit with input features X, for each x i ∈ X, its attended feature f i = MA(x i , X, X) can be understood as reconstructing x i by all the samples in X with respect to their normalized similarities to x i . Analogously, for a GA unit with input features X and Y , the attended feature  </p><formula xml:id="formula_3">f i = MA(x i , Y, Y ) for x i ∈ X is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modular Composition for VQA</head><p>Based on the two basic attention units in <ref type="figure">Figure 2</ref>, we composite them to obtain three modular co-attention (MCA) layers (see <ref type="figure" target="#fig_1">Figure 3</ref>) to handle the multimodal features for VQA. All three MCA layers can be cascaded in depth, such that the outputs of the previous MCA layer can be directly fed to the next MCA layer. This implies that the number of input features is equal to the number of output features without instance reduction.</p><p>The ID(Y)-GA(X,Y) layer in <ref type="figure" target="#fig_1">Figure 3a</ref> is our baseline. In ID(Y)-GA(X,Y), the input question features are directly passed through to the output features with an identity mapping, and the dense inter-modal interaction between each region x i ∈ X with each word y i ∈ Y is modeled in a GA(X,Y) unit. These interactions are further exploited to obtain the attended image features. Compared to the ID(Y)-GA(X,Y) layer, the SA(Y)-GA(X,Y) layer in <ref type="figure" target="#fig_1">Figure  3b</ref> adds a SA(Y) unit to model the dense intra-modal interaction between each question word pair {y i , y j } ∈ Y . The SA(Y)-SGA(X,Y) layer in <ref type="figure" target="#fig_1">Figure 3c</ref> continues to add a SA(X) unit to the SA(Y)-GA(X,Y) layer to model the intra-modal interaction between each image region pairs</p><formula xml:id="formula_4">{x i , x j } ∈ X. 1 .</formula><p>Note that the three MCA layers above have not covered all the possible compositions. We have also explored other MCA variants like the symmetric architectures GA(X,Y)-GA(Y,X) and SGA(X,Y)-SGA(Y,X). However, these MCA variants do not report comparative performance, so we do not discuss them further due to space limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modular Co-Attention Networks</head><p>In this section, we describe the Modular Co-Attention Networks (MCAN) architecture for VQA. We first explain the image and question feature representation from the input question and image. Then, we propose two deep co-attention models, namely stacking and encoder-decoder, which consists of multiple MCA layers cascaded in depth to gradually refine the attended image and question features. As we obtained the attended image and question features, we design a simple multimodal fusion model to fuse the multimodal features and finally feed them to a multi-label classifier to predict answer. An overview flowchart of MCAN is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>We name the MCAN model with the stacking strategy as MCAN sk -L and the MCAN model with the encoderdecoder strategy as MCAN ed -L, where L is the total number MCA layers cascaded in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Question and Image Representations</head><p>The input image is represented as a set of regional visual features in a bottom-up manner <ref type="bibr" target="#b0">[1]</ref>. These features are the intermediate features extracted from a Faster R-CNN model (with ResNet-101 as its backbone) <ref type="bibr" target="#b25">[26]</ref> pre-trained on the Visual Genome dataset <ref type="bibr" target="#b17">[18]</ref>. We set a confidence threshold to the probabilities of detected objects and obtain a dynamic number of objects m ∈ [10, 100]. For the i-th object, it is represented as a feature x i ∈ R dx by mean-pooling the convolutional feature from its detected region. Finally, the image is represented as a feature matrix X ∈ R m×dx .</p><p>The input question is first tokenized into words, and trimmed to a maximum of 14 words similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref>. Each word in the question is further transformed into a vector using the 300-D GloVe word embeddings <ref type="bibr" target="#b24">[25]</ref> pretrained on a large-scale corpus. This results in a sequence of words of size n×300, where n ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> is the number of words in the question. The word embeddings are then passed through a one-layer LSTM network <ref type="bibr" target="#b12">[13]</ref> with d y hidden units. In contrast to <ref type="bibr" target="#b27">[28]</ref> which only uses the final state (i.e., the output feature for the last word) as the question feature, we maintain the output features for all words and output a question feature matrix Y ∈ R n×dy .</p><p>To deal with the variable number of objects m and variable question length n, we use zero-padding to fill X and Y to their maximum sizes (i.e., m = 100 and n = 14, respectively). During training, we mask the padding logits with −∞ to get zero probability before every softmax layer to avoid the underflow problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Co-Attention Learning</head><p>Taking the aforementioned image features X and the question features Y as inputs, we perform deep co-attention learning by passing the input features though a deep coattention model consisting of L MCA layers cascaded in depth (denoted by MCA <ref type="bibr" target="#b0">(1)</ref> , MCA <ref type="bibr" target="#b1">(2)</ref> ... MCA (L) ). Denoting the input features for MCA (l) as X (l−1) and Y (l−1) respectively, their output features are denoted by X (l) and Y (l) , which are further fed to the MCA (l+1) as its inputs in a recursive manner.</p><formula xml:id="formula_5">[X (l) , Y (l) ] = MCA (l) ([X (l−1) , Y (l−1) ])<label>(4)</label></formula><p>For MCA <ref type="bibr" target="#b0">(1)</ref> , we set its input features X (0) = X and Y (0) = Y , respectively.</p><p>Taking the SA(Y)-SGA(X,Y) layer as an example (the other two MCA layers proceed in the same manner), we formulate two deep co-attention models in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>The stacking model <ref type="figure" target="#fig_3">(Figure 5a</ref>) simply stacks L MCA layers in depth and outputs X (L) and Y (L) as the final attended image and question features. The encoder-decoder model <ref type="figure" target="#fig_3">(Figure 5b)</ref> is inspired by the Transformer model proposed in <ref type="bibr" target="#b28">[29]</ref>. It slightly modifies the stacking model by replacing the input features Y (l) of the GA unit in each MCA (l) with the question features Y (L) from the last MCA layer. The encoder-decoder strategy can be understood as an encoder to learn the attended question features Y (L) with L stacked SA units and a decoder to use Y (L) to learn the attended image features X (L) with stacked SGA units.</p><p>The two deep models are of the same size with the same L. As a special case that L = 1, the two models are strictly equivalent to each other. example, the attended featurex is obtained as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multimodal Fusion and Output Classifier</head><formula xml:id="formula_6">α = softmax(MLP(X (L) )) x = m i=1 α i x (L) i<label>(5)</label></formula><p>where α = [α 1 , α 2 , ..., α m ] ∈ R m are the learned attention weights. We can obtain the attended featureỹ for Y (L) using an independent attentional reduction model by analogy.</p><p>Using the computedỹ andx, we design the linear multimodal fusion function as follows:</p><formula xml:id="formula_7">z = LayerNorm(W T xx + W T yỹ )<label>(6)</label></formula><p>where W x , W y ∈ R d×dz are two linear projection matrices. d z is the common dimensionality of the fused feature.</p><p>LayerNorm is used here to stabilize training <ref type="bibr" target="#b2">[3]</ref>. The fused feature z is projected into a vector s ∈ R N followed by a sigmoid function, where N is the number of the most frequent answers in the training set. Following <ref type="bibr" target="#b27">[28]</ref>, we use binary cross-entropy (BCE) as the loss function to train an N -way classifier on top of the fused feature z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct experiments to evaluate the performance of our MCAN models on the largest VQA benchmark dataset, VQA-v2 <ref type="bibr" target="#b10">[11]</ref>. Since the different MCA variants and deep co-attention models may influence final performance, we perform extensive quantitative and qualitative ablation studies to explore the reasons why MCAN performs well. Finally, with the optimal hyperparameters, we compare our best model with current stateof-the-art models under the same settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>VQA-v2 is the most commonly used VQA benchmark dataset <ref type="bibr" target="#b10">[11]</ref>. It contains human-annotated question-answer (a) MCA Variants: Accuracies of the MCAN model with different MCA variants under one layer. ID(Y)-GA(X,Y), SA(Y)-GA(X,Y) and SA(Y)-SGA(X,Y) denote the three MCA variants w/ or w/o the SA units for image and question (see <ref type="figure" target="#fig_1">Figure 3</ref>). Since the stacking and the encoder-decoder strategies are equivalent under one layer, we do not distinguish them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>All   pairs relating to the images from the MS-COCO dataset <ref type="bibr" target="#b18">[19]</ref>, with 3 questions per image and 10 answers per question. The dataset is split into three: train (80k images and 444k QA pairs); val (40k images and 214k QA pairs); and test (80k images and 448k QA pairs). Additionally, there are two test subsets called test-dev and test-standard to evaluate model performance online. The results consist of three per-type accuracies (Yes/No, Number, and Other) and an overall accuracy.</p><formula xml:id="formula_8">Y/N Num Other ID(Y)-GA(X,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>The hyper-parameters of our model used in the experiments are as follows. The dimensionality of input image features d x , input question features d y , and fused multimodal features d z are 2,048, 512, and 1,024, respectively. Following the suggestions in <ref type="bibr" target="#b28">[29]</ref>, the latent dimensionality d in the multi-head attention is 512, the number of heads h is set to 8, and the latent dimensionality for each head is d h = d/h = 64. The size of the answer vocabulary is set to N = 3, 129 using the strategy in <ref type="bibr" target="#b27">[28]</ref>. The number of MCA layers is L ∈ {1, 2, 4, 6, 8}.</p><p>To train the MCAN model, we use the Adam solver <ref type="bibr" target="#b16">[17]</ref> with β 1 = 0.9 and β 2 = 0.98. The base learning rate is set to min(2.5te −5 , 1e −4 ), where t is the current epoch number starting from 1. After 10 epochs, the learning rate is decayed by 1/5 every 2 epochs. All the models are trained up to 13 epochs with the same batch size 64. For the results on the val split, only the train split is used for training. For the results on the test-dev or test-standard splits, both train and val splits are used for training, and a subset of VQA samples from Visual Genome <ref type="bibr" target="#b17">[18]</ref> is also used as the augmented dataset to facilitate training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>We run a number of ablations to investigate the reasons why MCAN is effective. The results shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure" target="#fig_4">Figure 6</ref> are discussed in detail below. MCA Variants: From the results in <ref type="table" target="#tab_2">Table 1a</ref>, we can see that SA(Y)-GA(X,Y) outperforms ID(Y)-GA(X,Y) for all answer types. This verifies that modeling self-attention for question features benefits VQA performance, which is consistent with previous works <ref type="bibr" target="#b32">[33]</ref>. Moreover, we can see that SA(Y)-SGA(X,Y) also outperforms SA(Y)-GA(X,Y). This reveals, for the first time, that modeling selfattention for image features is meaningful. Therefore, we use SA(Y)-SGA(X,Y) as our default MCA in the following experiments unless otherwise stated. Stacking vs. Encoder-Decoder: From the results in <ref type="table" target="#tab_2">Table  1b</ref>, we can see that with increasing L, the performances of both deep co-attention models steadily improve and finally saturate at L = 6. The saturation can be explained by the unstable gradients during training when L &gt; 6, which</p><formula xml:id="formula_9">SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6 SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6</formula><p>Q: How many sheep we can see in this picture ?</p><formula xml:id="formula_10">A: 3 SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6 SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6</formula><p>Q: How many sheep we can see in this picture ?</p><formula xml:id="formula_11">A: 3 (a) Encoder-Decoder (P: 3) SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6 SA(Y)-1 SA(Y)-6 GA(X,Y)-1 SA(X)-1 SA(X)-6 GA(X,Y)-6</formula><p>Q: How many sheep we can see in this picture ? A: 3 makes the optimization difficult. Similar observations are also reported by <ref type="bibr" target="#b4">[5]</ref>. Furthermore, the encoder-decoder model steadily outperforms the stacking model, especially when L is large. This is because the learned self-attention from an early SA(Y) unit is inaccurate compared to that from the last SA(Y) unit. Directly feeding it to a GA(X,Y) unit may damage the learned guided-attention for images. The visualization in §5.4 supports this explanation. Finally, MCAN is much more parametric-efficient than other approaches, with MCAN ed -2 (27M) reporting a 66.2% accuracy, BAN-4 (45M) a 65.8% accuracy <ref type="bibr" target="#b13">[14]</ref>, and MFH (116M) a 65.7% accuracy <ref type="bibr" target="#b32">[33]</ref>. More in-depth comparisons can be found in the supplementary material. MCA vs. Depth: In <ref type="figure" target="#fig_4">Figure 6</ref>, we show the detailed performance of MCAN ed -L with different MCA variants. With increasing L, the performance gaps between the three variants increases. Furthermore, an interesting phenomenon occurs in <ref type="figure" target="#fig_4">Figure 6c</ref>. When L = 6, the number type accuracy of the ID(Y)-GA(X,Y) and SA(Y)-GA(X,Y) models are nearly identical, while the SA(Y)-SGA(X,Y) model reports a 4.5-point improvement over them. This verifies that self-attention for images plays a key role in object counting.</p><p>Question Representations: <ref type="table" target="#tab_2">Table 1c</ref> summarizes ablation experiments on different question representations. We can see that using the word embeddings pre-trained by GloVe <ref type="bibr" target="#b24">[25]</ref> significantly outperforms that by random initialization. Other trick like fine-tuning the GloVe embeddings or replacing the position encoding <ref type="bibr" target="#b28">[29]</ref> with a LSTM network to model the temporal information can slightly improve the performance further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Analysis</head><p>In <ref type="figure" target="#fig_5">Figure 7</ref>, we visualize the learned attentions from MCAN sk -6 and MCAN ed -6. Due to space limitations, we only show one example and visualize six attention maps from different attention units and different layers. More visualizations can be found in the supplementary material. From the results, we have the following observations. keyword 'sheep' is identified correctly. Image Self-Attention SA(X): Values in the attention maps of SA(X)-1 are uniformly distributed, suggesting that the key objects for sheep are unclear. The large values in the attention maps of SA(X)-6 occur on the 1st, 3rd, and 11th columns, which correspond to the three sheep in the image. This explains why introducing SA(X) can greatly improve object counting performance. Question Guided-Attention GA(X,Y): The attention maps of GA(X,Y)-1 do not focus on the current objects in the image; and the attention maps of GA(X,Y)-6 tend to focus on all values in the 'sheep' column. This can be explained by the fact that the input features have been reconstructed by the sheep features in SA(X)-6. Moreover, the GA(X,Y) units of the stacking model contain much more noise than the encoder-decoder model. This verifies our hypothesis presented in §5.3.</p><p>In <ref type="figure" target="#fig_6">Figure 8</ref>, we also visualize the final image and question attentions learned by Eq. <ref type="bibr" target="#b4">(5)</ref>. For the correctly predicted examples, the learned question and image attentions are usually closely focus on the key words and the most relevant image regions (e.g., the word 'holding' and the region of 'hand' in the first example, and the word 'vegetable' and the region of 'broccoli' in the second example). From the incorrect examples, we can draw some weaknesses of our approach. For example, it occasionally makes mistakes in distinguishing the key words in questions (e.g., the word 'left' in the third example and the word 'catcher' in the last example). These observations are useful to guide further improvements in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with State-of-the-Art</head><p>By taking the ablation results into account, we compare our best single model MCAN ed -6 with the current stateof-the-art methods in <ref type="table">Table 3</ref>. Using the same bottom-up attention visual features <ref type="bibr" target="#b0">[1]</ref>, MCAN ed -6 significantly outperforms the current best approach BAN [14] by 1.1 points in terms of overall accuracy. Compared to BAN+Counter <ref type="bibr" target="#b13">[14]</ref>, which additionally introduces the counting module <ref type="bibr" target="#b34">[35]</ref> to significantly improve object counting performance, our model is still 0.6 points higher. Moreover, our method obtains comparable object counting performance (i.e., the number type) to BAN+Counter, and in doing so does not use any auxiliary information like the bounding-box coordinates of each object <ref type="bibr" target="#b34">[35]</ref>. This suggests that MCAN is more general that can naturally learn to deduplicate the redundant objects based on the visual features alone. The comparative results with model ensembling are demonstrated in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we present a novel deep Modular Co-Attention Network (MCAN) for VQA. MCAN consists of a cascade of modular co-attention (MCA) layers, each of which consists of the self-attention and guided-attention units to model the intra-and inter-modal interactions synergistically. By stacking MCA layers in depth using the encoder-decoder strategy, we obtain a deep MCAN model that achieves new state-of-the-art performance for VQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Flowcharts of three MCA variants for VQA. (Y) and (X) denote the question and image features respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overall flowchart of the deep Modular Co-Attention Networks (MCAN). In the Deep Co-attention Learning stage, we have two alternative strategies for deep co-attention learning, namely stacking and encoder-decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>After the deep co-attention learning stage, the output image features X (L) = [x n ] ∈ R n×d already contain rich information about the attention weights over the question words and image regions. Therefore, we design an attentional reduction model with a two-layer MLP (FC(d)-ReLU-Dropout(0.1)-FC(1)) for Y (L) (or X (L) ) to obtain its attended featureỹ (orx). Taking X (L) as an Two deep co-attention models based on a cascade of MCA layers (e.g., SA(Y)-SGA(X,Y)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The overall and per-type accuracies of the MCAN ed -L models equipped with different MCA variants, where the number of layers L ∈ {1, 2, 4, 6}. All the reported results are evaluated on the val split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualizations of the learned attention maps (softmax(qK/ √ d)) of the attention units from typical layers. SA(Y)l, SA(X)-l and GA(X,Y)-l denote the question self-attention, image self-attention, and image guided-attention from the l-th layer, respectively. Q, A, P denote the question, answer and prediction respectively. The index within [0-19] shown on the axes of the attention maps corresponds to each object in the image (20 objects in total) . For better visualization effect, we highlight three objects in the image that are related to the answer (i.e., sheep).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Question Self-Attention SA(Y): The attention maps of SA(Y)-1 form vertical stripes, and the words like 'how' and 'see' obtain large attention weights. This unit acts as a question type classifier. Besides, the large values in the attention maps of SA(Y)-6 occur in the column 'sheep'. This reveals that all the attended features tend to use the feature of 'sheep' for reconstruction. That is to say, the Q: what is the lady holding ATypical examples of the learned image and question attentions by Eq.(5). For each example, the image, question (Q) and answer (A) are presented on the left; the learned image attention, question attention and prediction (P) are presented next to them. The brightness of regions and darkness of words represent their importance in the attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>obtained by reconstructing x i by all the samples in Y with respect to their normalized cross-modal similarity to x i .</figDesc><table><row><cell>Q: What is the made of? mustache</cell><cell>GloVe+ LSTM</cell><cell>Stacking</cell><cell>Att. Reduce</cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell>or</cell><cell></cell><cell>+</cell><cell>BCE Loss</cell></row><row><cell></cell><cell>Faster R-CNN</cell><cell>Encoder-Decoder</cell><cell>Att. Reduce</cell><cell>FC</cell><cell>A: Banana</cell></row><row><cell cols="2">Question and Image</cell><cell>Deep Co-Attention</cell><cell cols="3">Multimodal Fusion and Output</cell></row><row><cell cols="2">Representation ( §4.1)</cell><cell>Learning ( §4.2)</cell><cell cols="2">Classifier ( §4.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Stacking vs. Encoder-decoder: Overall accuracies and model sizes (i.e., number of parameters) of the MCAN sk -L models and the MCAN ed -L models, where number of layers L ∈ {2, 4, 6, 8}. With the same L, the sizes of the two models are equal. Question Representations: Accuracies of the MCAN ed -6 model with different question representations. Rand ft means the word embeddings are initialized randomly and then fine-tuned. PE denotes the positional encoding<ref type="bibr" target="#b28">[29]</ref>. GloVe pt+ft and GloVept mean the word embeddings are pre-trained with GloVe, while GloVe pt+ft is additionally fine-tuned.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) L</cell><cell>MCAN sk</cell><cell>MCAN ed</cell><cell>Size</cell><cell>(c) Model</cell><cell>All</cell><cell>Y/N</cell><cell>Num</cell><cell>Other</cell></row><row><cell>Y)</cell><cell>64.8</cell><cell>82.5</cell><cell>44.7</cell><cell>56.7</cell><cell>2</cell><cell>66.1</cell><cell>66.2</cell><cell>27M</cell><cell>Rand ft + PE</cell><cell>65.6</cell><cell>83.0</cell><cell>47.9</cell><cell>57.1</cell></row><row><cell>SA(Y)-GA(X,Y)</cell><cell>65.2</cell><cell>82.9</cell><cell>44.8</cell><cell>57.1</cell><cell>4</cell><cell>66.7</cell><cell>66.9</cell><cell>41M</cell><cell>GloVept + PE</cell><cell>67.0</cell><cell>84.6</cell><cell>49.4</cell><cell>58.2</cell></row><row><cell>SA(Y)-SGA(X,Y)</cell><cell>65.4</cell><cell>83.2</cell><cell>44.9</cell><cell>57.2</cell><cell>6</cell><cell>66.8</cell><cell>67.2</cell><cell>56M</cell><cell>GloVept + LSTM</cell><cell>67.1</cell><cell>84.8</cell><cell>49.4</cell><cell>58.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>66.8</cell><cell>67.2</cell><cell>68M</cell><cell>GloVe pt+ft + LSTM</cell><cell>67.2</cell><cell>84.8</cell><cell>49.3</cell><cell>58.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation experiments for MCAN. All the reported results are evaluated on the val split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracies of single-model on the test-dev and test-standard splits to compare with the state-of-the-art methods. All the methods use the same bottom-up attention visual features<ref type="bibr" target="#b0">[1]</ref>, and are trained on the train+val+vg sets (vg denotes the augmented VQA samples from Visual Genome). The best results on both splits are bolded.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Test-dev</cell><cell></cell><cell>Test-std</cell></row><row><cell></cell><cell>All</cell><cell>Y/N</cell><cell>Num</cell><cell>Other</cell><cell>All</cell></row><row><cell>Bottom-Up [28]</cell><cell cols="3">65.32 81.82 44.21</cell><cell>56.05</cell><cell>65.67</cell></row><row><cell>MFH [33]</cell><cell cols="3">68.76 84.27 49.56</cell><cell>59.89</cell><cell>-</cell></row><row><cell>BAN [14]</cell><cell cols="3">69.52 85.31 50.93</cell><cell>60.26</cell><cell>-</cell></row><row><cell>BAN+Counter [14]</cell><cell cols="2">70.04 85.42</cell><cell>54.04</cell><cell>60.52</cell><cell>70.35</cell></row><row><cell>MCAN ed -6</cell><cell cols="2">70.63 86.82</cell><cell>53.26</cell><cell>60.72</cell><cell>70.90</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our implementation, we omit the feed-forward layer and norm layer of the SA(X) unit to save memory costs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Ensembling</head><p>To compare MCAN to the best results on VQA-v2 leaderboard 2 , we train 4 MCAN ed -6 models with slightly different hyper-parameters for ensemble. The comparative results in <ref type="table">Table 3</ref> indicate that MCAN surpasses the top most solutions on the leaderboard. It is worth noting that our solution only use the basic bottom-up attention visual features <ref type="bibr" target="#b0">[1]</ref> and much fewer models for ensemble.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons of Model Stability and Computational Costs</head><p>We compare MCAN ed -6 with the best two approaches (MFH <ref type="bibr" target="#b32">[33]</ref> and BAN-8 <ref type="bibr" target="#b13">[14]</ref>) in <ref type="table">Table 4</ref> in terms of overall accuracy ±std, number of parameters and FLOPs, respectively. The accuracies are reported on the val split, and the standard deviation for each method is calculated by training three models with the same architecture but different initializations. The FLOPs are calculated for one testing sample. We can see that MCAN ed -6 outperforms the counterparts in both accuracy and stability, and is more parameteric-and computational-efficient at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Visualized Results</head><p>Similar to <ref type="figure">Figure 7</ref> in the main text, we visualize the learned attentions of two more examples from MCAN ed -6 in <ref type="figure">Figure 9</ref>. For each example, we visualize the attention maps from three attention units (SA(X), SA(Y), GA(X,Y)) and from two layers (1st and 6th). For each unit, we show the attention maps from 2 parallel heads (8 heads in total). From the results, we have the similar observations and explanations to those in the main text. The visualized attentions can well explain the reasoning process of MCAN to predict the correct answers. Furthermore, we find that different heads may provide complementary information to benefit VQA performance, which is similar to the 'multiglimpses' strategy in existing VQA approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is the man holding in his hands ?</head><p>A: Bat P: Bat Q: What colors are the stripes on the left ? A: red and white P: red and white </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07561</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Abc-cnn: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6087" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4613" to="4621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02711</idno>
	</analytic>
	<monogr>
		<title level="m">Learnings from the 2017 challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1839" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1114" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
		<title level="m">Learning to count objects in natural images for visual question answering. International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open-ended longform video question answering via adaptive hierarchical reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3683" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
