<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning of Video-Induced Visual Invariances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning of Video-Induced Visual Invariances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a general framework for self-supervised learning of transferable visual representations based on Video-Induced Visual Invariances (VIVI). We consider the implicit hierarchy present in the videos and make use of (i) frame-level invariances (e.g. stability to color and contrast perturbations), (ii) shot/clip-level invariances (e.g. robustness to changes in object orientation and lighting conditions), and (iii) video-level invariances (semantic relationships of scenes across shots/clips), to define a holistic selfsupervised loss. Training models using different variants of the proposed framework on videos from the YouTube-8M (YT8M) data set, we obtain state-of-the-art self-supervised transfer learning results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only 1000 labels per task. We then show how to co-train our models jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points with 10× fewer labeled images, as well as the previous best supervised model by 3.7 points using the full ImageNet data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised deep learning necessitates the collection and manual annotation of large amounts of data, which is often expensive, hard to scale, and may require domain expertise (e.g., in the context of medical data). Expensive data annotation hence presents a bottleneck which impedes the application of deep learning methods to diverse, previously under-explored problems. Learning transferable visual representations, namely representations obtained by training a model on one task (or collection of tasks) which can then be adapted to multiple unseen downstream tasks using few samples, is therefore a key research challenge <ref type="bibr" target="#b69">[69]</ref>.</p><p>An emerging body of work based on self-supervision has demonstrated that it is possible to learn such transferable visual representations. The idea is to carefully construct a pretext task which does not rely on manual annotation, yet encourages the model to extract useful features from the in-  <ref type="table">Table 1</ref>: Mean testing accuracy and per-category mean accuracy for models fine-tuned on the 19 diverse downstream tasks (based on NATural, SPECialized, STRuctured data sets) from the VTAB benchmark <ref type="bibr" target="#b69">[69]</ref> 1 , using only 1000 labels per task. The proposed unsupervised models (VIVI-Ex(4) / VIVI-Ex(4)-Big) trained on raw YT8M videos and variants co-trained with 10%/100% labeled ImageNet data (VIVI-Ex(4)-Co(10%) / VIVI-Ex(4)-Co(100%)), outperform the corresponding unsupervised (Ex-ImageNet), semi-supervised (Semi-Ex-10%) and fully supervised (Sup-100%, Sup-Rot-100%) baselines by a large margin.</p><p>put. Videos are a promising data modality to design such pretexts tasks for as they capture variations of the instances over time which are not present in images. In addition, there is an abundance of videos available on the Internet covering almost any imaginable domain. As a result, and with the recent emergence of research video data sets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">59]</ref>, videos have been investigated in the context of self-supervision (for example, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>). We believe that a holistic approach which captures these diverse efforts can be coupled with image-based pretext tasks to further improve the performance of self-supervised models.</p><p>In this work we propose a novel, versatile video-based self-supervision framework for learning image representations. We divide a video data set into its natural hierarchy of frames, shots, and videos. The intuition is that the model can leverage <ref type="bibr" target="#b0">(1)</ref> the frames to learn to be robust to color perturbations or contrast changes, <ref type="bibr" target="#b1">(2)</ref> the shot information to be robust to rigid and non-rigid transformations of objects in a scene, and that (3) explicitly accounting for the videolevel context should encourage the model to capture semantic relationships of scenes across shots/clips. In contrast to individual frame, shot, or video-level self-supervision objectives, our holistic approach yields a representation that transfers better to a large set of downstream tasks. As an additional benefit, our approach does not need to pre-compute optical flow or motion segmentation masks, nor does it rely on object tracking.</p><p>We train the proposed model on the YouTube-8M (YT8M) data set (without using video-level labels) and show that this approach leads to state-of-the-art selfsupervised results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB) <ref type="bibr" target="#b69">[69]</ref>. We then show how to co-train the model jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 with 10× fewer labeled images. We also investigate the robustness of our co-training models to natural perturbations as induced by the variations across nearby frames in videos <ref type="bibr" target="#b53">[54]</ref>. In summary, our contributions are:</p><p>• We propose a versatile framework to learn image representations from non-curated videos in the wild by learning frame-, shot-, and video-level invariances.</p><p>• We train a variety of models on 3.7M videos from the YT8M data set and achieve a 3.8% absolute improvement over image/frame-based baselines across the 19 diverse tasks of the VTAB benchmark <ref type="bibr" target="#b69">[69]</ref>, which sets new state of the art among unsupervised methods.</p><p>• We augment the self-supervised learning (SSL) training framework with a supervised classification loss using data from ImageNet. The resulting models outperform an ImageNet-pretrained network using only 10% labeled ImageNet images (and no additional unlabeled ones), and achieve a new state of the art when co-trained with the full ImageNet data set, outperforming the best previous supervised result by 3.7 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Self-supervised learning of image representations SSL is an active topic of research in the computer vision community. Recent methods <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b60">60]</ref> have advanced the state of the art in terms of learning representations that can linearly separate between the 1000 ImageNet categories <ref type="bibr" target="#b49">[50]</ref>. Prior work has explored diverse self-supervision cues such as predicting the spatialcontext <ref type="bibr" target="#b11">[12]</ref>, colorization <ref type="bibr" target="#b71">[71]</ref>, equivariance to transformations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref>; alongside unsupervised techniques such as clustering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b72">72]</ref>, generative modelling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>, and exemplar learning <ref type="bibr" target="#b14">[15]</ref>. We adopt some of these SSL losses in our framework at the frame-level. Learning image representations from videos More relevant to our contribution is the body of literature on SSL of image representations from videos. The temporal context of frames in video data has been widely exploited. For example, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b64">64</ref>] make use of the order in which frames appear in a video. Other forms of temporal context include its combination with spatial context <ref type="bibr" target="#b63">[63]</ref>, and the use of spatio-temporal co-occurrence statistics <ref type="bibr" target="#b27">[28]</ref>. Orthogonal to these efforts, which attempt to be selective of the differences between frames, prior work along the lines of slow feature analysis <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b73">73]</ref> also exploited videos as a means of learning invariant representations. Temporal coherence was exploited in a co-training setting by early work <ref type="bibr" target="#b40">[41]</ref> on learning convolutional neural networks (CNNs) for visual object recognition and face recognition. Slow and steady feature analysis <ref type="bibr" target="#b29">[30]</ref> attempts to learn representations that exhibit higher order temporal coherence. This object deformation signal can be separated from global camera motion by tracking objects using unsupervised methods. These tracked patches have been used to learn image representations <ref type="bibr" target="#b62">[62]</ref>. Tracking may also be replaced by spatio-temporally matched region proposals <ref type="bibr" target="#b16">[17]</ref>. Motivated by these works, we explore learning invariances from temporal information in video pixels. Some of the earliest work making use of temporal consistency used future frame prediction <ref type="bibr" target="#b55">[56]</ref> as a pretext task. A more challenging version of this task is single frame future synthesis. The ambiguity in single-frame prediction has been side-stepped via time-agnostic prediction <ref type="bibr" target="#b28">[29]</ref>, motion segmentation <ref type="bibr" target="#b46">[47]</ref>, cross-pixel matching <ref type="bibr" target="#b37">[38]</ref>, and by giving the model a motion cue as input <ref type="bibr" target="#b70">[70]</ref>. The latter two require distilling the temporal information from video pixels into optical-flow fields.</p><p>Optical flow has been treated as a separate modality from the RGB pixels in a multi-modal setting <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b60">60]</ref>. Beyond optical-flow, videos on the web are inherently multi-modal, as they contain audio and subtitles. Multi-modal learning methods that combine vision and audio <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref>, and vision and text <ref type="bibr" target="#b56">[57]</ref> achieve better performance than unimodal baselines. In a robotics setting, RGB pixels may be considered together with ego-motion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>. Timecontrastive networks <ref type="bibr" target="#b52">[53]</ref> consider two views of the same action to learn invariant representations.</p><p>Doersch et al. <ref type="bibr" target="#b12">[13]</ref> show that motion-based SSL may be combined with other self-supervision cues namely exemplar, colorization, and spatial-context, to pre-train models that perform better than each of these cues individually. Taking inspiration from their success our framework presents a synergistic combination of SSL methods. Transferable representations Fine-tuning models pre- is encoded using the frame encoder f . The frame embeddings f (x i k, ) are then aggregated for each shot using a pooling function p to obtain shot embeddings e i k . Predictions on the video level are then computed using the prediction functions g m . (right) Intuitively, we want to choose frame/shot-and video-level losses that embed frames from the same shot close to each other and frames from different shots or videos far apart, while encouraging shot embeddings from the same video to be predictive of each other using (simple) prediction functions. <ref type="bibr" target="#b1">2</ref> trained on ImageNet labels is a popular strategy for transferring representations to new tasks <ref type="bibr" target="#b25">[26]</ref>. Kornblith et al. <ref type="bibr" target="#b33">[34]</ref> show that better supervised models tend to transfer better when fine-tuned. Other supervised learning benchmarks focus on performance on multiple data sets, either via transfer learning, meta-learning, or multi-task learning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b61">61]</ref>. In the representation learning literature, models are usually evaluated in-domain, typically on ImageNet [70, and references therein]. However, self-supervised models are now performing well on tasks such as surface normal estimation, detection, and navigation <ref type="bibr" target="#b18">[19]</ref>. The VTAB benchmark evaluates the transferability of representations beyond object classification in the natural image domain to many domains and task semantics such as counting and localization <ref type="bibr" target="#b69">[69]</ref>. Similarly, recent developments in natural language processing (NLP) have lead to representations that transfer effectively to many diverse tasks <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning video-induced visual invariances</head><p>We start by giving an overview of the proposed framework in Sec. 3.1, and discuss frame/shot-level and videolevel losses in detail in Sec. 3.2 and Sec. 3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We consider a data set X containing N videos, each composed of multiple shots. For simplicity of exposition we assume that each video consists of K shots, and each shot has L frames. If we denote the -th frame in the kth shot of video i by x i k, , we can write the data set as X = {x i 1:K,1:L } N i=1 . Our framework consists of a frame-encoder f , a frame embedding pooling function p, and one or multiple shot-level prediction functions g m (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The pooling function computes an embedding e i k of the kth shot in video i by feeding each frame through the frame encoder and applying the pooling function,</p><formula xml:id="formula_0">e i k = p(f (x i k,1 ), . . . , f (x i k,L )).</formula><p>The pooling function can have different forms, ranging from simple average pooling to attention pooling taking the values of the individual frame embeddings f (x i k, ) into account. Shot-level prediction functions are trained to predict pretext (label-free) targets from shot embeddings.</p><p>We define a frame/shot-level loss and a video-level loss to learn invariances at different levels of abstraction. More specifically, the frame/shot-level loss takes the form</p><formula xml:id="formula_1">L S = i,k L S (f (x i k,1 ), . . . , f (x i k,L ); y i k,1 , . . . , y i k,L ),</formula><p>where y i k, are shot-level pretext labels and L S is a shot-level loss that can be instantiated as only acting on the frame level in the sense of L S decomposing into a sum over the frames = 1, . . . , L (see Sec. 3.2 for concrete instantiations of the losses). The video-level loss is given by</p><formula xml:id="formula_2">L V = i,m L V (g m (e i 1 , . . . , e i K )); y i m ),<label>(1)</label></formula><p>where the y i m are video-level pretext labels and L V is a video-level loss (see Sec. 3.3 for concrete losses). The total loss is then given by L SSL = L S + λL V , where λ &gt; 0 balances the shot level and video level losses. L SSL is minimized jointly w.r.t. the parameters of f , p, and g m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-training with labeled images</head><p>We also consider the case where one has access to a limited number of labeled images in addition to the video data. Combining imagebased SSL losses with a supervised loss applied to a subset of the images was studied previously by <ref type="bibr" target="#b68">[68]</ref>. They found that this approach leads to a state-of-the-art semi-supervised models, and improves the performance of supervised models when all images are labeled. Here, we consider the related setup where the SSL loss is computed on video data, and the supervised loss is based on image data from a different data set. Specifically, we additionally apply f followed by a linear classifier to mini-batches of labeled images and compute the cross-entropy loss L SUP between the predictions and the image labels. The total loss is then computed as L SSL + γL SUP , where γ &gt; 0 balances the contributions of the self-supervised and supervised loss terms. Relation to prior work We are not aware of prior work using the natural hierarchy of frame, shot, and video-level invariances in videos for self-supervised image representation learning. Further, our approach is geared towards reducing the need for curated datasets and expensive labeling procedures. In contrast, many existing methods for learning image representations from video data often rely on short curated videos consisting of single clips, or even the treat training set as a bag of frames <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning shot-level invariances</head><p>To define the frame/shot-level loss L S , we propose to build on any SSL loss designed for images, such as classifying exemplars <ref type="bibr" target="#b14">[15]</ref>, solving jigsaw puzzles of image patches <ref type="bibr" target="#b42">[43]</ref>, or rotation prediction <ref type="bibr" target="#b17">[18]</ref>. For learning shot-induced invariances, one can take two approaches:</p><p>(i) apply the image-based SSL loss independently to each frame so that the shot-induced invariances are learned implicitly through the combination of pooling function and video-level prediction task, or (ii) explicitly ensure that the embeddings of the frames from the same shot are similar by adding a triplet or a contrastive loss to the image-based SSL loss.</p><p>In this work, in the spirit of approach (i) we consider SSL by rotation prediction <ref type="bibr" target="#b17">[18]</ref> without additional explicit shotlevel loss. To explore approach (ii) we rely on a variant of exemplar SSL <ref type="bibr" target="#b14">[15]</ref>, where each image is associated with a different class, and a feature extractor is trained to classify each image into its own class after heavily augmenting it (random cropping, rotation, contrast, and color shifts). Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, to scale this approach to hundreds of millions of images (frames), we employ a triplet loss <ref type="bibr" target="#b51">[52]</ref> encouraging augmentations of the same image to be close and augmentations of different images to be far apart. To learn invariances from different frames of the same shot, rather than picking a random frame from the shot and applying M random augmentations to it, we pick M consecutive frames from the same shot and augment each frame once. As a result, our feature extractor learns both the invariances induced by temporal variation in video as well as those induced by the data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning video-level invariances</head><p>In contrast to action recognition networks, which learn video representations that have to be discriminative w.r.t. changes between frames, our framework targets learning representations that are invariant to such changes. Nevertheless, discriminative tasks useful for learning representations for action recognition, such as predicting whether a sequence of frames is played forward or backward <ref type="bibr" target="#b64">[64]</ref>, verifying whether the frames are ordered or shuffled <ref type="bibr" target="#b39">[40]</ref>, or predicting features corresponding to future frames <ref type="bibr" target="#b20">[21]</ref>, can be useful to learn abstract transferable representations when applied to sensibly chosen groups of aggregated frames. Following this intuition, our framework allows to apply any of these tasks to shot embeddings, rather than individual frame embeddings. Despite being discriminative at the video level, these tasks encourage the representation to be invariant to all except those cues that are necessary for the pretext task; and hence indirectly induce invariances. For example, determining whether a sequence of shot embeddings is played forward or backward requires understanding of the high-level semantics of the scene and objects in each shot. Similarly, predicting future shot embeddings from the past ones encourages learning an abstract summary of each shot. In this work we will explore these two approaches.</p><p>For shot order prediction, we randomly reverse the order of the shot embeddings and train a prediction function g to predict the shot order from concatenated shot embeddings, i.e., L V in (1) is the cross-entropy loss and y i m is 1 if the sequence of shot embeddings is reversed and 0 otherwise. To train g to predict future shot embeddings, we rely on noise-contrastive estimation <ref type="bibr" target="#b19">[20]</ref>. Specifically, we use the embeddings of the shots e i 1 , . . . , e i k to obtain a prediction e i k+m of the embedding e i k+m of the shot m steps in the future. Then, L V should quantify the quality of the prediction, which we accomplish using the InfoNCE loss <ref type="bibr" target="#b44">[45]</ref> </p><formula xml:id="formula_3">L NCE = − 1 N i log e g(ê i k+m ,e i k+m ) 1 N j e g(ê i k+m ,e j k+m ) ,<label>(2)</label></formula><p>where g is trained to assign high scores to pairs of shot embeddings from the same video, and low values to embeddings computed from different videos. <ref type="bibr" target="#b2">3</ref> Note that the terms in (2) can, up to an additive constant, be seen as the crossentropy loss of an N -class classification problem where the correct label is i, so that we could reformulate the loss in the form (1) using class labels y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>Our experiments encompass two training phases, which we refer to as upstream and downstream. First, in the upstream phase, we train our models on video (and image) data using the methods proposed in the previous section. Then, we fine-tune those trained models on a set of downstream problems in the second phase. We focus on the challenging scenario in which the downstream data is limited, and use only 1000 examples for each downstream task <ref type="bibr" target="#b69">[69]</ref>. Upstream training We train on the videos in the YT8M data set <ref type="bibr" target="#b0">[1]</ref>, which consists of millions of YouTube video IDs with over 3800 visual entities. We downloaded approximately 4.7M of these videos sampled at 1 Hz and split them into a training set of 3.7M and a testing set of 1M videos. We further split the videos into shots using a simple strategy based on color histograms, similarly to <ref type="bibr" target="#b38">[39]</ref> (see <ref type="table" target="#tab_4">Table 5</ref> in the supplementary material for data set statistics). No other pre-processing or filtering is performed as we target learning from real-world videos in the wild. We also present results of several baseline approaches applied to a data set obtained by selecting a single random frame from each video, which we refer to as YT8M frames.</p><p>Furthermore, in the co-training experiments we also use (a class-balanced fraction of) the ImageNet (ILSVRC-2012) training set <ref type="bibr" target="#b9">[10]</ref>. Downstream evaluation To evaluate the learned representations, we use the data sets and follow the protocol of the VTAB Version 1 (arXiv:1910.04867v1) <ref type="bibr" target="#b69">[69]</ref>. <ref type="bibr" target="#b3">4</ref> This protocol consists of 19 tasks categorized into three groups as follows (details and references are in the appendix).</p><p>• Natural -Six classical image classification problems on natural images (data sets: Caltech101, CIFAR-100, DTD, Flowers102, Pets, Sun397 and SVHN).</p><p>• Specialized -Image classification on data captured using specialist equipment, from the remote-sensing (data sets: Resisc45, EuroSAT) and medical (data sets: Patch Camelyon, Diabetic Retinopathy) domains.</p><p>• Structured -Eight tasks to predict properties of the objects appearing in an image (how many there are, their relative position and distance), on both rendered (Clevr, dSprites, SmallNORB, DMLab) and real (KITTI) data.</p><p>For each of these 19 tasks and each model that we propose, we launch a sweep over 4 hyper-parameters (learning rates and schedules, as in the lightweight mode of <ref type="bibr" target="#b69">[69]</ref>). Then, we choose the models that had the best validation accuracy when averaged over these 19 tasks. These bestperforming models are then re-trained for each data set on 1000 random points from the union of the train and validation set and evaluated on the testing set. To account for the randomness coming from the initialization of the fresh classification head and the order in which the data appears, we repeated this evaluation scheme three times and report the median test set accuracy (following <ref type="bibr" target="#b69">[69]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architectures and training details</head><p>The frame encoder f is modeled using the ResNet-50 v2 <ref type="bibr" target="#b22">[23]</ref> architecture with BatchNorm <ref type="bibr" target="#b26">[27]</ref>. We also investigated the effect of model capacity by widening the network by a factor of three. To avoid mismatch in batch statistics between the two data sources, in the co-training experiments we replace Batch-Norm with GroupNorm <ref type="bibr" target="#b66">[66]</ref> and also standardize <ref type="bibr" target="#b47">[48]</ref> the weights of the convolutions. We construct mini-batches by sampling either 2 or 4 consecutive shots from each video (dropping those videos with fewer shots), and randomly select 8 consecutive frames for exemplar-based shot-level SSL and 4 consecutive frames rotation-based frame-level SSL. For the L NCE loss, when we sample 2 shots, we predict the embedding of one from the embedding of the other one using a multilayer perceptron (MLP), i.e., the function g in (2) has the form g(e, e ) = φ 1 (e) φ 2 (e ), where φ 1 , φ 2 are MLPs with a single hidden layer with 256 units. In the experiments with 4 shots, we use a Long Short-Term Memory (LSTM) prediction function with 256 hidden units to predict every shot embedding from the previous ones. We use temporal order prediction only together with exemplarbased SSL and for data with 2 shots per video, relying on a single-hidden-layer MLP with 512 hidden units as prediction function. Throughout, we rely on (parameter-free) average pooling for p. For both frame and shot-level SSL approaches we use the augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref>. For models co-trained with a supervised loss based on a fraction of ImageNet we additionally use the same HSVspace color randomization as <ref type="bibr" target="#b68">[68]</ref>.</p><p>We also perform experiments where we replace the augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref> with AutoAugment (AA), which is an augmentation policy learned using a reinforcement learning algorithm from the full ImageNet data set. While this can cause label leakage when applied to unsupervised methods, we investigate it to understand how these automatically learned invariances compare to those induced by shot-based augmentation which are label-free.</p><p>In all cases we choose the batch size such that the product of the number of videos and the number of shots is 2048, i.e., N K = 2048. We train all unsupervised models for 120k iterations, using stochastic gradient descent (SGD) with a learning rate of 0.8 and momentum 0.9, multiplying the learning rate by 0.1 after 90k and 110k iterations. The co-trained models are trained for 100k iterations, and the schedule as well as the batch size is chosen depending on the amount of labeled data used. For the weight λ (and γ for co-trained models) we sweep over at most four different  <ref type="formula">(4)</ref>) and additionally 3×wider architecture (VIVI-Ex(4)-Big). Both shot and video-level losses improve the overall score, with the gains coming mostly from higher mean accuracy on the natural and structured subsets.</p><p>values. A complete description of all hyper-parameters and architectures can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We train a rotation and exemplar baseline model on ImageNet and a data set obtained by sampling one frame from each video in our training set (YT8M frames). We use the same training protocol as <ref type="bibr" target="#b32">[33]</ref> for the respective methods except that we increase the batch size to 2048 and stretch the schedule to 120k iterations to be comparable to our methods. Furthermore, for the exemplar-based model we ablate the video-level prediction task, which amounts to treating the shots independently and only using the frames from the same shot as exemplars. In addition, we consider 3 baselines from <ref type="bibr" target="#b69">[69]</ref>: A standard ResNet-50 v2 pretrained on ImageNet (achieving top-1/top-5 accuracy of 75.5%/92.6% on the ImageNet validation set), the exemplar model trained on ImageNet with 10% class-balanced labeled data from <ref type="bibr" target="#b68">[68]</ref> (Semi-Ex-10%), which achieves state-of-the-art semisupervised accuracy on ImageNet, and the rotation model trained on ImageNet with all labels <ref type="bibr" target="#b68">[68]</ref> (Sup-Rot-100%).</p><p>We further compare against three prior works that learn image representations from video data: The motion segmentation (MS) <ref type="bibr" target="#b46">[47]</ref> and the multi-task SSL (MT-SSL) models from <ref type="bibr" target="#b12">[13]</ref>, and the transitive invariance (TI) model from <ref type="bibr" target="#b63">[63]</ref>. MS learns representations based on a foreground-background segmentation pretext task. The segmentation maps are derived using an off-the-shelf offline video segmentation algorithm. MT-SSL combines MS and three other self supervision objectives to train a multi-task network. Its representation derives from a combination of colorization, spatial context, and motion segmentation cues. The MS and MT-SSL models fine-tuned in this evaluation have a ResNet-101 <ref type="bibr" target="#b21">[22]</ref> architecture up to the third block. TI builds a graph combining intra-instance and interinstance edges and exploits transitivity to learn invariant representations. The intra-instance edges are obtained by tracking patches in videos. We fine-tune their publicly available pre-trained VGG-16 <ref type="bibr" target="#b54">[55]</ref> checkpoint. We refer the reader to the supplementary material for implementation details regarding the evaluation of these baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we focus on the low sample-size regime, i.e., when each downstream data set consists of 1000 samples, and discuss the performance on the full data sets in the supplementary material <ref type="table">(Table 4</ref>). In brief, the ranking of the methods according to the VTAB mean score using all examples is similar to the ranking according to the VTAB 1000 example mean score. Further, here we only present the best configuration (w.r.t. the number of shots K and choice of prediction function) for each of our VIVI learning approaches, and defer the results for other configurations to the supplementary material <ref type="table">(Table 4</ref>). We also present an evaluation of the proposed methods on object detection in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Self-supervised learning</head><p>Exemplar <ref type="figure" target="#fig_1">Fig. 2</ref> shows the results for our models and the exemplar-based baselines. The baseline trained on YT8M frames only (Ex-YT-F), without leveraging any temporal information, achieves a mean VTAB 1000 example score of 59.4%. Exploiting the temporal variations within shots to create exemplars (Ex-YT-S) increases that score by about 1.9 points. Further, adding the video-level prediction loss on top adds another 1.2 points. It hence appears that leveraging both shot-and video-level invariances using our approach leads to significant gains over just using frames. In addition, increasing the model capacity (using a 3×wider model) leads to another increase by 0.8 points. Note that this model is only 2.0 points behind the semi-supervised model from <ref type="bibr" target="#b68">[68]</ref> (Semi-Ex-10%) which uses 128k labeled images from ImageNet for training (cf. <ref type="table">Table 1</ref>). The gains mostly come from improvements on the natural and struc-  <ref type="formula">(4)</ref>, and with a 3× wider architecture (VIVI-Ex(4)-Big)), with ImageNet-based exemplar (Ex-ImageNet) and rotation (Rot-ImageNet) baselines, as well as the multi-task SSL model from <ref type="bibr" target="#b12">[13]</ref>. Our models outperform all baselines on average, and in particular on the structured data sets.</p><p>tured data sets, whereas video-level losses do not notably improve the score on the specialized data sets (see <ref type="figure" target="#fig_1">Fig. 2</ref>). We observed the largest gains when using L NCE with K = 4 shots and more modest improvements for L NCE and temporal order prediction with K = 2 shots.</p><p>Rotation Similarly to the exemplar experiments, we observe gains of 2.0 points in the mean VTAB 1000 example score over the frame-based baseline (Rot-YT-F) when using a video-level prediction task (VIVI-Rot in <ref type="table">Table 2</ref>). The gains are smaller for K = 2 than for K = 4 shots when combined with L NCE , and temporal order prediction was not effective when combined with rotation prediction as frame-level loss for both K ∈ {2, 4}. We emphasize that the frame encoder trained via rotation SSL on YT8M frames performs considerably worse than the same model trained on ImageNet. This is not surprising as ImageNet images are carefully cropped and the data has a balanced class distribution. By contrast, frames sampled from YT8M are less balanced in terms of content and arguably provide many shortcuts for the rotation task such as black borders, overlaid logos, frames with text, or lack of orientation cues.</p><p>Effect of AutoAugment (AA) <ref type="table">Table 2</ref> shows the effect of using AA <ref type="bibr" target="#b7">[8]</ref> instead of the augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref>. The effect is strongest on the frame-based baselines, increasing the VTAB 1000-example score by at least 2, and weakest on models involving shot-and video-level losses, where the increase is between 0.5 and 1.5 points. Hence, the invariances induced by AA are, to some degree, complementary to the proposed shot-and video-level losses. However, note that AA is trained on labeled ImageNet images, which might introduce label leakage. Hence, methods relying on AA should not be considered fully unsupervised.</p><p>Comparison with related work <ref type="figure" target="#fig_2">Fig. 3</ref> presents a summary of the comparison with baselines. We omit MS and TI as they obtain a VTAB 1000 example mean score comparable to relative patch location prediction <ref type="bibr" target="#b11">[12]</ref> and jigsaw <ref type="bibr" target="#b42">[43]</ref> SSL trained on ImageNet. These two methods have a significantly lower VTAB 1000 example score than the  <ref type="table">Table 2</ref>: Effect of replacing the data augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref> with AA. Video-induced invariances learned by our method are complementary to AA in the sense that applying AA to different variants of our method consistently leads to improvements.</p><p>MT-SSL model, as well as rotation and exemplar SSL. Our VIVI models clearly outperform both the ImageNet baseline and the MT-SSL model. The score obtained by MT-SSL is comparable to that obtained by rotation-based SSL trained on ImageNet, which in turn scores 1.4 points higher than exemplar-based SSL. Both our models and MT-SSL significantly outperform rotation and exemplar-based SSL on the structured data sets, whereas the ImageNet-based exemplar baseline obtains the highest mean score on the specialized data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Co-training with ImageNet</head><p>In <ref type="table">Table 1</ref> we compare the scores obtained by our exemplar-based co-training models with the baselines from <ref type="bibr" target="#b69">[69]</ref>. Our model with frame/shot-level and video-level losses and a wider architecture (VIVI-Ex(4)-Big) reduces the gap between exemplar trained on ImageNet and the strong Semi-Ex-10% semi-supervised baseline model by more than a factor of 2. Moreover, our model co-trained with  <ref type="formula">(4)</ref>) and its counterpart cotrained with the full ImageNet data set (VIVI-Ex(4)-Co(100%)). The accuracy on most of the natural (red) and specialized (green) data sets improves, with the largest improvements observed on the latter, while the accuracy decreases for about half of the structured data sets (blue).</p><p>of the art on the VTAB benchmark. The largest gains from using (a subset of) ImageNet can generally be observed on the natural data sets, whereas the gains on the specialized and structured data sets are significantly lower. This result is not surprising given that many data sets in the natural category are semantically similar to ImageNet. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the per-data set increase/decrease in the VTAB 1000 example score when adding a classification loss computed on the entire ImageNet data set to VIVI-Ex <ref type="bibr" target="#b3">(4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to video perturbations</head><p>Our co-trained models are trained to both recognize 1000 ImageNet categories and be invariant to deformations found in video data. We therefore expect model predictions to be stable across neighbouring frames in a video. To measure if this is indeed the case, we evaluate our VIVI-Ex(4)-Co(100%) model on the ImageNet-Vid-Robust <ref type="bibr" target="#b53">[54]</ref> benchmark. This benchmark measures the drop in accuracy under a stricter definition of the 0-1 loss using videos from the ImageNet-Vid data set <ref type="bibr" target="#b49">[50]</ref>. Given a set of frames, the prediction on an "anchor" frame is considered correct only if all neighboring frames are predicted correctly. Intuitively, the drop in performance going from standard top-1 accuracy on anchor frames to this stricter loss function is indicative of a lack in model robustness. The lower the drop the more robust the model. In <ref type="table">Table 3</ref> we observe that our co-trained model is slightly more robust than its purely supervised counterpart, although the results are still within error bars. This is similar to the difference in performance drop observed for finetuning on ImageNet-Vid as reported in the benchmark paper itself [54, <ref type="table">Table 1</ref>]. These initial results suggest that our co-training approach leads to a similar effect as fine-tuning, despite the domain shift between YT8M and ImageNet-Vid.  <ref type="table">Table 3</ref>: ImageNet-Vid-Robust evaluation: We evaluate our VIVI-Ex(4)-Co(100%) model (co-trained using all labeled images available in the ImageNet training set), on the ImageNet-Vid-Robust benchmark <ref type="bibr" target="#b53">[54]</ref>. Accuracy original is the top-1 accuracy measured on "anchor" frames. Accuracy perturbed is the PM-10 accuracy from the benchmark. It is the worst case accuracy defined over neighbouring 20 frames <ref type="bibr" target="#b53">[54]</ref> around each "anchor" frame. ∆ is the absolute difference between these two. On this benchmark, lower difference is better. The small text in gray corresponds to the Clopper-Pearson confidence interval.</p><p>It seems that robustness to natural perturbations in videos is extremely challenging and worth investigating in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose and evaluate a versatile framework for learning transferable, data-efficient image representations by exploiting video-induced visual invariances at different levels of granularity. The framework can be instantiated with any image-based SSL loss at the frame/shot-level and arbitrary sequence prediction proxy tasks at the video-level. Our experiments reveal that purely self-supervised models benefit greatly from exploiting video-induced invariances, outperforming the SSL baselines trained on ImageNet by a large margin, in particular on problems that require predicting the structural properties of the data. Moreover, when augmenting the proposed framework with a supervised classification loss, the resulting models outperform a standard ImageNetpretrained model using 10× fewer labeled examples, and set a new state of the art on the VTAB benchmark when co-trained with the full ImageNet data set.</p><p>Future research could target better understanding of how the choice of losses and data sets used for upstream training impacts the performance on different tasks in downstream evaluation. While we found our co-trained models to be somewhat more robust to natural perturbations induced by videos than models trained only on images, further research is needed on learning models that overcome robustness issues related to perturbations induced by videos. <ref type="table">Table 4</ref>: Testing accuracy for every data set in the VTAB benchmark using 1000 and all samples for fine-tuning. Each number is the median of three fine-tuning runs. The proposed methods have the prefix VIVI. "Ex" and "Rot" stand for exemplar <ref type="bibr" target="#b14">[15]</ref> and rotation prediction <ref type="bibr" target="#b17">[18]</ref> frame-level self-supervision, respectively. These identifiers are followed with the number of shots in parentheses if an InfoNCE prediction loss across shots is used (except methods using shot order prediction have the suffix "-Ord"). Baseline methods only using frames and shots have the suffix "YT-F" and "YT-S", respectively. The suffix "-AA" denotes methods that use AutoAugment <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architectures</head><p>Here we expand on the short description in Section 4. The frame encoder f is modelled using the ResNet-50 v2 <ref type="bibr" target="#b22">[23]</ref> architecture with BatchNorm <ref type="bibr" target="#b26">[27]</ref>. We also investigate in several experiments the effect of model capacity by widening the network by a factor of three. To avoid mismatch in batch statistics between the two data sources, in the co-training experiments we replace the BatchNorm with GroupNorm <ref type="bibr" target="#b66">[66]</ref> and also standardize <ref type="bibr" target="#b47">[48]</ref> the weights of the convolutions.</p><p>For each prediction task, we attach a different linear head to the 2048-dimensional pre-logits ResNet representation before applying the respective loss or prediction function. For exemplar, following <ref type="bibr" target="#b32">[33]</ref>, we use a linear head with 1000 outputs with L2-normalization of the features before feeding into the triplet-loss. For rotation prediction we rely on a linear head with 4 outputs. For the video-level loss (prediction across shots using L NCE and temporal order prediction) we project the pre-logits, average-pooled across the frames of the same shot, to 512 dimensions using a linear head, and feed this representation to the prediction functions g m . Finally, in the experiments with co-training, we rely on an additional linear classification head with 1000 outputs.</p><p>For the L NCE loss, when we sample 2 shots, we predict one from the other using an MLP, i.e., the function g in (2) has the form g(e, e ) = φ 1 (e) φ 2 (e ), where φ 1 , φ 2 are MLPs with a single hidden layer with 256 units and 128 outputs. In the experiments with 4 shots, we use a 2-layer LSTM prediction function with 256 hidden units to predict every shot embedding from the previous ones. To match the dimension of the LSTM output (256) and that of the future shot embeddings (512) we employ another linear layer. We use temporal order prediction only together with exemplar-based SSL and for data with 2 shots per video, relying on a single-hidden-layer MLP with 512 hidden units as prediction function.</p><p>For both frame and shot-level SSL approaches we use the augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref>. For models co-trained with a supervised loss based on a fraction of ImageNet we additionally use the same HSV-space color randomization as <ref type="bibr" target="#b68">[68]</ref>. We also perform experiments where we replace the augmentation mechanism from <ref type="bibr" target="#b57">[58]</ref> with AA, which is an augmentation policy learned using a reinforcement learning algorithm from the full ImageNet data set. More specifically, we rely on the TF-Hub module publicly available at https://tfhub.dev/google/image_augmentation/nas_imagenet/1. <ref type="table" target="#tab_6">Table 6</ref> provides details about the schedules, batch size, loss weights, etc. used for the individual methods. When exploring the effect of AA we reduce the weight of the video-level loss, λ, by a factor of 2. The schedule for VIVI-Ex(4)-Co(10%) is motivated as follows. We take the schedule and batch size used for the ImageNet exemplar co-training experiments for 10% labeled ImageNet examples from <ref type="bibr" target="#b68">[68]</ref>, stretch the schedule to 100k iterations and reduce the batch size (as well as the learning rate) so that number of epochs over the 10% (128k example) data set matches that of <ref type="bibr" target="#b68">[68]</ref>. <ref type="table" target="#tab_4">Table 5</ref> shows some statistics of the YT8M subset we use for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training details</head><p>We set the margin parameter in the semi-hard triplet loss <ref type="bibr" target="#b51">[52]</ref> to 0.5. For rotation-based SSL, following common practice <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, we compute the predicted rotation after appending to the mini-batch 3 rotated copies of the mini-batch along the batch dimension and compute the rotation loss for all rotated copies.</p><p>We train all models on 128 cores of a Google TPU v3 Pod. For exemplar SSL the triplet loss is computed per core. For all frame/shot level loss variants, L NCE is computed across all cores when prediction is across 4 shots, and computed per core when prediction is across 2 shots as computing the loss across all cores led to instabilities for that case. <ref type="bibr">MEAN</ref> STD.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline fine-tuning details</head><p>As mentioned in the main manuscript we compared against two baseline methods: MT-SSL (Multi-Task Self-Supervised Learning) <ref type="bibr" target="#b12">[13]</ref>, and TI (Transitive Invariance) <ref type="bibr" target="#b63">[63]</ref>. For MT-SSL we considered two variants: MS which was pre-trained on motion segmentation only, and MT-SSL which combined MS with three other tasks in a multi-task setting. We obtained pre-trained checkpoints for all three methods (MS, MT-SSL, and TI) from the authors of their respective prior works.  C.1. Fine-tuning motion segmentation and multi-task SSL baselines MS and MT-SSL pre-trained a ResNet-101 up to block3. The representation at block3 is 7 × 7 × 1024, which is too big. In <ref type="bibr" target="#b12">[13]</ref>, the authors used max-pooling to down-sample this to 3 × 3 × 1024 and then trained a linear predictor for ImageNet classification. We experimented with this approach for VTAB evaluation. The default evaluation protocol for VTAB is to sweep over initial learning rates: 0.1 and 0.01. These were too high for the MS and MT-SSL models. For several downstream evaluation tasks fine-tuning diverged. We therefore modified the evaluation sweep minimally to sweep over initial learning rates: 0.1, 0.05, 0.01. We also evaluated a simpler alternative: Global average pooling the block3 representation into a 1 × 1 × 1024 dimensional vector. We found that global average pooling the representation achieved best results on the VTAB validation set. It also did not diverge at higher learning rates, so we could use the default learning rate schedule in this case. We therefore used this setting for the final evaluation on test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Fine-tuning the transitive invariance baseline</head><p>We exported the pre-trained caffe checkpoint into TensorFlow using the Caffe-TensorFlow tool <ref type="bibr" target="#b4">5</ref> . We found that the pretrained VGG-16 backbone diverges at higher learning rates when fine-tuning downstream on VTAB tasks. We therefore manually adjusted the sweep over initial learning rates and found 0.01, 0.005, 0.001 to work well. Another challenge with transferring this baseline model to several downstream data sets was that it is a patch-based model that expects 96 × 96 dimensional input, whereas the VTAB benchmark scales all images to 224 × 224. We experimented with three ways of deploying this downstream: (a) Resize the input image from 224×224 into 96×96, (b) apply the model fully convolutionally and compute a global average pool at the end, and (c) crop patches of size 96 × 96 at stride 32 from the input image and then average the representations across all of these. We found that (c) was computationally extremely expensive. (b) performed best and we report results for that approach on the VTAB test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results</head><p>Object detection We evaluate the proposed framework as a pre-training step for object detection. Specifically, we use different VIVI models and baselines as backbone for RetinaNet <ref type="bibr" target="#b35">[36]</ref> and evaluate it on the COCO-2017 data set <ref type="bibr" target="#b36">[37]</ref>. For all experiments, we rely on the standard ResNet-50 v2 architecture. We use the standard RetinaNet training protocol with fine-tuning and only adapt the learning rate in cases where the default learning rate is too high by halving it until the training loss decays smoothly (i.e., we tune the learning rate based on the training loss). <ref type="table" target="#tab_8">Table 7</ref> shows the standard COCO-2017 detection metric for different models. It can be seen that VIVI-Ex(4) outperforms the Ex-YT-F, MS, and MT-SSL baselines, and VIVI-Ex <ref type="formula">(4)</ref>   <ref type="bibr" target="#b36">[37]</ref> for different ways of pre-training the ResNet-50 v2 backbone (Sup-100% corresponds to standard supervised ImageNet pre-training). The reported AP values are the median over 3 runs.</p><p>Additional figures In <ref type="figure">Fig. 5 to 9</ref> we provide per-data set comparisons of different model pairs to better understand the effect of increasing the model size, using AA, and co-training with different amounts of labeled images. All numbers are accuracies when using 1000 labels for fine-tuning.  <ref type="formula">(4)</ref> Ex-ImageNet <ref type="figure">Figure 5</ref>: Per-data set comparison of ImageNet-based exemplar SSL (Ex-ImageNet) with VIVI-Ex <ref type="bibr" target="#b3">(4)</ref>. Training on YT8M rather than ImageNet and exploiting temporal information mostly helps on natural (red) and structured (blue) data sets, and slightly hurts for some specialized (green) data sets. Top-1 Accuracy Delta VIVI-Ex(4)-Big VIVI-Ex(4) <ref type="figure">Figure 6</ref>: Per-data set comparison of VIVI-Ex(4) and a 3× wider counterpart (VIVI-Ex(4)-Big). Increasing model capacity leads to an increase in accuracy for all natural (red) data sets and some structured (blue) and specialized (green) data sets. However, some structured and specialized data sets also incur a reduction in accuracy. Top-1 Accuracy Delta VIVI-Ex(4)-Co(10%) VIVI-Ex(4) <ref type="figure">Figure 8</ref>: Per-data set comparison of VIVI-Ex(4) and its counterpart co-trained with 10% class-balanced ImageNet data (VIVI-Ex(4)-Co(10%)). Most data sets from each category incur an increase in accuracy, but one data set from each the natural and structured categories suffer a significant loss in accuracy. VIVI-Ex(4)-Co(10%) <ref type="figure">Figure 9</ref>: Effect of increasing the number of ImageNet images used for co-training from 10% (VIVI-Ex(4)-Co(10%)) to 100% (VIVI-Ex(4)-Co(100%)). The accuracy on the majority of natural (red) data sets is significantly increased, whereas most of the structured data sets incur a slight drop in accuracy.  <ref type="table">Table 9</ref>: Overall and per group mean testing accuracy for fine-tuning hyper-parameter selection according to Version 2 (arXiv:1910.04867v2) of the VTAB benchmark. Each number is the median of three fine-tuning runs. See the caption of <ref type="table">Table 8</ref> for a description of the method abbreviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI-Dist</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dSpr-Loc</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sNORB-Elev</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD MEAN</head><p>NAT. SPEC. STR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(left) Illustration of the frame-, shot-, and video-level encoding pipeline used in this work. Each frame x i k,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>VTAB 1000 example mean score and per-category mean score of exemplar SSL from YT8M frames (Ex-YT-F), with additional shot-level self-supervision (Ex-YT-S), the proposed method with InfoNCE video-level prediction across 4 shots (VIVI-Ex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the VTAB 1000 example mean score of the proposed method with exemplar frame/shot-level SSL and InfoNCE video-level prediction across 4 shots (VIVI-Ex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>10% labeled ImageNet examples (class-balanced, no additional unlabeled ImageNet examples are used) outperforms both the Semi-Ex-10% baseline and the ImageNet pre-trained ResNet-50 on the VTAB 1000 examples mean score. Using the entire labeled ImageNet training set for co-training yields an increase of 2.1 points. Finally, scaling up the architecture and applying AA to pre-process the ImageNet data adds 2.3 points, leading to a clear new state Per-data set comparison of our exemplar-based unsupervised model (VIVI-Ex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Flowers102Figure 7 :</head><label>7</label><figDesc>Per-data set comparison of VIVI-Ex(4) and a variant using AA. AA seems to benefit all data set categories similarly, and also leads to reductions in accuracy for a few data sets from all categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>+1.6) 73.6 83.1 55.5 VIVI-Ex(4)-Co(100%) 69.4 (+3.0) 69.9 83.3 62.1 VIVI-Ex(4)-Co(100%)-Big 71.7 (+5.3) 72.5 84.3 64.7</figDesc><table><row><cell>METHOD</cell><cell>MEAN</cell><cell>NAT. SPEC. STR.</cell></row><row><cell>Ex-ImageNet</cell><cell>59.5</cell><cell>50.5 81.4 56.4</cell></row><row><cell>VIVI-Ex(4)</cell><cell cols="2">62.5 (+3.0) 55.9 80.9 59.1</cell></row><row><cell>VIVI-Ex(4)-Big</cell><cell cols="2">63.3 (+3.8) 57.5 81.0 59.5</cell></row><row><cell>Semi-Ex-10% [69]</cell><cell>65.3</cell><cell>70.2 81.9 52.7</cell></row><row><cell>VIVI-Ex(4)-Co(10%)</cell><cell cols="2">67.2 (+1.9) 63.3 82.6 62.9</cell></row><row><cell>Sup-100% [69]</cell><cell>66.4</cell><cell>73.5 82.5 52.1</cell></row><row><cell>Sup-Rot-100% [69]</cell><cell>68.0 (</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ImageNet 68.0 [65.2, 70.7] 49.9 [46.9, 52.9] 18.1 VIVI-Ex(4)-Co(100%) 62.2 [59.3, 65.1] 46.3 [43.3, 49.2] 15.9</figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>Accuracy</cell><cell></cell></row><row><cell>Model Type</cell><cell>Original</cell><cell>Perturbed</cell><cell>∆</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the YT8M subset we use for training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>LR #it. w. #it.</figDesc><table><row><cell></cell><cell></cell><cell>LR schedule</cell><cell>WD</cell><cell>λ</cell><cell>γ</cell><cell cols="2">batch size #exemp.</cell></row><row><cell>Ex-ImageNet</cell><cell>0.8 120k 17k</cell><cell>×0.1@52k;86k</cell><cell>10 −4</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>8</cell></row><row><cell>Ex-YT-F</cell><cell>0.8 120k 17k</cell><cell>×0.1@52k;86k</cell><cell>10 −4</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>8</cell></row><row><cell>Ex-YT-S</cell><cell>0.8 120k 5k</cell><cell cols="2">×0.1@90k;110k 10 −4</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>8 (sh.)</cell></row><row><cell>VIVI-Ex(2)-Ord</cell><cell>0.8 120k 5k</cell><cell cols="2">×0.1@90k;110k 10 −4</cell><cell>{2.0, 1.0, 0.5}</cell><cell>-</cell><cell cols="2">1024 · 2 sh. 8 (sh.)</cell></row><row><cell>VIVI-Ex(2)</cell><cell>0.8 120k 5k</cell><cell cols="2">×0.1@90k;110k 10 −4</cell><cell>{0.08, 0.04, 0.02}</cell><cell>-</cell><cell cols="2">1024 · 2 sh. 8 (sh.)</cell></row><row><cell>VIVI-Ex(4)</cell><cell>0.8 120k 5k</cell><cell cols="2">×0.1@90k;110k 10 −4</cell><cell>{0.04, 0.02, 0.01}</cell><cell>-</cell><cell cols="2">512 · 4 sh. 8 (sh.)</cell></row><row><cell>VIV-Ex(4)-Big</cell><cell>0.8 120k 5k</cell><cell cols="2">×0.1@90k;110k 10 −4</cell><cell>0.04</cell><cell>-</cell><cell cols="2">512 · 4 sh. 8 (sh.)</cell></row><row><cell>VIVI-Ex(4)-Co(10%)</cell><cell cols="3">0.1 100k 3k ×0.1@76k;88k;96k 10 −3</cell><cell>0.04</cell><cell cols="2">{1.0, 4.0, 8.0, 16.0} 512 · 4 sh.,</cell><cell>8 (sh.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256 im.</cell><cell></cell></row><row><cell>VIVI-Ex(4)-Co(100%)</cell><cell cols="3">0.8 100k 3k ×0.1@70k;85k;95k 10 −4</cell><cell>0.04</cell><cell cols="2">{0.1, 0.5, 1.0, 5.0} 512 · 4 sh.,</cell><cell>8 (sh.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2048 im.</cell><cell></cell></row><row><cell cols="4">VIVI-Ex(4)-Co(100%)-Big 0.8 100k 3k ×0.1@70k;85k;95k 10 −4</cell><cell>0.04</cell><cell cols="2">{0.1, 0.5, 1.0, 5.0} 512 · 4 sh.,</cell><cell>8 (sh.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2048 im.</cell><cell></cell></row><row><cell>Rot-ImageNet</cell><cell>0.8 120k 17k</cell><cell>×0.1@52k;86k</cell><cell>10 −4</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>1</cell></row><row><cell>Rot-YT-F</cell><cell>0.8 120k 17k</cell><cell>×0.1@52k;86k</cell><cell>10 −4</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>1</cell></row><row><cell>VIVI-Rot(2)</cell><cell>0.8 120k 5k</cell><cell cols="3">×0.1@90k;110k 10 −4 {0.2, 0.1, 0.05, 0.025}</cell><cell>-</cell><cell cols="2">1024 · 2 sh. 4 (sh.)</cell></row><row><cell>VIVI-Rot(4)</cell><cell>0.8 120k 5k</cell><cell cols="3">×0.1@90k;110k 10 −4 {0.32, 0.16, 0.08, 0.04}</cell><cell>-</cell><cell cols="2">512 · 4 sh. 4 (sh.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Learning rate (LR), number of training iterations (#it.), number of linear warm-up iterations (w. #it.), learning rate schedule (LR schedule), weight decay (WD), video-level loss weight (λ), supervised cross-entropy loss weight (γ), batch size, and the number of exemplars (#exemp.) for the different models considered in this paper. Lists of values indicate values explored in the parameter sweep, with the optimal value (in terms of validation VTAB 1000 example score) underlined. For the co-training methods we indicate video (suffix "sh.") and image (suffix "im.") batch size. If the number of exemplars is followed by "(sh.)" we use consecutive frames of the same shot to create exemplars.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>-Co(100%) improves over supervised ImageNet pre-training (Sup-100%).</figDesc><table><row><cell>METHOD</cell><cell>AP</cell></row><row><cell>RetinaNet (Random init.)</cell><cell>26.9</cell></row><row><cell>RetinaNet (MS)</cell><cell>32.3</cell></row><row><cell>RetinaNet (Ex-YT-F)</cell><cell>32.6</cell></row><row><cell>RetinaNet (MT-SSL)</cell><cell>32.7</cell></row><row><cell>RetinaNet (VIVI-Ex(4))</cell><cell>33.6</cell></row><row><cell>RetinaNet (Sup-100%)</cell><cell>35.3</cell></row><row><cell cols="2">RetinaNet (VIVI-Ex(4)-Co(100%)) 36.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Object detection performance of RetinaNet [36] on the COCO-2017 data set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>METHODMEAN NAT. SPEC. STR.</figDesc><table><row><cell>TI</cell><cell>44.2</cell><cell>35.0</cell><cell>61.8</cell><cell>43.3</cell></row><row><cell>MS</cell><cell>47.1</cell><cell>34.2</cell><cell>68.4</cell><cell>47.9</cell></row><row><cell>Rel.Pat.Loc</cell><cell>50.8</cell><cell>45.0</cell><cell>73.0</cell><cell>44.8</cell></row><row><cell>Jigsaw</cell><cell>51.1</cell><cell>42.5</cell><cell>75.6</cell><cell>46.4</cell></row><row><cell>Rot-YT-F</cell><cell>55.6</cell><cell>44.9</cell><cell>76.1</cell><cell>54.8</cell></row><row><cell>VIVI-Rot(4)</cell><cell>56.7</cell><cell>47.7</cell><cell>78.4</cell><cell>53.7</cell></row><row><cell>VIVI-Rot(2)</cell><cell>57.6</cell><cell>49.7</cell><cell>78.6</cell><cell>54.1</cell></row><row><cell>Rot-YT-F-AA</cell><cell>57.7</cell><cell>48.5</cell><cell>77.8</cell><cell>55.7</cell></row><row><cell>Ex-YT-F</cell><cell>58.1</cell><cell>49.4</cell><cell>79.5</cell><cell>54.9</cell></row><row><cell>BigBiGAN</cell><cell>59.1</cell><cell>56.6</cell><cell>79.1</cell><cell>51.3</cell></row><row><cell>Ex-ImageNet</cell><cell>59.2</cell><cell>52.9</cell><cell>80.9</cell><cell>53.8</cell></row><row><cell>MT-SSL</cell><cell>59.2</cell><cell>51.9</cell><cell>78.9</cell><cell>55.8</cell></row><row><cell>Rot-ImageNet</cell><cell>59.6</cell><cell>53.8</cell><cell>77.2</cell><cell>56.0</cell></row><row><cell>Ex-YT-S</cell><cell>60.2</cell><cell>54.8</cell><cell>81.3</cell><cell>54.3</cell></row><row><cell>VIVI-Ex(2)-TimeArrow</cell><cell>60.7</cell><cell>55.2</cell><cell>80.6</cell><cell>55.5</cell></row><row><cell>VIVI-Ex(4)</cell><cell>61.0</cell><cell>55.6</cell><cell>79.7</cell><cell>56.4</cell></row><row><cell>VIVI-Ex(2)</cell><cell>61.4</cell><cell>55.3</cell><cell>79.9</cell><cell>57.5</cell></row><row><cell>Ex-YT-S-AA</cell><cell>61.5</cell><cell>56.1</cell><cell>81.7</cell><cell>56.0</cell></row><row><cell>Sup-10%</cell><cell>61.6</cell><cell>65.9</cell><cell>80.2</cell><cell>48.5</cell></row><row><cell>VIVI-Ex(4)-AA</cell><cell>61.7</cell><cell>56.3</cell><cell>80.9</cell><cell>57.0</cell></row><row><cell>VIVI-Ex(4)-Big</cell><cell>61.9</cell><cell>56.9</cell><cell>80.6</cell><cell>57.1</cell></row><row><cell>Ex-YT-F-AA</cell><cell>61.9</cell><cell>56.1</cell><cell>81.7</cell><cell>57.2</cell></row><row><cell>VIVI-Ex(4)-Big-AA</cell><cell>63.5</cell><cell>58.2</cell><cell>81.5</cell><cell>59.0</cell></row><row><cell>Semi-Ex-10%</cell><cell>63.9</cell><cell>69.3</cell><cell>80.3</cell><cell>50.9</cell></row><row><cell>Sup-100%</cell><cell>65.6</cell><cell>72.6</cell><cell>82.2</cell><cell>51.1</cell></row><row><cell>VIVI-Ex(4)-Co(10%)</cell><cell>66.3</cell><cell>63.1</cell><cell>82.4</cell><cell>61.1</cell></row><row><cell>Sup-Rot-100%</cell><cell>67.5</cell><cell>73.1</cell><cell>83.2</cell><cell>54.8</cell></row><row><cell>VIVI-Ex(4)-Co(100%)</cell><cell>69.2</cell><cell>70.1</cell><cell>83.2</cell><cell>61.3</cell></row><row><cell>VIVI-Ex(4)-Co(100%)-Big</cell><cell>70.4</cell><cell>72.2</cell><cell>83.7</cell><cell>62.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use Version 1 of the VTAB benchmark (arXiv:1910.04867v1); please see Appendix E for Version 2 (arXiv:1910.04867v2) results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Video credit: https://vimeo.com/362621732 and https://en.wikipedia.org/wiki/Big_Buck_Bunny.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In practice, we use all shot embeddings from the other videos, not only those at time step k + m, which is known to improve performance<ref type="bibr" target="#b44">[45]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Please see Appendix E for Version 2 (arXiv:1910.04867v2) results; the relative improvements of our methods over baselines and the conclusions are similar.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/ethereon/caffe-tensorflow</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Raphael Marinier for help with preparing the YT8M data set. Further, we are grateful to Lucas Beyer for his implementation of GroupNorm with weight standardization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning classification with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virginia R De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding. NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object-centric representation learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<title level="m">Learning visual groups from co-occurrences in space and time</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Time-agnostic prediction: Predicting predictable video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning image representations tied to egomotion from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="161" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross pixel optical-flow similarity for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video shot boundary detection based on color histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notebook Papers TRECVID2003, NIST, 15</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Weight standardization</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nawid</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Timecontrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A systematic framework for natural perturbations from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02168</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<title level="m">The new data in multimedia research</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<title level="m">Alexey Dosovitskiy, et al. The Visual Task Adaptation Benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Self-supervised learning via conditional motion propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual invariance with temporal coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2011 Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<idno>68.5 19.1 52.2 69.0 41.3 60.9 11.1 77.5 92.6 65.4 70.7 43.5 59.6 33.6 68.2 70.7 29.3 47.2 35.2 53.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Pat.Loc</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Rot</surname></persName>
		</author>
		<idno>73.8 27.0 51.2 54.4 35.7 88.2 11.6 77.7 93.1 68.6 73.6 44.0 58.0 39.1 72.5 80.0 49.8 53.0 38.8 57.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Rot</surname></persName>
		</author>
		<idno>4) 73.4 25.8 52.0 53.1 42.2 88.5 10.3 84.1 93.7 66.3 73.6 49.9 58.3 38.7 73.6 88.9 52.4 52.8 40.6 58.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>Ord 76.0 29.0 49.0 77.7 54.7 88.5 13.6 80.5 94.2 73.2 73.6 55.9 60.2 39.1 72.0 91.5 52.1 51.5 42.8 61.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>75.3 28.8 48.7 77.5 55.5 87.9 12.4 81.6 94.1 73.6 73.6 56.3 60.6 38.9 73.1 91.9 52.2 50.6 44.6 62.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>4) 76.3 29.0 50.1 77.9 55.6 88.0 14.1 82.4 94.4 73.1 73.6 55.3 60.9 38.6 72.9 95.3 53.0 52.4 44.1 62.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 78.6 30.3 51.5 75.0 56.1 88.6 14.4 83.0 94.7 75.2 73.6 56.3 60.6 41.6 74.2 94.6 55.5 52.3 41.0 63.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>77.5 32.8 51.3 79.4 56.6 88.3 16.6 79.8 95.1 75.3 73.6 54.7 57.9 40.4 74.4 92.0 56.8 52.4 47.0 63.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 77.5 34.8 54.2 76.9 59.5 89.7 16.2 84.3 94.8 77.2 73.6 53.3 60.7 40.5 78.0 93.4 59.2 52.9 47.0 64.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
	<note>10%) 82.8 36.6 58.1 82.7 76.9 81.9 24.1 85.6 94.7 76.4 73.6 79.4 63.9 38.0 76.6 95.3 61.3 42.4 46.3</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>100%) 86.1 51.5 64.5 88.7 87.1 79.4 31.7 83.9 95.1 80.8 73.6 78.9 61.7 36.4 78.2 93.8 61.0 43.1 43.6 69.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>100%)-Big 88.0 53.3 69.0 90.4 88.4 84.4 34.1 86.2 95.9 81.7 73.6 79.9 63.5 37.3 82.9 95.3 67.4 46.2 44.9 71.7 MS 68.4 69.6 48.1 52.7 49.2 96.7 56.9 85.5 97.5 88.3 76.8 99.8 90.4 71.7 75.3 100.0 96.3 99.9</idno>
		<imprint/>
	</monogr>
	<note>4 80.0 TI 76.5 68.5 56.4 66.3 52.0 96.2 59.4 89.8 97.6 90.1 81.0 94.0 91.6 72.3 61.2 100.0 96.4 97.0 86.2 80.7 Jigsaw 79.1 65.3 63.9 77.9 65.4 93.9 59.2 83.0 97.9 92.0 80.1 99.6 88.6 72.0 74.7 100.0 90.3 99.9 93.6 83.0</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<idno>79.9 65.7 65.2 78.8 66.8 93.7 58.0 85.3 97.8 91.5 79.8 99.5 87.7 71.5 75.0 100.0 90.4 99.7 92.6 83.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Pat.Loc</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Rot</surname></persName>
		</author>
		<idno>92.1 76.3 79.1 100.0 96.5 100.0 97.7 85.3</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
	<note>7 74.1 61.6 75.1 67.6 97.0 61.9 86.7 98.4 92.6 77.7 99.8 92.5 76.4 81.3 100.0 96.6 99.9 97.1 85.4</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ex</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ImageNet 83.5 74.2 65.4 83.4 74.9 96.8 60.4 85.5 98.7 94.5 79.8 99.8 93.5 75.5 80.4 100.0 96.5 99.9 98.0 86.4</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">87</biblScope>
		</imprint>
	</monogr>
	<note>3 76.7 97.0 64.0 86.9 98.6 94.7 80.2 99.8 93.5 76.8 81.3 100.0 96.6 99.8 98.2</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Yt-F-Aa</forename><surname>Ex</surname></persName>
		</author>
		<idno>88.1 75.1 67.7 86.1 73.5 96.9</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>2 86.8 98.8 94.6 79.0 99.9 93.5 76.5 82.9 100.0 96.6 99.9 97.9 87.2 VIVI-Ex(2) 86.6 76.1 63.4 88.2 74.4 97.0 64.1 88.4 98.6 94.7 79.2 99.8 93.4 77.1 80.9 100.0 96.5 99.9 97.6</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 88.8 76.8 64.0 87.1 75.9</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>2 63.9 88.6 98.6 94.5 79.5 99.8 93.2 76.7 84.0 100.0 96.6 99.8 97.6</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>10%) 89.3 79.1 67.6 89.1 83.2 96.9 66.5 90.1 98.4 93.0 79.6 99.5 92.1 74.8 83.1 100.0 96.5 99.8 93.6 88.0</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>89.1 79.4 64.7 89.6 78.7 97.1 69.2 86.9</idno>
		<imprint/>
	</monogr>
	<note>6 95.6 80.2 99.8 93.6 77.2 81.8 100.0 96.6 99.9 98.6 88.3</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 90.5 80.4 68.5 87.5 78.3 97.3 68.7 88.7 98.7 95.3 80.5 99.9 92.8 77.8 81.0 100.0 96.7 100.0 98.1 88.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>100%) 92.5 82.0 73.2 92.7 90.9 96.8 70.7 87.4 98.5 93.7 80.2 99.4 91.2 73.4 82.1 100.0 96.5 98.9 96.5 89.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<idno>Sup-Rot-100% 94.6 84.8 75.9 94.7 91.5 97.0 70.2 85.9 98.8 94.9 79.5 99.8 92.5 76.5 82.3 100.0 96.5 100.0 98.4 90.2</idno>
		<title level="m">full VIVI-Ex(4)-Co(100%)</title>
		<imprint/>
	</monogr>
	<note>Big 93.5 85.9 77.2 94.4 91.6 97.3 73.7 89.4 98.8 95.1 81.0 99.7 92.5 76.7 84.8 100.0 96.6 99.7 94.6 90</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<idno type="arXiv">arXiv:1910.04867v2</idno>
		<title level="m">Table 8 we report complete testing results for the 1000 example transfer regime of Version 2</title>
		<imprint/>
	</monogr>
	<note>of the VTAB benchmark [69</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Note that the full data set evaluation is the same in both Versions 1 and 2. The 1000 example regime of Version 2 uses 800 samples for fine-tuning and 200 samples for validation to select the hyper-parameters for fine-tuning (see Section 4; the considered set of fine-tuning hyper-parameters remains the same). In contrast, Version 1 uses all the 1000 samples for fine-tuning, and the standard validation set for each data set. We emphasize that we do not re-select the training hyper-parameters (such as the video-level loss weight, see Table 6), but only the hyper-parameters for the transfer step. It can be seen that the relative improvements of our methods in terms of the VTAB 1000 example mean accuracy over the baselines obtained for Version 2 are comparable to Version 1, with few exceptions. Accordingly, the ranking of methods according to the mean accuracy remains mostly unchanged. Caltech101 CIFAR-100 DTD Flowers102 Pets SVHN Sun397 Camelyon EuroSAT Resisc45 Retinopathy Clevr-Count Clevr-Dist DM-Lab KITTI-Dist dSpr-Loc dSpr-Ori sNORB-Azim sNORB-Elev Mean TI 54</title>
	</analytic>
	<monogr>
		<title level="m">Table 9 shows the mean testing accuracy per data set category, and Table 10 is the Version 2-analog of Table 1</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>1 38.3 28.2 32.3 77.0 7.4 50.0 84.1 50.0 63.1 12.7 61.7 35.0 41.6 86.1 59.3 21.1 29.2 44.2 MS 52.3 12.7 37.3 32.6 15.8 81.8 6.8 76.8 89.7 49.7 57.3 43.2 55.7 38.4 48.4 81.2 46.4 34.8 35.1 47.1</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<idno>67.8 17.9 51.0 67.2 38.8 61.7 10.5 73</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Pat.Loc</note>
	<note>4 92.6 66.2 59.7 44.3 55.7 39.4 57.8 63.6 32.7 29.9 34.9 50.8 Jigsaw 66.2 14.8 50.7 65.3 34.0 54.9 11.4 73.0 91.5 66.7 71.3 44.1 56.2 42.2 63.8 66.0 34.2 32.9 31.7 51.1</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Rot</surname></persName>
		</author>
		<idno>4) 72.7 24.6 48.6 48.9 39.1 88.8 11.0 83.1 93.5 65.9 71.2 49.0 58.6 46.5 70.4 89.1 51.7 24.7 39.8 56.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Rot</surname></persName>
		</author>
		<idno>73.0 26.7 49.1 55.0 43.2 88.4 12.7 83.0 93.1 66.7 71.6 45.1 57.8 45.5 69.5 88.0 51.1 37.5 38.1 57.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>4) 75.0 29.3 49.0 77.8 55.3 87.9 15.1 80.3 94.2 73.9 70.4 55.2 58.8 46.4 70.9 91.3 51.4 34.9 42.1 61.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>74.8 28.8 49.5 77.5 54.0 88.2 14.0 80.0 94.0 73.7 71.9 56.0 61.1 46.5 70.1 91.8 51.0 38.2 45.0 61.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 78.6 30.5 50.9 74.8 55.8 88.8 14.5 81.2 94.9 75.5 71.8 54.3 61.0 45.7 76.3 93.0 54.1 31.7 39.7 61.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>76.5 33.0 51.6 75.2 57.1 87.5 17.1 80.1 94.7 75.1 72.4 55.4 57.1 46.6 73.3 89.4 55.1 35.6 44.1 61.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>AA 79.7 35.1 56.0 72.3 56.9 89.5 18.0 82.0 95.2 77.2 71.6 54.7 60.3 48.7 75.8 92.4 58.5 35.6 46.2 63.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>10%) 82.7 36.5 57.9 82.0 76.6 82.2 24.0 84.7 94.8 76.8 73.2 75.5 60.5 46.7 77.4 95.0 58.3 30.5 45.3 66.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<idno>100%) 86.7 51.6 64.8 88.2 86.3 80.4 32.5 84.3 95.1 80.9 72.6 78.0 60.4 45.4 80.2 92.9 61.7 30.2 41.8 69.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vivi-Ex</surname></persName>
		</author>
		<title level="m">Co(100%)-Big 87</title>
		<imprint/>
	</monogr>
	<note>6 53.6 68.9 89.9 87.0 84.1 34.2 85.9 95.5 81.6 71.9 75.8 62.8 45.9 83.9 95.3 62.1 27.1 45.3 70.4</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Ex&quot; and &quot;Rot&quot; stand for exemplar [15] and rotation prediction [18] frame-level self-supervision, respectively. These identifiers are followed with the number of shots in parentheses if an InfoNCE prediction loss across shots is used (except methods using shot order prediction have the suffix &quot;-Ord&quot;). Baseline methods only using frames and shots have the suffix &quot;YT-F&quot; and &quot;YT-S</title>
		<idno type="arXiv">arXiv:1910.04867v2)oftheVTABbenchmark[69</idno>
	</analytic>
	<monogr>
		<title level="m">Table 8: Testing accuracy for fine-tuning hyper-parameter selection according to Version</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Each number is the median of three fine-tuning runs. The proposed methods have the prefix VIVI. respectively. The suffix &quot;-AA&quot; denotes methods that use AutoAugment [8</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ex-Imagenet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
				<idno type="arXiv">arXiv:1910.04867v2</idno>
	</analytic>
	<monogr>
		<title level="m">Testing result summary as in Table 1 for fine-tuning hyper-parameter selection according to Version</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>of the VTAB benchmark. Each number is the median of three fine-tuning runs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
