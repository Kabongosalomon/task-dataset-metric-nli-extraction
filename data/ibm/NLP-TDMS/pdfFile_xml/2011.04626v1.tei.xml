<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Find it if You Can: End-to-End Adversarial Erasing for Weakly-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stammes</surname></persName>
							<email>erikstammes@me.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">TomTom</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">F H</forename><surname>Runia</surname></persName>
							<email>tomrunia@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">TomTom</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
							<email>mohsen.ghafoorian@tomtom.com</email>
							<affiliation key="aff1">
								<orgName type="institution">TomTom</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Find it if You Can: End-to-End Adversarial Erasing for Weakly-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is a task that traditionally requires a large dataset of pixel-level ground truth labels, which is time-consuming and expensive to obtain. Recent advancements in the weakly-supervised setting show that reasonable performance can be obtained by using only image-level labels. Classification is often used as a proxy task to train a deep neural network from which attention maps are extracted. However, the classification task needs only the minimum evidence to make predictions, hence it focuses on the most discriminative object regions. To overcome this problem, we propose a novel formulation of adversarial erasing of the attention maps. In contrast to previous adversarial erasing methods, we optimize two networks with opposing loss functions, which eliminates the requirement of certain suboptimal strategies; for instance, having multiple training steps that complicate the training process or a weight sharing policy between networks operating on different distributions that might be suboptimal for performance. The proposed solution does not require saliency masks, instead it uses a regularization loss to prevent the attention maps from spreading to less discriminative object regions. Our experiments on the Pascal VOC dataset demonstrate that our adversarial approach increases segmentation performance by 2.1 mIoU compared to our baseline and by 1.0 mIoU compared to previous adversarial erasing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is among the most fundamental tasks in computer vision, with applications ranging from autonomous vehicles <ref type="bibr" target="#b35">[36]</ref> to medical diagnosis <ref type="bibr" target="#b27">[28]</ref>. There has been remarkable progress in quality of semantic segmentation models in the era of deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b50">51]</ref>, in <ref type="bibr">Figure 1</ref>. Two examples of attention maps obtained from a classification network (left) and end-to-end adversarial erasing (right). Classification networks need only the minimum evidence to classify the objects that are present, hence they focus on the most discriminative objects regions. We call this the discriminative localization problem. Our proposed end-to-end adversarial erasing scheme resolves this problem by spreading the attention to less discriminative object regions. part due to the availability of large-scale datasets with pixellevel ground truth labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. However, labeling these datasets with pixel-level annotations is a laborious process. Weakly-supervised methods achieve reasonable performance with much coarser labels such as bounding boxes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">47]</ref>, scribbles <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>, points <ref type="bibr" target="#b4">[5]</ref> or even imagelevel labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. In this work we focus on leveraging image-level labels, which are the weakest form of supervision. It is common in methods that use only image-level labels to train a classification network and extract class activation maps (CAMs) as initial object locations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref>. However, learning semantic segmentation with only imagelevel labels is an ill-posed problem, since the labels indicate only the existence of a class instead of its location and shape. CNN − CNN + . . . <ref type="bibr">CNN</ref> − Adv. CNN <ref type="figure">Figure 2</ref>. High-level comparison of the iterative adversarial erasing methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50]</ref> (left) to our novel proposed approach (right) trained with a single iteration of adversarially trained models.</p><p>More specifically, the attended visual evidence generally corresponds to the most discriminative object regions and therefore fails to capture the complete object <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>We call this the discriminative localization problem, which is illustrated in the left images of <ref type="figure">Figure 1</ref>. This problem is especially prevalent in non-rigid object classes such as birds, cats, horses and sheep where the texture of the fur or skin is much less discriminative than other body parts such as heads or feet. Previous methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41]</ref> propose to alleviate this problem by introducing adversarial erasing, which sets a threshold on the attention map to generate a mask which can be used to remove the most discriminative object regions from the image. The resulting image is then fed into a second classification network to find less discriminative regions that belong to the same object. Some of the existing methods perform the erasing in multiple steps, either implemented as a multi-stage training approach <ref type="bibr" target="#b40">[41]</ref> or trained in an integrated fashion with multiple erasing networks trained jointly <ref type="bibr" target="#b49">[50]</ref>. This will result in either a complicated multistage training strategy or a more extensive memory footprint that might hinder leveraging state-of-the-art network architectures. <ref type="figure">Figure 2</ref> illustrates a high-level schematic comparison between the existing iterative erasing methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b40">41]</ref> and our proposed end-to-end approach. Other methods <ref type="bibr" target="#b24">[25]</ref> aim at avoiding this shortcoming by training a single erasing step while sharing the weights among the models operating on the input and erased input. The weight sharing, however, might result in suboptimal performance given the different distributions of data they are operating on.</p><p>Our proposed method follows the adversarial erasing methodology to recover the less discriminative object regions, but in contrast, we propose to train two separate networks, a localizer network and an adversarial network, in a truly adversarial manner. By involving the two networks in an adversarial game, we encourage the localizer networks to leave no visual clues for the adversarial networks to discover the existence of the corresponding class. Moreover, we regularize the localizer network to prefer solutions with smaller attention maps, to avoid low-specificity localization solutions that cover more than necessary in favor of winning the adversarial game. As a result, compared to previous methods, this setup eliminates the need for multiple consecutive localizer models during training and inference <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50]</ref> and weight sharing between models that operate on different data distributions <ref type="bibr" target="#b24">[25]</ref>. Furthermore, our proposed framework does not rely on extra supervision such as additional data or saliency estimation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18]</ref>. To demonstrate the effectiveness of our method we not only show improved results with the proposed adversarial training scheme as a stand-alone model, but also integrate our end-to-end adversarial erasing in Pixel-Level Semantic Affinity (PSA) <ref type="bibr" target="#b1">[2]</ref> and achieve better segmentation performance.</p><p>The main contributions of this paper are as follows: (1) we propose a novel end-to-end adversarial erasing method which helps capturing less discriminative object regions, <ref type="bibr" target="#b1">(2)</ref> we show how this approach can be integrated into existing weakly-supervised semantic segmentation methods, and <ref type="formula" target="#formula_3">(3)</ref> we demonstrate its effectiveness on the Pascal VOC 2012 benchmark, outperforming the baselines. The implementation is included as supplementary material and will be made publicly available upon the acceptance of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Visual Attention. Since the early days of the breakthrough of deep neural networks, considerable attention was given to shed light into these "black boxes" to better understand the decision making process. For instance, in visual tasks such as image classification it is often useful to highlight the image regions responsible for the network's decision. Earlier work achieved this by visualizing partial derivatives of predicted class scores w.r.t. the input image <ref type="bibr" target="#b36">[37]</ref> or by making modifications to raw gradients <ref type="bibr" target="#b47">[48]</ref>. CAMs <ref type="bibr" target="#b51">[52]</ref> can highlight relevant regions by adapting a global average pooling layer and a fully connected layer for classification. Selvaraju et al. <ref type="bibr" target="#b32">[33]</ref> extend this approach to Grad-CAM which utilizes gradients to make it possible to get visual explanations for tasks such as image captioning and visual question answering without any network architecture changes.</p><p>Adversarial Erasing. Visual attention techniques are often used for downstream tasks such as object detection and semantic segmentation when there is only a weak supervi-sion signal <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref>. Often, classification is used as a proxy task to generate the attention maps. Since classification needs only the minimum evidence to make a prediction, only the most discriminative regions of an image are used in the decision making process. In downstream tasks this leaves unsatisfactory results, as the goal is to capture the entire object in the image. To mitigate this issue, adversarial erasing was first introduced by <ref type="bibr" target="#b40">[41]</ref>. In adversarial erasing, the most discriminative object region is found using attention maps and then erased from the images. The erased images are then sent into another classification network to find less discriminative object regions belonging to the same entity. Finally, the attention maps are combined to create a segmentation mask. This approach has been improved by <ref type="bibr" target="#b49">[50]</ref>, which integrated the erasing step into training by erasing from the feature map instead of the image. However, both of these approaches still require multiple training and/or inference steps and the fusion of attention maps into segmentation masks. Li et al. <ref type="bibr" target="#b24">[25]</ref> resolve this by sharing weights between two classifiers and applying a soft thresholding technique, which allows the attention maps of the initial classifier to grow to less discriminative object regions <ref type="bibr" target="#b24">[25]</ref>. In adversarial erasing approaches, the attention often starts to spread to the background regions that are highly correlated with the corresponding objects. This can be fixed by using extra supervision in the form of saliency masks <ref type="bibr" target="#b17">[18]</ref>. In contrast to the previous approaches, we utilize adversarial erasing without the need of multiple localizer models during training/inference, weight sharing or saliency masks. We achieve this by training two models using distinct optimizers with adversarial objectives. Our approach is simple and can be easily plugged into existing weakly-supervised methodologies.</p><p>Weakly-Supervised Semantic Segmentation. In weaklysupervised semantic segmentation (WSSS), the supervision signal is reduced from pixel-level labels to bounding boxes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">47]</ref>, scribbles <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>, points <ref type="bibr" target="#b4">[5]</ref> or even imagelevel labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. In this work we focus on image-level labels, as it is the hardest task and reduces the labeling efforts the most. A number of methods utilize adversarial erasing to produce semantic segmentation masks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>. Furthermore, there are methods that randomly hide parts of the feature map <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8]</ref> and methods that utilize cross-image features <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>. In many weakly-supervised methods class agnostic saliency methods are used as cues of object and background <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>. Common to many WSSS methods, the output segmentation masks are used as proxy labels to train a fully supervised semantic segmentation model <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref>. Our framework does not require saliency masks and is agnostic to the choice of training a fully supervised semantic segmentation model. Adversarial Training. The idea of adversarial training has gained significant attention in recent years after the introduction of Generative Adversarial Nets (GANs) <ref type="bibr" target="#b14">[15]</ref>, consisting of two competing networks, the generator and the discriminator. The task of the discriminator is to predict whether a given input image is coming from the real data distribution or the distribution of fake images generated by the generator, while the task of the generator is to fool the discriminator by matching the distribution of real data. This approach to image synthesis has been proven to be powerful and has resulted in generating convincing looking images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. The idea of adversarial training has since been extended to different tasks, such as image-to-image translation <ref type="bibr" target="#b18">[19]</ref>, reconstructing 3D objects from images <ref type="bibr" target="#b43">[44]</ref>, image super-resolution <ref type="bibr" target="#b39">[40]</ref> and semantic segmentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Similar to our approach, adversarial training has been used to find complete segmentation masks from weak supervision <ref type="bibr" target="#b33">[34]</ref>, but unlike our method this setup imposes shape priors on the generator and is used for the task of automatic object removal. The term adversarial training has also been loosely used in the WSSS field, where it denotes erasing part of an image and training an auxiliary model on this new image, despite not having any adversarial objective formulation and/or independent and competing models with different parameterization. Our approach to adversarial training is closer to the original adversarial training formulation, as we use two distinct models with opposing objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section the proposed method, end-to-end adversarial erasing (EADER), is described. First, we present our novel adversarial training formulation for weaklysupervised semantic segmentation. Then we illustrate the effectiveness of our method, by integrating end-to-end adversarial erasing into an existing weakly supervised semantic segmentation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">End-to-End Adversarial Erasing</head><p>Our proposed method consists of two image classifiers: an image classifier and an adversarial model. Both can be instantiated by any appropriate convolutional neural network. The first image classifier network is used to localize the target object using attention maps, hence we call this network the localizer network. The attention maps are then converted to masks by a soft, differentiable thresholding operation. Next, the masks are used to create a new image where the the most discriminative object regions are erased. These images are then forwarded through the second network, which we call the adversarial network. Its goal is to classify the images correctly, even when the target classes are erased. The localizer and adversarial networks' image classifiers are optimized using binary cross entropy loss, but in alternating fashion using distinct optimizers. To force the localizer network to not only classify the image correctly, but also to spread its attention to less discriminative object regions, we add an adversarial loss term to the localizer. This term captures the ability of the adversarial network to still classify the erased object. A trivial solution for the localizer would then be to hide the entire image from the adversarial, hence we regularize the localizer with an additional regularization loss term. This limits the attention of the localizer and thus forces it to only erase the regions that belong to the target class. An overview of the end-to-end adversarial framework is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><formula xml:id="formula_0">Consider a dataset D = {x i , y i } N i=1</formula><p>, where N is the number of images, x i the input image and y i a multi-hot vector of length C, with C being the number of classes, and in which y i,c = 1, if class c is present in x i and y i,c = 0, otherwise. Note that being in a multi-label setup, multiple classes can be present in an input image and hence c y i,c ≥ 1.</p><p>Localizer network. The localizer can be instantiated by any convolutional neural network from which (Grad-)CAMs can be extracted. For simplicity we assume the usage of CAMs but it is straightforward to extend this approach to more advanced attention extraction methods such as Grad-CAM. The localizer G with trainable parameters ϕ is trained as multi-label classifier using binary cross entropy loss on each label class:</p><formula xml:id="formula_1">L loc (G ϕ (x i ), y i ) = − 1 C c y i,c ln (G ϕ (x i )) +(1 − y i,c ) ln (1 − G ϕ (x i ))<label>(1)</label></formula><p>Attention maps. Given a trained localizer network G ϕ , the attention map A c for class c can be obtained using its feature map of the final convolutional layer g final ϕ and the classification weights w c as follows:</p><formula xml:id="formula_2">A c (x i ) = ReLU w T c g final ϕ (x i ) .<label>(2)</label></formula><p>A c is then normalized so that the maximum activation equals 1.</p><p>Soft masks. Only the attention maps for ground truth classes are kept, which are then resized to the input image dimensions and a soft thresholding operation is applied to generate class specific masks M c</p><formula xml:id="formula_3">M c (x i ) = σ (ω (A c (x i ) − ψ)) ,<label>(3)</label></formula><p>where σ is the sigmoid non-linearity, ψ is the threshold value and ω is a scaling parameter that ensures that values above the threshold are (close to) 1 and values below are (close to) 0. In contrast to a regular thresholding operation, this soft threshold is differentiable which allows the gradients from any further computations to backpropagate to the localizer.</p><p>Erasing. The input images for the adversarial network, where the attention maps have been erased, are computed as follows:</p><formula xml:id="formula_4">x i,c = x i (1 − M c (x i ))<label>(4)</label></formula><p>Note here that only the attention map of one particular class is erased, which is why multiple images are created in cases where there is more than one target.</p><p>Adversarial network. The adversarial network F with trainable parameters θ is then trained as multi-label classifier using the same binary cross entropy loss function:</p><formula xml:id="formula_5">L adv (F θ (x i ), y i ) = − 1 C c y i,c ln (F θ (x i,c )) +(1 − y i,c ) ln (1 − F θ (x i,c ))<label>(5)</label></formula><p>Hence, the goal of this network is to classify the same targets as before, despite the erased evidence.</p><p>Attention mining loss. To encourage the model to erase the object evidence thoroughly, we engage the localizer network in an adversarial game with the adversarial model. We follow <ref type="bibr" target="#b24">[25]</ref> by utilizing attention mining loss, which is the mean of the logits of the classes that have been erased:</p><formula xml:id="formula_6">L am (x i , y i ) = 1 C c∈yi F θ (x i,c )<label>(6)</label></formula><p>Regularization loss. Finally, to regularize the localizer, we impose an additional loss term:</p><formula xml:id="formula_7">L reg (x i , y i ) = 1 W × H × C c∈yi j,k A c (x i ) j,k ,<label>(7)</label></formula><p>where W , and H represent the width and height of the activations. Incorporating this regularization loss in the optimization process encourages the localizer to find a minimum attention map that covers the target class and hence prevents the localizer from the trivial solution where it erases the entire image to globally minimize the attention mining loss.</p><p>Total loss. The total loss function for the localizer then becomes:</p><formula xml:id="formula_8">L total = L loc + αL am + βL reg ,<label>(8)</label></formula><p>where α and β are hyper-parameters to tune the importance of the adversarial and regularization losses respectively. While the localizer is trained to minimize its adversarial loss term, the adversarial model tries to maximize it, by minimizing its loss in Equation 5.  The images x are forwarded through the localizer Gϕ to extract per-class (c) attention maps Ac. Using a soft-thresholding operation they are converted to masks Mc, which are used to create images where the most discriminative object parts have been erased (x). These are forwarded through the adversarial F θ , which is optimized using a classification loss Ladv. The localizer is optimized using a classification loss Lloc and an adversarial loss term Lam. This forces the localizer to spread its attention to less discriminative object parts, while the Lreg loss encourages the model to bound the activation to the minimum necessary area.</p><p>Segmentation maps. After training the model with the described loss terms, we convert the attention maps to segmentation maps. We first upsample and stack all the attention maps into the image resolution with C + 1 channels. Since we do not train the classification models for the background class, we set the first channel to a threshold value of ρ. To obtain the segmentation masks we take the argmax over the class dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrability of End-to-End Adversarial Erasing</head><p>The proposed method is simple and integrable, and we showcase this by integrating the proposed end-to-end adversarial erasing scheme into an existing WSSS method. We integrate it into Pixel-level Semantic Affinity (PSA) <ref type="bibr" target="#b1">[2]</ref>, a multi-stage method which suffers from the discriminative localization problem in its first stage. In this stage a classification network is trained from which CAMs are extracted. This stage does not utilize specific methods to improve the segmentation masks, but training and test-time data augmentations increase performance in this regard. The latter two stages train AffinityNet, which generates pseudo segmentation masks, and a fully-supervised segmentation model, which uses the pseudo masks as training data.</p><p>The CAM-generation stage of PSA is suitable for adversarial training as it suffers from the discriminative localization problem and because the classification network is suitable as a localizer, i.e. the CAMs are generated from the final convolutional layer without the need of any postprocessing or other gradient-breaking computations. As before, we apply a soft threshold on the attention maps to create masks, which are then used to erase the most discriminative object regions from the input images. The resulting images are forwarded through the adversarial network and attention mining loss is applied as adversarial loss on the localizer network.</p><p>Note that a baseline method need not be multi-stage to make it suitable for integrability of EADER. As long as the attention map can be obtained without breaking the gradients, EADER can be integrated into the method to find less discriminative object regions to improve the attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset. We evaluate the performance of the proposed method on the Pascal VOC 2012 segmentation dataset <ref type="bibr" target="#b9">[10]</ref>, the most widely used benchmark on weakly supervised semantic segmentation. The dataset consists of 20 object classes and one background class and contains 1464, 1449 and 1456 images in the train, validation and test sets respectively. Following the previous works in the WSSS literature, we augment the dataset with annotations from Hariharan et al. <ref type="bibr" target="#b15">[16]</ref>, resulting in a total of 10582 training images. We report the mean intersection-over-union (mIoU) for the validation and test sets. The test set results are obtained using the official Pascal VOC evaluation server.</p><p>In contrast to the previous adversarial erasing methods we do not employ any post-processing and keep the tricks to a minimum to keep our method simple. More specifically, we leave out tricks such as test-time augmentations, postprocessing and saliency cues to the method we integrate with.</p><p>Network architecture details. We test the adversarial training approach with a ResNet-101 <ref type="bibr" target="#b16">[17]</ref>  localizer is a WideResNet <ref type="bibr" target="#b45">[46]</ref> with 38 convolutional layers, while the new adversarial model is a ResNet-18. In the final stage we train a fully supervised semantic segmentation network on proxy labels. We utilize DeepLabV3+, which is a modern segmentation model, with ResNet-101 and Xception-65 backbones and the default training strategy from <ref type="bibr" target="#b6">[7]</ref>.</p><p>Training specifications. We train the localizer with a batch size of 16, while the batch size is dynamic for the adversarial model as it depends on the number of objects in each image. For example, when each image in the batch of 16 has two object classes, both objects are erased from each image separately and the batch size for the adversarial network will be 32. We randomly resize and crop the input images into 448 × 448 for both the localizer and the adversarial model. Both networks are optimized for 10 epochs with stochastic gradient descent with a learning rate of 0.01. We alternately train the localizer and adversarial per 200 training steps. Throughout the experiments, unless specified otherwise, we have used an α value of 0.05 and β is set to 10 −5 <ref type="figure">(Equation 8</ref>). Further hyperparameter values are ω = 100, ψ = 0.5 (both Equation 3) and ρ = 0.3. We follow the training settings of <ref type="bibr" target="#b1">[2]</ref> when training PSA and use an initial learning rate of 0.01 for the adversarial network. To show that our method is agnostic to attention map generation method, we utilize Grad-CAM <ref type="bibr" target="#b32">[33]</ref> in our experiments and CAM <ref type="bibr" target="#b51">[52]</ref> when integrating into PSA. This also ensures a fair comparison to PSA, which utilizes CAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Our first experiment is an ablation study to verify our hypothesis that the adversarial network forces the localizer network to spread its attention to less discriminative object regions. Recall from Equation 8 that α controls the strength of the adversarial loss term. In <ref type="table" target="#tab_0">Table 1</ref> we vary the α parameter and report the mIoU, precision and recall of the segmentation masks. We make the following observations: first, we find that a higher α indeed increases the recall, i.e. it forces the localizer to spread its attention to less discriminative object regions. Second, we observe that this increase in recall also increases performance in terms of mIoU. The α = 0 α = 0.01 α = 0.05 α = 0.1 <ref type="figure">Figure 4</ref>. Attention maps obtained using Grad-CAMs from the end-to-end adversarial erasing method using different values for the adversarial loss term α. As the α value increases, the attention spreads to less discriminative object regions.</p><p>highest mIoU is obtained at α = 0.05, which strikes the right balance between precision and recall. A higher α value further increases the recall but the degradation in precision is stronger, resulting in a lower mIoU score. Example attention maps generated for different α values are shown in <ref type="figure">Figure 4</ref>. Consistent with the previous observation, we see that a higher α value forces the attention map to spread to less discriminative object regions. However, when the value is too high some pixels belonging to other classes and background regions receive high responses and therefore cause a drop in the precision. A similar effect can be achieved by tuning the threshold (ρ) without needing an adversarial model. A lower threshold value increases the recall and decreases the precision, and vice versa. However, decreasing the threshold value to obtain higher recall is unsatisfactory, as the localizer is not trained to find less discriminative regions belonging to the same object. As a result, the localizer only focuses on the most discriminative object regions, failing to capture the entire object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to PSA</head><p>We now compare the original PSA results to the results where we have integrated end-to-end adversarial erasing. <ref type="table" target="#tab_1">Table 2</ref> shows the improvements in terms of mIoU. Additionally, we report precision and recall after the CAM generation stage. End-to-end adversarial erasing improves performance in this stage for all metrics. In other words, the combination of the adversarial and regularization loss terms forces the attention map to spread to less discriminative object regions without spreading to background areas. Besides, the mIoU scores in this stage are higher than those reported in <ref type="table" target="#tab_0">Table 1</ref>, which is caused by the extensive test-time augmentations used by PSA. In the next stage, training AffinityNet with the improved outputs of the first stage again results in better mIoU scores. Finally, we report results when training a fully supervised semantic segmentation model on the proxy labels generated by AffinityNet. We report the results of training DeepLabV3+ on the proxy labels generated from PSA with and without end-to-end adversarial erasing. Again, with end-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth PSA PSA w/ EADER <ref type="figure">Figure 5</ref>. Qualitative results on the Pascal VOC 2012 validation set. The white edges in the ground truth mask denote pixels that are ignored during evaluation. End-to-end adversarial erasing strategy increases the recall without sacrificing the precision.</p><p>to-end adversarial erasing the mIoU improves, showing the integrability of end-to-end adversarial erasing into existing WSSS methods. In <ref type="table" target="#tab_2">Table 3</ref> we make a per-class comparison of mIoU scores on the validation set. Recall that the discriminative localization problem is especially prevalent in non-rigid object classes. We find that end-to-end adversarial erasing significantly improves the results in many non-rigid object classes such as bird, cat, cow and horse. Typically in these object classes the most discriminative object region is the head or the feet, which causes the attention map to cover only a small portion of these object classes. With end-to-end adversarial erasing, the localizer is forced to capture the entire object region, as the fur or skin of these object classes are less discriminative, but still recognizable. For outdoor object classes the results are often similar to PSA, while for indoor object classes the performance is often degraded. Overall, end-to-end adversarial erasing increases the performance. In <ref type="figure">Figure 5</ref> we show some qualitative results demonstrating the increase in precision, recall and mIoU. In the first four rows we find that end-to-end adversarial erasing better segments objects by capturing less discriminative object regions, especially for non-rigid object classes. The increased specificity, as for instance observed in the last samples, can be attributed to the regularization term that forces the attention to spread only to areas where the localizer is confident that it is an object region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to Adversarial Erasing Methods</head><p>In <ref type="table" target="#tab_5">Table 5</ref> we compare our results to previous WSSS methods that follow an adversarial erasing strategy. Note that in each method the adversarial erasing is a component in a multi-stage setup. We outperform all existing adversarial erasing methods, even when most of them use stronger supervision signals in the form of saliency masks. When comparing to ACoL <ref type="bibr" target="#b49">[50]</ref>, the only other adversarial erasing methodology without saliency masks, we significantly outperform their method on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison to the State-of-the-Art</head><p>In <ref type="table">Table 4</ref> we compare to the previous WSSS methods, where we report the feature extractor that is used to gener-  <ref type="table">Table 4</ref>. Comparison of WSSS methods on the Pascal VOC 2012 dataset. For the supervision, I denotes image-level labels, S denotes saliency masks and F denotes pixel-level labels, which is the upper bound for fully supervised semantic segmentation. For the feature extractor, the mentioned architecture is the one that is used to generate the initial object locations (e.g. by utilizing CAMs).  ate object locations and the fully supervised model that is trained on proxy labels. The methods denoted with supervision signal F denote the upper bound of the segmentation performance. Note that our method outperforms a fully supervised FCN <ref type="bibr" target="#b28">[29]</ref> network and we achieve ≈ 73% of the upper bound, set by a fully supervised DeepLabV3+ <ref type="bibr" target="#b5">[6]</ref> model. Further, we show that we outperform many existing WSSS methods, but are outperformed by some others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Supervision Validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we presented a novel end-to-end adversarial erasing method to resolve the discriminative localization problem, an inherent issue in weakly-supervised semantic segmentation methods. This approach is easily integrable to existing methods, not requiring iterative classifiers, postprocessing, weight sharing or saliency masks, unlike many previous adversarial erasing methods. We further show that end-to-end adversarial erasing improves performance on the Pascal VOC 2012 dataset, especially on most non-rigid object classes, which suffer the most from the discriminative localization problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>xy:</head><label></label><figDesc>{cat, dog} localizer (G ϕ ) attention map (A c ) L loc w.r.t. ϕ mask (M c ) L reg w.r.t. ϕ −x adversarial (F θ ) L adv w.r.t. θ L am w.r.t. ϕ − = erasing = soft thresholding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An overview of our end-to-end adversarial erasing framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>localizer network, while the adversarial model is a ResNet-18 network. We utilize ImageNet pre-trained weights for both networks. When integrating with PSA, to ensure fair comparisons, we do not change any of the existing networks, which means the α mIoU Precision Recall 0 41.37 ± 0.26 58.26 ± 0.50 58.18 ± 0.66 0.01 42.51 ± 0.41 57.79 ± 0.72 60.88 ± 0.52 0.05 43.89 ± 0.40 54.78 ± 1.03 68.13 ± 1.51 0.1 42.88 ± 0.99 52.68 ± 2.30 69.31 ± 1.84 Performance of the model with different α values on the Pascal VOC 2012 validation set. We report both the mean and standard deviation over 6 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison to our baseline, Pixel-level Semantic Affinity (PSA), on the Pascal VOC 2012 validation set. To enable a fair comparison we reproduce the PSA numbers and train the proxy labels from AffinityNet on DeepLabV3+. The numbers with a † denote our reproduced results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAM</cell><cell></cell><cell cols="4">AffinityNet DeepLabV3+</cell><cell></cell></row><row><cell></cell><cell cols="2">Model</cell><cell cols="5">mIoU Precision Recall</cell><cell>mIoU</cell><cell cols="2">mIoU</cell><cell></cell></row><row><cell></cell><cell>PSA</cell><cell></cell><cell cols="2">46.8</cell><cell>60.3  †</cell><cell cols="2">66.7  †</cell><cell>58.7</cell><cell cols="2">60.7  †</cell><cell></cell></row><row><cell></cell><cell cols="4">PSA w/ EADER 48.6</cell><cell>61.3</cell><cell></cell><cell>68.7</cell><cell>60.1</cell><cell>62.8</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>bkg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell></cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell></row><row><cell>PSA</cell><cell>86.7</cell><cell>53.2</cell><cell>29.1</cell><cell>76.7</cell><cell>44.2</cell><cell></cell><cell>67.7</cell><cell>85.2</cell><cell>72.4</cell><cell>71.7</cell><cell>26.7</cell><cell>76.5</cell></row><row><cell cols="2">PSA w/ EADER 88.2</cell><cell>54.9</cell><cell>31.3</cell><cell>84.1</cell><cell>58.2</cell><cell></cell><cell>70.9</cell><cell>83.0</cell><cell>76.2</cell><cell>82.1</cell><cell>24.4</cell><cell>80.6</cell></row><row><cell>Method</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell cols="2">person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>PSA</cell><cell>40.9</cell><cell>72.2</cell><cell>68.2</cell><cell>70.2</cell><cell>66.4</cell><cell></cell><cell>37.8</cell><cell>80.9</cell><cell>38.5</cell><cell>62.8</cell><cell>45.4</cell><cell>60.7</cell></row><row><cell cols="2">PSA w/ EADER 35.8</cell><cell>80.7</cell><cell>76.4</cell><cell>73.7</cell><cell>70.8</cell><cell></cell><cell>15.4</cell><cell>77.2</cell><cell>34.6</cell><cell>66.4</cell><cell>52.6</cell><cell>62.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Per-class comparison with Pixel-level Semantic Affinity (PSA) on Pascal VOC 2012 validation set with only image-level supervision.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the previous adversarial erasing methods for WSSS on the Pascal VOC 2012 dataset. For supervision, I denotes image-level labels and S denotes saliency masks. The result with † was obtained from<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10762" to="10769" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Msg-gan: Multi-scale gradients for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7799" to="7808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ciompi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efstratios Gavves, and Mohsen Ghafoorian. I bet you are wrong: Gambling adversarial networks for structured semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Samson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Nanne Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial scene editing: Automatic object removal from weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Rakshith R Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7706" to="7716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th international conference on intelligent transportation systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep clustering for weakly-supervised semantic segmentation in autonomous driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to segment with image-level annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="234" to="244" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scribble-supervised segmentation of aerial building footprints using adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenrui</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="58898" to="58911" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-toend weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
