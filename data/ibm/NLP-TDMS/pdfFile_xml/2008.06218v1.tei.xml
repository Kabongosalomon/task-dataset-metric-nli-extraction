<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Which Strategies Matter for Noisy Label Classification? Insight into Loss and Uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyoung</forename><surname>Shin</surname></persName>
							<email>wonyoung.shin@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Naver Shopping</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
							<email>jungwoo.ha@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengzhe</forename><surname>Li</surname></persName>
							<email>s.li@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Naver Shopping</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwoo</forename><surname>Cho</surname></persName>
							<email>yongwoo.cho@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Naver Shopping</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoyean</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunyoung</forename><surname>Kwon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Which Strategies Matter for Noisy Label Classification? Insight into Loss and Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label noise is a critical factor that degrades the generalization performance of deep neural networks, thus leading to severe issues in real-world problems. Existing studies have employed strategies based on either loss or uncertainty to address noisy labels, and ironically some strategies contradict each other: emphasizing or discarding uncertain samples or concentrating on high or low loss samples. To elucidate how opposing strategies can enhance model performance and offer insights into training with noisy labels, we present analytical results on how loss and uncertainty values of samples change throughout the training process. From the in-depth analysis, we design a new robust training method that emphasizes clean and informative samples, while minimizing the influence of noise using both loss and uncertainty. We demonstrate the effectiveness of our method with extensive experiments on synthetic and real-world datasets for various deep learning models. The results show that our method significantly outperforms other state-of-the-art methods and can be used generally regardless of neural network architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in deep learning have significantly improved performance in numerous tasks due to large quantities of human-annotated data. While standard large-scale benchmark datasets used for deep learning research such as ImageNet <ref type="bibr" target="#b0">[1]</ref> are generally clean and error-free, most real-world data contain noisy labels, which refer to observed labels that are incorrect <ref type="bibr" target="#b1">[2]</ref>. Because obtaining reliably labeled data is expensive, labor-intensive and time-consuming, label noise is common and inevitable in most real-world datasets.</p><p>The ubiquity of noise is all the more a critical issue for it is known that learning with noisy labels severely degrades model performance. As reported by Zhang et al. <ref type="bibr" target="#b2">[3]</ref>, deep neural networks are capable of fitting random noisy labels. If even a small portion of noisy labels exists within the training data, deep learning models can eventually memorize the wrongly given labels, thus deteriorating performance. It is, therefore, necessary to design methods that are robust to label noise such that negative consequences are minimized.</p><p>One approach for dealing with noisy labels is to focus on samples according to their uncertainty during the training phase (see <ref type="figure">Figure 1</ref>). Some methods emphasize uncertain samples, the predictions of which are inconsistent during training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. As reported in previous studies on active learning, these uncertain samples are informative and require more training than other samples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, while samples that are well-trained and have consistent predictions have less information in improving the model. Performance can consequently be boosted by preferring uncertain samples which are near the class decision boundary. On the other hand, some methods reduce the importance or exclude uncertain samples so that only highly certain samples remain in the training data <ref type="bibr" target="#b7">[8]</ref>. Although the <ref type="figure">Figure 1</ref>: Label noise and sample uncertainty. Uncertain samples are located near the decision boundary and cause predictions to change constantly. Samples with high certainty are far from the decision boundary and lead to consistent predictions. Noisy samples can be close to or far from the decision boundary and are easily identifiable when they are distant from the decision boundary. Each color represents a different class.  <ref type="bibr" target="#b0">[1]</ref> impact of informative samples is minimized, it can produce a coherent model and be a safer way of training.</p><p>Another way to address noisy labels is by managing samples depending on their loss. Loss can signify the difficulty and the confidence of predictions, so giving precedence to samples with low loss or samples with high loss can work well depending on the amount of noise in the data or the complexity of the problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Difficult samples are known to accelerate training, especially for datasets with a small amount of noise <ref type="bibr" target="#b10">[11]</ref>. For this reason, there have been studies that increase the weights of high loss samples so that the network focuses on difficult samples <ref type="bibr" target="#b11">[12]</ref>. More recently, however, researchers have somewhat ironically taken the opposite approach by emphasizing easy samples <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Because easy samples are likely to be clean, favoring low loss samples has been proven to enhance performance, especially when solving a difficult task such as training with severe label noise <ref type="bibr" target="#b3">[4]</ref>.</p><p>In the literature, it is shown that contrasting strategies effectively diminish the effect of noisy samples, leading to improved performance over the baseline. Motivated to understand how all approaches can enhance accuracy, we analyze the changing loss and uncertainty of samples in the course of training for CIFAR-10, CIFAR-100, and Tiny ImageNet with different noise types. Data show that symmetric noise is easy to identify using either loss or uncertainty, whereas asymmetric noise is challenging to distinguish from clean samples, indicating the need for an efficient alternative method.</p><p>Inspired by the finding that only a minority of samples with low loss and high uncertainty have noisy labels, we propose FOCI (Focus On Clean and Informative samples), a novel robust training method. Our key idea is to emphasize the samples that are likely to be clean and informative. FOCI prioritizes samples with low loss and high uncertainty and minimizes the impact of samples of high loss since they are very likely to be noisy. To validate our method, we conducted extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet with diverse noise types from 40% to 70% of noise levels, as well as on a real-world dataset Clothing1M. Moreover, we observed the performance of training with various deep learning models to check generalizability. Our empirical analysis demonstrates the enhanced robustness of FOCI on noisy datasets, and its generalizability to any network architecture, making FOCI a useful addition to real-world deep learning pipelines.</p><p>The contribution of this paper is three-fold. (1) We identify insights on how loss and uncertainty affect noisy label classification via an in-depth analysis. (2) Inspired by these insights, we design a novel lightweight method that robustly learns by focusing on clean and informative samples from data with various conditions and types of noisy labels without any additional clean data. (3) With thorough experiments, we demonstrate our FOCI's robustness to label noise that substantially outperforms state-of-the-art methods on a real-world dataset and three benchmark datasets injected with diverse synthetic noise.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Loss and Uncertainty in Noisy Datasets</head><p>We explore how loss and uncertainty differ for various label noise by training DenseNet (L=25, k=12, momentum optimizer) on three benchmark datasets, listed in <ref type="table" target="#tab_0">Table 1</ref>: CIFAR-10, CIFAR-100, and Tiny ImageNet. These datasets are commonly used to evaluate noisy labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. We artificially corrupted the data by following typical protocols <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. In accordance with prior studies, for k classes, noise is given by swapping true labels for other class labels with some constant probability, namely, noise rate τ <ref type="bibr" target="#b12">[13]</ref>. In this experiment, we set τ = 0.4. While labels are swapped between two classes for asymmetric noise, labels are swapped to classes other than the true class label with probability of τ k−1 for symmetric noise <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Each noise type is described in Appendix A. To analyze uncertainty in noisy datasets, we explore various representations of uncertainty. Uncertainty can be quantified by the variance of loss or predicted probabilities for a given class in a q-sized history <ref type="bibr" target="#b3">[4]</ref>. Another definition is by the variance of the predicted probabilities over all the classes at one step <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The normalized distributions of each definition where q = 15 are displayed in <ref type="figure" target="#fig_1">Figure 2</ref>. The difference of distributions between noisy and clean samples is more pronounced for variance based on the history of predicted probabilities, so we use this definition for uncertainty.</p><p>As can be seen from <ref type="figure" target="#fig_2">Figure 3</ref>, we check samples based on their loss and uncertainty (prediction variance) by dividing them into four groups. Samples are split into low loss and high loss with a ratio of 1 − τ : τ as suggested by Han et al. <ref type="bibr" target="#b12">[13]</ref>. The same applies for uncertainty in which the ratio of low uncertainty and high uncertainty samples is 1 − τ : τ . The proportions concerning loss and uncertainty did not change much during training, so we only display the results at epoch 50 due to lack of space. The detailed results can be found in Appendix B.</p><formula xml:id="formula_0">Algorithm 1 FOCI Algorithm Input: mini-batch D b from dataset D 1: for t ← 1 to T do 2: if t ≤ γ then during warm-up (γ) phase 3: θ ← θ − α∇ 1 N b L(x, y; θ) parameter update by L from D b 4: else after warm-up, FOCI phase 5: W ← normalize( P t (y|x) · Var(P t−q+1:t (y|x))) sample weighting (W ) 6: θ ← θ − α∇ 1 N b W (x, q)L(x, y; θ) parameter update by W L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">.1 Analysis of symmetric noise</head><p>As shown in Figures 2a and 3a, clean and noisy samples have distinct characteristics for symmetric noise, seemingly due to the easiness of symmetric noise and deep neural networks' capability of generalizing on data with symmetric noise <ref type="bibr" target="#b22">[23]</ref>. The majority of clean samples have lower loss and relatively higher uncertainty than noisy samples, providing evidence for enhanced accuracy of approaches which emphasize high uncertainty. In contrast to clean samples, most noisy samples have higher loss and lower uncertainty. The loss of noisy samples tends to be higher than those of clean samples because predictions are different from the given labels, and the uncertainty of noisy samples is close to 0 because the predicted probabilities for the given noisy labels maintain a very small value. Approaches that emphasize easy samples (low loss) or uncertain samples can thus benefit from this fact. These findings not only support our idea of emphasizing low loss and high uncertainty samples, but also confirm that symmetric noise is an easy problem to be solved and less practical as stated in prior works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">.Analysis of asymmetric noise</head><p>Inspection of <ref type="figure" target="#fig_2">Figure 3b</ref> indicates that noisy samples can have high or low loss and uncertainty, thus justifying the enhanced performance of strategies that contradicted each other. However, according to <ref type="figure" target="#fig_1">Figure 2b</ref>, it is not effective to distinguish clean and noisy samples solely based on loss or uncertainty; the loss of clean and noisy samples are alike, and the difference between the uncertainty of clean and noisy samples is very subtle. Taking both loss and uncertainty into consideration seems more effective and plausible when training data with asymmetric noise, which is problematic and similar to real-world noise <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">.1 Overview</head><p>To validate findings from our analysis, we design a novel lightweight method FOCI that aims at focusing on clean and informative samples. The overall procedure of our method is summarized in Algorithm 1.</p><p>Let the training set be D = {(x, y)} of size N , and the dataset for a mini-batch be</p><formula xml:id="formula_1">D b of size N b .</formula><p>When training a network parameter θ in the warm-up phase with learning rate α (Lines 2-3), updating parameters can be formulated as:</p><formula xml:id="formula_2">θ ← θ − α∇( 1 N b x∈D b L(x, y; θ)),<label>(1)</label></formula><p>The algorithm starts by updating the network in the conventional way stated above. This is because deep neural networks can learn simple and common patterns, even with the presence of noisy labels during the early warm-up phase <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>. However, since real-world datasets are bound to have noise, our method pursues robust training after the warm-up phase (Lines 4-6) by reweighting samples so clean and informative samples are emphasized and the impact of noisy samples are minimized:</p><formula xml:id="formula_3">θ ← θ − α∇( 1 N b x∈D b W (x, q)L(x, y; θ)),<label>(2)</label></formula><p>where W (x, q) is the reweighting function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">.2 Sample weighting</head><p>Our aim is to ensure that clean and informative samples contribute more to training, so we place more importance on samples with low loss and high uncertainty. Because the loss value and the predicted softmax probability are inversely proportional to each other, emphasizing samples with a high predicted probability of the given label would more or less be identical to focusing on samples with low loss. As a result, to favor clean samples, we compute W (x, q) using P t (y|x), the predicted probability of the given label, and Var(P t−q+1:t (y|x)), the variance of predicted probabilities in the history queue for epochs from t − q + 1 to t.</p><formula xml:id="formula_4">W (x, q) = normalize P t (y|x) · Var(P t−q+1:t (y|x))<label>(3)</label></formula><p>The weights are subsequently standardized (i.e., mean is 0 and standard deviation is 1), and bounded with the sigmoid function to give a clipping effect, and are further divided by a normalizing factor to have unit mean. These normalized sample weights W (x, q) are multiplied to the loss function, allowing cleaner samples to contribute more when updating the network (Line 6).</p><p>We also reduce the impact of samples that are likely to be noisy using methods partially based on SELFIE. We screen samples with inconsistent predictions and high loss or samples with consistent predictions but the predicted label disagrees from the given label, and set their weights to zero.</p><p>Inconsistency is represented by a normalized information entropy of label frequency i.e., − 1 log(q) k y=1 F (ŷ) log F (ŷ), where F denotes the frequency proportion of labelŷ in the q sized prediction history, andŷ denotes each predicted class label of k classes. Samples that have inconsistency values higher than a certain threshold ∈ [0, 1] are considered noisy because their predicted classes have changed constantly during training. To identify high loss samples, we adopt the widely used lossbased separation method. Loss values ranked in the top τ × 100% within the minibatch are classified as high loss. The noise rate τ can be estimated through cross-validation if unknown <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets and label corruption schemes. To validate the effectiveness of our method, we perform an image classification task on three benchmark datasets: CIFAR-10 2 , CIFAR-100 2 , and Tiny ImageNet 3 and a real-world dataset Clothing1M <ref type="bibr" target="#b28">[29]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>). For the benchmark datasets, we use four label corruption schemes: symmetric noise, asymmetric noise, mixed noise, and nearest label transfer. In this work, we are concerned with scenarios of abundant data with very poor but realistic label quality. Because labelers make mistakes within very few and similar classes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, asymmetric noise is injected to these datasets with varying noise rates τ ∈ {0.1, 0.2, 0.3, 0.4}, and symmetric noise is mixed with asymmetric noise. To simulate confusions between visually similar classes, we also employ nearest label transfer <ref type="bibr" target="#b29">[30]</ref>, in which labels are swapped according to a confusion matrix of a pretrained network. All the noise types are detailed in Appendix A. Experimental settings. We use two different schemes for the learning rate policy and number of epochs depending on the type of noise that is used. For symmetric noise, we follow the experimental settings of Arazo et al. <ref type="bibr" target="#b30">[31]</ref> and train PreAct ResNet-18 using SGD with a momentum of 0.9, weight decay of 10 −4 , and a batch size of 128 for 300 epochs. The initial learning rate is 0.1 and reduced by a factor of 10 at epoch 100 and 250. We set = 0.1, q = 25, γ = 250 for CIFAR-10 and γ = 100 for CIFAR-100. Data preprocessing and augmentation is also applied, including mean subtraction, horizontal random flip, 32x32 random crops after padding with 4 pixels on each side, and mixup augmentation <ref type="bibr" target="#b31">[32]</ref>. We report the best classification accuracy (i.e., the percentage of correct predictions out of the entire test dataset) across epochs following prior works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. For other types of noise, we trained DenseNet (L=25, k=12) for 100 epochs using SGD with a momentum of 0.9 in line with experiments conducted by Huang et al. <ref type="bibr" target="#b32">[33]</ref>. The initial learning rate is 0.1 and divided by 5 at epoch 50 and 75. We use batch size of 128, = 0.1, q = 15, and γ = 25. Each image is scaled to have zero mean and unit variance. We measure performance by the mean of last classification accuracies over three runs for it is common to measure the robustness of noisy labels with the test error at the end of training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. We also compute the label precision by the fraction of true clean samples among all the samples selected for training or samples that have non-zero weights. All of the experiments were executed using NAVER Smart Machine Learning (NSML) platform <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">.1 Performance Comparison</head><p>Baselines. We compare FOCI with a baseline algorithm (denoted by Default), which trains noisy data without any strategies, an uncertainty-based approach Active Bias, loss-based approach Coteaching, and a hybrid approach SELFIE. Active Bias <ref type="bibr" target="#b3">[4]</ref> emphasizes uncertain samples with high prediction variances. Coteaching <ref type="bibr" target="#b12">[13]</ref> uses two networks that feed each other low loss samples. SELFIE <ref type="bibr" target="#b7">[8]</ref> selects low loss samples and relabels samples with high certainty. It is fair to compare algorithms with the same number of epochs, so we did not restart SELFIE, which caused different results from the paper. FOCI can handle label noise with only a noisy train dataset, so we did not compare methods that require an additional clean dataset <ref type="bibr" target="#b35">[36]</ref>.</p><p>Asymmetric noise. <ref type="figure">Figure 4</ref> displays the test accuracies of FOCI along with other baseline methods for varying rates of asymmetric noise. It appears that the performance of Default degrades drastically as the noise rate increases. Although other methods achieve higher accuracies than those of Default, our method outperforms all other baselines with significant margins for each dataset and noise rate. Moreover, as can be seen from <ref type="table" target="#tab_1">Table 2</ref>, there is a remarkable improvement in performance for  CIFAR-100 where the accuracy differs with the second-best algorithm by 7%. <ref type="figure">Figure 5</ref> also shows that our method is effective at detecting and filtering out noise even for the difficult scenario of asymmetric noise.</p><p>Mixed noise. According to <ref type="table" target="#tab_1">Table 2</ref>, the performance of FOCI achieves the best performance for mixed noise. As symmetric noise increases under the same noise level, the accuracy of Default increases, presumably resulting from that symmetric noise is easy to distinguish. This result is in good agreement with results from Section 2 . We can also observe that the difference between Default and other baselines reduces with more symmetric noise, indicating that symmetric noise does not require developed algorithms and lacks significance. Furthermore, FOCI can identify noise with high precision than other methods as indicated in <ref type="figure">Figure 5</ref>. These results of mixed noise imply our model's advantages against noisy real-world data, where symmetric and asymmetric noise may coexist.</p><p>Nearest noise. We can observe from <ref type="table" target="#tab_1">Table 2</ref> that FOCI yields higher accuracy for nearest noise compared to other methods. The label precision of our method also surpasses other methods and continues to increase as training proceeds, while other methods appear to converge towards the end of training as shown in <ref type="figure">Figure 5</ref>.</p><p>High level noise. To validate our method for another challenging problem, we conducted experiments on CIFAR-100 with larger noise rates for mixed noise and nearest noise. As shown in <ref type="table" target="#tab_3">Table 3</ref>, FOCI outperforms other state-of-the-art methods for larger noise rates. These results confirm that our method effectively downplays noisy samples and emphasizes clean and informative samples for all noise types.</p><p>Symmetric noise. When adding symmetric noise, the true label can be included or excluded from the candidates of labels to be swapped, so we evaluated our method for both cases. We present the results of both definitions of symmetric noise in Appendix C due to lack of space and show that FOCI achieves comparable or better performance than other state-of-the-art methods for symmetric noise.</p><p>Model architectures. We evaluated whether FOCI is generic by comparing the performance of each method using various model architectures trained on CIFAR-100 with 40% asymmetric noise. As shown in <ref type="table" target="#tab_4">Table 4</ref>, FOCI obtains the highest accuracy while producing consistent results despite changes in architectures. DenseNet (L=25, k=12) had the smallest architecture, thereby yielding better performance than those of other models such as ResNet50, which suffered severe overfitting. These results suggest that FOCI can be reliably applied to different model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">.2 Empirical Analysis of Algorithm</head><p>To comprehensively understand loss and uncertainty in our method, we conducted experiments on CIFAR-100 with 40% asymmetric noise.   Forward loss <ref type="bibr" target="#b36">[37]</ref> 69.84 LCCN <ref type="bibr" target="#b37">[38]</ref> 71.63 Joint Optim. <ref type="bibr" target="#b38">[39]</ref> 72.16 DMI <ref type="bibr" target="#b39">[40]</ref> 72.46 MLNT <ref type="bibr" target="#b40">[41]</ref> 73.47 PENCIL <ref type="bibr" target="#b24">[25]</ref> 73.49 FOCI 73.78</p><p>number of noisy samples, on the contrary, decreases (see also <ref type="figure">Figure 5</ref>). These results demonstrate our approach's effectiveness towards minimizing the impact of label noise and boosting the benefits of informative clean samples.</p><p>We also evaluated the weighting module for three approaches: placing larger weights on low loss, on high uncertainty, and treating every sample equally. As shown in <ref type="table" target="#tab_5">Table 5</ref>, our weighting method outperforms other cases. Interestingly, giving equal weights achieved the best accuracy out of the three alternatives, while emphasizing samples with low loss led to the lowest accuracy. These results are parallel to <ref type="figure">Figure 4</ref> in that Coteaching, which focuses on low loss samples, performs worse than Active Bias, which emphasizes high uncertainty samples. The detailed results of the ablation study is in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">.3 Experiments on Clothing1M</head><p>To further demonstrate our method's effectiveness to realistic noise, we test on Clothing1M <ref type="bibr" target="#b28">[29]</ref>, which comprises clothing data crawled from online shopping websites. Clothing1M consists of 1M images with real-world noisy labels and additional 50K, 14K, 10K verified clean data for training, validation and testing respectively. We retrain ResNet50 pretrained on ImageNet for 20 epochs using the 1M noisy dataset without any clean data in the training process. We use SGD with momentum of 0.9, = 0.1, q = 5, γ = 5, and τ = 0.4 because the estimated noise rate is 38% <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. The initial learning rate is 0.002 and is decreased by 10 every 5 epochs. For preprocessing, we resize images to 256x256 and randomly crop 224x224 from the resized images. This dataset is greatly imbalanced so we randomly select a relatively balanced subset of up to 35,000 samples for each class.</p><p>As shown in <ref type="table" target="#tab_6">Table 6</ref>, our method achieves 73.8% accuracy, which is higher than recent state-of-the-art methods. For fair comparison, we do not include methods using different backbone models or any clean data during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have investigated the behavior of loss and uncertainty of samples for various noise. We have shown that for symmetric noise, noisy samples can be clearly identified with respect to either loss or uncertainty. For asymmetric noise-a more complex noisy label scenario that commonly occurs in real-world datasets-it is observed that considering both loss and uncertainty is necessary. Inspired by the findings, we have designed a novel method that aims at downplaying noisy samples while emphasizing clean and informative samples. Through series of experiments, we have demonstrated the effectiveness of FOCI when training with realistic synthetic label noise and real-world datasets as well as its generalizability in that it can be applied to any model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Types of Noise</head><p>There are four types of noise used in this paper: asymmetric noise, symmetric noise, mixed noise, and nearest noise. The noise rate is denoted by τ . <ref type="figure" target="#fig_5">Figure 7</ref> displays the noise transition matrices of each noise type.</p><p>As can be seen from <ref type="figure" target="#fig_5">Figure 7a</ref>, asymmetric noise swaps labels between two classes with a probability of τ . Asymmetric noise is problematic and similar to real-world noise <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. We, therefore, place more emphasis on the results of asymmetric noise to provide a promising method for realistic noise.</p><p>For symmetric noise, which is less practical than asymmetric noise <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, the true label can be swapped to any other label. There are two definitions of symmetric noise in prior works. As shown in <ref type="figure" target="#fig_5">Figure 7b</ref>, one popular label noise criterion is random labeling while the true label is not selected <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. In this case, the probability of being swapped to another label is uniformly distributed with value τ k−1 , so the sum of probabilities being swapped becomes the noise rate τ . As displayed in <ref type="figure" target="#fig_5">Figure 7c</ref>, another criterion for symmetric noise addition consists of randomly selecting labels for a percentage of the training data using all possible labels (i.e. the true label could be randomly maintained) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>. The probability of being swapped to another label is thus τ k . Mixed noise is when asymmetric noise and symmetric noise are added together as can be seen in <ref type="figure" target="#fig_5">Figure 7d</ref>. Mixed noise represents the scenario of noise mainly injected from another class along with some random noise. We have experimented with mixed noise to produce more realistic noise.</p><p>Nearest noise is used to simulate confusions between visually similar classes <ref type="bibr" target="#b29">[30]</ref>. As shown in <ref type="figure" target="#fig_5">Figure 7e</ref>, the probabilities of being swapped are different for each class. For the nearest neighbor search, we use a confusion matrix of a pretrained network of the dataset. The validation accuracy of the pretrained network trained on CIFAR-100 was 53.12%.   As can be seen from the figures, after 10 epochs, there are subtle changes in proportions concerning loss and uncertainty during training. For symmetric noise, the proportions remain constant throughout the training process (see <ref type="figure" target="#fig_6">Figure 8</ref>). For asymmetric noise, although there are slight changes throughout training, the changes are negligible (see <ref type="figure" target="#fig_9">Figure 9</ref>). We can also observe that asymmetric noise is more problematic than symmetric noise, because samples for each combination are distributed fairly evenly for noisy and clean samples.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Loss and Uncertainty During Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Symmetric Noise Results</head><p>More research has focused on symmetric noise or random labeling, so we compare our results with recent state-of-the-art methods: bootstrapping, forward loss, mixup, MentorNet, D2L, and MD-DYR-SH, which uses dynamic mixup, soft to hard dynamic bootstrapping with regularization. As explained in Appendix A, when adding symmetric noise, the true label can be included or excluded from the candidates of labels to be swapped. We have hence evaluated our method for both cases. <ref type="table">Table 7</ref> displays the classification accuracy results of 40% symmetric noise for CIFAR-10 and CIFAR-100. FOCI was trained on datasets using PreAct ResNet-18 following the experimental settings of Arazo et al. <ref type="bibr" target="#b30">[31]</ref>. We report the results from the paper and as shown from the table, our method achieves accuracy higher than or equivalent to other state-of-the-art methods for both symmetric noise types. However, the results on symmetric noise excluding true labels should be interpreted with care because some methods employed different architectures and used clean data during training such as in <ref type="bibr" target="#b18">[19]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Normalized distribution of loss and uncertainty on CIFAR-100 with 40% noise at epoch 50. noise Low loss, Low uncertainty Low loss, High uncertainty High loss, Low uncertainty High loss, High uncertainty</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Combinations of loss and uncertainty with 40% noise rate at epoch 50. Clean (60%) and noisy (40%) samples are divided vertically. The green and red colors represent low and high loss samples, respectively. The solid and stripe patterns represent low and high variation samples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 Figure 6 :</head><label>66</label><figDesc>displays how sample weights of FOCI change from soon after the warm-up stage (epoch 25) to convergence (epoch 75). We can observe that clean samples are allocated with larger weights, while noisy samples are allocated with smaller weights as training progresses. Moreover, the number of clean samples with non-zero weights increases, and the Changes of weight distribution on clean and noisy samples with non-zero weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Types of 40% noise transition matrices for 5 classes. Mixed noise consists of 20% asymmetric noise and 20% symmetric noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8</head><label>8</label><figDesc>and 9 show how loss and uncertainty change throughout the training process. Clean (60%) and noisy (40%) samples are divided vertically. The green and red colors represent low and high loss samples, and the solid and stripe patterns represent low and high uncertainty samples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Loss and uncertainty of datasets with 40% symmetric noise during training. variance Low loss, High variance High loss, Low variance High loss, High variance (a) CIFAR-10. (b) CIFAR-100. (c) Tiny ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Loss and uncertainty of datasets with 40% asymmetric noise during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of datasets used in the experiments.</figDesc><table><row><cell>Dataset</cell><cell>Train #</cell><cell cols="3">Test # Class # Image size</cell></row><row><cell>CIFAR-10</cell><cell>50K</cell><cell>10K</cell><cell>10</cell><cell>32x32</cell></row><row><cell>CIFAR-100</cell><cell>50K</cell><cell>10K</cell><cell>100</cell><cell>32x32</cell></row><row><cell>Tiny ImageNet  *</cell><cell>100K</cell><cell>10K</cell><cell>200</cell><cell>64x64</cell></row><row><cell>Clothing1M</cell><cell>1M noisy</cell><cell>10K</cell><cell>14</cell><cell>256x256</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* a subset of ImageNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) on benchmark datasets with 40% noise. Asymmetric and symmetric noise are denoted by A and S respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell cols="2">Active Bias Coteaching</cell><cell>SELFIE</cell><cell>FOCI</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>71.8±1.5</cell><cell>78.0±1.5</cell><cell>83.7±1.4</cell><cell cols="2">84.9±0.1 86.2±0.4</cell></row><row><cell>Asymmetric noise</cell><cell>CIFAR-100</cell><cell>45.3±1.4</cell><cell>50.3±0.6</cell><cell>47.3±1.4</cell><cell cols="2">52.8±0.5 59.5±0.9</cell></row><row><cell></cell><cell cols="2">Tiny ImageNet 30.8±0.1</cell><cell>33.2±0.9</cell><cell>30.3±0.5</cell><cell cols="2">36.1±0.3 37.6±0.9</cell></row><row><cell>Mixed noise (A-30, S-10)</cell><cell cols="2">CIFAR-10 CIFAR-100 Tiny ImageNet 35.0±0.8 79.6±0.8 49.9±0.7</cell><cell>84.9±0.5 56.2±0.5 36.2±0.4</cell><cell>80.6±1.7 50.9±0.8 34.0±0.6</cell><cell cols="2">84.9±0.9 85.7±0.4 58.5±0.3 61.5±0.5 39.0±0.6 39.7±0.7</cell></row><row><cell>Mixed noise (A-20, S-20)</cell><cell cols="2">CIFAR-10 CIFAR-100 Tiny ImageNet 36.8±0.9 81.2±0.8 53.0±0.4</cell><cell>84.8±0.3 58.2±1.1 37.6±0.6</cell><cell>82.1±0.3 54.2±1.0 37.1±1.5</cell><cell cols="2">84.8±0.4 86.1±0.9 59.1±0.5 60.6±0.5 37.9±0.2 37.9±0.2</cell></row><row><cell>Nearest noise</cell><cell>CIFAR-100</cell><cell>45.8±0.8</cell><cell>54.8±0.7</cell><cell>55.9±0.8</cell><cell cols="2">57.8±0.3 57.9±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Classification accuracy comparison on three benchmark datasets with varying rates of asymmetric noise. The shaded area represents the standard deviation of three repeated experiments.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell>Tiny ImageNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.75 0.80 0.85 0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.50 0.55 0.60 0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.45 0.35 0.40</cell><cell>Default ActiveBias Coteaching SELFIE FOCI</cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.30</cell></row><row><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 Noise rate</cell><cell>0.3</cell><cell>0.4</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.1</cell><cell>0.2 Noise rate</cell><cell>0.3</cell><cell>0.4</cell><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 Noise rate</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell cols="5">60 Epochs Asymmetric Noise 80 Figure 4: 40 0.63 0.88 0.93 0.68 0.73 0.78 0.83 Label Precision</cell><cell>100</cell><cell>Label Precision</cell><cell>0.68 0.88 0.73 0.78 0.83</cell><cell>40</cell><cell cols="3">60 Epochs Mixed Noise 80</cell><cell>100</cell><cell>Label Precision</cell><cell>0.80 0.90 0.85</cell><cell>40</cell><cell>60 Epochs Nearest Noise 80</cell><cell>100</cell><cell>Coteaching SELFIE FOCI</cell></row></table><note>Figure 5: Label precision comparison on CIFAR-100 with 40% noise after the warmup phase (epoch 25). Mixed noise comprises 30% asymmetric noise and 10% symmetric noise. Default and Active Bias do not distinguish clean samples, so they are not included for comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (%) on CIFAR-100 with high-level noise. Asymmetric and symmetric noise are denoted by A and S respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell cols="2">Active Bias Coteaching</cell><cell>SELFIE</cell><cell>FOCI</cell></row><row><cell>Mixed noise</cell><cell cols="2">50% (A-40, S-10) 38.0±1.2</cell><cell>41.2±1.0</cell><cell>37.3±0.9</cell><cell cols="2">42.1±2.7 48.8±2.1</cell></row><row><cell>Mixed noise</cell><cell cols="2">60% (A-30, S-30) 35.9±0.4</cell><cell>40.8±1.5</cell><cell>37.0±0.7</cell><cell cols="2">43.8±0.4 48.5±1.4</cell></row><row><cell>Mixed noise</cell><cell cols="2">70% (A-20, S-50) 32.9±0.7</cell><cell>35.8±0.5</cell><cell>32.2±0.2</cell><cell cols="2">41.5±0.9 42.0±2.2</cell></row><row><cell>Nearest noise</cell><cell>60%</cell><cell>36.3±0.9</cell><cell>43.2±0.1</cell><cell>44.0±1.6</cell><cell cols="2">46.6±1.1 47.1±0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (%) of various architectures on CIFAR-100 with 40% asymmetric noise.</figDesc><table><row><cell></cell><cell>Default</cell><cell cols="2">Active Bias Coteaching</cell><cell>SELFIE</cell><cell>FOCI</cell></row><row><cell>DenseNet</cell><cell>45.3±1.4</cell><cell>50.3±0.6</cell><cell>47.3±1.4</cell><cell cols="2">52.8±0.5 59.5±0.9</cell></row><row><cell>VGG-19</cell><cell>35.4±1.9</cell><cell>31.4±0.7</cell><cell>35.6±0.5</cell><cell cols="2">39.3±0.3 43.1±0.6</cell></row><row><cell>ResNet50</cell><cell>29.8±0.2</cell><cell>28.2±0.5</cell><cell>32.6±0.2</cell><cell cols="2">32.8±0.2 35.9±0.4</cell></row><row><cell cols="2">MobileNetV2 32.9±0.6</cell><cell>38.1±0.6</cell><cell>31.9±0.8</cell><cell cols="2">35.1±0.2 39.1±0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Results of variations in sample</cell></row><row><cell cols="2">weighting on CIFAR-100 with 40% asymmet-</cell></row><row><cell>ric noise.</cell><cell></cell></row><row><cell>Variations</cell><cell>Acc.[%]</cell></row><row><cell>None (all the same)</cell><cell>57.1±1.0</cell></row><row><cell>Low loss</cell><cell>52.4±1.0</cell></row><row><cell>High uncertainty</cell><cell>55.7±1.7</cell></row><row><cell>Low loss, high uncertainty</cell><cell>59.5±0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on Clothing1M.</figDesc><table><row><cell>Methods</cell><cell>Acc.[%]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cs.toronto.edu/~kriz/cifar.html 3 https://www.kaggle.com/c/tiny-imagenet</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Study on Hyperparameters</head><p>We present an ablation study on hyperparameters: q (queue size), γ (warm-up), and (threshold). We recorded the classification accuracies (%) over three runs for CIFAR-100 with 40% asymmetric noise (default q=15, g=25, e=0.1). As shown in <ref type="table">Table 8</ref>, the results do not greatly depend on hyperparameters except for the strictest case (e=0). Therefore, it can be concluded that our method is practical due to its insensitivity to hyperparameters. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active learning for logistic regression: an evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="235" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">To recognize shapes, first learn to generate images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Brain Research</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">O2u-net: A simple noisy label detection approach for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongfei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3326" to="3334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Brendan Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning active learning from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ksenia</forename><surname>Konyushkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4225" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A variance maximization criterion for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="358" to="370" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning is robust to massive label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10694</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combinatorial inference against label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geeho</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05902</idno>
		<title level="m">A machine learning platform that enables you to focus on your models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metaweight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1917" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9103" to="9110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">l DM I : A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classification accuracy on benchmark datasets with symmetric noise using all labels (top) and excluding true labels (bottom)</title>
	</analytic>
	<monogr>
		<title level="j">Generic CNN is GCNN</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Wide ResNet is denoted by WRN. ResNet is RN, and PreAct ResNet is PRN. Bootstrapping [17] Forward loss [37] mixup [32</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md-Dyr-Sh</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md-Dyr-Sh</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<idno>WRN-101 GCNN-12/RN-44 PRN-18 PRN-18</idno>
	</analytic>
	<monogr>
		<title level="j">Architecture</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
