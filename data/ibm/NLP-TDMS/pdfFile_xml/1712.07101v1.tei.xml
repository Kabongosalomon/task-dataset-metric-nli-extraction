<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING END-TO-END SPEECH RECOGNITION WITH POLICY LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING END-TO-END SPEECH RECOGNITION WITH POLICY LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-end-to-end speech recognition</term>
					<term>LVCSR</term>
					<term>policy gradient</term>
					<term>deep neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Connectionist temporal classification (CTC) is widely used for maximum likelihood learning in end-to-end speech recognition models. However, there is usually a disparity between the negative maximum likelihood and the performance metric used in speech recognition, e.g., word error rate (WER). This results in a mismatch between the objective function and metric during training. We show that the above problem can be mitigated by jointly training with maximum likelihood and policy gradient. In particular, with policy learning we are able to directly optimize on the (otherwise non-differentiable) performance metric. We show that joint training improves relative performance by 4% to 13% for our end-to-end model as compared to the same model learned through maximum likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and 5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Deep neural networks are the basis for some of the most accurate speech recognition systems in research and production <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Neural network based acoustic models are commonly used as a sub-component in a Gaussian mixture model (GMM) and hidden Markov model (HMM) based hybrid system. Alignment is necessary to train the acoustic model, and a two-stage (i.e. alignment and frame prediction) training process is required for a typical hybrid system. A drawback of such setting is that there is a disconnect between the acoustic model training and the final objective, which makes the system level optimization difficult.</p><p>The end-to-end neural network based speech models bypass this two-stage training process by directly maximizing the likelihood of the data. More recently, the end-to-end models have also shown promising results on various datasets <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>. While the end-to-end models are commonly trained with maximum likelihood, the final performance metric for a speech recognition system is typically word error rate (WER) or character error rate (CER). This results a mismatch between the objective that is optimized and the evaluation metric. In an ideal setting the model should be trained to optimize the final metric. However, since the metrics are commonly discrete and non-differentiable, it is very difficult to optimize them in practice.</p><p>Lately, reinforcement learning (RL) has shown to be effective on improving performance for problems that have non-differentiable metric through policy gradient. Promising results are obtained in machine translation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, image captioning <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10]</ref>, summarization <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11]</ref>, etc.. In particular, REINFORCE algorithm <ref type="bibr" target="#b12">[12]</ref> enables one to estimate the gradient of the expected reward by sampling from the model. It has also been applied for online speech recognition <ref type="bibr" target="#b13">[13]</ref>. Graves and Jaitly <ref type="bibr" target="#b4">[4]</ref> propose expected transcription loss that can be used to optimize on WER. However, it is more computationally expensive. For example, for a sequence of length T with vocabulary size K, at least T samples and K metric calculations are required for estimating the loss.</p><p>We show that jointly training end-to-end models with self critical sequence training (SCST) <ref type="bibr" target="#b10">[10]</ref> and maximum likelihood improves performance significantly. SCST is also efficient during training, as only one sampling process and two metric calculations are necessary. Our model achieves 5.53% WER on Wall Street Journal dataset, and 5.42% and 14.70% WER on Librispeech test-clean and test-other sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL STRUCTURE</head><p>The end-to-end model structure used in this work is very similar to that of Deep Speech 2 (DS2) <ref type="bibr" target="#b6">[6]</ref>. It is mainly composed of 1) a stack of convolution layers in the front-end for feature extraction, and 2) a stack of recurrent layers for sequence modeling. The structure of recurrent layers is the same as in DS2, and we illustrate the modifications in convolution layers in this section.</p><p>We choose to use time and frequency convolution (i.e. 2-D convolution) as the front-end of our model, since it is able to model both the temporal transitions and spectral variations in speech utterances. We use depth-wise separable convolution <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> for all the convolution layers, due to its computational efficiency and performance advantage <ref type="bibr" target="#b15">[15]</ref>. The depthwise separable convolution is implemented by first convolving over the input channel-wise, and then convolve with 1 × 1 filters with the desired number of output channels. Stride size only influences the channel-wise convolution; the following 1 × 1 convolutions always have stride size of one. More pre-  cisely, let x ∈ R F ×T ×D , c ∈ R W ×H×D and w ∈ R D×N denote an input sample, the channel-wise convolution and the 1 × 1 convolution weights respectively. The depth-wise separable convolution with D input channels and N output channels performs the following operations:</p><formula xml:id="formula_0">s(i, j, d) = F −1 f =0 T −1 t=0 x(f, t, d)c(i − f, j − t, d) (1) o(i, j, n) = D−1 k=0 s(i, j, k)w(k, n)<label>(2)</label></formula><p>where d ∈ {1, . . . , D} and n ∈ {1, 2, . . . , N }, s is the channel-wise convolution result, and o is the result from depth-wise separable convolution. In addition, we add a residual connection <ref type="bibr" target="#b16">[16]</ref> between the input and the layer output for the depth-wise separable convolution to facilitate training.</p><p>Our model is composed of six convolution layers -one standard convolution layer that has larger filter size, followed by five residual convolution blocks <ref type="bibr" target="#b16">[16]</ref>. The convolution features are then fed to four bidirectional gated recurrent units (GRU) <ref type="bibr" target="#b17">[17]</ref> layers, and finally two fully connected layers that make the final per-character prediction. The full end-to-end model structure is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL OBJECTIVE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Maximum Likelihood Training</head><p>Connectionist temporal classification (CTC) <ref type="bibr" target="#b18">[18]</ref> is a popular method for doing maximum likelihood training on sequence labeling tasks, where the alignment information is not provided in the label. The alignment is not required since CTC marginalizes over all possible alignments, and maximizes the likelihood P (y|x). It achieves this by augmenting the original label set L to set Ω = L ∪ {blank} with an additional blank symbol. A mapping M is then defined to map a length T sequence of label Ω T to L ≤T by removing all blanks and repeated symbols along the path. The likelihood can then be recovered by</p><formula xml:id="formula_1">P (y |x) = t P (y t |x), y t ∈ Ω T P (y|x) = y ∈M −1 (y) P (y |x)</formula><p>where x, y and y denote an input example of length T , the corresponding label of length ≤ T and one of the augmented label with length T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Policy Learning</head><p>The log likelihood reflects the log probability of getting the whole transcription completely correct. What it ignores are the probabilities of the incorrect transcriptions. In other words, all incorrect transcriptions are equally bad, which is clearly not the case. Furthermore, the performance metrics typically aim to reflect the plausibility of incorrect predictions. For example, WER penalizes less for transcription that has less edit distance to the ground truth label. This results in a disparity between the optimization objective of the model and the (commonly discrete) evaluation criteria. This mismatch is mainly attributed to the inability to directly optimize the criteria.</p><p>One way to remedy this mismatch is to view the above problem in the policy learning framework. In this framework, we can view our model as an agent and the training samples as the environment. The parameters of the model θ defines a policy P θ (y|x), the model interacts with the environment by following this policy. The agent then performs an action based on its current state, in which case the action is the generated transcription and the state is the model hidden representation of the data. It then observes a reward that is defined from the evaluation metric calculated on the current sample (e.g. 1−WER for the current transcription). The goal of learning is to obtain a policy that minimizes the negative expected reward:</p><formula xml:id="formula_2">L p (θ) = −E y s ∼P θ (y|x) [r(y s )]<label>(3)</label></formula><p>where r(·) denotes the reward function. Gradient of eq. 3 can be obtained through REINFORCE <ref type="bibr" target="#b12">[12]</ref> as</p><formula xml:id="formula_3">∇ θ L p (θ) = −E y s ∼P θ (y|x) [r(y s )∇ θ log P θ (y s |x)] (4) ≈ −r(y s )∇ θ log P θ (y s |x)<label>(5)</label></formula><p>Eq. 5 shows the Monte Carlo approximation of the gradient with a single example, which is a common practice when training model with stochastic gradient descent. The policy gradient obtained from eq. 5 is often of high variance, and the training can get unstable. To reduce the variance, Rennie et al. <ref type="bibr" target="#b10">[10]</ref> proposed self-critical sequence training (SCST). In SCST, the policy gradient is computed with a baseline, which is the greedy output from the model. Formally, the policy gradient is calculated using</p><formula xml:id="formula_4">∇ θ L p (θ) = −E y s ∼P θ (y|x) [(r(y s ) − r(ŷ)) ∇ θ log P θ (y s |x)]<label>(6)</label></formula><formula xml:id="formula_5">≈ − (r(y s ) − r(ŷ)) ∇ θ log P θ (y s |x)<label>(7)</label></formula><p>whereŷ is the greedy decoding output from the model for the input sample x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-objective Policy Learning</head><p>A potential problem with policy gradient methods (including SCST) is that the learning can be slow and unstable at the beginning of training. This is because it is unlikely for the model to have reasonable output at that stage, which leads to implausible samples with low rewards. Learning will be slow in case of small learning rate, and unstable otherwise. One way to remedy this problem is to incorporate maximum likelihood objective along with policy gradient, since in maximum likelihood the probability is evaluated on the ground truth targets, and hence will get large gradients when the model output is incorrect. This leads to the following objective for training our end-to-end speech model:</p><formula xml:id="formula_6">L(θ) = − log P θ (y|x) + λL scst (θ) where (8) L scst (θ) = − {g(y s , y) − g(ŷ, y)} log P θ (y s |x)</formula><p>where g(·, ·) is the reward function and λ ∈ (0, +∞) is the coefficient that controls the contribution from SCST. In our case we choose g(·, y) = 1 − max(1, WER(·, y)). Training with eq. 8 is also efficient, since both sampling and greedy decoding is cheap. The only place that might be computationally more demanding is the reward calculation, however, we only need to compute it twice per batch of examples, which adds only a minimal overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate the proposed objective by performing experiments on the Wall Street Journal (WSJ) and LibriSpeech <ref type="bibr" target="#b19">[19]</ref> datasets. The input to the model is a spectrogram computed with a 20ms window and 10ms step size. We first normalize each spectrogram to have zero mean and unit variance. In addition, we also normalize each feature to have zero mean and unit variance based on the training set statistics. No further preprocessing is done after these two steps of normalization.</p><p>We denote the size of the convolution layer by the tuple (C, F, T, SF, ST), where C, F, T, SF, and ST denote number of channels, filter size in frequency dimension, filter size in time dimension, stride in frequency dimension and stride in time dimension respectively. We have one convolutional layer with size (32,41,11,2,2), and five residual convolution blocks of size <ref type="figure" target="#fig_0">(32,7,3,1,1), (32,5,3,1,1), (32,3,3,1,1), (64,3,3,2,1)</ref>, (64,3,3,1,1) respectively. Following the convolutional layers we have 4 layers of bidirectional GRU RNNs with 1024 hidden units per direction per layer. Finally, we have one fully connected hidden layer of size 1024 followed by the output layer. Batch normalization <ref type="bibr" target="#b20">[20]</ref> is applied to all layers' preactivations to facilitate training. Dropout <ref type="bibr" target="#b21">[21]</ref> is applied to inputs of each layer, and for layers that take sequential input (i.e. the convolution and recurrent layers) we use the dropout variant proposed by Gal and Ghahramani <ref type="bibr" target="#b22">[22]</ref>. The convolutional and fully connected layers are initialized uniformly following He et al. <ref type="bibr" target="#b23">[23]</ref>. The recurrent layer weights are initialized with a uniform distribution U(−1/32, 1/32). The model is trained in an end-to-end fashion to minimize the mixed objective as illustrated in eq. 8. We use mini-batch stochastic gradient descent with batch size 64, learning rate 0.1, and with Nesterov momentum 0.95. The learning rate is reduced by half whenever the validation loss has plateaued. We set λ = 0.1 at the beginning of training, and increase it to 1 after the model has converged (i.e. the validation loss stops improving). The gradient is clipped <ref type="bibr" target="#b24">[24]</ref> to have a maximum 2 norm of 1. For regularization, we use 2 weight decay of 10 −5 for all parameters. Additionally, we apply dropout for inputs of each layer (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The dropout probabilities are set as 0.1 for data, 0.2 for all convolution layers, and 0.3 for all recurrent and fully connected layers. Furthermore, we also augment the audio training data through random perturbations of tempo, pitch, volume, temporal alignment, along with adding random noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of Policy Learning</head><p>To study the effectiveness of our multi-objective policy learning, we perform experiments on both datasets with various settings. The first set of experiments was carried out on the WSJ corpus. We use the standard si284 set for training, dev93 for validation and eval92 for test evaluation. We use the provided language model and report the result in the 20K closed vocabulary setting with beam search. The beam width is set to 100. Results are shown in table 1. Both policy gradient methods improve results over baseline. In particular, the use of SCST results in 13.8% relative performance improvement on the eval92 set over the baseline.</p><p>On LibriSpeech dataset, the model is trained using all 960 Method dev93 eval92 CER WER CER WER Baseline 4.07% 9.93% 2.59% 6.42% Policy (eq. 5) 3.71% 9.46% 2.31% 5.85% Policy (eq. 7) 3.52% 9.21% 2.10% 5.53% 9.30% Graves and Jaitly <ref type="bibr" target="#b4">[4]</ref> 8.20% Wu et al. <ref type="bibr" target="#b26">[26]</ref> 8.20% Miao et al. <ref type="bibr" target="#b5">[5]</ref> 7.34% Chorowski and Jaitly <ref type="bibr" target="#b27">[27]</ref> 6.70% Human <ref type="bibr" target="#b6">[6]</ref> 5.03% Amodei et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Other Methods</head><p>We also compare our performance with other end-to-end models. Comparative results from WSJ and LibriSpeech dataset are illustrated in tables 3 and 4 respectively. Our model achieved competitive performance with other methods on both datasets. In particular, with the help of policy learning we achieved similar results as Amodei et al. <ref type="bibr" target="#b6">[6]</ref> on LibriSpeech without using additional data. To see if the model generalizes, we also tested our LibriSpeech model on the WSJ dataset. The result is significantly better than the model trained on WSJ data (see <ref type="table">table 3</ref>), which suggests that the end-to-end models benefit more when more data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we try to close the gap between the maximum likelihood training objective and the final performance metric for end-to-end speech models. We show this gap can be reduced by using the policy gradient method along with the negative log-likelihood. In particular, we apply a multi-objective training with SCST to reduce the expected negative reward that is defined by using the final metric. The joint training is computationally efficient. We show that the joint training is effective even with single sample approximation, which improves the relative performance on WSJ and LibriSpeech by 13% and 4% over the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Model architecture of our end-to-end speech model. Different colored blocks represent different layers as shown on the right, the lightning symbol indicates dropout happens between the two layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performance from WSJ dataset. Baseline denotes model trained without CTC only; policy indicates model trained using the multi-objective policy learning. Equation in parenthesis indicates the way used to obtain policy gradient. Performance from LibriSpeech dataset. Policy denotes model trained with multi-objective shown in eq. 8.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">Baseline Policy</cell></row><row><cell>dev-clean</cell><cell>CER WER</cell><cell>1.76% 5.33%</cell><cell>1.69% 5.10%</cell></row><row><cell>test-clean</cell><cell>CER WER</cell><cell>1.87% 5.67%</cell><cell>1.75% 5.42%</cell></row><row><cell>dev-other</cell><cell cols="3">CER WER 14.88% 14.26% 6.60% 6.26%</cell></row><row><cell>test-other</cell><cell cols="3">CER WER 15.18% 14.70% 6.58% 6.25%</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>WER</cell></row><row><cell cols="2">Hannun et al. [25]</cell><cell></cell><cell>14.10 %</cell></row><row><cell cols="2">Bahdanau et al. [7]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparative results with other end-to-end methods on WSJ eval92 dataset. LibriSpeech denotes model trained using LibriSpeech dataset only, and test on WSJ. Amodei et al. used more training data. hours of training data. Both dev-clean and dev-other are used for validation and results are reported in table 2. The provided 4-gram language model is used for final beam search decoding. The beam width is also set to 100 for decoding. Overall, a relative ≈ 4% performance improvement over the baseline is observed. Word error rate comparison with other end-to-end methods on LibriSpeech dataset. Amodei et al. used more training data.</figDesc><table><row><cell>6]*</cell><cell>3.60%</cell></row><row><cell>Ours</cell><cell>5.53%</cell></row><row><cell>Ours (LibriSpeech)</cell><cell>4.67%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G E</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The ibm 2015 english conversational telephone speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hk J Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picheny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05899</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5255" to="5259" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eesen: End-toend speech recognition using deep rnn models and wfstbased decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4945" to="4949" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An actorcritic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S J Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning online alignments with continuous rewards policy gradient,&quot; in ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2801" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Proceedings</title>
		<editor>ICML, Francis R. Bach and David M. Blei</editor>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Firstpass large vocabulary continuous speech recognition using bi-directional recurrent dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2873</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2856" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
