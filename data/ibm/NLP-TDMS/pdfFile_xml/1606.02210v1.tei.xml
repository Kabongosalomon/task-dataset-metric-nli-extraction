<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective Unsupervised Feature Learning with Convolutional Neural Network (S-CNN)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghaderi</surname></persName>
							<email>amir.ghaderi@mavs.uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Arlington Arlington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Athitsos</surname></persName>
							<email>athitsos@uta.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>at Arlington</addrLine>
									<settlement>Arlington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selective Unsupervised Feature Learning with Convolutional Neural Network (S-CNN)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Artificial Neural Networks</term>
					<term>Classification and Clustring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning of convolutional neural networks (CNNs) can require very large amounts of labeled data. Labeling thousands or millions of training examples can be extremely time consuming and costly. One direction towards addressing this problem is to create features from unlabeled data. In this paper we propose a new method for training a CNN, with no need for labeled instances. This method for unsupervised feature learning is then successfully applied to a challenging object recognition task. The proposed algorithm is relatively simple, but attains accuracy comparable to that of more sophisticated methods. The proposed method is significantly easier to train, compared to existing CNN methods, making fewer requirements on manually labeled training data. It is also shown to be resistant to overfitting. We provide results on some well-known datasets, namely STL-10, CIFAR-10, and CIFAR-100. The results show that our method provides competitive performance compared with existing alternative methods. Selective Convolutional Neural Network (S-CNN) is a simple and fast algorithm, it introduces a new way to do unsupervised feature learning, and it provides discriminative features which generalize well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>A popular method in machine learning is Convolutional Neural Networks (CNNs). CNN had was of high interest to the research community in the 1990s, but after that its popularity receded compared to the Support Vector Machines (SVMs) <ref type="bibr" target="#b0">[1]</ref>. One of the reasons was the relatively lower computational demands of SVMs. Training CNNs requires significantly more computational power and time than training SVMs.</p><p>With increased availability of powerful GPU processing, and using several improvements in network structure, Krizhevsky et al. <ref type="bibr" target="#b1">[2]</ref> used CNNs to achieve the highest image classification accuracy on ImageNet Large Scale Visual Recognition Challenge(ILSVRC) <ref type="bibr" target="#b2">[3]</ref>. After that result, CNNs have become widely popular in the computer vision and pattern recognition community, and have been applied to a variety of classification problems, including detection and localization <ref type="bibr" target="#b3">[4]</ref>. CNNs have achieved the best results for detection on the PASCAL VOC dataset <ref type="bibr" target="#b0">[1]</ref>, and for classification on the Caltech-256 <ref type="bibr" target="#b4">[5]</ref> and Caltech-101 datasets <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>. Based on such results, CNNs have emerged as a leading method for supervised learning.</p><p>At the same time, a weakness of supervised learning using CNNs is the need for much larger amounts of labeled training data, compared to alternative methods. Acquiring a large number of labeled instances requires oftentimes significant time spent by humans to provide the labels, and significant costs. Furthermore, when training instances are labeled by humans, errors and inconsistency in labeling become an issue, especially when labeling large scale datasets. On the other hand, in many settings it is easy to obtain vast amounts of unlabeled data, making unsupervised learning an attractive alternative, provided of course that unsupervised learning can attain satisfactory accuracy.</p><p>In this paper, we propose an algorithm that learns features using CNNs that train on unlabeled data. We evaluate this algorithm on the STL, CIFAR-10, CIFAR-100 datasets, obtaining competitive performance compared to other methods.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> we show the overview of algorithm. Selective search finds the important parts of the object. Then CNN learns the features to classify those important parts. At the final step, an SVM is trained on the features. The following sections describe each of these components in detail. CNN typically consist of different types of layers, with each layer performing some specialized functionality. Examples of such types of layers are convolutional layers, rectifier layers(max(0,x)) (also known as ReLU layers), max-pooling layers for reducing the number of inputs, and normalization layers <ref type="bibr" target="#b1">[2]</ref>. The speed of training Deep CNN with ReLUs is much higher than the speed of training ReLUs with tanh units <ref type="bibr" target="#b1">[2]</ref>. In fully connected layers, each element is calculated based on the values of all components of the input. The last layer calculates the loss function of the network. The main role of training is on the convolutional layers, and classification is performed by the fully connected layers. After training a CNN, instead of performing classification using the fully connected layers, one can feed features from the last convolution layer into an SVM classifier.</p><p>CNNs can be combined with both supervised and unsupervised methods in an end-to-end system. In supervised methods, data augmentation can be used to increase the number of instances for training, so as to reduce overfitting <ref type="bibr" target="#b1">[2]</ref>. Coates et al. <ref type="bibr" target="#b6">[7]</ref> point out that the effect of certain factors, such as the number of hidden nodes, may be more vital for performance than the depth of the model. In <ref type="bibr" target="#b7">[8]</ref>, researchers use the temporal slowness constraint with and employ a linear autoencoder in order to learn features from video. In the category of unsupervised methods, Bo et al. propose the hierarchical matching pursuit (HMP) method, which uses sparse coding and learns hierarchical feature representations in an unsupervised manner on depth data <ref type="bibr" target="#b8">[9]</ref>. Unsupervised feature learning is used by Netzer et al. for recognizing digits cropped from street view images. Features invariant to transformations are learned by Sohn et al. <ref type="bibr" target="#b9">[10]</ref>. Le et al. <ref type="bibr" target="#b10">[11]</ref> have trained features robust to translation, scaling, and rotation for face detection using a deep sparse auto encoder on a large dataset, without having to label images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Object detection in many methods is based on exhaustive search for specific object types. Alternatively, some methods output possible locations of objects, without being trained to detect specific types of objects. Such methods include objectness <ref type="bibr" target="#b11">[12]</ref>, selective search <ref type="bibr" target="#b12">[13]</ref>, and categoryindependent object proposals <ref type="bibr" target="#b13">[14]</ref>. Selective search identifies potential object locations which can be used for object recognition. It combines advantages of both exhaustive search and segmentation <ref type="bibr" target="#b14">[15]</ref> and achieves relatively high speed compared to alternative methods. It uses the structure of the image for sampling, and it creates scores by merging low-level superpixels <ref type="bibr" target="#b15">[16]</ref>. The goal of selective search is to find all locations in the image that have high probability to be an object. The output of selective search given an image is a set of bounding boxes, representing possible locations of objects.</p><p>As we mentioned earlier, annotating large sets of images can be an important bottleneck for training supervised methods, but large amounts of unlabeled data may be easy to obtain. E.g. in the STL dataset there are 100K unlabeled images. Let x i be an unlabeled image, that we give as input to the selective search algorithm. Selective search outputs a set w i of bounding boxes for x i . We treat each bounding box as a subimage of x i . Thus, set w i consists of many images a ij , which are all subimages of x i . w i = {a ij |a ij is output of selective search with input x i } If selective search creates T i subimages from x i then j = {1,2,3 … . T i } and w i = {a i1 , a i2 , . . . a iT i }. Then, we assign training label i to all these images in set w i . In other words, w i generates T i image-label pairs [ a ij , i] for our training set. Intuitively, all subimages from the same original image x i are assigned i as their training label. Thus, training labels are assigned fully automatically, with no need for manual intervention.</p><p>Set T contains as elements the numbers T i of subimages extracted from all unlabeled images x i . We have 100,000 unlabeled images in the STL dataset, so T has 100,000 members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T = {T 1 , T 2 , … , T 100000 }</head><p>Suppose that we want to train a CNN to recognize C classes, where C is a user-specified parameter. We want to find the C members of T that contain the most elements. For reaching this goal we sort set T in descending order, and we put the indices in set TS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TS = indices of sorted T in descending order</head><p>= {ts 1 , ts 2 , … ts 100000 } Note that TS stores indices of elements in T , not the elements themselves. So T ts 1 is the maximum element of the T. We choose the top C indices of TS to train the CNN. In our experiments, we try C= 5000, 10000, 15000, 20000, 25000, 30000. Our goal is to train a CNN to discriminate between C classes, and to choose features that can discriminate among various types of objects. Therefore, the input for training the CNN is a set of images X and labels as below: X = {w ts 1 , w ts 2 , … , w ts C } labels for images in w ts i = ts i</p><p>The loss function which should be minimized is:</p><formula xml:id="formula_0">l(i, a ij )</formula><p>is the softmax loss based on the image a ij and the label i. In the following section we provide more details about the trained CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>For comparison to other methods, we evaluate performance on the STL-10 dataset <ref type="bibr" target="#b6">[7]</ref>, which has 10 classes, and the CIFAR-10, and CIFAR-100 datasets <ref type="bibr" target="#b16">[17]</ref> that have 10 and 100 classes respectively. STL-10 contains 100,000 unlabeled data we use it as source of data for unsupervised feature learning.</p><p>We extract the surrogate classes for training the CNN from the unlabeled set of STL-10. Each image in the unlabeled STL set is given an input for selective search. The output images of the selective search have different sizes, which would cause features created in fully connected layers to have different numbers of elements. To deal with this problem there are two options. The first is resizing the images to P*P fixed size, where P is a preselected parameter. The second is to use images with different sizes at beginning of the network, and to use spatial pyramid pooling <ref type="bibr" target="#b17">[18]</ref> at the last layer before the fully connected layers, so as to create fixed number of features in fully connected layers. Here we select the first option and resize the input images to 32*32.</p><p>We try two network architectures. The first one has three convolutional layers, each of them with 64, 128, and 256 filters respectively. The kernel size for the first convolutional layer is 5*5. We use stride 1 and padding 2 for this layer. An ReLU filter is after each convolutional layer. After the first and the second ReLU layer we have the max pooling layer. Here we have kernel size 3*3, stride 2, and zero padding. The third ReLU layer is followed by two fully connected layers with 512 and C neurons respectively, where C is the number of the class labels that are assigned automatically (see Section III). Note that C varies in different experiments, as described later. Dropout <ref type="bibr" target="#b18">[19]</ref> is employed at the fully connected layers to reduce overfitting. At the end there is a softmax layer for calculating the loss function. We named this network 64-128-256_512.</p><p>The second network, which is larger than the first one, has three convolutional layers with 92, 256, and 512 filters, followed by a fully connected layer with 1024 neurons. We named this network 92-256-512_1024. The kernel size for the first convolutional layer is 5*5. We use stride 1 and padding 2 for this layer. Again, a Rectified Linear Unit (ReLU) is used after each convolutional layer. After the first ReLU layer there is a max pooling layer with kernel size 3*3, stride 2, and zero padding. The second convolutional layer is like the first one, except that it consists of 256 kernels instead of 92. The ReLU and pooling layers applied to second convolutional layer are the same as for the first layer. The third convolutional layer has 512 kernels. At the end we have two fully connected layers with 1024 and C neurons, where again C is the number of classes and is different in each experiment. As in the first network, we have a softmax layer at the end for calculating the loss function. <ref type="figure" target="#fig_1">Figure 2</ref> shows the second network in details. The figure is created by NVIDIA Deep Learning GPU Training System (DIGITS). We implement CNNs based on the caffe framework <ref type="bibr" target="#b19">[20]</ref> .</p><p>For each dataset, each image of the test set of that dataset is given as input to the network. Then, we compute the output of all the network layers expect the top softmax one. We use the pooling method which is usually used for the STL-10 dataset. 4-quadrant max-pooling, to obtain 4 values per feature map. This is the standard procedure for STL-10 <ref type="bibr" target="#b20">[21]</ref>. We use the pooled features for training a one-vs-all linear support vector machine (SVM). To train the SVM we use the standard training and testing protocols for each dataset. For the STL dataset, we use the 10 predefined folds for training the SVM, and final accuracy is calculated as the average accuracy over the 10 splits. Code is available at http://vlm1.uta.edu/~amir/scnn.</p><p>Here we investigate the impact of different parameters on the results. We run different experiments by varying the number of classes, the network structure, and the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Number of classes</head><p>As described in Section III, parameter C is the number of classes that are assigned in an automatic manner, so as to train the CNN. We experimented with C equal to 5K, 10k, 15K, 20K, 25K, and 30K. A larger C can increase accuracy, because the neural network receives more training data. At the same time, when C is too large, the network can be fed with conflicting data (since class labels are assigned automatically) and not converge. <ref type="table" target="#tab_0">Table 1</ref> shows the accuracy obtained on the STL dataset for different values of C. Indeed we notice that accuracy improves as C increases from 5000 to 20000, and then it starts decreasing. The best values of C are in the range between 20K and 25K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generality of features</head><p>We have also used the features learned on the STL dataset for recognition on the CIFAR-10 and CIFAR-100 datasets. These two are popular datasets, used by several researchers. Both datasets are split into a training set and a test set. In contrast to the STL dataset, the CIFAR datasets do not have any unlabeled data. We do not use their training set to learn features by CNN, using instead the trained features from the STL dataset. The results are comparable to other methods which use the CIFAR training sets directly. <ref type="table">Table 2</ref> shows the results for classification on CIFAR-10 and CIFAR-100 with learned features from STL-10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Different network architectures</head><p>We have conducted additional experiments to investigate the impact of the network architecture on classification performance. Since we established the best range for parameter C (number of classes) is 20K-25K, we decided to run two different architectures for the neural network, trained with C equal to 20000 and 25000. The details of the architectures are explained at the beginning of Section IV. The 92-128-512_1024 network has more parameters to learn and more power to discriminate between classes relative to the 64-128-256_512 network. We only change the parameters of the layers, and the number of layers is fixed for both network architectures. The 64-128-512_1024 network with 25K classes has 61.94 percent accuracy on STL test set. It shows that this architecture has more power for creating more distinguishing features. Classification accuracy improves with increasing network size. This is evidence that our algorithm works well with larger networks and avoids overfitting. The results of these experiments with different neural network architecture on the STL-10 dataset are shown in table 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison to other methods</head><p>In <ref type="table" target="#tab_3">Table 4</ref> we compare the results of our algorithm with other learning methods on the STL-10 dataset. Our approach appears to be competitive with the others, despite the fact that our model only uses 3 convolutional layers and requires learning only few parameters. Note that better result than ours which reported in the table have been obtained by using external data, achieving an accuracy rate of 70.10% on STL-10 <ref type="bibr" target="#b22">[23]</ref>. In that work, knowledge gained from previous optimizations is transferred to new tasks in order to find optimal hyperparameter settings more efficiently. We find it particularly promising that our results are more accurate than those of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and <ref type="bibr" target="#b23">[24]</ref>. In this paper we have proposed a new method for unsupervised feature learning, tailored for image classification in large datasets. We show that results are compatible to previously proposed methods, while our results use a simpler architecture and no data augmentation or use of external data. Also, the features learned on the STL-10 dataset are tested on the CIFAR-10 and CIFAR-100 datasets, and results show that the learned features generalize well and can extend to other sets of data.</p><p>There are several interesting directions for improvements. One such direction is trying bigger and deeper architectures for CNN. Using CNNs with more layers may learn more powerful features for distinguishing among different objects. Another interesting direction is to try learning features from a bigger dataset, with more images and classes than the STL-10 unlabeled dataset, to see if that would lead to learning better features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the algorithm II. RELATED WORK</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of network 92-256-512_1024</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy percentages on the STL dataset using different values of C (number of classes).</figDesc><table><row><cell>#classes</cell><cell>CNN</cell><cell>SVM(linear)</cell></row><row><cell>5000</cell><cell>64-128-256_512</cell><cell>58.01</cell></row><row><cell>10000</cell><cell>64-128-256_512</cell><cell>58.10</cell></row><row><cell>15000</cell><cell>64-128-256_512</cell><cell>58.29</cell></row><row><cell>20000</cell><cell>64-128-256_512</cell><cell>61.04</cell></row><row><cell>25000</cell><cell>64-128-256_512</cell><cell>60.38</cell></row><row><cell>30000</cell><cell>64-128-256_512</cell><cell>58.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy percentages of different architectures on the STL dataset.</figDesc><table><row><cell>Architecture</cell><cell>#classes</cell><cell>Accuracy</cell></row><row><cell>64-128-256_512</cell><cell>20000</cell><cell>61.04</cell></row><row><cell>64-128-256_512</cell><cell>25000</cell><cell>60.38</cell></row><row><cell>92-256-512_1024</cell><cell>20000</cell><cell>60.36</cell></row><row><cell>92-256-512_1024</cell><cell>25000</cell><cell>61.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Classification accuracy percentages on the</cell></row><row><cell></cell><cell></cell><cell cols="2">STL-10 dataset</cell><cell></cell><cell></cell></row><row><cell>Our method</cell><cell>[23]</cell><cell>[8]</cell><cell>[24]</cell><cell>[10]</cell><cell>[7]</cell></row><row><cell>61.94</cell><cell cols="2">70.10 61.0</cell><cell cols="3">60.1 58.70 51.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evaluation of Deep Learning based Pose Estimation for Sign Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gattupalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghaderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.09065</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning for RGB-D Based Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics (ISER)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning invariant representation with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building highlevel features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How good are detection proposals, really?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
