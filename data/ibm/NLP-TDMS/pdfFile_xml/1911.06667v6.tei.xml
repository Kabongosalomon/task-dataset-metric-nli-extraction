<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterMask : Real-Time Anchor-Free Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
							<email>yw.lee@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
							<email>jongyoul@etri.re.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CenterMask : Real-Time Anchor-Free Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple yet efficient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchorfree one stage object detector (FCOS [33]) in the same vein with Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise. We also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet <ref type="bibr" target="#b18">[19]</ref> and <ref type="formula">(2)</ref> effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE. With SAG-Mask and VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted each to large and small models, respectively. Using the same ResNet-101-FPN backbone, Cen-terMask achieves 38.3%, surpassing all previous state-ofthe-art methods while at a much faster speed. CenterMask-Lite also outperforms the state-of-the-art by large margins at over 35fps on Titan Xp. We hope that CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance segmentation and backbone network for various vision tasks, respectively. The Code is available at https: //github.com/youngwanLEE/CenterMask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, instance segmentation has made great progress beyond object detection. The most representative method, Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>, extended on object detection (e.g., Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>), has dominated COCO <ref type="bibr" target="#b22">[23]</ref> benchmarks since instance segmentation can be easily solved by detecting objects and then predicting pixels on each box. However, even if there have been many works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref> for improving the Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>, few works exist for considering the speed of the instance segmentation. Although YOLACT <ref type="bibr" target="#b0">[1]</ref> is the first real-time one-stage instance * Corresponding author.  <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_6">Table 5</ref> for details. segmentation due to its parallel structure and extremely lightweight assembly process, the accuracy gap from Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> is still significant. Thus, we aim to bridge the gap by improving both accuracy and speed. While Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> is based on a two-stage object detector (e.g., Faster R-CNN) that first generates box proposals and then predicts box location and classification, YOLACT <ref type="bibr" target="#b0">[1]</ref> is built on one-stage detector (RetinaNet <ref type="bibr" target="#b21">[22]</ref>) where P3 (stride of 2 3 ) to P7 (stride of 2 7 ) denote the feature map in feature pyramid of backbone network. Using the features from the backbone, FCOS <ref type="bibr" target="#b32">[33]</ref> predicts bounding boxes. Spatial Attention-Guided Mask (SAG-Mask) predicts segmentation mask inside of the each detected box with Spaital Attention Module (SAM) helping to focus on the informative pixels but also suppress the noise.</p><p>that directly predicts boxes without proposal step. However, these object detectors rely heavily on pre-define anchors, which are sensitive to hyper-parameters (e.g., input size, aspect ratio, scales, etc.) and different datasets. Besides, since they densely place anchor boxes for higher recall rate, the excessively many anchor boxes cause the imbalance of positive/negative samples and higher computation/memory cost.</p><p>To cope with these drawbacks of anchor boxes, recently, many works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref> tend to escape from the anchor boxes toward anchor-free by using corner/center points, which leads to more computation-efficient and better performance compared to anchor box based detectors. Therefore, we design a simple yet efficient anchorfree one stage instance segmentation called CenterMask that adds a novel spatial attention-guided mask branch to the more efficient one-stage anchor-free object detector (FCOS <ref type="bibr" target="#b32">[33]</ref>) in the same way with Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overview of our CenterMask. Plugged into the FCOS <ref type="bibr" target="#b32">[33]</ref> object detector, our spatial attentionguided mask (SAG-Mask) branch takes the predicted boxes from the FCOS <ref type="bibr" target="#b32">[33]</ref> detector to predict segmentation masks on each Region of Interest (RoI). The spatial attention module (SAM) in the SAG-Mask helps the mask branch to focus on meaningful pixels and suppressing uninformative ones.</p><p>When extracting features on each RoI for mask prediction, each RoI pooling should be assigned considering the RoI scales. Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> proposes a new assignment function, called RoIAlign, that does not consider the input scale. Thus, we design a scale-adaptive RoI assignment function that considers the input scale and is a more suitable one-stage object detector. We also propose a more effective backbone network VoVNetV2 based on VoVNet <ref type="bibr" target="#b18">[19]</ref> that shows better performance and faster speed than ResNet <ref type="bibr" target="#b9">[10]</ref> and DenseNet <ref type="bibr" target="#b13">[14]</ref> due to its One-shot Aggregation (OSA).</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> (bottom), We found that stacking the OSA modules in VoVNet makes the performance degradation (e.g., VoVNetV1-99). We see this phenomenon as the motivation of ResNet <ref type="bibr" target="#b9">[10]</ref> because the backpropagation of gradient is disturbed. Thus, we add the residual connection <ref type="bibr" target="#b9">[10]</ref> into each OSA module to ease optimization, which makes the VoVNet deeper and in turn, boosts the performance.</p><p>In the Squeeze-Excitation (SE) <ref type="bibr" target="#b12">[13]</ref> channel attention module, it was found that the fully connected layers reduce the channel size, thereby reducing computational burden and unexpectedly causing channel information loss. Thus, we re-design the SE module as effective SE (eSE) replacing the two FC layers with one FC layer maintaining channel dimension, which prevents the information loss and in turn, improves the performance. With residual connection and eSE modules, We propose VoVNetV2 on various scales; from lightweight VoVNetV2-19, base VoVNetV2-39/57 and large model VoVNetV2-99 that are correspond with MobileNet-V2 <ref type="bibr" target="#b10">[11]</ref>, ResNet-50/101 <ref type="bibr" target="#b9">[10]</ref> &amp; HRNet-W18/32 <ref type="bibr" target="#b31">[32]</ref>, and ResNeXt-32x8d <ref type="bibr" target="#b35">[36]</ref>.</p><p>With SAG-Mask and VoVNetV2, we design Center-Mask and CenterMask-Lite that are targeted each to large and small models, respectively. The Extensive experiments demonstrate the effectiveness of CenterMask &amp; CenterMask-Lite and VoVNetV2. Using the same ResNet-101 backbone <ref type="bibr" target="#b9">[10]</ref>, CenterMask outperforms all previous state-of-the-art single models on the COCO <ref type="bibr" target="#b22">[23]</ref> instance and detection tasks while at a much faster speed. CenterMask-Lite with VoVNetV2-39 bakcbone also achieves 33.4% mask AP / 38.0% box AP, outperforming the state-of-the-art real-time instance segmentation YOLACT [1] by 2.6 / 7.0 AP gain, respectively, at over 35fps on Titan Xp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CenterMask</head><p>In this section, first, we review the anchor-free object detector, FCOS <ref type="bibr" target="#b32">[33]</ref>, which is a fundamental object detection part of our CenterMask. Next, we demonstrate the architecture of the CenterMask and describe how the proposed spatial attention-guided mask branch (SAG-Mask) is designed to plug into the FCOS <ref type="bibr" target="#b32">[33]</ref> detector. Finally, a more effective backbone network, VoVNetV2, is proposed to boost the performance of CenterMask in terms of accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">FCOS</head><p>FCOS <ref type="bibr" target="#b32">[33]</ref> is an anchor-free and proposal-free object detection in a per-pixel prediction manner as like FCN <ref type="bibr" target="#b25">[26]</ref>. Almost state-of-the-art object detectors such as Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, YOLO <ref type="bibr" target="#b28">[29]</ref>, and RetinaNet <ref type="bibr" target="#b21">[22]</ref> use the concept of the pre-defined anchor box which needs elaborate parameter tunning and complex calculation associated with box IoU in training. Without the anchor-box, the FCOS <ref type="bibr" target="#b32">[33]</ref> directly predicts a 4D vector plus a class label at each spatial location on a level of feature maps. As shown in <ref type="figure" target="#fig_1">Figure  2</ref>, the 4D vector embeds the relative offsets from the four sides of a bounding box to the location (e.g., left, right, top and bottom). In addition, FCOS <ref type="bibr" target="#b32">[33]</ref> introduces the centerness branch to predict the deviation of a pixel to the center of its corresponding bounding box, which improves the detection performance. Avoiding complex computation of anchor-boxes, FCOS <ref type="bibr" target="#b32">[33]</ref> reduces memory/computation cost but also outperforms the anchor box based object detectors. Because of the efficiency and good performance of the FCOS <ref type="bibr" target="#b32">[33]</ref>, we design the proposed CenterMask built upon the FCOS <ref type="bibr" target="#b32">[33]</ref> object detector. <ref type="figure" target="#fig_1">Figure 2</ref> shows overall architecture of the CenterMask. CenterMask consists of three-part:(1) backbone for feature extraction, (2) FCOS <ref type="bibr" target="#b32">[33]</ref> detection head, and (3) mask head. The procedure of masking objects is composed of detecting objects from the FCOS <ref type="bibr" target="#b32">[33]</ref> box head and then predicting segmentation masks inside the cropped regions in a per-pixel manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adaptive RoI Assignment Function</head><p>After object proposals are predicted in the FCOS <ref type="bibr" target="#b32">[33]</ref> box head, CenterMask predicts segmentation masks using the predicted box regions in the same vein as Mask R-CNN. As the RoIs are predicted from different levels of feature maps in Feature Pyramid Network (FPN <ref type="bibr" target="#b20">[21]</ref>), RoI Align <ref type="bibr" target="#b8">[9]</ref> that extracts features should be assigned at different scales of feature maps with respect to RoI scales. Specifically, an RoI with a large scale has to be assigned to a higher feature level and vice versa. Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> based two-stage detector uses Equation 1 in FPN <ref type="bibr" target="#b20">[21]</ref> to determine which feature map (P k ) to be assigned.</p><formula xml:id="formula_0">k = k 0 + log 2 √ wh/224 ,<label>(1)</label></formula><p>where k 0 is 4 and w, h are the width and height of the each RoI. However, Equation 1 is not suitable for CenterMask based one-stage detector because of two reasons. First, Equation 1 is tuned to two-stage detectors (e.g.,FPN <ref type="bibr" target="#b20">[21]</ref>) that use different feature levels compared to one-stage detectors (e.g, FCOS <ref type="bibr" target="#b32">[33]</ref>, RetinaNet <ref type="bibr" target="#b21">[22]</ref>). Specifically, twostage detectors use feature levels of P2 (stride of 2 2 ) to P5 (2 5 ) while one-stage detectors use from P3 (2 3 ) to P7 ( <ref type="formula" target="#formula_1">2 7 )</ref> that is larger receptive fields with lower-resolution. Besides, the canonical ImageNet pretraining size 224 in Equation 1 is hard-coded and not adaptive to feature scale variation. For example, when the input dimension is 1024×1024 and the area of an RoI is 224 2 , the RoI is assigned to relative higher feature P4 despite its small size of the area with respect to input dimension, which results in reducing small object AP. Therefore, we define Equation 2 as a new RoI assignment function suited for CenterMask based one-stage detectors.</p><formula xml:id="formula_1">k = k max − log 2 A input /A RoI ,<label>(2)</label></formula><p>where k max is the last level (e.g., 7) of feature map in backbone and A input , A RoI are area of input image and the RoI, respectively. Without the canonical size 224 in Equation 1, Equation 2 adaptively assign RoI pooling scale by the ratio of input/RoI area. If k is lower than minimum level (e.g., P3), k is clamped to the minimum level. Specifically, if the area of an RoI is bigger than half of the input area, the RoI is assigned to the highest feature level(e.g., P 7 ). Inversely, while Equation 1 assigns P 4 to the RoI with 224 2 , Equation 2 determine k max -5 level which maybe minimum feature level for area of the RoI that is about ×20 smaller than input size. We can find that the proposed RoI assignment method improves the small object AP than Equation 1 because of its adaptive and scale-aware assignment strategy in <ref type="table">Table 2</ref>. From an ablation study, we set k max to P5 and k min to P3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Spatial Attention-Guided Mask</head><p>Recently, attention methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref> have been widely applied to object detections because it helps to focus on important features, but also suppress unnecessary ones. In particular, channel attention <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> emphasizes 'what' to focus across channels of feature maps while spaital attention <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b3">4]</ref> focuses 'where' is an informative regions. Inspired by the spatial attention mechanism, we adopt a spatial attention module to guide the mask head for spotlighting meaningful pixels and repressing uninformative ones.</p><p>Thus, we design a spatial attention-guided mask (SAG-Mask), as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Once features inside the predicted RoIs are extracted by RoI Align <ref type="bibr" target="#b8">[9]</ref> with 14×14 resolution, those features are fed into four conv layers and </p><formula xml:id="formula_2">input + 1×1 3×3 3×3 3×3 3×3 3×3 input 1×1 3×3 3×3 3×3 3×3 3×3 input 1×1 3×3 3×3 3×3 3×3 3×3 x × × × × × × × × ×1×1 Figure 3: Comparison of OSA modules. F 1×1 , F 3×3 denote 1 × 1, 3 × 3 conv layer respectively, F avg is global average pooling, W C is fully-connected layer,</formula><p>A eSE is channel attention map, ⊗ indicates element-wise multiplication and ⊕ denotes element-wise addition.</p><p>spatial attention module (SAM) sequentially. To exploit the spatial attention map A sag (X i ) ∈ R 1×W ×H as a feature descriptor given input feature map X i ∈ R C×W ×H , the SAM first generates pooled features P avg , P max ∈ R 1×W ×H by both average and max pooling operations respectively along the channel axis and aggregates them via concatenation. Then it is followed by a 3 × 3 conv layer and normalized by the sigmoid function. The computation process is summarized as follow:</p><formula xml:id="formula_3">A sag (X i ) = σ(F 3×3 (P max • P avg )),<label>(3)</label></formula><p>where σ denotes the sigmoid function, F 3×3 is 3 × 3 conv layer and • represents concatenate operation. Finally, the attention guided feature map X sag ∈ R C×W ×H is computed as:</p><formula xml:id="formula_4">X sag = A sag (X i ) ⊗ X i ,<label>(4)</label></formula><p>where ⊗ denotes element-wise multiplication. After then, a 2 × 2 deconv upsamples the spatially attended feature map to 28 × 28 resolution. Lastly, a 1 × 1 conv is applied for predicting class-specific masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">VoVNetV2 backbone</head><p>In this section, we propose more effective backbone networks, VoVNetV2, for further boosting the performance of CenterMask. VoVNetV2 is improved from VoVNet <ref type="bibr" target="#b18">[19]</ref> by adding residual connection <ref type="bibr" target="#b9">[10]</ref> and the proposed effective Squeeze-and-Excitation (eSE) attention module to the VoVNet. VoVNet is a computation and energy efficient backbone network that can efficiently present diversified feature representation because of One-Shot Aggrega-tion (OSA) modules. As shown in <ref type="figure">Figure 3</ref>(a) OSA module consists of consecutive conv layers and aggregates the subsequent feature maps at once, which can capture diverse receptive fields efficiently and in turn outperforms DenseNet and ResNet in terms of accuracy and speed. Residual connection: Even with its efficient and diverse feature representation, VoVNet has a limitation in terms of optimization. As OSA modules are stacked (i.g., deeper) in VoVNet, we observe the accuracy of the deeper models is saturated or degradation. Specifically, <ref type="table" target="#tab_4">Table 4</ref> shows the accuracy of VoVNetV1-99 is lower than that of VoVNetV1-57. Based on the motivation of ResNet <ref type="bibr" target="#b9">[10]</ref>, We conjecture that stacking OSA modules make the backpropagation of gradient gradually hard due to the increase of transformation functions such as conv. Therefore, as shown in <ref type="figure">Figure 3</ref>(b), we also add the identity mapping <ref type="bibr" target="#b9">[10]</ref> to OSA modules. Correctly, the input path is connected to the end of an OSA module that is able to backpropagate the gradients of every OSA module in an end-to-end manner on each stage like ResNet. Boosting the performance of VoVNet, the identity mapping also makes the VoVNet possible to enlarge its depth such as VoVNet-99. Effective Squeeze-Excitation (eSE): For further boosting the performance of VoVNet, We also propose a channel attention module, effective Squeeze-Excitation (eSE), improving original SE <ref type="bibr" target="#b12">[13]</ref> more effectively. Squeeze-Excitation (SE) <ref type="bibr" target="#b12">[13]</ref>, a representative channel attention method adopted in CNN architectures, explicitly models the interdependency between the channels of feature maps to enhance its representation. The SE module squeezes the spatial dependency by global average pooling to learn a channel specific descriptor and then two fully-connected (FC) layers followed by a sigmoid function are used to rescale the input feature map to highlight only useful channels. In short, given input feature map X i ∈ R C×W ×H , the channel attention map A ch (X i ) ∈ R C×1×1 is computed as:</p><formula xml:id="formula_5">A ch (X i ) = σ(W C (δ(W C/r (F gap (X i )))),<label>(5)</label></formula><p>where F gap (X) = 1 W H W,H i,j=1 X i,j is channel-wise global average pooling, W C/r , W C ∈ R C×1×1 are weights of two fully-connected layers, δ denotes ReLU non-linear operator and σ indicates sigmoid function.</p><p>However, it is assumed that the SE module has a limitation: channel information loss due to dimension reduction. For avoiding high model complexity burden, two FC layers of the SE module need to reduce channel dimension. Specifically, while the first FC layer reduces input feature channels C to C/r using reduction ratio r, the second FC layer expands the reduced channels to original channel size C. As a result, this channel dimension reduction causes channel information loss.</p><p>Therefore, we propose effective SE (eSE) that uses only one FC layer with C channels instead of two FCs without channel dimension reduction, which rather maintains channel information and in turn improves performance. the eSE process is defined as:</p><formula xml:id="formula_6">A eSE (X div ) = σ(W C (F gap (X div ))),<label>(6)</label></formula><formula xml:id="formula_7">X ref ine = A eSE (X div ) ⊗ X div ,<label>(7)</label></formula><p>where X div ∈ R C×W ×H is the diversified feature map computed by 1 × 1 conv in OSA module. As a channel attentive feature descriptor, the A eSE ∈ R C×1×1 is applied to the diversified feature map X div to make the diversified feature more informative. Finally, when using the residual connection, the input feature map is element-wise added to the refined feature map X ref ine . The details of How the eSE module is plugged into the OSA module are shown in <ref type="figure">Figure 3</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Implementation details</head><p>Since CenterMask is built on FCOS [33] object detector, we follow hyper-parameters of the FCOS <ref type="bibr" target="#b32">[33]</ref> except for positive score threshold 0.03 instead of 0.05 Since FCOS <ref type="bibr" target="#b32">[33]</ref> does not generate positive RoI samples well in initial training time. While using FPN levels 3 through 7 with 256 channels in the detection step, we use P3 ∼ P7 in the masking step, as mentioned in 2.3. We also use mask scoring <ref type="bibr" target="#b14">[15]</ref> that recalibrates classification score considering predicted mask quality (e.g., mask IoU) in Mask R-CNN. CenterMask-Lite: To achieve real-time processing, we try to make the proposed CenterMask lightweight. We downsize three parts: backbone, box head, and mask head. In the backbone, first, we reduce the channels C of FPN from 256 to 128, which can decrease the output of 3×3 conv in FPN but also input dimension of box and mask head. And then, we replace the backbone network with more lightweight VoVNetV2-19 that has 4 OSA modules on each stage comprised of 3 conv layers instead of 5 as in VoVNetv2-39/57. In the box head, there are four 3 × 3 conv layers with 256 channels on each classification and box branch where the centerness branch is shared with the box branch. We reduce the number of conv layer from 4 to 2 with 128 channels. Lastly, in the mask head, we also reduce the number of conv layers and channels in the feature extractor and mask scoring part from (4, 256) to (2, 128), respectively. Training: We set the number of detection boxes from the FCOS <ref type="bibr" target="#b32">[33]</ref> to 100, and the highest-scoring boxes are fed into the SAG-mask branch for training mask branch. We use the same mask target as Mask R-CNN that is made by the intersection between an RoI and its associated groundtruth mask. During training time, we define a multi-task loss on each RoI as:</p><formula xml:id="formula_8">L = L cls + L center + L box + L mask ,<label>(8)</label></formula><p>where the classification loss L cls , centerness loss L center , and box regression loss L box are same as those in <ref type="bibr" target="#b32">[33]</ref> and L mask is the average binary cross-entropy loss identical as in <ref type="bibr" target="#b8">[9]</ref>. Unless specified, the input image is resized to have 800 pixels <ref type="bibr" target="#b20">[21]</ref> along the shorter side and their longer side less or equal to 1333. We train CenterMask by using Stochastic Gradient Descent (SGD) for 90K iterations (∼12 epoch) with a mini-batch of 16 images and initial learning rate of 0.01 which is decreased by a factor of 10 at 60K and 80K iterations, respectively. We use a weight decay of 0.0001 and a momentum of 0.9, respectively. All backbone models are initialized by ImageNet pre-trained weights. Inference: At test time, the FCOS detection part yields 50 high-score detection boxes, and then the mask branch uses them to predict segmentation masks on each RoI. CenterMask/CenterMask-Lite use a single scale of 800/600 pixels for the shorter side, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we evaluate the effectiveness of Center-Mask on COCO <ref type="bibr" target="#b22">[23]</ref> benchmarks. All models are trained on the train2017 and val2017 are used for ablation studies. Final results are reported on test-dev for comparison with state-of-the-arts. We use AP mask as mask average precision AP (averaged over IoU thresholds), AP S , AP M , and AP L (AP at different scale). We also denote box AP as AP box . All ablation studies are conducted using CenterMask with ResNet-50-FPN. We report the inference time of models using one thread (1 batch size) on the same enviroment equipped with Titan Xp GPU, CUDA v10.0,  With the prementioned scale-adaptive RoI mapping strategy, our spatial attention module, SAM, makes the mask performance forward because the spatial attention module helps the mask predictor to focus on informative pixels but also suppress noise. It can also be seen that the detection performance is boosted when using SAM. We suggest that  <ref type="table">Table 2</ref>: Feature level ranges for RoIAlign <ref type="bibr" target="#b8">[9]</ref> in CenterMmask. P3∼P7 denotes the feature maps with output stride of 2 3 ∼ 2 7</p><p>result from the SAM, the refined feature maps of mask head would also have a secondary effect on the detection branch that shares feature maps of the backbone. The SAG-mask also deploys the mask scoring <ref type="bibr" target="#b14">[15]</ref> that recalibrates the score regarding the predicted mask IoU. As a result, the mask scoring increases performance by 0.7% AP mask . We note that the mask scoring cannot boost detection performance because the recalibrated mask score adjusts the ranks of mask results in the evaluation step, not refines the features of the mask head like the SAM. Besides, SAM rarely causes extra computation while the mask scoring leads to computation overhead (e.g., +5ms).</p><p>Feature selection. We also ablate which feature level range is suitable for our CenterMask based one-stage detector. Since FCOS <ref type="bibr" target="#b32">[33]</ref> detector extract features from P3 ∼ P7, we start the same feature levels in the SAG-mask branch. As shown in <ref type="table">Table 2</ref>, the performance of the P3 ∼ P7 range is not as good as other ranges. We speculate P7 feature map is too small to extract fine features for pixel-level prediction (e.g., 7 × 7). We observe that P3 ∼ P5 feature range achieves the best result, which means feature maps with a bigger resolution are advantageous for the mask prediction.   residual connection and the proposed effective SE (eSE) module into the VoVNet. <ref type="table" target="#tab_4">Table 4</ref> shows residual connection consistently improves VoVNet-39/57/99. In particular, the reason that the improved AP margin of VoVNet-99 is bigger than VoVNet-39/57 is that VoVNet-99 comprised of more OSA modules can have more effect of residual connection that alleviates the optimization problem.</p><p>To validate eSE, we also apply the SE <ref type="bibr" target="#b12">[13]</ref> to the VoVNet and compare it with the proposed eSE. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the SE worsens the performance of VoVNet or has no effect because the diversified feature map of OSA module losses channel information due to channel dimension reduction in the SE. Contrary to the SE, our eSE maintaining channel information using only 1 FC layer boosts both AP mask and AP box from VoVNetV1 with slight computation.</p><p>Comparison to other backbones: We expand VoVNetV2 on various scales; large (V-99), base (V-39/57), and lightweight (V-19) which correspond to ResNeXt-32-8d <ref type="bibr" target="#b35">[36]</ref> &amp; HRNet-W48 <ref type="bibr" target="#b31">[32]</ref>, ResNet-50/101 <ref type="bibr" target="#b9">[10]</ref> &amp; HRNet-W18/W32 <ref type="bibr" target="#b31">[32]</ref>, and MobileNetV2 <ref type="bibr" target="#b30">[31]</ref>, respectively. Table <ref type="bibr" target="#b2">3</ref> and <ref type="figure" target="#fig_0">Figure 1 (bottom)</ref> demonstrate VoVNetV2 is well-balanced backbone network in terms of accuracy and speed. While VoVNetV1-39 already outperforms its counterparts, VoVNetV2-39 shows better performance than ResNet-50/HRNet-W18 by a large margin of 1.2%/2.6% at faster speeds, respectively. Especially, the gain of AP box is bigger than AP mask , 1.5%/3.3%, respectively. A similar result pattern is shown in VoVNetV2-57 with its counterparts.</p><p>For large model, showing much faster run time (1.5×), VoVNetV2-99 achieves competitive AP mask or higher AP box than ResNeXt-101-32x8d despite fewer model parameters. For small model, VoVNetV2-19 outperforms MobileNetV2 by a large margin of 1.7% AP mask /3.3%AP box , with comparable speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with state-of-the-arts methods</head><p>For further validation of the CenterMask, we compare the proposed CenterMask with state-of-the-art instance segmentation methods. As most methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref> use train augmentation, we also adopt the scale-jitter where the shorter image side is randomly sampled from [640, 800] pixels <ref type="bibr" target="#b7">[8]</ref>. For Centermask-Lite, [580, 600] scale jittering is used for training. We train CenterMask and CenterMask-Lite for 24/36 epochs and 48 epochs, respectively. Note that we do not use test-time augmentation <ref type="bibr" target="#b7">[8]</ref> (multi-scale). The other hyper-parameters are kept same as ablation study. For fair speed comparison, we inference models on the same GPU as counterparts. Specifically, since most large models are tested on V100 GPU and YOLACT <ref type="bibr" target="#b0">[1]</ref> models are reported on Titan Xp GPU, we also report CenterMask models on V100 and CenterMask-Lite models on Xp.  Under the same ResNet-101 backbone, CenterMask outperforms all other counterparts in terms of both accuracy (AP mask , AP box ) and speed. In particular, compared to RetinaMask <ref type="bibr" target="#b6">[7]</ref> that has similar architecture (i.g., one-stage detector + mask branch), CenterMask achieves 3.6%AP mask gain. In less than half training epochs, Cen-terMask also surpasses the dense sliding window method, TensorMask <ref type="bibr" target="#b4">[5]</ref>, by 1.2%AP mask at ×5 faster speed. Furthermore, to the best of our knowledge, the CenterMask with VoVNetV2-99 is the first method to achieves 40% AP mask at over 10 fps. It is noted that after first submission, Detectron2 <ref type="bibr" target="#b34">[35]</ref> has been released that is a better baseline code. Thus, we also re-implement our CenterMask* on top of Detectron2 <ref type="bibr" target="#b34">[35]</ref> and obtain further performance gain.</p><p>We also compare with YOLACT <ref type="bibr" target="#b0">[1]</ref> that is the representative real-time instance segmentation. We use four kinds of backbones (e.g., MobileNetV2, VoVNetV2-19, VoVNetV2-39, and ResNet-50), which have a different accuracy-speed tradeoff. <ref type="table" target="#tab_6">Table 5</ref> and <ref type="figure" target="#fig_0">Figure 1</ref> (top) demonstrate CenterMask-Lite is consistently superior to YOLACT in terms of accuracy and speed. Compared to YOLACT, all CenterMask-Lite models achieve over 30 fps speed with large margins of both AP mask and AP box .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>In <ref type="table" target="#tab_6">Table 5</ref>, we observe that using the same ResNet-101 backbone, Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> shows better performance than <ref type="bibr" target="#b0">1</ref> After the initial submission, Detectron2 <ref type="bibr" target="#b34">[35]</ref> has been released and we has developed the improved CenterMask* on top of the Detectron2 <ref type="bibr" target="#b34">[35]</ref>.</p><p>CenterMask on small object. We conjecture that Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> uses larger feature maps (P2) than Center-Mask (P3) in which the mask branch can extract much finer spatial layout of an object than the P3 feature map. We note that there are still rooms for improving one-stage instance segmentation performance like techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> of Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a real-time anchor-free one-stage instance segmentation and more effective backbone networks. Adding spatial attention guided mask branch to the anchorfree one stage instance detection, CenterMask achieves state-of-the-art performance at real-time speed. The newly proposed VoVNetV2 backbone spanning from lightweight to larger models makes CenterMask well-balanced performance in terms of speed and accuracy. We hope Center-Mask will serve as a baseline for real-time instance segmentation. We also believe our proposed VoVNetV2 can be used as a strong and efficient backbone network for various vision tasks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy-speed Tradeoff. across various instance segmentation models (top) and backbone networks (bottom) on COCO. The inference speed of Cen-terMask &amp; CenterMask-Lite is reported on the same GPU (V100/Xp) with their counterparts. Note that all backbone networks in the bottom are compared under the proposed CenterMask. Please refer to section 3.2,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of CenterMask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>OSA module in VoVNet (b) OSA + identity mapping (c) OSA + identity mapping + eSE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Results of CenterMask with VoVNetV2-99 on COCO test-dev2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Spatial Attention Guided Mask (SAG-Mask) These models use ResNet-50 backbone. We note that the mask heads with Eq.1 is same as the mask branch of Mask R-CNN. SAM and Scoring denotes the proposed Spatial Attention Module and mask scoring<ref type="bibr" target="#b14">[15]</ref>.Table 1demonstrates the influence of each component in building Spatial Attention Guided Mask (SAG-Mask). The baseline, FCOS<ref type="bibr" target="#b32">[33]</ref> object detector, starts from 37.8% AP box with the run time of 57 ms. Adding only naive mask head improves the box performance by 0.5% AP box and obtains 33.4% AP mask .</figDesc><table><row><cell>Component</cell><cell cols="3">AP mask AP box Time (ms)</cell></row><row><cell>FCOS (baseline), ours</cell><cell>-</cell><cell>37.8</cell><cell>57</cell></row><row><cell>+ mask head (Eq. 1 [21])</cell><cell>33.4</cell><cell>38.3</cell><cell>67</cell></row><row><cell>+ mask head (Eq. 2, ours)</cell><cell>33.8</cell><cell>38.7</cell><cell>67</cell></row><row><cell>+ SAM</cell><cell>34.0</cell><cell>38.9</cell><cell>67</cell></row><row><cell>+ Mask scoring</cell><cell>34.7</cell><cell>38.8</cell><cell>72</cell></row><row><cell cols="4">cuDNN v7.3, and pytorch1.1. The Qualitative results of</cell></row><row><cell cols="2">CenterMask are shown in Figure 4.</cell><cell></cell><cell></cell></row><row><cell>3.1. Ablation study</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Scale-adaptive RoI assignment function: Comparing to</cell></row><row><cell cols="4">Equation 1 [21], we validate the proposed Equation 2 in</cell></row><row><cell cols="4">CenterMask. Table 1 shows that our scale-adaptive RoI as-</cell></row><row><cell cols="4">signment function considering the input scale improves by</cell></row><row><cell cols="4">0.4% AP mask over the counterpart. It means that Equation 2</cell></row><row><cell cols="4">regarding the ratio of input/RoI is more scale-adaptive than</cell></row><row><cell>Equation 1.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Spatial Attention Guided Mask:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>VoVNetV2:We extend VoVNet to VoVNetV2 by using Params. AP mask AP mask</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>S</cell><cell>AP mask M</cell><cell>AP mask L</cell><cell cols="2">AP box AP box S</cell><cell>AP box M</cell><cell>AP box L</cell><cell>Time (ms)</cell></row><row><cell>MobileNetV2 [31]</cell><cell>28.7M</cell><cell>29.5</cell><cell>12.0</cell><cell>31.4</cell><cell>43.8</cell><cell>32.6</cell><cell>17.8</cell><cell>35.2</cell><cell>43.2</cell><cell>56</cell></row><row><cell>VoVNetV2-19 [19]</cell><cell>37.6M</cell><cell>32.2</cell><cell>14.1</cell><cell>34.8</cell><cell>48.1</cell><cell>35.9</cell><cell>20.8</cell><cell>39.2</cell><cell>47.6</cell><cell>59</cell></row><row><cell cols="2">HRNetV2-W18 [32] 36.4M</cell><cell>33.0</cell><cell>14.3</cell><cell>34.7</cell><cell>49.9</cell><cell>36.7</cell><cell>20.7</cell><cell>39.4</cell><cell>49.3</cell><cell>80</cell></row><row><cell>ResNet-50 [10]</cell><cell>51.2M</cell><cell>34.7</cell><cell>15.5</cell><cell>37.6</cell><cell>51.5</cell><cell>38.8</cell><cell>22.4</cell><cell>42.5</cell><cell>51.1</cell><cell>72</cell></row><row><cell>VoVNetV1-39 [19]</cell><cell>49.0M</cell><cell>35.3</cell><cell>15.5</cell><cell>38.4</cell><cell>52.1</cell><cell>39.7</cell><cell>23.0</cell><cell>43.3</cell><cell>52.7</cell><cell>68</cell></row><row><cell>VoVNetV2-39</cell><cell>52.6M</cell><cell>35.6</cell><cell>16.0</cell><cell>38.6</cell><cell>52.8</cell><cell>40.0</cell><cell>23.4</cell><cell>43.7</cell><cell>53.9</cell><cell>70</cell></row><row><cell cols="2">HRNetV2-W32 [32] 56.2M</cell><cell>36.2</cell><cell>16.0</cell><cell>38.4</cell><cell>53.0</cell><cell>40.6</cell><cell>23.0</cell><cell>43.8</cell><cell>53.1</cell><cell>95</cell></row><row><cell>ResNet-101[10]</cell><cell>70.1M</cell><cell>36.0</cell><cell>16.5</cell><cell>39.2</cell><cell>54.4</cell><cell>40.7</cell><cell>23.4</cell><cell>44.3</cell><cell>54.7</cell><cell>91</cell></row><row><cell>VoVNetV1-57 [19]</cell><cell>63.0M</cell><cell>36.1</cell><cell>16.2</cell><cell>39.2</cell><cell>54.0</cell><cell>40.8</cell><cell>23.7</cell><cell>44.2</cell><cell>55.3</cell><cell>74</cell></row><row><cell>VoVNetV2-57</cell><cell>68.9M</cell><cell>36.6</cell><cell>16.9</cell><cell>39.8</cell><cell>54.5</cell><cell>41.5</cell><cell>24.1</cell><cell>45.2</cell><cell>55.2</cell><cell>76</cell></row><row><cell cols="2">HRNetV2-W48 [32] 92.3M</cell><cell>38.1</cell><cell>17.6</cell><cell>41.1</cell><cell>55.7</cell><cell>43.0</cell><cell>25.8</cell><cell>46.7</cell><cell>55.9</cell><cell>126</cell></row><row><cell>ResNeXt-101 [36]</cell><cell>114.3M</cell><cell>38.3</cell><cell>18.4</cell><cell>41.6</cell><cell>55.4</cell><cell>43.1</cell><cell>26.1</cell><cell>46.8</cell><cell>55.7</cell><cell>157</cell></row><row><cell>VoVNetV1-99 [19]</cell><cell>83.6M</cell><cell>31.5</cell><cell>13.5</cell><cell>33.5</cell><cell>46.5</cell><cell>35.3</cell><cell>19.7</cell><cell>38.1</cell><cell>46.6</cell><cell>101</cell></row><row><cell>VoVNetV2-99</cell><cell>96.9M</cell><cell>38.3</cell><cell>18.0</cell><cell>41.8</cell><cell>56.0</cell><cell>43.5</cell><cell>25.8</cell><cell>47.8</cell><cell>57.3</cell><cell>106</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CenterMask with other backbones on COCO val2017. Note that all mdoels are trained with a same manner (e.g., 12 epoch, 16 batch size, without train &amp; test augmentation). The inference time is reported on same Titan Xp GPU.</figDesc><table><row><cell>Backbone</cell><cell>Params.</cell><cell>AP mask</cell><cell>AP box</cell><cell>Time (ms)</cell></row><row><cell cols="2">VoVNetV1-39 49.0M</cell><cell>35.3</cell><cell>39.7</cell><cell>68</cell></row><row><cell>+ residual</cell><cell cols="3">49.0M 35.5 (+0.2) 39.8 (+0.1)</cell><cell>68</cell></row><row><cell>+ SE [13]</cell><cell>50.8M</cell><cell cols="2">34.6 (-0.7) 39.0 (-0.7)</cell><cell>70</cell></row><row><cell>+ eSE, ours</cell><cell cols="3">52.6M 35.6 (+0.3) 40.0 (+0.3)</cell><cell>70</cell></row><row><cell cols="2">VoVNetV1-57 63.0M</cell><cell>36.1</cell><cell>40.8</cell><cell>74</cell></row><row><cell>+ residual</cell><cell cols="3">63.0M 36.4 (+0.3) 41.1 (+0.3)</cell><cell>74</cell></row><row><cell>+ SE [13]</cell><cell>65.9M</cell><cell>35.9 (-0.2)</cell><cell>40.8</cell><cell>77</cell></row><row><cell>+ eSE, ours</cell><cell cols="3">68.9M 36.6 (+0.5) 41.5 (+0.7)</cell><cell>76</cell></row><row><cell cols="2">VoVNetV1-99 83.6M</cell><cell>31.5</cell><cell>35.3</cell><cell>101</cell></row><row><cell>+ residual</cell><cell cols="3">83.6M 37.6 (+6.1) 42.5 (+7.2)</cell><cell>101</cell></row><row><cell>+ SE [13]</cell><cell cols="3">88.0M 37.1 (+5.6) 41.9 (+6.6)</cell><cell>107</cell></row><row><cell>+ eSE, ours</cell><cell cols="3">96.9M 38.3 (+6.8) 43.5 (+8.2)</cell><cell>106</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>VoVNetV2</figDesc><table><row><cell>Start from VoVNetV1, VoVNetV2 is im-</cell></row><row><cell>proved by adding residual connection [10] and the proposed ef-</cell></row><row><cell>fetive SE (eSE).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>CenterMask instance segmentation and detection performance on COCO test-dev2017. Mask R-CNN, RetinaMask, and CenterMask are implemented on the same base code<ref type="bibr" target="#b26">[27]</ref> and CenterMask* is implemented on top of Detectron2 1<ref type="bibr" target="#b34">[35]</ref>. R, X, V, and M denote ResNet, ResNeXt-32x8d, VoVNetV2, and MobileNetV2, respectively. For fair compariosn, these results are tested with one thread and single-scale.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Retinamask: Learning to predict masks improves stateof-the-art single-shot detection for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03353</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sc-fegan: Face editing generative adversarial network with user&apos;s sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1745" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shapemask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An energy and gpu-computation efficient backbone network for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangrok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01892</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Thundernet: Towards realtime generic object detection on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vision-based garbage dumping action detection for real-world surveillance platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchan</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="494" to="505" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
