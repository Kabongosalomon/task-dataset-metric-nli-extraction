<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Convolution Designs into Visual Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<email>wuwei@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research, ‡ S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Convolution Designs into Visual Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting lowlevel features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.</p><p>Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and stateof-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b37">[38]</ref> have become the de-facto standard for natural language processing (NLP) tasks due to their abilities to model long-range dependencies and to train in parallel. Recently, there exist some attempts to apply Trans- <ref type="bibr" target="#b0">1</ref> Code and models will be released upon acceptance.  <ref type="table" target="#tab_7">Table 8</ref>.</p><p>formers to vision domains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51]</ref>, leading promising results in different tasks. Among them, Vision Transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> is the first pure Transformer architecture that is directly inherited from NLP, and applied to image classification. It obtains promising results compared to many state-of-the-art CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b18">19]</ref>. But it relies heavily on the large amount of dataset of JFT-300M <ref type="bibr" target="#b33">[34]</ref>, which limits the application in the scenarios with limited computing resources or labeled training data. To alleviate the dependence on a large amount of data, the Data-efficient image Transformers (DeiT) <ref type="bibr" target="#b35">[36]</ref> introduce a CNN model as a teacher and applies knowledge distillation <ref type="bibr" target="#b13">[14]</ref> to improve the student model of ViT. Thus DeiT that is only trained on ImageNet can obtain satisfactory results. But the requirement of trained high-performance CNN models is a potential computation burden. Besides, the choice of teacher models, distillation types may affect the final performance. Therefore, we intend to design a new visual Transformer that can overcome these limitations.</p><p>Some existing observations in these work can help us design desired architectures. In ViT, Transformer-based models underperform CNNs in the realm of ∼10M training samples. It claims that "Transformers lack some of the inductive biases inherent to CNNs, and therefore do not generalize well when trained on insufficient data". In DeiT, a CNN teacher gives better performance than using a Transformer one, which probably due to "the inductive bias inherited by the Transformer through distillation". These observations make us rethink whether it is appropriate to remove all convolutions from the Transformer. And should the inductive biases inherited in the convolution be forgotten?</p><p>Looking back to the convolution, the main characteristics are translation invariance and locality <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>. Translation invariance is relevant to the weight sharing mechanism, which can capture information about the geometry and topology in vision tasks <ref type="bibr" target="#b22">[23]</ref>. For the locality, it is a common assumption in visual tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref> that neighboring pixels always tend to be correlated. However, pure Transformer architectures do not fully utilize these prior biases that existed in images. First, ViT performs direct tokenization of patches from the raw input image with a size of 16 × 16 or 32 × 32. It is difficult to extract the low-level features which form some fundamental structures in images (e.g. corners and edges). Second, the self-attention modules concentrate on building long-range dependencies among tokens, ignoring the locality in the spatial dimension.</p><p>To address these problems, we design a Convolutionenhanced image Transformer (CeiT) to combine the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in associating long-range dependencies. Three modifications are made compared to the vanilla ViT. To solve the first problem, instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features, where patches are in a smaller size and then flattened into a sequence of tokens. Due to a well-designed structure, the I2T module does not introduce more computation costs. To solve the second problem, the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension. To exploit the ability of self-attention, a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations to improve the final representation. In summary, our contributions are as follows: • As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, CeiT models demonstrate better convergence than pure Transformer models with 3× fewer training iterations, which can reduce the training cost significantly.</p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformer in Vision. iGPT <ref type="bibr" target="#b6">[7]</ref> first introduce transformer to auto-regressively predict pixels, and obtaining pre-trained models without incorporating knowledge of the content in 2D images. However, it can only achieve reasonable performance in a tiny dataset (CIFAR10) with an extremely large model ( , T2T-ViT <ref type="bibr" target="#b46">[47]</ref> and PVT <ref type="bibr" target="#b39">[40]</ref>. Besides, recent work also apply Transformers to various vision tasks, including object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">53]</ref>, segmentation <ref type="bibr" target="#b41">[42]</ref>, image enhancement <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref> and video processing <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Hybrid Models of Convolution and Self-attention. To utilize the advantages of self-attention in building longrange dependencies, some work introduces attention modules into CNNs <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43]</ref>. Among these works, the Non-local network <ref type="bibr" target="#b40">[41]</ref> insert non-local layers into the last several blocks of ResNet <ref type="bibr" target="#b11">[12]</ref> and improve the performance on video recognition and instance segmentation. CCNet <ref type="bibr" target="#b17">[18]</ref> attaches a criss-cross attention module at the top of a segmentation network. SASA <ref type="bibr" target="#b28">[29]</ref>, SANet <ref type="bibr" target="#b48">[49]</ref> and Axial-SASA <ref type="bibr" target="#b38">[39]</ref> propose to replace all convolutional layers by self-attention module to form a stand-alone self-attention network. Recent work also combines Transformers with CNNs. DETR <ref type="bibr" target="#b4">[5]</ref> uses Transformer blocks outside the CNN backbone with the motivation to get rid of region proposals and non-maximal suppression for simplicity. ViLBERT <ref type="bibr" target="#b23">[24]</ref> and VideoBERT <ref type="bibr" target="#b32">[33]</ref> construct crossmodality models using CNN and BERT. Different from the above methods, CeiT incorporates convolutional designs into the basic building blocks of Transformer to inherit the inductive bias in CNNs, which a more elaborate design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our CeiT is designed based on the ViT. First, we give a brief overview of the basic components of ViT in section 3.1. Next, we introduce three modifications that incorporate convolution designs and benefit visual Transformers, including an Image-to-Tokens (I2T) module in section 3.2, a Locally-enhanced FeedForwad (LeFF) module in section 3.3 and a Layer-wise Class token Attention (LCA) module in section 3.4. Last, we analyze the computation complexity of these proposed modules in section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Vision Transformer</head><p>We first revisit the basic components in ViT, including tokenization, encoder blocks, multi-head self-attention (MSA) layers, and feed-forward network (FFN) layers.</p><p>Tokenization. The standard Transformer <ref type="bibr" target="#b37">[38]</ref> receives a sequence of token embeddings as input. To handle 2D images, ViT reshapes the image x ∈ R H×W ×3 into a sequence of flattened 2D patches</p><formula xml:id="formula_1">x p ∈ R N ×(P 2 ·3) , where (H, W )</formula><p>is the resolution of the original image, 3 is the number of channels of RGB images, (P, P ) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. And these patches are flattened and mapped to latent embeddings with a size of C. Then an extra class token is added to the sequence and serves as the image representation, resulting in the input of sequence with a size of x t ∈ R (N +1)×C .</p><p>In practice, ViT splits each image with a patch size of 16 × 16 or 32 × 32. But the straightforward tokenization of input images with large patches may have two limitations: 1) it is difficult to capture low-level information in images (such as edges and corners); 2) large kernels are over-parameterized and are often hard to optimize, thus requires much more training samples or training iterations.</p><p>Encoder blocks. ViT is composed of a series of stacked encoders. Each encoder has two sub-layers of MSA and FFN. A residual connection <ref type="bibr" target="#b11">[12]</ref> is employed around each sub-layer, followed by layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref>. The output for each encoder is:</p><formula xml:id="formula_2">y = LN(x + FFN(x )), and x = LN(x + MSA(x)) (1)</formula><p>Different from CNNs where feature maps are downsampled at the beginning of each stage, the length of tokens is not reduced in different Encoder blocks. The effective receptive field cannot be expanded efficiently, which may affect the efficiency of optimization in visual Transformers.</p><p>MSA. For a self-attention (SA) module, the sequence of input tokens x t ∈ R (N +1)×C are linear transformed into qkv spaces, i.e., queries Q ∈ R (N +1)×C , keys K ∈ R (N +1)×C and values V ∈ R (N +1)×C . Then a weighted sum over all values in the sequence is computed through:</p><formula xml:id="formula_3">Attention(Q, K, V) = softmax( QK T √ C )V (2)</formula><p>And a linear transformation is performed to the weighted values. MSA is an extension of SA. It splits queries, keys, and values for h times and performs the attention function in parallel, then projects their concatenated outputs. Through computing dot-product, the similarity between different tokens is calculated, resulting in long-range and global attention. And a linear aggregation is performed for corresponding values V.</p><p>FFN. FFN performs point-wise operations, which are applied to each token separately. It consists of two linear transformations with a non-linear activation in between:</p><formula xml:id="formula_4">FFN(x) = σ(xW 1 + b 1 )W 2 + b 2<label>(3)</label></formula><p>where W 1 ∈ R C×K is the weight of the first layer, projecting each token into a higher dimension K. And W 2 ∈ R K×C is the weight of the second layer. b 1 ∈ R K and b 2 ∈ R C are the biases. And σ(·) is the non-linear activation of GELU <ref type="bibr" target="#b12">[13]</ref> in ViT.</p><p>Complementary to the MSA module, the FFN module performs dimensional expansion/reduction and non-linear transformation on each token, thereby enhancing the representation ability of tokens. However, the spatial relationship among tokens, which is important in vision, is not considered. This leads that the original ViT needs a mass of training data to learn these inductive biases. Locally Enhanced Feed-forward Identity <ref type="figure">Figure 3</ref>. Illustration of the Locally-enhanced Feed-Forward module. First, patch tokens are projected into a higher dimension. Second, they are restored to "images" in the spatial dimension based on the original positions. Third, a depth-wise convolution is performed on the restored tokens as shown in the yellow region. Then the patch tokens are flattened and projected to the initial dimension. Besides, the class token conducts an identical mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image-to-Tokens with Low-level Features</head><p>To solve the above-mentioned problems in tokenization, we propose a simple but effective module named as Imageto-Tokens (I2T) that extracts patches from feature maps instead of raw input images. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the I2T module is an lightweight stem that consists of a convolutional layer and a max-pooling layer. Ablation studies also suggest that a BatchNorm layer following the convolution layer benefits the training process. It can be denoted as:</p><formula xml:id="formula_5">x = I2T(x) = MaxPool(BN(Conv(x))) (4) where x ∈ R H S × W S ×D ,</formula><p>S is the stride w.r.t the raw input images, and D is the number of enriched channels. Then the learned feature maps are extracted into a sequence of patches in the spatial dimension. To keep the number of generated tokens consistent with ViT, we shrink the resolution of patches into ( P S , P S ). In practice, we set S = 4. I2T fully utilizes the advantage of CNNs in extracting low-level features and reduces the training difficulty of embedding by shrinking the patch size. This is also different from the hybrid type of Transformer proposed in ViT, where a regular ResNet-50 is used to extract high-level features from the last two stages. Our I2T is much lighter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Locally-Enhanced Feed-Forward Network</head><p>To combine the advantage of CNNs to extract local information with the ability of Transformer to establish longrange dependencies, we propose a Locally-enhanced Feed-Forward Network (LeFF) layer. In each Encoder block, we keep the MSA module unchanged, remaining the ability to capture global similarities among tokens. Instead, the original feed-forward network layer is replaced with the LeFF. The structure is given in <ref type="figure">Figure 3</ref>.</p><p>A LeFF module performs following procedures. First, given tokens x h t ∈ R (N +1)×C generated from the preceding MSA module, we split them into patch tokens x h p ∈ R (N +1)×C and a class token x h c ∈ R C accordingly. A linear projection is conducted to expand the embeddings of patch tokens to a higher dimension of x l1 p ∈ R N ×(e×C) , where e is the expand ratio. Second, the patch tokens are restored to "images" of x s p ∈ R √ N × √ N ×(e×C) on spatial dimension based on the position relative to the original image. Third, we perform a depth-wise convolution with kernel size of k on these restored patch tokens, enhancing the representation correlation with neighboring k 2 − 1 tokens, obtaining</p><formula xml:id="formula_6">x d p ∈ R √ N × √ N ×(e×C)</formula><p>. Fourth, these patch tokens are flattened into sequence of x f p ∈ R N ×(e×C) . Last, the patch tokens are projected to the initial dimension with x l2 p ∈ R N ×C , and concatenated with the class token, resulting in x h+1 t ∈ R (N +1)×C . Following each linear projection and depth-wise convolution, a BatchNorm and a GELU is added. These procedures can be noted as:</p><formula xml:id="formula_7">x h c , x h p = Split(x h t )<label>(5)</label></formula><p>x l1 p = GELU(BN(Linear1(x h p )))</p><p>x s p = SpatialRestore(x l1 p )</p><p>x d p = GELU(BN(DWConv(x s p )))</p><p>x f p = Flatten(x d p )</p><p>x l2 p = GELU(BN(Linear2(x f p ))) (10)</p><formula xml:id="formula_12">x h+1 t = Concat(x h c , x l2 p )<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Layer-wise Class-Token Attention</head><p>In CNNs, as the network deepens, the receptive field of the feature map increases. Similar observations are also  found in ViT, whose "attention distance" increases with depth. Therefore, feature representations will be different at different layers. To integrate information across different layers, we design a Layer-wise Class-token Attention (LCA) module. Unlike the standard ViT that takes the class token x (L) c at the last L-th layer as the final representation, LCA makes attention over class tokens at different layers.</p><p>As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, LCA gets a sequence of class tokens as the input, which can be denoted as X c = [x <ref type="bibr" target="#b0">(1)</ref> c , · · · , x </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Computational Complexity Analysis</head><p>We analyze the extra computational complexity (in terms of FLOPs) brought by our modifications. Generally, with a small increase in computational cost, our CeiT model can efficiently combine the advantage of CNNs and Transformers, resulting in higher performance and better convergence.  O(N 2 C) in practice as given in <ref type="table">Table 1</ref>.</p><p>LCA vs Encoder Block. Compared with the standard Encoder block, the LCA only computes attention over the L-th class token. Both the computation cost in MSA and FFN has been reduced to 1 N . The cost can be ignored compared with the other 12 encoder blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform extensive experiments to demonstrate the effectiveness of our proposed CeiT. In section 4.1, we give the details of used visual datasets and training settings. In section 4.2, we compare CeiT with other state-of-the-art architectures including CNNs and Transformers in ImageNet. In section 4.3, we transfer CeiT models trained on ImageNet to other benchmark datasets, showing the strong generalization ability. In section 4.4, we conduct ablation studies on our modifications. In section 4.5, we show the fast convergence ability of our CeiT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Network Architectures. We build our CeiT architectures by following the basic configurations of ViT and DeiT. The details are given in <ref type="table">Table 1</ref>. The I2T module consists of a convolutional layer with a kernel size of 7 and a stride of 2, generating enriched channels of 32. And a BatchNorm layer is added for stable training. Then a max-pooling layer with a kernel size of 3 and a stride of 2 is followed, resulting in feature maps with 4× smaller than the input image. Compared to the patch size of 16×16 in ViT, we use a patch size of 4 × 4 in generating a sequence of tokens. We follow the standard setting in the number of the depth of 12. For the LeFF module, we set the expand ratio e to be 4. And the kernel size for the depth-wise convolution is 3 × 3. For the LCA module, the number of heads and the ratio of MLP follow those of the standard Encoder blocks. Implementation Details. All of our experiments are performed on the NVIDIA Tesla V100 GPUs. We adopt the same training strategy in DeiT. We list the detailed settings for training, fine-tuning, and transfer learning in <ref type="table">Table 2</ref>.</p><p>Datasets. Instead of using a large-scale training dataset of JFT300M or ImageNet22K, we adopt the mid-sized  ImageNet <ref type="bibr" target="#b30">[31]</ref> dataset. It consists of 1.2M training images belonging to 1000 classes, and 50K validation images. Besides, we also test on some downstream benchmarks to evaluate the transfer ability of our trained CeiT models. These datasets consist different scenes, including fine-grained recognition (Standford Cars <ref type="bibr" target="#b19">[20]</ref>, Oxford-102 Followers <ref type="bibr" target="#b26">[27]</ref> and Oxford-IIIT-Pets <ref type="bibr" target="#b27">[28]</ref>), long-tailed classification (iNaturalist18 <ref type="bibr" target="#b15">[16]</ref>, iNaturalist19 <ref type="bibr" target="#b15">[16]</ref>) and superordinate level classification (CIFAR10 <ref type="bibr" target="#b20">[21]</ref>, CIFAR100 <ref type="bibr" target="#b20">[21]</ref>). The details are given in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on ImageNet</head><p>We report the results on ImageNet validation dataset and ImageNet Real dataset <ref type="bibr" target="#b3">[4]</ref> in <ref type="table" target="#tab_4">Table 4</ref>. For comparison, we select CNNs (ResNets <ref type="bibr" target="#b11">[12]</ref>, EfficieNets <ref type="bibr" target="#b34">[35]</ref>, RegNets <ref type="bibr" target="#b29">[30]</ref>) and Transformers (ViTs, DeiTs) to evaluate the effectiveness of our CeiT models.</p><p>CeiT vs CNNs. We first compare CeiT models with CNN models. CeiT-T achieves a Top-1 accuracy of 76.4% in ImageNet, which is close to the performance of ResNet-50. But CeiT-T only requires 3× fewer FLOPs and 4× fewer Params than ResNet-50. For the CeiT-S of a sim-ilar size as ResNet-50, its performance is 82.0%, achieving a higher performance (+5.3%) than that of ResNet-50 (76.7%). This performance also outperforms larger CNN models of ResNet-152 and RegNetY-8GF. When trained on the resolution of 384 × 384, CeiT-S↑ 384 surpasses EfficientNet-B4 by 0.4%. It shows that we have obtained comparable results with EfficientNets, and have almost closed the gap between vision Transformers and CNNs.</p><p>CeiT vs ViT/DeiT/T2T/PVT. CeiT-T achieves a similar result of 76.4% with ViT-L/16 of 76.5%. This is a surprising result since the size of the CeiT-T model is only one-fifth the size of ViT-L/16. But this result is produced by the improvements of the training strategy and the modifications of the model structure. To further demonstrate the improvements brought by the structure, we compare CeiT with DeiT. CeiT models follow the same training strategy as given in section 4.1. Our modifications only increase the number of parameters by about 10%, and have almost no effect on FLOPs. In this way, CeiT-T outperforms DeiT-T by a large margin of 4.2% for the Top-1 accuracy. And CeiT-S obtains higher results than that of DeiT-S and DeiT-B by 2.1% and 0.2% respectively. We also compare CeiT with concurrent work of T2T-ViT and PVT. CeiT-S achieves slightly higher accuracy than T2T-ViT-19 with much fewer FLOPs and parameters. For PVT models, CeiT-T obtains higher accuracy of 1.3% than PVT-T with fewer FLOPs and parameters. CeiT-S also outperforms PVT-S/M/L models separately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning</head><p>To demonstrate the generalization power of pre-trained CeiT models, we conduct experiments of transfer learning in 7 downstream benchmarks. And the results are given in <ref type="table">Table 6</ref>. Training details are given in the previous <ref type="table">Table  2</ref>. It can be seen that CeiT-S outperforms DeiT-B in most datasets with fewer parameters and FLOPs. CeiT-S↑ 384 achieves state-of-the-arts results in most datasets. Notably, CeiT-S↑ 384 get comparable results with EfficientNet-B7 with an input size of 600. It suggests that the pretrained models of Transformers can be well migrated to downstream tasks, showing the strong potential of visual Transformers against CNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To further identify the effects of the proposed modules, we conduct ablation studies on the main components of I2T, LeFF, and LCA. All of our ablation experiments are based on the DeiT-T model on ImageNet. Different types of I2T module. The influencing factors in I2T include the kernel size of the convolution, the stride of the convolution, the existence of Max-pooling and Batch-Norm layers. The results are given in <ref type="table" target="#tab_5">Table 5</ref>. Without the Max-pooling layer, one convolution layer with a kernel of k7s4 and k5s4 each decreases the performance. An I2T with two convolution layers with a kernel of k3s2 also suffers from a drop. Both the Max-pooling and BatchNorm layers benefit the training. Therefore, we adopt the best structure (in the last row) in all of our experiments.</p><p>Different types of LeFF module. In a LeFF module, the size of the kernel determines the region size in which patch tokens establish local correlation. So we test using kernel sizes of 1 × 1, 3 × 3 and 5 × 5 in <ref type="table">Table 7</ref>. Compared to the baseline without the middle depth-wise convolution, the type of 1 × 1 shows poor performance with a drop of 1.9%. This shows that simply increasing the number of layers for the Transformer does not certainly bring improvements. When increasing the kernel size to larger ones, each token can accumulate with neighboring tokens through the non-linear transformation. Both the types of 3 × 3 and 5 × 5 obtain gains. When adopting the BatchNorm layer, the model can achieve further accuracy improvements up to 2.2% of Top-1 accuracy. Based on the trade-off between the number of parameters and accuracy, we choose the kernel </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Fast Convergence</head><p>The standard visual Transformers, such as ViT and DeiT, usually require a large number of training epochs to converge. Using 3× fewer training epochs, the performances of DeiT suffer significant declines. As shown in <ref type="table" target="#tab_7">Table 8</ref>, CeiT models demonstrate better convergence than DeiT models, resulting in higher performances in a large margin. And CeiT models trained in 100 epochs can obtain comparable results with DeiT models trained in 300 epochs. It shows that incorporating these inductive biases inherent in CNNs benefits the optimization procedure of visual Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a new CeiT architecture that combines the advantages of CNNs in extracting lowlevel features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. CeiT obtains state-of-the-art performances on ImageNet and various downstream tasks, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence than pure Transformer with 3× fewer training iterations, reducing the training cost significantly. Through incorporating convolution designs, we provide a new perspective for more effective visual Transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The fast convergence ability of CeiT models. CeiT models trained with 100 epochs obtain comparable results with DeiT models trained with 300 epochs. Other model settings are given in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparisons of different tokenization methods. The upper one extracts patches from raw input images. The below one (I2T) uses the low-level features generated by a convolutional stem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The proposed Layer-wise Class-token Attention block. It integrates information across different layers through receiving a sequence of class tokens as inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where l denotes the layer depth. LCA follows the standard Transformer block, which contains a MSA and a FFN layer. Particularly, it only computes unidirectional similarities between the L-th class token x (L) c and other class tokens. These modifications reduce the complexity of computing attention from O(n 2 ) to O(n). And the corresponding value of x (L) c is aggregated with others through attention. Then the aggregated value is sent into a FFN layer, resulting in the final representations of x (L) c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>I2T vs Original. The type of tokenization affects the computational cost of embedding. For the original one with a patch size of 16 × 16, the FLOPs are 3C(HW ) 2 . For I2T, the FLOPs are consist of two parts, including feature generation and embedding. In this paper, the generated features are 4× smaller than the input. And the detailed architecture of I2T is given in section 4.1. The total FLOPs of I2T are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>+ 1 64 DCHW . For a ViT-B/16 model, the ratio between I2T and the original one is around 1.1. In this way, the extra computational cost is negligible. LeFF vs FFN. In a FFN layer with e = 4, the FLOPs are 8(N + 1)C 2 . The main extra computation cost of LeFF is introduced by the depth-wise convolution, whose FLOPs are 4k 2 N 2 C. The increase of FLOPs is small since O((N + 1)C 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>CeiT vs DeiT-Teacher. DeiT introduces a CNN teacher model as the extra supervision to optimize the Transformer, achieving higher performances. But it requires extra computation cost to obtain the trained CNN model. While CeiT does not need an additional CNN model to provide supervision information, except for the ground truth. Meanwhile, CeiT-T surpasses DeiT-T-Teacher by 1.9% of the Top-1 ac-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. For example, with a similar model size as ResNet-50, CeiT-S obtains a Top-1 accuracy of 82.0% on Im-ageNet. And the result boosts into 83.3% when finetuned in the resolution of 384 × 384.</figDesc><table><row><cell>We design a new visual Transformer architecture</cell></row><row><cell>namely Convolution-enhanced image Transformer</cell></row><row><cell>(CeiT). It combines the advantages of CNNs in extract-</cell></row><row><cell>ing low-level features, strengthening locality, and the</cell></row><row><cell>advantages of Transformers in establishing long-range</cell></row><row><cell>dependencies.</cell></row></table><note>• Experimental results on ImageNet and seven down- stream tasks show the effectiveness and generaliza- tion ability of CeiT compared with previous Trans- formers and state</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Variants of our CeiT architecture. The FLOPs are calculated for images at resolution 224 × 224. And 7ks2 means a convolution/pooling with kernel size of 7 and stride of 2. The hyper-paramters that varies depends on tasks. The training and fine-tuning on ImageNet in our experienet adopt the same setting in DeiT. We use the same settings for finetuning on different downstream datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">I2T conv maxpool channels</cell><cell cols="4">encoder embedding heads blocks dimension</cell><cell cols="2">LeFF Params FLOPs e k (M) (G)</cell></row><row><cell cols="2">CeiT-T k7s2</cell><cell>k3s2</cell><cell>32</cell><cell></cell><cell>12</cell><cell>192</cell><cell>3</cell><cell>4 3</cell><cell>6.4</cell><cell>1.2</cell></row><row><cell cols="2">CeiT-S k7s2</cell><cell>k3s2</cell><cell>32</cell><cell></cell><cell>12</cell><cell>384</cell><cell>6</cell><cell>4 3</cell><cell>24.2</cell><cell>4.5</cell></row><row><cell cols="2">CeiT-B k7s2</cell><cell>k3s2</cell><cell>32</cell><cell></cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>4 3</cell><cell>86.6</cell><cell>17.4</cell></row><row><cell>Task</cell><cell>dataset</cell><cell>input size</cell><cell cols="2">Epochs</cell><cell cols="2">batch learning size rate</cell><cell>LR scheduler</cell><cell cols="2">warmup weight repeated epoch decay aug [15]</cell></row><row><cell>training</cell><cell>ImageNet</cell><cell>224</cell><cell>300</cell><cell></cell><cell>1024</cell><cell>1e-3</cell><cell>cosine</cell><cell>5</cell><cell>0.05</cell></row><row><cell>fine-tuning</cell><cell>ImageNet</cell><cell>384</cell><cell>30</cell><cell></cell><cell>1024</cell><cell>5e-6</cell><cell>constant</cell><cell>0</cell><cell>1e-8</cell></row><row><cell cols="3">transferring downstream 224&amp;384</cell><cell>100</cell><cell></cell><cell>512</cell><cell>5e-4</cell><cell>cosine</cell><cell>2</cell><cell>1e-8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Details of used visual datasets.</figDesc><table><row><cell>dataset</cell><cell cols="3">classes train data val data</cell></row><row><cell>ImageNet</cell><cell>1000</cell><cell>1,281,167</cell><cell>50000</cell></row><row><cell>iNaturalist2018</cell><cell>8142</cell><cell>437513</cell><cell>24426</cell></row><row><cell>iNaturelist2019</cell><cell>1010</cell><cell>265240</cell><cell>3003</cell></row><row><cell>Standford Cars</cell><cell>196</cell><cell>8133</cell><cell>8041</cell></row><row><cell>Oxford-102 Followers</cell><cell>102</cell><cell>2040</cell><cell>6149</cell></row><row><cell>Oxford-IIIT-Pets</cell><cell>37</cell><cell>3680</cell><cell>3669</cell></row><row><cell>CIFAR100</cell><cell>100</cell><cell>50000</cell><cell>10000</cell></row><row><cell>CIFAR10</cell><cell>10</cell><cell>50000</cell><cell>10000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Accuracies on ImageNet and ImageNet Real of CeiT and of several SOTA CNNs and Transformers, for models trained with no extra data. The notation ↑ 384 means the model is fine-tuned on the resolution of 384 × 384. CeiT models outperform CNNs with similar FLOPs. Directly trained in ImageNet, CeiT models also achieve higher performances than DeiT models that are trained with extra CNN teachers.</figDesc><table><row><cell>Group</cell><cell>Model</cell><cell cols="3">FLOPs Params input (G) (M) size</cell><cell cols="2">ImageNet Top-1 Top-5</cell><cell>Real Top-1</cell></row><row><cell></cell><cell>ResNet-18 [12]</cell><cell>1.8</cell><cell>11.7</cell><cell>224</cell><cell>70.3</cell><cell>86.7</cell><cell>77.3</cell></row><row><cell></cell><cell>ResNet-50 [12]</cell><cell>4.1</cell><cell>25.6</cell><cell>224</cell><cell>76.7</cell><cell>93.3</cell><cell>82.5</cell></row><row><cell></cell><cell>ResNet-101 [12]</cell><cell>7.8</cell><cell>44.5</cell><cell>224</cell><cell>78.3</cell><cell>94.1</cell><cell>83.7</cell></row><row><cell></cell><cell>ResNet-152 [12]</cell><cell>11.5</cell><cell>60.2</cell><cell>224</cell><cell>78.9</cell><cell>94.4</cell><cell>84.1</cell></row><row><cell></cell><cell>EfficientNet-B0 [35]</cell><cell>0.4</cell><cell>5.3</cell><cell>224</cell><cell>77.1</cell><cell>93.3</cell><cell>83.5</cell></row><row><cell>CNNs</cell><cell>EfficientNet-B1 [35]</cell><cell>0.7</cell><cell>7.8</cell><cell>240</cell><cell>79.1</cell><cell>94.4</cell><cell>84.9</cell></row><row><cell></cell><cell>EfficientNet-B2 [35]</cell><cell>1.0</cell><cell>9.1</cell><cell>260</cell><cell>80.1</cell><cell>94.9</cell><cell>85.9</cell></row><row><cell></cell><cell>EfficientNet-B3 [35]</cell><cell>1.8</cell><cell>12.2</cell><cell>300</cell><cell>81.6</cell><cell>95.7</cell><cell>86.8</cell></row><row><cell></cell><cell>EfficientNet-B4 [35]</cell><cell>4.4</cell><cell>19.3</cell><cell>380</cell><cell>82.9</cell><cell>96.4</cell><cell>88.0</cell></row><row><cell></cell><cell>RegNetY-4GF [30]</cell><cell>4.0</cell><cell>20.6</cell><cell>224</cell><cell>80.0</cell><cell>-</cell><cell>86.4</cell></row><row><cell></cell><cell>RegNetY-8GF [30]</cell><cell>8.0</cell><cell>39.2</cell><cell>224</cell><cell>81.7</cell><cell>-</cell><cell>87.4</cell></row><row><cell></cell><cell>ViT-B/16 [10]</cell><cell>18.7</cell><cell>86.5</cell><cell>384</cell><cell>77.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ViT-L/16 [10]</cell><cell>65.8</cell><cell>304.33</cell><cell>384</cell><cell>76.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeiT-T [36]</cell><cell>1.2</cell><cell>5.7</cell><cell>224</cell><cell>72.2</cell><cell>91.1</cell><cell>80.6</cell></row><row><cell></cell><cell>DeiT-S [36]</cell><cell>4.5</cell><cell>22.1</cell><cell>224</cell><cell>79.9</cell><cell>95.0</cell><cell>85.7</cell></row><row><cell></cell><cell>DeiT-B [36]</cell><cell>17.3</cell><cell>86.6</cell><cell>224</cell><cell>81.8</cell><cell>95.6</cell><cell>86.7</cell></row><row><cell></cell><cell>DeiT-T + Teacher [36]</cell><cell>1.2</cell><cell>5.7</cell><cell>224</cell><cell>74.5</cell><cell>91.9</cell><cell>82.1</cell></row><row><cell></cell><cell>DeiT-S + Teacher [36]</cell><cell>4.5</cell><cell>22.1</cell><cell>224</cell><cell>81.2</cell><cell>95.4</cell><cell>86.8</cell></row><row><cell></cell><cell>DeiT-B↑384 [36]</cell><cell>52.8</cell><cell>86.6</cell><cell>384</cell><cell>83.1</cell><cell>96.2</cell><cell>87.7</cell></row><row><cell></cell><cell>T2T-ViT-14 [47]</cell><cell>5.2</cell><cell>21.5</cell><cell>224</cell><cell>81.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformers</cell><cell>T2T-ViT-19 [47]</cell><cell>8.9</cell><cell>39.2</cell><cell>224</cell><cell>81.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>T2T-ViT-24 [47]</cell><cell>14.1</cell><cell>64.1</cell><cell>224</cell><cell>82.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PVT-T [40]</cell><cell>1.9</cell><cell>13.2</cell><cell>224</cell><cell>75.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PVT-S [40]</cell><cell>3.8</cell><cell>24.5</cell><cell>224</cell><cell>79.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PVT-M [40]</cell><cell>6.7</cell><cell>44.2</cell><cell>224</cell><cell>81.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PVT-L [40]</cell><cell>9.8</cell><cell>61.4</cell><cell>224</cell><cell>81.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CeiT-T</cell><cell>1.2</cell><cell>6.4</cell><cell>224</cell><cell>76.4</cell><cell>93.4</cell><cell>83.6</cell></row><row><cell></cell><cell>CeiT-S</cell><cell>4.5</cell><cell>24.2</cell><cell>224</cell><cell>82.0</cell><cell>95.9</cell><cell>87.3</cell></row><row><cell></cell><cell>CeiT-T↑384</cell><cell>3.6</cell><cell>6.4</cell><cell>384</cell><cell>78.8</cell><cell>94.7</cell><cell>85.6</cell></row><row><cell></cell><cell>CeiT-S↑384</cell><cell>12.9</cell><cell>24.2</cell><cell>384</cell><cell>83.3</cell><cell>96.5</cell><cell>88.1</cell></row><row><cell cols="3">curacy. And CeiT-S also outperforms DeiT-S-Teacher by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0.8%. These experimental results demonstrate the effec-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tiveness of our CeiT.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study results on the type of I2T. Top-1 accuracy and changes are reported.</figDesc><table><row><cell></cell><cell>I2T Type</cell><cell></cell><cell>Top-1</cell></row><row><cell>conv</cell><cell cols="2">maxpool BN channels</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3</cell><cell>72.2</cell></row><row><cell>k7s4</cell><cell></cell><cell>64</cell><cell>71.4 (-0.8)</cell></row><row><cell>k5s4</cell><cell></cell><cell>64</cell><cell>71.1 (-1.1)</cell></row><row><cell>k3s2 + k3s2</cell><cell></cell><cell>64</cell><cell>70.4 (-1.8)</cell></row><row><cell>k7s2</cell><cell>k3s2</cell><cell>32</cell><cell>72.9 (+0.7)</cell></row><row><cell>k7s2</cell><cell>k3s2</cell><cell>32</cell><cell>73.4 (+1.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Results on downstream tasks with ImageNet pre-training. CeiT models achieve state-of-the-arts performance. The results with the first two highest accuracies are bolded. Ablation study results on the type of LeFF. Top-1 accuracy and changes are reported.</figDesc><table><row><cell>Model</cell><cell>FLOPs</cell><cell cols="5">ImageNet iNat18 iNat19 Cars Followers</cell><cell>Pets</cell><cell cols="2">CIFAR10 CIFAR100</cell></row><row><cell>Grafit ResNet-50 [37]</cell><cell>4.1G</cell><cell>79.6</cell><cell>69.8</cell><cell>75.9</cell><cell>92.5</cell><cell>98.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Grafit RegNetY-8GF [37]</cell><cell>8.0G</cell><cell>-</cell><cell>76.8</cell><cell>80.0</cell><cell>94.0</cell><cell>99.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EfficientNet-B5 [35]</cell><cell>10.3G</cell><cell>83.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.5</cell><cell>-</cell><cell>98.1</cell><cell>91.1</cell></row><row><cell>EfficientNet-B7 [35]</cell><cell>37.3G</cell><cell>84.3</cell><cell>-</cell><cell>-</cell><cell>94.7</cell><cell>98.8</cell><cell>-</cell><cell>98.9</cell><cell>91.7</cell></row><row><cell>ViT-B/16 [10]</cell><cell>18.7G</cell><cell>77.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.5</cell><cell>93.8</cell><cell>98.1</cell><cell>87.1</cell></row><row><cell>ViT-L/16 [10]</cell><cell>65.8G</cell><cell>76.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.7</cell><cell>93.6</cell><cell>97.9</cell><cell>86.4</cell></row><row><cell>Deit-B [36]</cell><cell>17.3G</cell><cell>81.8</cell><cell>73.2</cell><cell>77.7</cell><cell>92.1</cell><cell>98.4</cell><cell>-</cell><cell>99.1</cell><cell>90.8</cell></row><row><cell>Deit-B↑384 [36]</cell><cell>52.8G</cell><cell>83.1</cell><cell>79.5</cell><cell>81.4</cell><cell>93.3</cell><cell>98.5</cell><cell>-</cell><cell>99.1</cell><cell>90.8</cell></row><row><cell>CeiT-T</cell><cell>1.2G</cell><cell>76.4</cell><cell>64.3</cell><cell>72.8</cell><cell>90.5</cell><cell>96.9</cell><cell>93.8</cell><cell>98.5</cell><cell>88.4</cell></row><row><cell>CeiT-T↑384</cell><cell>3.6G</cell><cell>78.8</cell><cell>72.2</cell><cell>77.9</cell><cell>93.0</cell><cell>97.8</cell><cell>94.5</cell><cell>98.5</cell><cell>88.0</cell></row><row><cell>CeiT-S</cell><cell>4.5G</cell><cell>82.0</cell><cell>73.3</cell><cell>78.9</cell><cell>93.2</cell><cell>98.2</cell><cell>94.6</cell><cell>99.0</cell><cell>90.8</cell></row><row><cell>CeiT-S↑384</cell><cell>12.9G</cell><cell>83.3</cell><cell>79.4</cell><cell>82.7</cell><cell>94.1</cell><cell>98.6</cell><cell>94.9</cell><cell>99.1</cell><cell>90.8</cell></row><row><cell>LeFF Type</cell><cell>Top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>kernel size BN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>72.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 × 1</cell><cell>70.3 (-1.9)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 × 3</cell><cell cols="2">72.7 (+0.5)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 × 5</cell><cell cols="2">73.1 (+0.9)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 × 3</cell><cell cols="2">74.3 (+2.1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 × 5</cell><cell cols="2">74.4 (+2.2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of the ability of convergence between DeiT and CeiT models. CeiT models trained with 100 epochs obtain comparable results with DeiT models trained with 300 epochs. 1× means 100 epochs. × 3. The same as I2T, the presence of Batch-Norm layers following transformation layers significantly improves the performance.Effectiveness of LCA. We compare the performances w/wo the LCA module. Through adopting LCA, the performance improves from 72.2% to 72.8%, showing multi-level information contributes to the final image representation.</figDesc><table><row><cell>3×</cell><cell>Top-1</cell><cell>1×</cell><cell>Top-1</cell><cell>1×</cell><cell>Top-1</cell></row><row><cell>DeiT-T</cell><cell>72.2</cell><cell>DeiT-T</cell><cell>65.3</cell><cell>CeiT-T</cell><cell>72.2 (+6.9)</cell></row><row><cell>DeiT-S</cell><cell>79.9</cell><cell>DeiT-S</cell><cell>74.5</cell><cell>CeiT-S</cell><cell>78.9 (+4.4)</cell></row><row><cell>DeiT-B</cell><cell>81.8</cell><cell>DeiT-B</cell><cell>76.8</cell><cell cols="2">CeiT-B 81.8 (+5.0)</cell></row><row><cell>size of 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3285" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2012.00364</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aˆ2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8126" to="8135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The inaturalist challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1707.06642</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3463" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11206</biblScope>
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-complexity image denoising based on statistical modeling of wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mehmet Kivanç Mihçak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Kozintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="300" to="303" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Grafit: Learning finegrained image representations with coarse labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5790" to="5799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (16)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12361</biblScope>
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10073" to="10082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="270" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2012.15840, 2020. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
