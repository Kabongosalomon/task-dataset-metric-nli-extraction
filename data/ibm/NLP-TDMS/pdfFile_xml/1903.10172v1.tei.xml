<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
							<email>masonliuw@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
							<email>menglong@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>White</surname></persName>
							<email>mariewhite@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
							<email>yinxiao@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
							<email>dkalenichenko@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the "gist" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone. 1 * This work was done while interning at Google.</p><p>1 Source code will be released under <ref type="bibr" target="#b22">[23]</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in image object detection have followed a trend of increasingly elaborate convolutional neural network <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9]</ref> designs to improve either accuracy or speed. Though accuracy was initially the primary concern and continues to be a key metric <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, the importance of improving the speed of these models has steadily risen as deep learning techniques have been increasingly deployed in practical applications. On the far end of the speed spectrum, substantial work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref> has been done on allowing neural networks to run on mobile devices, which represent an environment with extreme computation and energy constraints. Despite significant advances, the ultimate goal of being able to run neural networks in real-time on mobile devices without substantial accuracy loss has yet to be achieved by any single-frame detection model. However, the human visual system provides intuition that such a result should be achievable, as experiments have shown that humans can process basic information about a scene at a single glance, which notably includes recognizing a few categories of objects <ref type="bibr" target="#b26">[27]</ref>. One critical advantage of human vision is that it does not operate on single images, but rather a stream of images. By introducing a temporal dimension to the problem, humans can rely on contextual cues and memory to supplement their understanding of the image. This work examines the question of whether neural networks are similarly able to perform video object detection with very little computation when assisted by memory.</p><p>A key observation is that since adjacent video frames tend to be similar, running a single feature extractor on multiple frames is likely to result in mostly redundant computation. Therefore, a simple idea is to keep a memory of previously computed features and only extract a small amount of necessary features from new frames. This par-allels the role of gist in the human visual system in that both require minimal computation and rely on memory to be effective. To follow this idea, a system requires multiple feature extractors-an accurate extractor initializes and maintains the memory, while the other rapidly extracts features representing the gist of new images.</p><p>From these observations, we propose a simple yet effective pipeline for efficient video object detection, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, we introduce a novel interleaved framework where two feature extractors with drastically different speeds and recognition capacities are run on different frames. The features from these extractors are used to maintain a common visual memory of the scene in the form of a convolutional LSTM (ConvLSTM) layer, and detections are generated by fusing context from previous frames with the gist from the current frame. Furthermore, we show that the combination of memory and gist contains within itself the information necessary to decide when the memory must be updated. We learn an interleaving policy of when to run each feature extractor by formulating the task as a reinforcement learning problem.</p><p>While prior works, notably flow-based methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39]</ref>, also provide approaches for fast video object detection based on interleaving fast and slow networks, these approaches are based on the CNN-specific observation that intermediate features can be warped by optical flow. Meanwhile, our method relies on the biological intuition that fast, memory-guided feature extractors exist in the human visual system. This intuition translates naturally into a simple framework which is not dependent on optical flow. Our method runs at an unprecedented 72.3 FPS postoptimization on a Pixel 3 phone, while matching stateof-the-art mobile performance on the Imagenet VID 2015 benchmark.</p><p>In summary, this paper's contributions are as follows:</p><p>• We present a memory-guided interleaving framework where multiple feature extractors are run on different frames to reduce redundant computation, and their outputs are fused using a common memory module.</p><p>• We introduce an adaptive interleaving policy where the order to execute the feature extractors is learned using Q-learning, which leads to a superior speed/accuracy trade-off.</p><p>• We demonstrate on-device the fastest mobile video detection model known to date at high accuracy level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video object detection has received much attention in recent years. Current methods focus on extending singleimage detection approaches by leveraging the temporal properties of videos to improve accuracy and speed. These approaches can be roughly categorized into three families.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Postprocessing Methods</head><p>Initial work for extending single-image detection to the video domain usually centered on a postprocessing step where per-frame detections are linked together to form tracks, and detection confidences are modified based on other detections in the track. Seq-nms <ref type="bibr" target="#b6">[7]</ref> finds tracks via dynamic programming and boosts the confidence of weaker predictions. TCNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> provides a pipeline with optical flow to propagate detections across frames and a tracking algorithm to find tubelets for rescoring. These early approaches yielded sizeable performance improvements, but did not fundamentally change the underlying per-frame detection process, which limited their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Flow Methods</head><p>Later, Zhu et al. <ref type="bibr" target="#b40">[41]</ref> discovered that intermediate features in a convolutional neural network could be directly propagated between video frames via optical flow. The DFF framework <ref type="bibr" target="#b40">[41]</ref> demonstrated that it is sufficient to compute detections on sparse keyframes and perform feature propagation on all other frames by computing optical flow, which is substantially cheaper. FGFA <ref type="bibr" target="#b39">[40]</ref> showed that this idea can also be used to improve accuracy if perframe detections are densely computed and features from neighboring frames are warped to the current frame and aggregated with weighted averaging. Impression networks <ref type="bibr" target="#b9">[10]</ref> balance speed and accuracy by using sparse keyframes but retaining an "impression feature" which is aggregated across keyframes and stores long-term temporal information. Further work by Zhu et al. <ref type="bibr" target="#b37">[38]</ref> introduces efficient feature aggregation as well as a measure of feature quality after warping, which is used to improve keyframe selection and sparsely replace poorly warped features.</p><p>This paradigm has also been applied to mobile-focused video object detection, which is particularly relevant to this paper. In <ref type="bibr" target="#b38">[39]</ref>, flow-guided feature propagation is used on a GRU module with very efficient feature extractor and flow networks to demonstrate that flow-based methods are viable in computationally constrained environments. Our work also applies to the mobile setting, but by interleaving specialized feature extractors rather than using flow to propagate features, we remove the dependence on optical flow and hence the need for optical flow training data and the additional optical flow pre-training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-frame Methods</head><p>A third class of video object detection methods involve methods which explicitly process multiple frames of the video simultaneously. D&amp;T <ref type="bibr" target="#b4">[5]</ref> combines detection and tracking by adding an RoI tracking operation and loss on pairs of frames, while STSN <ref type="bibr" target="#b1">[2]</ref> uses deformable convolutions to sample features from adjacent frames. Chen et al. <ref type="bibr" target="#b2">[3]</ref> propose using a scale-time lattice to generate detections in a coarse-to-fine manner. Though D&amp;T and Chen et al.'s methods can improve detection speed by sampling sparse keyframes and propagating results to intermediate frames, all of these works are still focused on high-accuracy detection and are nontrivial to generalize to a mobile setting. Our approach also extracts features from each frame rather than entirely propagating results from keyframes, which allows access to a greater quantity of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Adaptive Keyframe Selection</head><p>There have been a variety of methods to select keyframes when sparsely processing videos. These methods range from fixed intervals <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39]</ref> to heuristics <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17]</ref> to learned policies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36]</ref>. Most of these works address the problems of semantic segmentation and tracking, and adaptive keyframe selection has been less explored in video object detection. We propose a different formulation for constructing a learned adaptive policy by leveraging the information contained in our memory module, creating a complete and principled detection pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interleaved Models</head><p>Our paper addresses the task of video object detection. For this task, we must generate frame-level bounding boxes and class predictions on each frame of a video V = {I 0 , I 1 , . . . I n }. We further restrict our task to an online setting where only {I 0 , I 1 , . . . I k } are available when generating detections for the k-th frame.</p><p>The primary contribution of this paper is an interleaved model framework where multiple feature extractors are run sequentially or concurrently. These frame-level features are then aggregated and refined using a memory mechanism. Finally, we apply SSD-style <ref type="bibr" target="#b23">[24]</ref> detection on the refined features to produce bounding box results.</p><p>Each of the these steps can be defined as a function. Let the m feature extractors be f i :</p><formula xml:id="formula_0">R I → R F | m i=0</formula><p>, mapping the image space to separate feature spaces in R F . The memory module m : R F × R S → R R × R S , maps features from f and an internal state representation to a common, refined feature space while also outputting an updated state. The SSD detector d : R R → R D maps refined features to final detection anchor predictions.</p><p>The use of multiple feature extractors has several advantages. Different feature extractors can specialize on different image features, creating a temporal ensembling effect. Since we are focused on the mobile setting, we study the case where the features extractors have drastically different computational costs, which dramatically decreases the runtime of the model. In particular, the remainder of our paper focuses on the case where m = 2, with f 0 optimized for accuracy and f 1 optimized for speed.</p><p>To obtain detection results D k on the k-th frame given the previous frame's state s k−1 , run m(f i (I k ), s k−1 ) to obtain a feature map M k and the updated state s k . Then,</p><formula xml:id="formula_1">D k = d(M k )</formula><p>. This is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Note that running any feature extractor (i.e. choosing any valid i) will yield valid detection results, but the quality of the detections and the updated state representation will vary. One crucial problem is finding an interleaving policy such that the amortized runtime of our method is similar to f 1 while retaining the accuracy of exclusively running f 0 . A simple fixed interleaving policy involves defining a hyperparameter τ , the interleave ratio, and running f 0 after f 1 is run τ times. Though we find that even this simple policy achieves competitive results, we also present a more advanced learned policy in section 3.4.</p><p>The architecture of f 0 is a standard MobileNetV2 [30] with a depth multiplier of 1.4 and an input resolution of 320 × 320. f 1 also uses a MobileNetV2 architecture with a depth multiplier of 0.35 and a reduced input resolution of 160 × 160. We remove the striding on the last strided convolution so that the output dimensions match. The SS-DLite layers are similar to <ref type="bibr" target="#b29">[30]</ref>, with the difference that the SSD feature maps have a constant channel depth of 256 and share the same convolutional box predictor. We also limit the aspect ratios of the anchors to {1, 0.5, 2.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Module</head><p>Our method requires a memory module for aggregating features from the two extractors across timesteps, especially to augment features from the small network with previous temporal context. Though Liu and Zhu <ref type="bibr" target="#b21">[22]</ref> showed that LSTMs can be used to propagate temporal information for object detection, our memory module serves an additional purpose of fusing features from different feature extractors and feature spaces, posing an additional challenge. Moreover, we require this mechanism to be extremely fast, as it must be executed on all frames. To this end, we modify an LSTM cell to be faster and better at preserving long-term dependencies. To improve the speed of the standard LSTM, we make three modifications. We adopt the bottlenecking proposed in <ref type="bibr" target="#b21">[22]</ref> and add a skip connection between the bottleneck and output so that the bottleneck is part of the output. We also divide the LSTM state into groups and use grouped convolutions to process each one separately. Given the previous state h t−1 and input feature map x t , we partition the state channel-wise into G equal partitions 1 h t−1 , 2 h t−1 . . . G h t−1 . We concatenate each partition with x t and compute bottlenecked LSTM gates b t , f t , i t , o t as in <ref type="bibr" target="#b21">[22]</ref>. The updated LSTM states g c t , g h t are also computed the same way, but are now just slices of the final updated state. We generate the output slice with a skip connection:</p><formula xml:id="formula_2">g M t = [ g h t , b t ],</formula><p>where brackets denote concatenation. Finally, concatenate the slices channel-wise to obtain c t , h t , and M t . This is visualized in <ref type="figure" target="#fig_2">Figure 3</ref>. The grouped convolutions provide a speed-up by sparsifying layer connections, while the skip connection allows less temporally relevant features to be included in the output without being stored in memory. This reduces the burden on the memory and allows us to scale down the state dimensions, whereas other LSTM variants suffer substantial accuracy drops if the state is too small <ref type="bibr" target="#b21">[22]</ref>. In our model, we choose G = 4 and use a 320-channel state. The speed benefits of our modified LSTM are detailed in <ref type="table">Table 1</ref>.</p><p>We also observe that one inherent weakness of the LSTM is its inability to completely preserve its state across updates in practice. The sigmoid activations of the input and forget gates rarely saturate completely, resulting in a slow state decay where long-term dependencies are gradually lost. When  <ref type="table">Table 1</ref>: Performance of a MobilenetV2-SSDLite with a recurrent layer as in <ref type="bibr" target="#b21">[22]</ref>. Each RNN variant has a 1024channel input and 640-channel output. mAP@0.5IOU is reported on a subset of Imagenet VID validation containing a random sequence of 20 frames from each video. MAC contains only multiply-adds from the RNN.</p><p>compounded over many steps, predictions using the f 1 degrade unless f 0 is rerun. We propose a simple solution to this problem by simply skipping state updates when f 1 is run, i.e. the output state from the last time f 0 was run is always reused. This greatly improves the LSTM's ability to propagate temporal information across long sequences, resulting in minimal loss of accuracy even when f 1 is exclusively run for tens of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Procedure</head><p>Our training procedure consists of two phases. First, we pretrain our interleaved model without detection layers on Imagenet classification in order to obtain a good initialization of our LSTM weights. To adapt the network for classification, we remove the detection layers d and add one average pooling and fully connected layer immediately after the LSTM, followed by a softmax classifier. During training, we duplicate each frame three times and unroll the LSTM to three steps. At each step, we uniformly select a random feature extractor to run.</p><p>Next, we perform SSD detection training. Again, we unroll the LSTM to six steps and uniformly select a random feature extractor at each step. We train on a mix of video and image data. For image data, we augment the image by cropping a specific region at each step and shifting the crop between steps to mimic translations and zooms, in order to aid the model in learning the relation between motion and box displacement. Otherwise, the training procedure is similar to that of the standard SSD <ref type="bibr" target="#b23">[24]</ref>. We use a batch size of 12 and a learning rate of 0.002 with cosine decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptive Interleaving Policy</head><p>Though we show that a simple interleaving policy already achieves competitive results, a natural question is whether it is possible to optimize the interleaving policy to further improve results. We propose a novel approach for learning an adaptive interleaving policy using reinforcement learning. The key observation is that in order to effectively aid the smaller network, the memory module must contain some measure of detection confidence, which we can leverage as part of our interleaving policy. Therefore, we construct a policy network π which examines the LSTM state and outputs the next feature extractor to run, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Then, we train the policy network using Double Q-learning (DDQN) <ref type="bibr" target="#b33">[34]</ref>.</p><p>To formulate a reinforcement learning problem, it is necessary to define an action space, a state space, and a reward function. The action space consists of m actions, where action a corresponds to running f a at the next timestep. We denote the state as:</p><formula xml:id="formula_3">S = (c t , h t , c t − c t−1 , h t − h t−1 , η t )<label>(1)</label></formula><p>which includes the current LSTM states c t and h t , as well as their changes during the current step (c t − c t−1 ) and (h t − h t−1 ). We also add an action history term η, so the policy network is aware of its previous actions and can avoid running f 0 excessively. The action history is a binary vector of length 20. For all k, the k-th entry of η is 1 if f 1 was run k steps ago and 0 otherwise. Our reward function must reflect our aim to find a balance between running f 1 as frequently as possible while maintaining accuracy. Therefore, we define the reward as the sum of a speed reward and an accuracy reward. For the speed reward, we simply define a positive constant γ and give γ reward when f 1 is run. For the accuracy reward, we compute the detection losses after running each feature extractor and take the loss difference between the minimum-loss feature extractor and the selected feature extractor. The final reward can be expressed as:</p><formula xml:id="formula_4">R(a) =    min i L(D i ) − L(D 0 ) a = 0 γ + min i L(D i ) − L(D 1 ) a = 1 ,<label>(2)</label></formula><p>where L(D i ) denotes the loss for detections D i using features from f i . Our policy network is a lightweight convolutional neural network which predicts the Q-value of each state-action pair given the state. We first perform a grouped convolution using each of the four feature maps in S as separate groups. Then, we perform a depthwise separable convolution and use max pooling to remove the spatial dimensions. Finally, we concatenate the action feature vector and apply a fully connected layer to obtain m outputs, the Q-values for each state-action pair. The architecture is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>To train the policy network, we generate batches of (S t , a, S t+1 , R t ) examples by running the interleaved network in inference mode. Though the entire system could potentially be trained end-to-end, we simplify the training process by using pretrained weights for the base interleaved model and freezing all weights outside of the policy network. After obtaining the batched examples, we use standard DDQN with experience replay as described in <ref type="bibr" target="#b33">[34]</ref>. The training process is detailed in Algorithm 1. for every feature extractor f i do 6:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Adaptive Interleaving Training</head><formula xml:id="formula_5">M i t , s i t ← m(f i (I t ), s t−1 ) 7:</formula><p>if i = a t then 8:</p><p>s t ← s i t 9:</p><formula xml:id="formula_6">D i t ← d(M i t ) 10:</formula><p>Construct S t from s t and η t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>With probability , select a random action a t+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>otherwise a t+1 ← arg max π(S t )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Construct η t+1 with a t+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Compute R t (a t ) <ref type="bibr">15:</ref> Add (S t−1 , a t , S t , R t ) to replay buffer <ref type="bibr">16:</ref> t ← t + 1 <ref type="bibr">17:</ref> Sample batch B from replay buffer <ref type="bibr">18:</ref> π ← DDQN(B) Update policy using Double Q-learning <ref type="bibr">19:</ref> until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference Optimizations</head><p>We explore two additional optimizations geared towards practical usage which triple the frame rate of our method while preserving accuracy and ease of deployment.</p><p>Asynchronous Inference One problem with keyframebased detection methods is that they only consider amortized runtime. However, since these methods perform the bulk of their computation on keyframes, the latency across frames is extremely inconsistent. When worst-case runtime on a single frame is considered, these methods are no faster than single-frame methods, limiting their applicability in practical settings. Li et al. <ref type="bibr" target="#b16">[17]</ref> address this problem in semantic video segmentation by running networks in parallel. Likewise, the interleaved framework naturally suggests an asynchronous inference approach which removes the gap between the amortized and worst-cases runtimes, allowing our method to run smoothly in real-time on a mobile device.</p><p>When running an interleaved model synchronously, one feature extractor is run at each timestep, so the maximum potential latency depends on f 0 . However, this process is easily parallelizable by running feature extractors in separate threads, which we term asynchronous mode. In asynchronous mode, f 1 is run at each step and exclusively used to generate detections, while f 0 continues to be run every τ frames and updates the memory when it completes. The lightweight feature extractor uses the most recent available memory at each step and no longer has to wait for the larger feature extractor to run. This is illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>Quantization Another benefit of relying on multiple feature extractors instead of optical flow is that standard inference optimization methods can be applied with minimal changes. In particular, we demonstrate that our interleaved framework can be quantized using the simulatedquantization training procedure in <ref type="bibr" target="#b12">[13]</ref>. The Tensorflow <ref type="bibr" target="#b0">[1]</ref> quantization library is used out-of-the-box for the Mo-bileNet and SSDLite layers. For the LSTM, fake quantization operations are inserted after all mathematical operations (addition, multiplication, sigmoid and ReLU6), following Algorithm 1 in <ref type="bibr" target="#b12">[13]</ref>. Ranges after activations are fixed to [0, 1] for sigmoid and [0, 6] for ReLU6 to ensure that zero is exactly representable. We also ensure that the ranges of all inputs for concatenation operations are the same to remove the need for rescaling (as described in A.3. <ref type="bibr" target="#b12">[13]</ref>). Our final quantized asynchronous model runs at 72.3 FPS on a Pixel 3 phone, over three times the frame rate of our unoptimized model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present results on the Imagenet VID 2015 dataset <ref type="bibr" target="#b28">[29]</ref>, which includes 30 object classes. For training, we use Imagenet VID training data and relevant classes from the Imagenet DET <ref type="bibr" target="#b28">[29]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> training sets, amounting to 3862 videos, 147K images from Imagenet DET, and 43K images from COCO. We also provide results without COCO data as <ref type="bibr" target="#b38">[39]</ref> does not include it, though they train on additional optical flow data. For evaluation, we use the 555 videos in the Imagenet VID validation set. <ref type="table" target="#tab_2">Table 2</ref> contains comparisons of our results with singleframe baselines, LSTM-based methods, and the state-ofthe-art mobile video object detection method by Zhu et al. <ref type="bibr" target="#b38">[39]</ref>. We retrain the LSTM-based models of <ref type="bibr" target="#b21">[22]</ref> using publicly available code <ref type="bibr" target="#b22">[23]</ref> on our combined dataset, including COCO data. We report accuracy in terms of mAP @0.5IOU, theoretical complexity in the form of multiplyadd count (MAC), and practical runtime by deploying our model in a Pixel 3 phone using Tensorflow Lite <ref type="bibr" target="#b0">[1]</ref>. Our method matches the accuracy of the most accurate flowbased mobile models with COCO data and achieves comparable accuracy even without COCO data. None of our models require any optical flow data, whereas <ref type="bibr" target="#b38">[39]</ref> also uses the Flying Chairs dataset with 22K examples during training. Our method also has comparable theoretical complexity, but contains far fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Imagenet VID</head><p>Our adaptive model is discussed in detail in Section 4.3, but we include one variant (γ = 1) which successfully reduces the frequency f 0 is run while maintaining accuracy, though it is less amenable to quantization. Our inference optimizations triple the frame rate at a small cost of accuracy, allowing our method to run much faster in practice. Though the runtimes for <ref type="bibr" target="#b38">[39]</ref> are measured using a different phone and not directly comparable, it is safe to say that our inference-optimized method provides unprecedented realtime runtime on mobile devices.</p><p>We also include results for only running the large and small models in our interleaved framework (i.e. τ = 0 and τ = ∞). Our results show that both feature extractors are able to perform detection individually, but f 1 performs extremely poorly without temporal context supplied by f 0 . This demonstrates the necessity of fusing features from both models to create a fast and accurate detector, and the effectiveness of our memory-guided framework in doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speed/Accuracy Tradeoffs</head><p>Our method provides a simple way to trade off between speed and accuracy by varying the interleave ratio τ . Notably, several other works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> have observed that it is possible to sparsely pro-   cess frames on Imagenet VID using a variety of methods, suggesting that interleaving at moderate ratios is not very difficult. However, our method incurs less accuracy degradation even at extreme interleave ratios, suggesting that our memory-guided detection approach is superior at capturing the long-term temporal dynamics inherent to videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Reinforcement Learning</head><p>We present results for a variety of learned interleaving policies in <ref type="figure" target="#fig_8">Figure 7</ref>. We are able to vary how often f 0 is run by adjusting the speed reward γ, with γ ∈ {1.5, 1.0, 0.4, 0.3, 0.2, 0.1}. The adaptive policy is superior to the fixed policy at all percentages, especially when more large models can be run. This improvement comes at a negligible cost of 89.6K extra parameters and 1.76M multiply-adds from the policy network. <ref type="figure">Figure 8</ref> shows the change in predicted Q-values, mAP and percentage of f 0 run during training. Notably, the mAP reaches a constant value, while Q-values steadily increase and interleave ratio (percentage) decreases. This observation suggests the policy network gradually learns how to use the small model without hurting the overall accuracy. <ref type="figure" target="#fig_10">Figure 9</ref> visualizes the adaptive policy on the Imagenet VID validation set, ordered by how often the large model is run. The learned policy tends to expend more computation when objects are difficult to detect while running the small network frequently on easy scenes, demonstrating the merits of this approach over a fixed policy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a method for video object detection by interleaving multiple feature extractors and aggregating their results in memory. By interleaving extremely lightweight and conventional feature extractors, we construct a model optimized for computationally constrained environments. The presence of the memory module allows the lightweight fea-ture extractor, which performs poorly by itself, to be run frequently with minimal accuracy loss. We also propose a method for learning the interleaving policy using reinforcement learning. We demonstrate that our method is competitive with the state-of-the-art for mobile video object detection while enjoying a substantial speed advantage and removing the dependency on optical flow, making it effective and straightforward to deploy in a mobile setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of our proposed memory-guided interleaved model. Given a video stream, a visual memory module fuses very different visual features produced by fast and slow feature extractors across frames to generate detections in an online fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the proposed interleaved model with heavy and lightweight feature extractors using a fixed interleave policy with τ = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Detailed illustration of one group of our speedoptimized LSTM cell. The illustrated operations are performed once for each of G groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Our adaptive interleaved model uses an ultralightweight policy network to decide which feature extractor to run at each time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 :</head><label>4</label><figDesc>Define: action a, timestep t, action history η, LSTM state s, observation S, reward R, policy π 1: repeat 2: sample video frames I 1 , . . . I k 3:a 0 ← 0; t ← 0; η 0 ← 0 while t &lt; k do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of asynchronous mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 6 plots our model's accuracy compared to the accuracy of Zhu et al.'s method using different keyframe durations. Between all interleaved models from τ = 1 to τ = 39, a drop of 3.75 mAP is observed. Meanwhile, the flow-based method suffers a drop of at least 4.5 mAP after only increasing the keyframe duration to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Speed/Accuracy trade-off comparison between flow-guided and our memory-guided approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Speed/Accuracy trade-off comparison between fixed and adaptive interleaving policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>of f 0 Figure 8 :</head><label>08</label><figDesc>Q-values, mAP and percentage of f 0 run during RL training. The blue curves correspond to training and the red curves evaluation. The x-axis is the number of training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of a learned adaptive policy for feature extractor selection on Imagenet VID validation (best viewed in color). Frames where the policy runs the large model are highlighted in red. Clips are ordered by how often the large model triggers. The scene complexity increases correspondingly, which shows that the policy allocates computation intelligently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the Imagenet VID validation set. All of our non-adaptive models use a fixed interleave policy with τ = 9. α is the feature extractor width multiplier described in<ref type="bibr" target="#b10">[11]</ref>, while β is the flow network width multiplier.*Runtime results of Zhu et al. are reported with HuaWei Mate 8 phone, while the rest are reported on a Pixel 3 phone. † The effective MAC for asynchronous inference (84) for each frame includes only f 1 plus the LSTM and SSDLite detection layers, while 190 is the amortized MAC including f 0 .</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Impression network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02532</idno>
		<title level="m">Tubelets with convolutional neural networks for object detection from videos</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tensorflow mobile video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models/tree/master/research/lstmobjectdetection" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Budget-aware deep semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gist of the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurobiology of attention</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tracking as online decision-making: Learning a policy from streaming videos with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-adaptive computational policies for efficient visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMMCVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards high performance video object detection for mobiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05830</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
