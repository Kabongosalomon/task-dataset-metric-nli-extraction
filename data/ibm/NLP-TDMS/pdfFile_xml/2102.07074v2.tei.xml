<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransGAN: Two Transformers Can Make One Strong GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Uni-versity of Texas at Austin</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
								<address>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Uni-versity of Texas at Austin</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransGAN: Two Transformers Can Make One Strong GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Correspondence to: Yifan Jiang &lt;yi-fanjiang97@utexas.edu&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent explosive interest on transformers has suggested their potential to become powerful "universal" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go -are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformerbased. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets new state-of-the-art IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.63 IS score and 11.89 FID score on CIFAR-10, and 12.23 FID score on CelebA 64 × 64, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at https:// github.com/VITA-Group/TransGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) have gained considerable success on numerous tasks including image synthesis <ref type="bibr" target="#b49">(Radford et al., 2015;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b23">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b45">Miyato et al., 2018;</ref><ref type="bibr" target="#b3">Brock et al., 2018)</ref>, image translation <ref type="bibr" target="#b28">(Isola et al., 2017;</ref><ref type="bibr" target="#b73">Zhu et al., 2017a;</ref><ref type="bibr" target="#b6">Chen et al., 2020b)</ref>, and image editing <ref type="bibr" target="#b66">(Yang et al., 2019;</ref><ref type="bibr" target="#b29">Jiang et al., 2021)</ref>. Unfortunately, GANs suffer from the notorious training instability, and numerous efforts have been devoted to stabilizing GAN training, introducing various regularization terms <ref type="bibr" target="#b37">(Kurach et al., 2019;</ref><ref type="bibr" target="#b50">Roth et al., 2017;</ref><ref type="bibr" target="#b70">Zhang et al., 2019b;</ref><ref type="bibr" target="#b44">Mescheder et al., 2018)</ref>, better losses <ref type="bibr" target="#b23">(Gulrajani et al., 2017;</ref><ref type="bibr" target="#b43">Mao et al., 2017;</ref><ref type="bibr" target="#b30">Jolicoeur-Martineau, 2018;</ref><ref type="bibr" target="#b39">Li et al., 2017)</ref>, and training recipes <ref type="bibr" target="#b51">(Salimans et al., 2016;</ref><ref type="bibr" target="#b31">Karras et al., 2017)</ref>.</p><p>Another parallel route to improving GANs examines their neural architectures. <ref type="bibr" target="#b42">(Lucic et al., 2018;</ref><ref type="bibr" target="#b37">Kurach et al., 2019)</ref> reported a large-scale study of GANs and observed that when serving as (generator) backbones, popular neural architectures perform comparably well across the considered datasets. Their ablation study suggested that most of the variations applied in the ResNet family architectures lead to marginal improvements in the sample quality. However, further research introduced neural architecture search (NAS) to GANs and suggests that enhanced backbone designs are also important for improving GANs further, just like for other computer vision tasks. Those works are consistently able to discover stronger GAN architectures beyond the standard ResNet topology <ref type="bibr" target="#b19">(Gong et al., 2019;</ref><ref type="bibr" target="#b17">Gao et al., 2020;</ref><ref type="bibr" target="#b54">Tian et al., 2020)</ref>. Other efforts include customized modules such as self-attention <ref type="bibr" target="#b69">(Zhang et al., 2019a)</ref>, stylebased generator <ref type="bibr" target="#b32">(Karras et al., 2019)</ref>, and autoregressive transformer-based part composition <ref type="bibr" target="#b16">(Esser et al., 2020)</ref>.</p><p>However, there is one last "commonsense" that seems to have seldomly been challenged: using convolutional neural networks (CNNs) as GAN backbones. The original GAN <ref type="bibr" target="#b20">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b13">Denton et al., 2015)</ref> used fullyconnected networks and can only generate small images. DCGAN <ref type="bibr" target="#b49">(Radford et al., 2015)</ref> was the first to scale up GANs using CNN architectures, which allowed for stable training for higher resolution and deeper generative models. Since then, in the computer vision domain, nearly every successful GAN relies on CNN-based generators and dis-arXiv:2102.07074v2 [cs.CV] 16 Feb 2021 criminators. Convolutions, with the strong inductive bias for natural images, crucially contribute to the appealing visual results and rich diversity achieved by modern GANs.</p><p>Can we build a strong GAN completely free of convolutions? This is a question not only arising from intellectual curiosity, but also of practical relevance. Fundamentally, a convolution operator has a local receptive field, and hence CNNs cannot process long-range dependencies unless passing through a sufficient number of layers. However, that could cause the loss of feature resolution and fine details, in addition to the difficulty of optimization. Vanilla CNNbased models (including conventional GANs) are therefore inherently not well suited for capturing an input image's "global" statistics, as demonstrated by the benefits from adopting self-attention <ref type="bibr" target="#b69">(Zhang et al., 2019a)</ref> and non-local <ref type="bibr" target="#b61">(Wang et al., 2018b)</ref> operations in computer vision.</p><p>We are inspired by the emerging trend of using Transformer architectures for computer vision tasks <ref type="bibr" target="#b4">(Carion et al., 2020;</ref><ref type="bibr" target="#b68">Zeng et al., 2020;</ref><ref type="bibr" target="#b15">Dosovitskiy et al., 2020)</ref>. Transformers <ref type="bibr" target="#b58">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b14">Devlin et al., 2018)</ref> have prevailed in natural language processing (NLP), and lately, start to perform comparably or even better than their CNN competitors in a variety of vision benchmarks. The charm of the transformer to computer vision lies in at least two-fold:</p><p>(1) it has strong representation capability and is free of human-defined inductive bias. In comparison, CNNs exhibit a strong bias towards feature locality, as well as spatial invariance due to sharing filter weights across all locations;</p><p>(2) the transformer architecture is general, conceptually simple, and has the potential to become a powerful "universal" model across tasks and domains <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>. It can get rid of many ad-hoc building blocks commonly seen in CNN-based pipelines <ref type="bibr" target="#b4">(Carion et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Our Contributions</head><p>This paper aims for the first pilot study to build a GAN completely free of convolutions, using only pure transformerbased architecture. Our ambitious goal is clearly distinguished from the previous works that only applied selfattention or transformer encoder block in conjunction with CNN-based generative models <ref type="bibr" target="#b69">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b16">Esser et al., 2020)</ref>. However, as all previous pure transformerbased models in computer vision are focused on discriminative tasks such as classification and detection, our goal faces several daunting gaps ahead. First and foremost, although a pure transformer architecture applied directly to sequences of image patches can perform very well on image classification tasks <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>, it is unclear whether the same way remains effective in generating images, which poses a high demand for spatial coherency in structure, color, and texture. The handful of existing transformers that output images have unanimously leveraged CNN-based part encoders <ref type="bibr" target="#b16">(Esser et al., 2020)</ref> or convolutional feature extractors <ref type="bibr" target="#b5">Chen et al., 2020a)</ref>. Moreover, even given well-designed CNN-based architectures, training GANs is notoriously unstable and prone to mode collapse <ref type="bibr" target="#b51">(Salimans et al., 2016)</ref>. Training visual transformers are also known to be tedious, heavy, and data-hungry <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>. Mingling the two will undoubtedly amplify the challenges of training.</p><p>In view of those challenges, this paper presents a coherent set of efforts and innovations towards building the pure transformer-based GAN architectures, dubbed TransGAN. A naive option may directly stack multiple transformer blocks from raw pixel inputs, but that would require prohibitive memory and computation. Instead, we start with a memory-friendly transformer-based generator by gradually increasing the feature map resolution while decreasing the embedding dimension in each stage. The discriminator, also transformer-based, tokenizes the image patches rather than pixels as its inputs and classifies between real and fake images. This vanilla TransGAN architecture naturally inherits the advantages of the global receptive field by the self-attention, but practically it leads to degraded generation and broken visual smoothness. To close the performance gap between CNN-based GANs, we then demonstrate Trans-GAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy with a self-supervised auxiliary loss, and a locally initialized selfattention that emphasizes the neighborhood smoothness of natural images. Our contributions are outlined below:</p><p>• Model Architecture: We build the first GAN using purely transformers and no convolution. To avoid overwhelming memory overheads, we create a memoryfriendly generator and a patch-level discriminator, both transformer-based without bells and whistles. Trans-GAN can be effectively scaled up to larger models.</p><p>• Training Technique: We study a number of techniques to train TransGAN better, ranging from data augmentation, multi-task co-training for generator with self-supervised auxiliary loss, and localized initialization for self-attention. Extensive ablation studies, discussions, and insights are presented. None of them requires any architecture change.</p><p>• Performance: TransGAN achieves highly competitive performance compared to current state-of-the-art CNN-based GANs. Specifically, it sets new state-ofthe-art IS score of 10.10 and FID score of 25.32 on STL-10 and also reaches competitive 8.63 IS score and 11.89 FID score on CIFAR-10, and 12.23 FID score on CelebA 64 × 64, respectively. We also summarize the current limitations and future potential of TransGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relative Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative Adversarial Networks</head><p>GANs <ref type="bibr" target="#b22">(Gui et al., 2020)</ref> can be generalized to minimizing a large family of divergences, and are practically formulated as minimax optimization. After its origin, GANs quickly embrace fully convolutional backbones <ref type="bibr" target="#b49">(Radford et al., 2015)</ref>, and inherited most successful designs from CNNs such as batch normalization, pooling, ReLU/Leaky ReLU and more. GANs are widely adopted in image-toimage translation <ref type="bibr" target="#b28">(Isola et al., 2017;</ref><ref type="bibr" target="#b73">Zhu et al., 2017a)</ref>, image enhancement <ref type="bibr" target="#b29">(Jiang et al., 2021;</ref><ref type="bibr" target="#b38">Ledig et al., 2017;</ref><ref type="bibr" target="#b36">Kupyn et al., 2018)</ref>, and image editing <ref type="bibr" target="#b47">(Ouyang et al., 2018;</ref><ref type="bibr" target="#b67">Yu et al., 2018)</ref>. To alleviate its unstable training, a number of techniques have been studied, including the Wasserstein loss , the style-based generator <ref type="bibr" target="#b32">(Karras et al., 2019)</ref>, progressive training <ref type="bibr" target="#b31">(Karras et al., 2017)</ref>, and Spectual Normalization <ref type="bibr" target="#b45">(Miyato et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual Transformer</head><p>The original transformer was built for NLP <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>, where the multi-head self-attention and feed-forward MLP layer are stacked to capture the long-term correlation between words. Its popularity among computer vision tasks rises recently <ref type="bibr" target="#b48">(Parmar et al., 2018;</ref><ref type="bibr" target="#b64">Yang et al., 2020;</ref><ref type="bibr" target="#b68">Zeng et al., 2020;</ref><ref type="bibr" target="#b4">Carion et al., 2020;</ref><ref type="bibr" target="#b63">Wu et al., 2020;</ref><ref type="bibr" target="#b5">Chen et al., 2020a)</ref>. The core of a transformer is the self-attention mechanism, which characterizes the dependencies between any two distant tokens. It could be viewed as a special case of non-local operations in the embedded Gaussian <ref type="bibr" target="#b61">(Wang et al., 2018b)</ref>, that captures long-range dependencies of pixels in image/video. A recent work <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref> implements highly competitive ImageNet classification using pure transformers, by treating an image as a sequence of 16 × 16 visual words. However, its success relies on pretraining on large-scale external data. <ref type="bibr" target="#b55">(Touvron et al., 2020)</ref> improves the data efficiency for its training. Besides image classification task, transformer and its variants are also explored on image processing <ref type="bibr" target="#b5">(Chen et al., 2020a)</ref>, point cloud <ref type="bibr" target="#b71">(Zhao et al., 2020a)</ref>, object detection <ref type="bibr" target="#b4">(Carion et al., 2020;</ref><ref type="bibr">Zhu et al., 2020)</ref> and so on. A comprehensive review is referred to <ref type="bibr" target="#b24">(Han et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformer Modules for Image Generation</head><p>There exist several related works combining the transformer modules into image generation models, by replacing certain components of CNNs. <ref type="bibr" target="#b48">(Parmar et al., 2018)</ref> firstly formulated image generation as autoregressive sequence generation, for which they adopted a transformer architecture. <ref type="bibr" target="#b8">(Child et al., 2019)</ref> propose sparse factorization of the attention matrix to reduce its complexity. While those two works did not tackle the GANs, one recent (concurrent) work <ref type="bibr" target="#b16">(Esser et al., 2020</ref>) used a convolutional GAN to learn a codebook of context-rich visual parts, whose composition is subsequently modeled with an autoregressive transformer architecture. The authors demonstrated success in synthesizing high-resolution images. However, the overall CNN architecture remains in place (including CNN encoder/decoder for the generators, and a fully CNN-based discriminator), and the customized designs (e.g, codebook and quantization) also limit their model's versatility. To our best knowledge, no existing work has tried to completely remove convolutions from their generative frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach: A Journey Towards GAN with Pure Transformers</head><p>A GAN consists of a generator G and a discriminator D. We start by replacing G or D with a transformer to understand the design sensitivity; then we replace both of them and optimize our design for memory efficiency. On top of the vanilla TransGAN with both G and D being transformers, we gradually introduce a series of training techniques to fix its weakness, including data augmentation, an auxiliary task for co-training, and injecting locality to self-attention. With those aids, TransGAN can be scaled up to deeper/wider models, and generate images of high quality.</p><p>3.1. Vanilla Architecture Design for TransGAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">TRANSFORMER ENCODER AS BASIC BLOCK</head><p>We choose the transformer encoder <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref> as our basic block, and try to make minimum changes. An encoder is a composition of two parts. The first part is constructed by a multi-head self-attention module and the second part is a feed-forward MLP with GELU non-linearity. We apply layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> before both of the two parts. Both parts employ residual connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">MEMORY-FRIENDLY GENERATOR</head><p>Transformers in NLP taking each word as inputs <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref>. However, if we similarly generate an image in a pixel-by-pixel manner through stacking transformer encoders, even a low-resolution image (e.g. 32×32) can result in a long sequence (1024), and then even more explosive cost of self-attention (quadratic w.r.t. the sequence length).</p><p>To avoid this prohibitive cost, we are inspired by a common design philosophy in CNN-based GANs, to iteratively upscale the resolution at multiple stages <ref type="bibr" target="#b13">(Denton et al., 2015;</ref><ref type="bibr" target="#b31">Karras et al., 2017)</ref>. Our strategy is to gradually increase the input sequence and reduce the embedding dimension.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref> left, we propose a memory-friendly transformer-based generator that consists of multiple stages (default 3 for CIFAR-10). Each stage stacks several encoder blocks (5, 2, and 2 by default). By stages, we gradually  We show 9 patches for discriminator as an example while in practice we use 8 × 8 patches across all datasets.</p><p>increase the feature map resolution until it meets the target resolution H T × W T . Specifically, the generator takes the random noise as its input, and passes it through a multiplelayer perceptron (MLP) to a vector of length H × W × C. The vector is reshaped into a H × W resolution feature map (by default we use H = W = 8), each point a Cdimensional embedding. This "feature map" is next treated as a length-64 sequence of C-dimensional tokens, combined with the learnable positional encoding.</p><p>Similar to BERT <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref>, the transformer encoders take embedding tokens as inputs and calculate the correspondence between each token recursively. To synthesize higher resolution images, we insert an upsampling module after each stage, consisting of a reshaping and pixelshuffle <ref type="bibr" target="#b52">(Shi et al., 2016)</ref> module. The upsampling module firstly reshapes the 1D sequence of token embedding back to a 2D feature map X 0 ∈ R H×W ×C and then adopt the pixelshuffle module to upsample its resolution and downsample the embedding dimension, resulting in the output X 0 ∈ R 2H×2W ×C/4 . After that, the 2D feature map X 0 is again reshaped into the 1D sequence of embedding tokens where the token number becomes 4HW and the embedding dimension is C/4. Therefore, at each stage the resolution (H, W ) becomes 2 times larger, while the embedded dimension C is reduced to a quarter of the input. This trade-off mitigates the memory and computation explosion. We repeat multiple stages until the resolution reaches (H T , W T ), and then we will project the embedding dimension to 3 and obtain the RGB image Y ∈ R H T ×W T ×3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">TOKENIZED-INPUT FOR DISCRIMINATOR</head><p>Unlike the generator which needs to synthesize each pixel precisely, the discriminator is only expected to distinguish between real/fake images. This allows us to semantically tokenizing the input image into the coarser patch-level <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> right, the discriminator takes the patches of an image as inputs. Following <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>, we split the input images Y ∈ R H×W ×3 into 8 × 8 patches where each patch can be regarded as a "word". The 8 × 8 patches are then converted to the 1D sequence of token embeddings through a linear flatten layer, with token number N = 8 × 8 = 64 and embedding dimension equal to C. After that, the learnable positional encoding is added and a [cls] token is appended at the beginning of the 1D sequence. After passing through the transformer encoders, only [cls] token is taken by the classification head to output the real/fake prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">EVALUATION OF TRANSFORMER-BASED GAN</head><p>To put the performance of transformer-based generator G and discriminator D into context, we take a reference to one of the state-of-the-art GANs, AutoGAN <ref type="bibr" target="#b19">(Gong et al., 2019)</ref>, which has convolutional G and D. We study four combinations: i) AutoGAN G + AutoGAN D (i.e., original AutoGAN); ii) Transformer G + AutoGAN D; iii) Auto-GAN G + Transformer D; and iv) Transformer G + Transformer D (i.e., our vanilla TransGAN). Our transformer G has {5,2,2} encoder blocks in each stage and transformer D only has one stage with 7 encoder blocks. For all models, we train them on CIFAR-10 to evaluate Inception Score <ref type="bibr" target="#b51">(Salimans et al., 2016)</ref> and FID <ref type="bibr" target="#b26">(Heusel et al., 2017)</ref>. We try our best on tuning hyperparameters to reach their best performance, detailed setting is shown in the Appendix A.1. <ref type="table" target="#tab_1">Table 1</ref> reveals a few very intriguing findings:</p><p>• Transformer-based G has a strong capacity: when training with the mature AutoGAN D, its performances already get on par with the original AutoGAN. That is also aligned with <ref type="bibr" target="#b16">(Esser et al., 2020)</ref> who found putting transformers in the generator is successful.</p><p>• However, Transformer-based D seems to be an inferior "competitor" and unable to push AutoGAN G towards good generation results. After replacing AutoGAN G with Transformer G , the results slightly improves, possibly benefiting more symmetric G and D structures. However, the numbers still largely lag behind when using convolutional D.</p><p>Note that although not yet a pure transformer, the promising result of Transformer G + AutoGAN D already has practical relevance -considering in most GAN applications, the discriminator is discarded after training and only the generator is kept for testing use. If one's goal is to simply obtain a transformer-based G, then the goal can be fulfilled by Transformer G + AutoGAN D. However, for our much more ambitious goal of making GAN completely free of convolutions, our journey has to continue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augmentation is Crucial for TransGAN</head><p>The preliminary findings in <ref type="table" target="#tab_1">Table 1</ref> inspire us to reflect on the key barrier -it seems that D is not well trained, no matter with CNN-or transformer-based G. Note the transformer-based classifiers were known to be highly datahungry <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref> due to the removal of human-designed bias: they were inferior to CNNs until much larger external data was used for pre-training. To remove this roadblock, data augmentation was revealed as a blessing in <ref type="bibr" target="#b55">(Touvron et al., 2020)</ref>, which showed that different types of strong data augmentation can lead us to data-efficient training for visual transformers.</p><p>Traditionally, contrary to training image classifiers, training GANs hardly refers to data augmentation. Lately, there is an interest surge in training GANs in the "few-shot" regime, aiming to match state-of-the-art GAN results with orders of magnitude fewer real images, using well-crafted data augmentation <ref type="bibr" target="#b72">(Zhao et al., 2020b;</ref><ref type="bibr" target="#b33">Karras et al., 2020a)</ref>.</p><p>We target in a different setting: comparing the influence of data augmentation for CNN-and transformer-based GANs, in the full-data regime. We use the whole training set of CIFAR-10, and compare TransGAN with three state-of-theart CNN-based GANs: WGAN-GP <ref type="bibr" target="#b23">(Gulrajani et al., 2017)</ref>, AutoGAN and StyleGAN v2 <ref type="bibr" target="#b34">(Karras et al., 2020b)</ref>. The data augmentation method is DiffAug <ref type="bibr" target="#b72">(Zhao et al., 2020b)</ref>. As shown in <ref type="table" target="#tab_2">Table 2</ref>, for three CNN-based GANs, the performance gains of data augmentation seems to diminish in the full-data regime. Only the largest model StyleGAN-V2 seems to gain visibly in both IS and FID. In sharp contrast, TransGAN, also trained on the same training set, sees a shockingly large margin of improvement: IS improving from 6.95 to 8.15 and from 41.41 to 19.85, respectively. That re-confirms the findings <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b55">Touvron et al., 2020</ref>) that transformer-based architectures are much more data-hungry than CNNs, and that can be helped by stronger data augmentation to a good extent. Transformers in the NLP domain benefit from multiple pretraining tasks <ref type="bibr" target="#b14">(Devlin et al., 2018;</ref><ref type="bibr" target="#b53">Song et al., 2020)</ref>. Interestingly, adding a self-supervised auxiliary task (e.g., rotation prediction) was previously found to stabilize GAN training too . That makes it a natural idea to incorporate self-supervised auxiliary co-training into TransGAN, which may help it capture more image priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Co-Training with Self-Supervised Auxiliary Task</head><p>Specifically, we construct an auxiliary task of super resolution, in addition to the GAN loss. This task comes "for free", since we can just treat the available real images as highresolution, and downsample them to obtain low-resolution counterparts. The generator loss is added with a auxiliary term λ * L SR , where L SR is the mean-square-error (MSE) loss and the coefficient λ is empirically set as 50. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the idea of multi-task co-training (MT-CT) , and it improves TransGAN from 8.15 IS and 19.85 FID to 8.20 IS and 19.12 FID, respectively, as in <ref type="table" target="#tab_3">Table 3</ref>.  <ref type="bibr" target="#b57">(Ulyanov et al., 2018)</ref> which was believed to contribute to natural image generation. That was lacked by the transformer architecture which features full learning flexibility. However, <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref> observed that transformers still tend to learn convolutional structures from images. Therefore, a meaningful question arises as, whether we can efficiently encode inductive image biases while still retaining the flexibility of transformers. <ref type="bibr" target="#b16">(Esser et al., 2020)</ref> pursued so by keeping a convolutional architecture to encode the low-level image structure. In this paper, we show that a similar effect may be achieved without changing the pure transformer architecture at all, yet instead by warm-starting the self-attention properly.</p><p>To inject this particular prior, we introduce the localityaware initialization for self-attention. Our specific strategy is illustrated in <ref type="figure">Figure 3</ref>. We introduce a mask by which each query is only allowed to interact with its local neighbors that are not "masked". Different from previous methods <ref type="bibr" target="#b12">(Daras et al., 2020;</ref><ref type="bibr" target="#b48">Parmar et al., 2018;</ref><ref type="bibr" target="#b8">Child et al., 2019;</ref><ref type="bibr" target="#b2">Beltagy et al., 2020)</ref> during training we gradually reduce the mask until diminishing it, and eventually the self-attention is fully global 1 . That strategy stems from our observation that a localized self-attention <ref type="bibr" target="#b48">(Parmar et al., 2018)</ref> is most helpful at the early training stage, but can hurt the later training 1 Implementation-wise, we control the window size for the allowable interactive neighborhood tokens. The window size is 8 for epochs 0-20, then 10 for epochs 20-30, 12 for epochs 30-40, 14 for epochs 40-50, and then the full image all afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradually Increasing Receptive Field Early Stage</head><p>Middle Stage Final Stage <ref type="figure">Figure 3</ref>. Locality-aware initialization for self-attention. The red block indicates a query location, the transparent blocks are its allowable key locations to interact with, and the gray blocks indicate the masked region. TransGAN gradually increases the allowable region during the training process.</p><p>stage and the final achievable performance. We consider this locality-aware initialization as a regularizer that comes for the early training dynamics and then gradually fades away <ref type="bibr" target="#b18">(Golatkar et al., 2019)</ref>. It will enforce TransGAN to learn image generation, by prioritizing on the local neighborhood first (which provides the "necessary details"), followed by exploiting non-local interactions more broadly (which may supply more "finer detail" and also noise). <ref type="table" target="#tab_3">Table 3</ref> shows that it improves both IS and (more notably) FID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Scaling up to Large Models</head><p>All previous training techniques have contributed to a better and more stable TransGAN, consisting of only transformerbased G and D. Equipped with them all (DA, MT-CT, and Local Init.), we are now ready to scale TransGAN up and see how much further we could gain from bigger models. <ref type="table" target="#tab_4">Table 4</ref>, we firstly enlarge the embedded dimension of the transformer-based G, from (default) 384 to 512 and then 768 2 , and denote the resultant models as TransGAN-S, TransGAN-M and TransGAN-L, respectively. That comes with a consistent and remarkable gain in IS (up to 0.28), and especially FID (up to 4.08) -without any extra hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We then increase the depth (number of transformer encoder blocks) on top of TransGAN-L. The original transformer G has {5,2,2} encoder blocks in each stage. We increase the number of encoder blocks to {5,4,2} and the embedded dimension to 1024 as well, obtaining TransGAN-XL from TransGAN-L. Still, both IS and FID benefit, and FID is reduced by another nice margin of 2.57.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison with State-of-the-art GANs</head><p>Datasets and Implementation We adopt CIFAR-10 <ref type="bibr" target="#b35">(Krizhevsky et al., 2009</ref>) dataset as the main testbed during the ablation study. The CIFAR-10 dataset consists of 60k 32×32 color images in 10 classes, with 50k training and 10k testing images respectively. We follow the standard setting to use the 50k training images without labels for training the TransGAN. We further consider the STL-10 <ref type="bibr" target="#b10">(Coates et al., 2011)</ref> and CelebA <ref type="bibr" target="#b41">(Liu et al., 2015)</ref> datasets to scale up TransGAN to higher resolution image generation tasks. For the STL-10 dataset, we use both the 5k training images and 100k unlabeled images for training TransGAN, with each image at 48 × 48 resolution. For the CelebA dataset, we use 200k unlabeled face images (aligned and cropped version), and all are resized to 64 × 64 resolution.</p><p>We follow the training setting of WGAN , and use the WGAN-GP loss <ref type="bibr" target="#b23">(Gulrajani et al., 2017)</ref>. We adopt a learning rate of 1e − 4 for both generator and discriminator, an Adam optimizer, and a batch size of 128 for generator and 64 for discriminator. For higher resolution (64 × 64), we decrease the batch sizes to 32 (generator) and 16 (discriminator) to fit the GPU memory. Using 4 V100 GPUs, our training takes 2 days on CIFAR-10, and 3 days on STL-10 and CelebA. We focus on the unconditional image generation setting for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on CIFAR-10</head><p>We compare TransGAN with recently published results by ConvNet-based GANs on the CIFAR-10 dataset, shown in <ref type="table" target="#tab_5">Table 5</ref>. The results are all collected from the original papers with their best hand-tuned training settings. <ref type="table" target="#tab_5">Table 5</ref>, TransGAN surpasses the strong model of AutoGAN <ref type="bibr" target="#b19">(Gong et al., 2019)</ref>, and many other latest competitors such as SN-GAN <ref type="bibr" target="#b45">(Miyato et al., 2018)</ref>, improving MMD-GAN <ref type="bibr" target="#b60">(Wang et al., 2018a)</ref>, and MGAN <ref type="bibr" target="#b27">(Hoang et al., 2018)</ref>, in terms of inception score (IS).It is only next to the huge and heavily engineered Progressive GAN <ref type="bibr" target="#b31">(Karras et al., 2017)</ref> and StyleGAN v2 <ref type="bibr" target="#b34">(Karras et al., 2020b)</ref>. Once we look at the FID results, TransGAN is even found to outperform Progressive GAN <ref type="bibr" target="#b31">(Karras et al., 2017)</ref>, while being slightly inferior to StyleGAN v2 <ref type="bibr" target="#b34">(Karras et al., 2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Examples generated on CIFAR-10 are shown in <ref type="figure" target="#fig_3">Figure 4</ref>, from which we observe pleasing visual details and diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on STL-10</head><p>We then apply TransGAN on another popular benchmark STL-10, which is larger in scale and higher in resolution. To this end, we increase the input feature map of the generator's first stage from (8 × 8) = 64 to (12 × 12) = 144 to fit the target resolution. We use our TransGAN-XL, and compare it with both the automatic searched and hand-crafted ConvNetbased GANs, shown in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Different from the results on CIFAR-10, we find that Trans-GAN outperforms all current ConvNet-based GAN models, and sets new state-of-the-art results in terms of both IS and FID score. This is thanks to the fact that the STL-10 dataset size is 2× larger than CIFAR-10, re-confirming our assumption that transformer-based architectures benefit much more notably from larger-scale data. The visual examples generated on STL-10 are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generation on Higher Resolution</head><p>As TransGAN shows to benefit from more data samples, we now examine another challenge for TransGAN: scaling up to higher-resolution, using the more challenging CelebA dataset (64 × 64 resolution). To synthesize 64 × 64 output images, we increase the stage number of the generator from 3 to 4, to let the new generator contain {5, 3, 3, 2} encoder blocks in each stage. Detailed network configurations are in the Appendix B. We do not tune any other hyperparameter.</p><p>On this new testbed, TransGAN-XL reaches a FID score of 12.23, and its visual results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>, which appear to be natural, visually pleasing and diverse in de-   tails. In comparison, we also train another ConvNet-based GAN baseline on the same dataset, DCGAN <ref type="bibr" target="#b49">(Radford et al., 2015)</ref>, using the TTUR algorithm <ref type="bibr" target="#b26">(Heusel et al., 2017)</ref>. That baseline achieves a slightly inferior 12.50 FID score.</p><p>Although stronger ConvNet-based GANs can achieve better FID scores, we believe that the performance of TransGAN can also be boosted more by tuning its training recipe more specifically for the higher-resolution cases, and we will continue to push forward on this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions, Limitations, and Discussions</head><p>We have proposed TransGAN, a new GAN paradigm based on pure transformers. We have carefully crafted the architectures and thoughtfully designed training techniques. As a result, TransGAN has achieved comparable performance to some state-of-the-art CNN-based GAN methods across multiple popular datasets. We show that the traditional re-liance on CNN backbones and many specialized modules may not be necessary for GANs, and pure transformers can be sufficiently capable for image generation.</p><p>The pure transformer-based architecture brings versatility to TransGAN. As we turn from specialized to general-purpose architectures, one strong motivation is to simplify and unify various task pipelines, so one general suite of models could be extensively reused by many applications, avoiding the need of re-inventing wheels. There is an appealing potential that many associated techniques developed for improving transformers, originally proposed for NLP applications, could become available to TransGAN as well.</p><p>Building a GAN using only transformers appears to be more challenging than other transformer-based vision models <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020)</ref>, due to the higher bar for realistic image generation (compared to classification) and the high instability of GAN training itself. Considering that existing transformer-based models <ref type="bibr" target="#b4">(Carion et al., 2020;</ref><ref type="bibr" target="#b15">Dosovitskiy et al., 2020)</ref> are mostly on par or slightly inferior to their strongest CNN competitors (assuming no extra data used), we find TransGAN to provide an encouraging starting point.</p><p>Still, there is a large room for TransGAN to improve further, before it can outperform the best hand-designed GANs with more margins. We point out a few specific items that call for continuing efforts:</p><p>• More sophisticated tokenizing for both G and D, e.g. using some semantic grouping <ref type="bibr" target="#b63">(Wu et al., 2020)</ref>.</p><p>• Pre-training transformers using pretext tasks , which may improve over our current MT-CT.</p><p>• Stronger attention forms, e.g., <ref type="bibr">(Zhu et al., 2020)</ref>.</p><p>• More efficient self-attention forms <ref type="bibr" target="#b9">Choromanski et al., 2020)</ref>, which not only help improve the model efficiency, but also save memory costs and hence help higher-resolution generation. • Conditional image generation <ref type="bibr" target="#b40">(Lin et al., 2019</ref> Since the best hyperparameter setting for each combination is different, we list the specific setting in <ref type="table" target="#tab_8">Table 7</ref>. The most noticeable difference is that we apply hinge loss for Auto-GAN D following the original paper <ref type="bibr" target="#b19">(Gong et al., 2019)</ref>, and switch to WGAN-GP <ref type="bibr" target="#b23">(Gulrajani et al., 2017)</ref> loss when applying Transformer D as the discriminator. This is due to that AutoGAN D contains Spectral Normalization (SN) <ref type="bibr" target="#b45">(Miyato et al., 2018)</ref> layer but Transformer D does not. And we apply WGAN-GP loss for Transformer D to achieve the similar goal, the Lipschitz constraint. Here we do not make any change of the AutoGAN architectures. We apply Adam optimizer for all four combinations. Due to that STL-10 and CelebA datasets contain higher resolution images (e.g., 48 × 48 and 64 × 64), the memory cost of TransGAN will also increase. As discussed in Sec. 3, since increasing the model size of TransGAN's generator shows a more significant improvement than the multi-task co-training (MT-CT) strategy, we remove the MT-CT loss in order to save the memory for a larger model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>We include the specific architecture configurations of TransGAN-XL in <ref type="table">Table 8</ref> and 9. Here we take the architec-ture used for CIFAR-10 dataset as an example to describe the detailed configuration. The "Encoder" represents the basic Transformer Encoder block constructed by self-atention, LayerNormalization, and Feed-forward MLP. PixelShuffle layer is adopted for feature map upsampling. The "input shape" and "output shape" denotes the shape of input feature map and output feature map, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Augmentation Strategy</head><p>We mainly follow the way of differentiable augmentation <ref type="bibr" target="#b72">(Zhao et al., 2020b)</ref> to apply the data augmentation on our GAN training framework. Specifically, we conduct {T ransition, Cutout, Color} augmentation for Trans-GAN with probability p, while p is empirically set to be {1.0, 0.3, 1.0}. However, we find that T ransition augmentation will hurt the performance of ConvNet-based GAN when 100% data is utilized. Therefore, we remove it and only conduct {Cutout, Color} augmentation for Auto-GAN <ref type="bibr" target="#b19">(Gong et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Efficiency Comparison with ConvNet-based GANs</head><p>We further compare the computational cost of TransGAN with current state-of-the-art ConvNet-based GANs, including both the smallest model TransGAN-S and largest model TransGAN-XL. As shown in <ref type="table" target="#tab_1">Table 10</ref>, we firstly compare TransGAN-S with SN-GAN <ref type="bibr" target="#b45">(Miyato et al., 2018)</ref>, since they achieve similar performance. Specifically, TransGANs reaches a similar IS score and better FID score comparing with SN-GAN, while only costs about half FLOPs of SN-GAN. We then compare our largest model TransGAN-XL with AutoGAN <ref type="bibr" target="#b19">(Gong et al., 2019)</ref> and Progressive-GAN <ref type="bibr" target="#b31">(Karras et al., 2017)</ref>. TransGAN-XL reaches the best FID score with much smaller FLOPs compared to Progressive GAN and slightly larger FLOPs (1.06G) compared to Auto-GAN. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The pipeline of the pure transform-based generator and discriminator of TransGAN. Here H = W = 8 and HT = WT = 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Co-training the transformer G with an auxiliary task of super resolution. "LR" and "SR" represent low-resolution input and high-resolution output respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual Results of TransGAN on CIFAR-10, STL-10, and CelebA 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Inception Score (IS) and FID results on CIFAR-10. The first row shows the AutoGAN results<ref type="bibr" target="#b19">(Gong et al., 2019)</ref>; the second and thirds row show the mixed transformer-CNN results; and the last row shows the pure-transformer GAN results.</figDesc><table><row><cell>GENERATOR</cell><cell>DISCRIMINATOR</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell>AUTOGAN</cell><cell>AUTOGAN</cell><cell cols="2">8.55± 0.12 12.42</cell></row><row><cell>TRANSFORMER</cell><cell>AUTOGAN</cell><cell cols="2">8.59± 0.10 13.23</cell></row><row><cell>AUTOGAN</cell><cell>TRANSFORMER</cell><cell cols="2">6.17± 0.12 49.83</cell></row><row><cell cols="4">TRANSFORMER TRANSFORMER 6.95 ± 0.13 41.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The effectiveness of Data Augmentation (DA) on both CNN-based GANs and TransGAN. We used the full CIFAR-10 training set and DiffAug<ref type="bibr" target="#b72">(Zhao et al., 2020b)</ref>.</figDesc><table><row><cell>METHODS</cell><cell>DA</cell><cell>IS ↑</cell><cell>FID ↓</cell></row><row><cell>WGAN-GP (GULRAJANI ET AL., 2017)</cell><cell>× √</cell><cell cols="2">6.49 ± 0.09 39.68 6.29 ± 0.10 37.14</cell></row><row><cell>AUTOGAN (GONG ET AL., 2019)</cell><cell>× √</cell><cell cols="2">8.55 ± 0.12 12.42 8.60 ± 0.10 12.72</cell></row><row><cell>STYLEGAN V2 (ZHAO ET AL., 2020B)</cell><cell>× √</cell><cell>9.18 9.40</cell><cell>11.07 9.89</cell></row><row><cell>TRANSGAN</cell><cell>× √</cell><cell cols="2">6.95 ± 0.13 41.41 8.15 ± 0.14 19.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation</figDesc><table><row><cell cols="3">studies for multi-task co-training (MT-CT) and</cell></row><row><cell cols="3">locality-aware self-attention initialization on TransGAN.</cell></row><row><cell>MODEL</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell>TRANSGAN + DA (*)</cell><cell cols="2">8.15± 0.14 19.85</cell></row><row><cell>(*) + MT-CT</cell><cell cols="2">8.20± 0.14 19.12</cell></row><row><cell cols="3">(*) + MT-CT + LOCAL INIT. 8.22± 0.12 18.58</cell></row><row><cell cols="3">3.4. Locality-Aware Initialization for Self-Attention</cell></row><row><cell cols="3">CNN architectures have the built-in prior of natural image</cell></row><row><cell>smoothness</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Scaling-up the model size of TransGAN on CIFAR-10.</figDesc><table><row><cell cols="5">Here "Dim" represents the embedded dimension of transformer</cell></row><row><cell cols="5">and "Depth" is the number of transformer encoder block in each</cell></row><row><cell>stage.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODEL</cell><cell>DEPTH</cell><cell>DIM</cell><cell>IS ↑</cell><cell>FID ↓</cell></row><row><cell>TRANSGAN-S</cell><cell>{5,2,2}</cell><cell>384</cell><cell cols="2">8.22 ± 0.14 18.58</cell></row><row><cell>TRANSGAN-M</cell><cell>{5,2,2}</cell><cell>512</cell><cell cols="2">8.36 ± 0.12 16.27</cell></row><row><cell>TRANSGAN-L</cell><cell>{5,2,2}</cell><cell>768</cell><cell cols="2">8.50 ± 0.14 14.46</cell></row><row><cell cols="5">TRANSGAN-XL {5,4,2} 1024 8.63 ± 0.16 11.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Unconditional image generation results on CIFAR-10.</figDesc><table><row><cell>METHODS</cell><cell>IS</cell><cell>FID</cell></row><row><cell>WGAN-GP (GULRAJANI ET AL., 2017)</cell><cell cols="2">6.49 ± 0.09 39.68</cell></row><row><cell>LRGAN (YANG ET AL., 2017)</cell><cell>7.17 ± 0.17</cell><cell>-</cell></row><row><cell>DFM (WARDE-FARLEY &amp; BENGIO, 2016)</cell><cell>7.72 ± 0.13</cell><cell>-</cell></row><row><cell>SPLITTING GAN (GRINBLAT ET AL., 2017)</cell><cell>7.90 ± 0.09</cell><cell>-</cell></row><row><cell>IMPROVING MMD-GAN (WANG ET AL., 2018A)</cell><cell>8.29</cell><cell>16.21</cell></row><row><cell>MGAN (HOANG ET AL., 2018)</cell><cell>8.33 ± 0.10</cell><cell>26.7</cell></row><row><cell>SN-GAN (MIYATO ET AL., 2018)</cell><cell>8.22 ± 0.05</cell><cell>21.7</cell></row><row><cell>PROGRESSIVE-GAN (KARRAS ET AL., 2017)</cell><cell cols="2">8.80 ± 0.05 15.52</cell></row><row><cell>AUTOGAN (GONG ET AL., 2019)</cell><cell cols="2">8.55 ± 0.10 12.42</cell></row><row><cell>STYLEGAN V2 (ZHAO ET AL., 2020B)</cell><cell>9.18</cell><cell>11.07</cell></row><row><cell>TRANSGAN-XL</cell><cell cols="2">8.63 ± 0.16 11.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Unconditional image generation results on STL-10.</figDesc><table><row><cell>METHODS</cell><cell>IS ↑</cell><cell>FID ↓</cell></row><row><cell>DFM (WARDE-FARLEY &amp; BENGIO, 2016)</cell><cell>8.51 ± 0.13</cell><cell>-</cell></row><row><cell>D2GAN (NGUYEN ET AL., 2017)</cell><cell>7.98</cell><cell>-</cell></row><row><cell>PROBGAN (HE ET AL., 2019)</cell><cell>8.87 ± 0.09</cell><cell>47.74</cell></row><row><cell>DIST-GAN (TRAN ET AL., 2018)</cell><cell>-</cell><cell>36.19</cell></row><row><cell>SN-GAN (MIYATO ET AL., 2018)</cell><cell>9.16 ± 0.12</cell><cell>40.1</cell></row><row><cell>IMPROVING MMD-GAN (WANG ET AL., 2018A)</cell><cell>9.23 ± 0.08</cell><cell>37.64</cell></row><row><cell>AUTOGAN (GONG ET AL., 2019)</cell><cell>9.16 ± 0.12</cell><cell>31.01</cell></row><row><cell>ADVERSARIALNAS-GAN (GAO ET AL., 2020)</cell><cell>9.63 ± 0.19</cell><cell>26.98</cell></row><row><cell>TRANSGAN-XL</cell><cell cols="2">10.10 ± 0.17 25.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>).Zhu, J.-Y., Zhang, R., Pathak, D., Darrell, T., Efros, A. A.,  Wang, O., and Shechtman, E. Toward multimodal imageto-image translation. In Advances in neural information processing systems, pp. 465-476, 2017b.Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.</figDesc><table><row><cell>A. Implementation Details</cell></row><row><cell>A.1. Specific Setting of Transformer/CNN</cell></row><row><cell>Combinations</cell></row><row><cell>In Sec. 3.1.4, we evaluate the performance of transformer-</cell></row><row><cell>based G and D by studying four combinations: i) AutoGAN</cell></row><row><cell>G + AutoGAN D (original AutoGAN); ii) Transformer G</cell></row><row><cell>+ AutoGAN D; iii) AutoGAN G + Transformer D; and</cell></row><row><cell>iv) Transformer G + Transformer D (TransGAN-S).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Detailed setting of four combinations.</figDesc><table><row><cell>Combination</cell><cell>Loss</cell><cell>lr</cell><cell>IS</cell><cell>FID</cell></row><row><cell>i)</cell><cell cols="4">Hinge Loss 2e − 4 8.55 12.42</cell></row><row><cell>ii)</cell><cell cols="4">Hinge Loss 2e − 4 8.59 13.23</cell></row><row><cell>iii)</cell><cell cols="4">WGAN-GP 1e − 4 6.17 49.83</cell></row><row><cell>iv)</cell><cell cols="4">WGAN-GP 1e − 4 6.95 41.41</cell></row><row><cell cols="5">A.2. Experiments on STL-10 and CelebA 64 x 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Specific configuration of the generator of TransGAN-XL Specific configuration of the discriminator of TransGAN-XL</figDesc><table><row><cell>Stage</cell><cell>Layer Type</cell><cell>input shape</cell><cell>output shape</cell></row><row><cell>-</cell><cell>MLP</cell><cell>1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell></cell><cell>Encoder</cell><cell>(8 × 8) × 1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell></cell><cell>Encoder</cell><cell>(8 × 8) × 1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell>1</cell><cell>Encoder</cell><cell>(8 × 8) × 1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell></cell><cell>Encoder</cell><cell>(8 × 8) × 1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell></cell><cell>Encoder</cell><cell>(8 × 8) × 1024</cell><cell>(8 × 8) × 1024</cell></row><row><cell></cell><cell>PixelShuffle</cell><cell>(8 × 8) × 1024</cell><cell>(16 × 16) × 256</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(16 × 16) × 256 (16 × 16) × 256</cell></row><row><cell>2</cell><cell>Encoder</cell><cell cols="2">(16 × 16) × 256 (16 × 16) × 256</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(16 × 16) × 256 (16 × 16) × 256</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(16 × 16) × 256 (16 × 16) × 256</cell></row><row><cell></cell><cell cols="2">PixelShuffle (16 × 16) × 256</cell><cell>(32 × 32) × 64</cell></row><row><cell>3</cell><cell>Encoder</cell><cell>(32 × 32) × 64</cell><cell>(32 × 32) × 64</cell></row><row><cell></cell><cell>Encoder</cell><cell>(32 × 32) × 64</cell><cell>(32 × 32) × 64</cell></row><row><cell>-</cell><cell>Linear Layer</cell><cell>(32 × 32) × 64</cell><cell>32 × 32 × 3</cell></row><row><cell>Stage</cell><cell>Layer Type</cell><cell>input shape</cell><cell>output shape</cell></row><row><cell>-</cell><cell>Linear Flatten</cell><cell>32 × 32 × 3</cell><cell>(8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell>1</cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell></cell><cell>Encoder</cell><cell cols="2">(8 × 8 + 1) × 384 (8 × 8 + 1) × 384</cell></row><row><cell>-</cell><cell>-</cell><cell>(8 × 8 + 1) × 384</cell><cell>1 × 384</cell></row><row><cell>-</cell><cell>Classification Head</cell><cell>1 × 384</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Efficiency comparison between TransGAN and ConvNetbased GANs. Results are reported on CIFAR-10 dataset with 32 × 32 resolution.</figDesc><table><row><cell>METHODS</cell><cell>FLOPS (G)</cell><cell>IS</cell><cell>FID</cell></row><row><cell>SNGAN (MIYATO ET AL., 2018)</cell><cell>1.57</cell><cell>8.22 ± 0.05</cell><cell>21.7</cell></row><row><cell>TRANSGAN-S</cell><cell>0.68</cell><cell cols="2">8.22 ± 0.14 18.58</cell></row><row><cell>AUTOGAN (GONG ET AL., 2019)</cell><cell>1.77</cell><cell cols="2">8.55 ± 0.10 12.42</cell></row><row><cell>PROGRESSIVE-GAN (KARRAS ET AL., 2017)</cell><cell>6.39</cell><cell cols="2">8.80 ± 0.05 15.52</cell></row><row><cell>TRANSGAN-XL</cell><cell>2.83</cell><cell cols="2">8.63 ± 0.16 11.89</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We find enlarging G to significantly improve performance, while enlarging D seems to have negligible impact. Therefore, we increase G size by default and keep D the same.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distilling portable generative adversarial networks for image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3585" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12154" to="12163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Your local gan: Designing two dimensional local attention mechanisms for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14531" to="14539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05751</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5680" to="5689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3224" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Grinblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Uzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Granitto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07359</idno>
		<title level="m">Classsplitting generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06937</idno>
		<title level="m">A review on generative adversarial networks: Algorithms, theory, and applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probgan: Towards probabilistic gan with theoretical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training generative adversarial nets with multiple generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deblurgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A large-scale study on regularization and normalization in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Póczos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coco-gan: Generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4512" to="4521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03831</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pedestrian-synthesis-gan: Generating pedestrian data in real scene and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2018" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mpnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09297</idno>
		<title level="m">Masked and permuted pre-training for language understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Off-policy reinforcement learning for efficient and effective gan architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="175" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dist-gan: An improved gan using distance constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Improving mmdgan training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halgamuge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Visual transformers: Tokenbased image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lr-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01560</idno>
		<title level="m">Layered recursive generative adversarial networks for image generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Controllable artistic text style transfer via shape-matching gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4442" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning joint spatialtemporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027</idno>
		<title level="m">Consistency regularization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koltun</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">V. Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
