<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blending-target Domain Adaptation by Adversarial Meta-Adaptation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Zhuang</surname></persName>
							<email>zhuangjy6@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Blending-target Domain Adaptation by Adversarial Meta-Adaptation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. The comparison of MTDA and BTDA (color orange and blue denote source and target) setups. In MTDA (a), target domains are explicitly separated and we are informed by which target an unlabeled sample originates from. In BTDA (b), sub-target IDs are encrypted. If we treat them as a combined single target, transfer learning will lead to the adaptation on a multi-target mixture instead of each hidden target (gray distribution curves in (a), (b)). It implies category-shifted adaptation and negative transfer in practice. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>(Unsupervised) Domain Adaptation (DA) seeks for classifying target instances when solely provided with source labeled and target unlabeled examples for training. Learning domain-invariant features helps to achieve this goal, whereas it underpins unlabeled samples drawn from a single or multiple explicit target domains (Multi-target DA). In this paper, we consider a more realistic transfer scenario: our target domain is comprised of multiple sub-targets implicitly blended with each other, so that learners could not identify which sub-target each unlabeled sample belongs to. This Blending-target Domain Adaptation (BTDA) scenario commonly appears in practice and threatens the validities of most existing DA algorithms, due to the presence of domain gaps and categorical misalignments among these hidden sub-targets.</p><p>To reap the transfer performance gains in this new scenario, we propose Adversarial Meta-Adaptation Network (AMEAN). AMEAN entails two adversarial transfer learning processes. The first is a conventional adversarial transfer to bridge our source and mixed target domains. To circumvent the intra-target category misalignment, the second process presents as "learning to adapt": It deploys an unsupervised meta-learner receiving target data and their ongoing feature-learning feedbacks, to discover target clus-ters as our "meta-sub-target" domains. These meta-subtargets auto-design our meta-sub-target DA loss, which empirically eliminates the implicit category mismatching in our mixed target. We evaluate AMEAN and a variety of DA algorithms in three benchmarks under the BTDA setup. Empirical results show that BTDA is a quite challenging transfer setup for most existing DA algorithms, yet AMEAN significantly outperforms these state-of-the-art baselines and effectively restrains the negative transfer effects in BTDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>1 Despite achieving a growing number of successes, deep supervised learning algorithms remain restrictive in a variety of new application scenarios due to their vulnerabilities towards domain shifts <ref type="bibr" target="#b16">[17]</ref>: when evaluated on newlyemerged unlabeled target examples drawn from a distribution non-identical with the training source density, supervised learners inevitably perform inferior. In terms of visual data, this issue stems from diverse domain-specific appearance variabilities, e.g. differences in background and camera poses, occlusions and volatile illumination conditions, etc. Hence the variabilities are highly relevant in most machine vision implementations and obstacle their advancements. To address these problems, (unsupervised) Domain Adaptations (DAs) choose suitable statistical measures between source and target domains, e.g., maximum mean discrepancy (MMD) <ref type="bibr" target="#b24">[25]</ref> and adversarial-network distance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>, to learn domain-invariant features in pursuit of consistent cross-domain model performances. The related studies increasingly attract a large amount of interests from the areas of domain-adaptive perception <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>, autonomous steering <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref> and robotic vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Most existing DA approaches indeed have evidenced impressive performances in a laboratory, whereas the domain shifts that frequently occur in reality, are far from being settled through these techniques. One explanation is the ideal target-domain premise these DA techniques start from. Particularly, DAs are conventionally established on a "singletarget" preset, namely, all target examples are drawn from an identical distribution. Some recent researches focus on Multi-target DA (MTDA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>, where target examples stem from multiple distributions, whereas we exactly know which target they belong to (See <ref type="figure">Fig.1.(a)</ref>).</p><p>In this paper, we argue these target-domain preconditions always taken for granted in most previous transfer learning literatures. After revisiting widespread presences, we discover that target unlabeled examples are often too diverse to well suit a "single-target" foresight. For instance, virtualto-real researches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47]</ref> encourage robots and driver agents trained on a simulation platform to adaptively perform in the real-world environment. However, the target real-world environment includes extensive arrays of scenarios and continuously changes as time goes by. Another case is an encrypted dataset stored in a cloud server <ref type="bibr" target="#b12">[13]</ref>, where the unlabeled examples are derived from multiple origins whereas due to a privacy protection, users have no access to identify these origins. These facts imply the existence of multiple sub-target domains while unlike standard MTDA, these sub-targets are blended with each other so that learners are not able to identify which sub-target each unlabeled example belongs to (See <ref type="figure">Fig.1.(b)</ref>). This so-called blendingtarget domain adaptation (BTDA) scenario regularly occurs in more other circumstances and during adaptation process, it commonly arouses notorious negative transfer effects <ref type="bibr" target="#b32">[33]</ref> by two reasons:</p><p>• Hidden sub-target domains are organized as a mixture distribution, whereas without the knowledge of subtarget ID, it is quite difficult to align the category to reduce their mismatching across the sub-targets. • Regardless of the domain gaps among the hidden targets, existing DA approaches will suffer from the category misalignments among these sub-targets.</p><p>. Due to the blending-target preset, BTDA can not be solved by existing multi-target transfer learning methods.</p><p>To reap transfer performance gains and simultaneously prevent the negative transfer effects in BTDA scenario, we propose Adversarial Meta-Adaptation Network (AMEAN) attempting to solve this problem under the context of visual recognition. AMEAN evolves from popular adversarial adaptation frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> and opts for minimizing the discrepancy between our source and mixed target domains. But distinguished from these existing pipelines, our AMEAN is inspired by meta-learning and AutoML <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>, which concurrently deploys an unsupervised metalearner to learn deep target cluster embeddings by receiving the mixed target data and their ongoing-learned features as feedbacks. The incurred clusters are treated as metasub-target domains. Hence, our AMEAN auto-designs its multi-target adversarial adaptation loss functions and to this end, dynamically train itself to obtain domain-invariant features from a source to a mixed target and among the multiple meta-sub-target domains derived from the mixed target. This bi-level optimization endows more diverse and flexible adaptation within the mixed target and effectively mitigates its latent category mismatching.</p><p>Our contributions mainly present in three aspects: 1. On account of practical cross-domain applications, we consider a new transfer scenario termed Blending-target Domain Adaptation (BTDA), which is common in reality and more difficult to settle.</p><p>2. We propose AMEAN, a adversarial transfer framework incorporating meta-learner dynamically inducing metasub-targets to auto-design adversarial adaptation losses, which effectively achieve transfers in BTDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Our experiments are conducted on three widely-applied DA benchmarks. Our results show that BTDA setup definitely brings more transfer risks towards existing DA algorithms, while AMEAN significantly outperforms the state-of-the-art and present more robust in BTDA setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Before introducing BTDA problem setup, we would like to briefly revisit (unsupervised) Domain Adaptation (DA) under the modern visual learning background.</p><p>Single-source-single-target DA. DAs are derived from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15]</ref>, where shallow models are deployed to achieve data transfer across visual domains. The development of deep learning enlightens the tunnel to learn nonlinear transferable feature mappings in DAs. Up-to-date deep DA methods have been branched into two mainstreams: explicit and implicit statistical measure matching. The former employs MMD <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, CMD <ref type="bibr" target="#b48">[49]</ref>, JMMD <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>, etc, as the domain regularizer to chase for consistent model performances both on source and target datasets. The latter designates domain discriminators to perform adversarial learning, where the feature extractor is trained to optimize the transferable feature spaces. It includes amounts of avenues to present diverse adversarial manners, e.g., CoGAN <ref type="bibr" target="#b23">[24]</ref>, DDC <ref type="bibr" target="#b19">[20]</ref>, RevGred <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, ADDA <ref type="bibr" target="#b38">[39]</ref>, VADA <ref type="bibr" target="#b37">[38]</ref>, GTA <ref type="bibr" target="#b36">[37]</ref>, etc. Beside of the two branches, there are some approaches in virtue of other ideologies, e.g., reconstruction <ref type="bibr" target="#b11">[12]</ref>, semi-supervised learning <ref type="bibr" target="#b35">[36]</ref> to optimize a domainaligned feature space. It is worth noting that, these methods agree with the "single-source-single-target" precondition when they learns transferable features for DA .</p><p>Multi-source DA (MSDA). MSDA aims at boosting the target-adaptive accuracy of a model by introducing multiple source domains in a transfer process. It is a historical topic <ref type="bibr" target="#b44">[45]</ref> and refers to a part of DA theories <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b3">[4]</ref>. Some recent work place the problem under the deep visual learning background. For instance, <ref type="bibr" target="#b43">[44]</ref> invented a adversarial reweighting strategy to infer a source-ensemble target classifier; <ref type="bibr" target="#b49">[50]</ref> developed an old-fashion theory to suit deep MSDA and provided a target error upper bound; <ref type="bibr" target="#b28">[29]</ref> stacked multi-DA layers in a network to obtain robust multisource domain alignment.</p><p>Multi-target DA (MTDA). Similar to MSDA, the goal of MTDA is to enhance data transfer efficacy by bridging the cross-target semantic. <ref type="bibr" target="#b1">[2]</ref> used a semantic disentangler to facilitate an adversarial MTDA approach; <ref type="bibr" target="#b47">[48]</ref> considered the visible semantic gap between multiple targets and propose a dictionary learning algorithm to suit this problem. MTDA is still a fresh area and awaits more explorations. i is an m-dimensional one-hot vector corresponding to its label. Besides, a target set includes n t unlabeled ex-</p><formula xml:id="formula_0">amples T = {x (t) i } nt i=1 where x (t) i</formula><p>∈ R d + denotes the i th target image. S and T underly distributions P S (x, y) and P T (x), in which P T (x) = P T (x, y)dy indicates target labels unobservable during training. DA seeks for learning a classifier along with a domain-invariant feature extractor across S and T , which is capable to predict the correct labels of given images sampled from P T (x, y). As of now DA assumes all unlabeled images derived from a single target distribution P T .</p><p>Here we turn to consider the multi-target DA (MTDA) setup: every unlabeled target instance x (t) underly k distributions {P Tj (x (t) )} k j=1 , as they are drawn from the mixture P T (x (t) ) = k j=1 π j P Tj (x (t) ) where ∀j ∈ [k], π j ∈ [0, 1]&amp; k j π j = 1. However, The learning goal of MTDA is to simultaneously adapt k targets {T j } k j=1 instead of their mixed target T . Since the multi-target proportions {π j } k j=1 are known in MTDA, target set T j is explicitly provided by drawing from the posterior</p><formula xml:id="formula_1">πj P T j (x (t) ) k j =1 πj P T j (x (t) )</formula><p>. Hence exist-ing DA algorithms can address the problem by training k target-specific DA models respectively, and using the j thtarget model to classify the examples from the j th target.</p><p>From MTDA to BTDA. Like MTDA, BTDA is also established on a target mixture distribution and expected to adapt k targets {T j } k j=1 . However, multi-target proportions {π j } k j=1 in BTDA are unobservable. In other words, BTDA learners are solely provided with a mixed target set T drawn from a k-mixture density and required to classify a mixed target test set drawn from the same mixture. If we directly leverage existing DA techniques to transfer category information from S to T , the learning objective will guide domain-invariant features to adapt a mixed target set T instead of k target {T j } k j=1 . Since sub-targets from k distributions could be quite distinct in visual realism, adapting to a mixed target probably would result in drastic category mismatching and arouses serious negative transfer effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adversarial MEta-Adaptation Network</head><p>To reap positive transfer performance in BTDA setup, we propose Adversarial MEta-Adaptation Network (AMEAN). AMEAN is coupled of two adversarial learning processes, which are parallely executed to obtain domain-invariant features by transferring data from source S to mixed target T , and among k "meta-sub-targets" {T j } k j=1 within the mixed target T . The pipeline of AMEAN is concisely illustrated in <ref type="figure">Fig.2</ref> and we elaborate the methodology as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptation from a source to a mixed target</head><p>Suppose that C is a m-slot softmax classifier proposed on a transferable feature space and F denotes the feature extractor we long to optimize in BTDA. We employ a sourcetarget domain discriminator D st to deploy an adversarial learning scheme, where F is trained to match a source set S and a mixed target set T (the unification of {T j } k j=1 ) at the feature level, while D st is demanded to separate the source and the target features.</p><formula xml:id="formula_2">min F,C max Dst V st (F, D st , C) = λ E x∼S log D st F (x) + E x∼T log 1 − D st F (x) Adversarial DA loss −E (x,y)∼S y T log C F (x)</formula><p>Classification loss <ref type="bibr" target="#b0">(1)</ref> . This minimax objective can be optimized in two ways:</p><p>1. C, F , D st are jointly trained by inducing a reversed gradient layer <ref type="bibr" target="#b8">[9]</ref> [10] across D st and F , which inversely back-propagates the gradients from a sourcetarget domain discriminator D st . 2. C, F and D st are updated by an alternative optimization like GAN <ref type="bibr" target="#b13">[14]</ref> [38]. <ref type="figure">Figure 2</ref>. The learning pipeline of our Adversarial MEta-Adaptation Network (AMEAN). AMEAN receives source samples with ground truth and unlabeled samples drawn from an unknown target mixture to synchronously execute two transfer learning processes. In the first process, we propose an adversarial learning bridging our source S and the mixed target T , so as to learn a feature extractor F that maps them into a common feature space along with a classifier training. In the second stream, AMEAN uses an unsupervised meta-learner U to accept target data and their representations (student feedbacks) and then, learns to separate the mixed target domain into k clusters as meta-sub-targets {Tj} k j=1 . These meta-sub-targets are iteratively updated to auto-design the multi-target adaptation objectives (Eq. <ref type="bibr">6 7)</ref>, which operates to progressively eliminate the category mismatching behide the mixed target. Best viewed in color.</p><p>Our experiments show them both well-suited in AMEAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Meta-adaptation among meta-sub-targets</head><p>The pure source-to-target transfer is blind to the domain gaps and category misalignment among {T j } k j=1 , which the negative transfer mainly ascribes to. Here we interpret the key module of AMEAN to address this problem: an unsupervised meta-learner trained to obtain k embedded clusters <ref type="bibr" target="#b41">[42]</ref> as our "meta-sub-targets", which play the roles of automatically and dynamically outputting multi-target adversarial adaptation loss functions (Eq.6, 7) in pursuit of reducing the category mismatching inside the mixed target T .</p><p>Meta-learning for dynamic loss design.. Meta-learning ("learning to learn") <ref type="bibr" target="#b0">[1]</ref> become a thriving topic in machine learning community. It refers to all learnable technique to improve model generalization, which includes fast adaptation to rare categories <ref type="bibr" target="#b40">[41]</ref> and unseen tasks <ref type="bibr" target="#b7">[8]</ref>, as well as auto-tuning the hyper-parameters accompanied with training, e.g., learning rate <ref type="bibr" target="#b0">[1]</ref>, architecture <ref type="bibr" target="#b50">[51]</ref>, loss function <ref type="bibr" target="#b42">[43]</ref>, etc. Inspired by them, our meta-learner U is an unsupervised net learning to find deep clustering embeddings that incurs k clusters based on the ongoing feature extractor feedbacks. It induces k meta-sub-targets {T j (U )} k j=1 to take place of {T j } k j=1 . Via this auxiliary, meta-learner has access to auto-design the adversarial multi-target DA loss with respect to the k meta-sub-targets and, dynamically alter the losses according to the change of these meta-subtargets as our meta-learner updates the k clusters.</p><p>Two major concerns are probably raised to our methodology: 1). Are {T j (U )} k j=1 and {T j } k j=1 similar ? 2). Does the clustering finally lead to target samples of the same category staying together? Towards the first concern, our answer is not and unnecessary. The technical difficulty in BTDA arises from the category misalignment instead of the hidden sub-target domains. As long as a DA algorithm performs appropriate category alignment in a mixed target, it is unnecessary to explicitly discover these hidden sub-target domains. AMEAN receives ongoing learned features that implicitly conceive label information from Eq 1 , to adaptively organizes its meta-adaptation objective. In our ablation, This manner shows powerful to overcome the category misalignment in T . The second concern raises a dilemma to our meta-transfer: as learned features become more classifiable, target features of the same class will be closer and closer, then clustering will more probably select them as a newly-updated meta-target domain. It causes category shift if we apply them to learn AMEAN. To remove this hidden threat, our meta-learner simultaneously receives x (t) and F (x (t) ) to learn deep clustering embeddings, where x (t) denotes the primitive state of a sample without DA and F (x (t) ) implies the adaptation feedback from the ongoing learned feature. Since x (t) is feature-agnostic, the induced clusters inherit the merit of meta-learning and concurrently gets rid of this classifiable feature dilemma.</p><p>Discovering k meta-sub-targets via deep k-clustering. Our unsupervised meta-learner U is derived from the base model in DEC <ref type="bibr" target="#b41">[42]</ref>, a denoising auto-encoder (DAE) composed of encoder U 1 and decoder U 2 . Distinguished from the DAE in DEC, U takes a target couple x t , F (x (t) ) as inputs and learns to satisfy the self-reconstruction as well as obtain deep clustering embedding U 1 x (t) , F (x (t) ) . More specifically, suppose {µ j } k j=1 is the k cluster centroids, and we define a soft cluster assignment</p><formula xml:id="formula_3">{q i,j } k j=1 to x (t) i ∈ T . q i,j = (1 + ||U1(x (t) i ,F (x (t) i ))−µj || 2 α ) − α+1 2 k j (1 + ||U1(x (t) i ,F (x (t) i ))−µ j || 2 α ) − α+1 2 (2)</formula><p>where α indicates the degree of freedom in a Student's tdistribution. Eq.2 is aimed to learn U by iteratively inferring deep cluster centroids {µ j } k j=1 . This EM-like learning operates with the help of auxiliary distributions {p i,j , ∀i ∈ [n t ]}, which are computed by raising q i to the second power and normalize it by the frequency in per cluster, i.e.,</p><formula xml:id="formula_4">pi,j = q 2 i,j /fj k j q 2 i,j /f j , s.t. fj = n t i qi,j<label>(3)</label></formula><p>where f j denotes the j th cluster frequency. Compared with q i , p i endows more emphasis on data points with high confidence and thus, is more appropriate to supervise the soft cluster inference. We employ KL divergence to restrict q i and p i for the meta-learner clustering network learning:</p><formula xml:id="formula_5">min U1,U2,{µj } k j=1 E xi∼T L rec (x i ; F ) − k j=1 p i,j log p i,j q i,j<label>(4)</label></formula><p>where L rec (x; F ) denotes a l 2 self-reconstruction w.r.t. a target feedback pair x, F (x) and the second term denotes a KL divergence term for clustering. Parameter learning and k cluster centroid ({µ j } k j=1 ) update are facilitated by backpropagation with a SGD solver. (See more in Appendix.A)</p><p>After meta-learner U converges, we apply the incurred clustering assignments to separate T into k meta-sub-target domains, i.e., x</p><formula xml:id="formula_6">(t) i in a mixed target T will be classed into meta-sub-targetT j (U ) if q i,j is the maximum in {q i,j } k j =1 : ∀j ∈ [k],T j (U ) = {x i ∈ T &amp;j = arg max j q * i,j } (5)</formula><p>Meta-sub-target adaptation. Given k meta-sub-target domains {T j (U )} k j=1 , AMEAN auto-designs the k-sub-target DA losses to re-align the features in T . More detailedly, AMEAN designates a k-slot softmax classifier D mt sharing parameters with D st , as meta-sub-target domain discriminator. In order to obtain k meta-sub-target adaptations, features extracted by F seek to "maximally confuse" the discriminative decision of D mt :</p><formula xml:id="formula_7">min F max D mt Vmt(F, Dmt) = k j=1 E x∼T j (U ) 1 T j log Dmt F (x)<label>(6)</label></formula><p>where 1 j indicates a k-dimensional one-hot vector implying that sample x belongs toT j (U ). In the case of joint parameter learning, due to the mutual architectures of D st and D mt , Eq.6 is implemented by the same reversed gradient layer originally for source-to-target transfer. However, if Divide T into k meta-sub-targets {Tj(U )} k j=1 by Eq.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Collaborative Advesarial Meta-Adaptation:</p><p>10:</p><formula xml:id="formula_8">for 1:M do 11:</formula><p>Sample a mini-batch Xs from S and k mini-batches {X</p><formula xml:id="formula_9">(j) t } k j=1 from {Tj(U )} k j=1 respectively; Xt = ∪X (j) t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if alternating domain adaptation then <ref type="bibr">13:</ref> Update Dst and Dmt with Eq.15 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Update F and C with Eq.16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>else Update Dst, Dmt, F and C with Eq.14 . <ref type="bibr" target="#b15">16</ref>:</p><formula xml:id="formula_10">end if 17: end for 18: end while 19: return F * = F ; C * = C; D * st = Dst; D * mt = Dmt.</formula><p>subnets F and D mt are alternatively trained, Eq.6 is solely used to update D mt while we prefer to optimize feature extractor F by maximizing the cross-entropy of D mt F (x) :</p><formula xml:id="formula_11">min F Vmt(F, Dmt) = k j=1 E x∼T j (U ) Dmt F (x) T log Dmt F (x)<label>(7)</label></formula><p>It implies that F learns to "confuse" multi-target domain discriminator D mt , namely, D mt could not identify which meta-sub-target an unlabeled example belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Collaborative Advesarial Meta-Adaptation</head><p>In order to learn domain-invariant features as well as resist the negative transfer from a mixed target, the previous transfer processes should be combined to battle the domain shifts. In particular, we retrain our meta-learner to update meta-DA losses V mt (F, D mt ) and V mt (F, D mt ) per M iteration during feature learning. After that, if we employ a reverse gradient layer as the adversarial DA implementation (Eq.1), the collaborative learning objective is formulated as</p><formula xml:id="formula_12">max Dst,Dmt min F,C V joint (D st , D mt , F, C) = V st (F, D st , C) + γV mt (F, D mt )<label>(8)</label></formula><p>where γ indicates the balance factor between two transfer processes. Eq.14 suits joint learning w.r.t. F , D st , D mt , C, while in an alternating adversarial manner, it would be more appropriate to iteratively update {F, C} and {D st , D mt } by </p><formula xml:id="formula_13">V alter (D st , D mt ) = V st (F, D st , C) + V mt (F, D mt ) (9) min F,C V alter (F, C) = V st (F, D st , C) + γ V mt (F, D mt )<label>(10)</label></formula><p>In a summary, the stochastic learning pipeline of AMEAN is described by Algorithm.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we elaborate comprehensive experiments in the BTDA setup and compare AMEAN with state-of-theart DA baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>Benchmarks. Digit-five <ref type="bibr" target="#b43">[44]</ref> is composed of five domain sets drawn from mt (MNIST) <ref type="bibr" target="#b22">[23]</ref>, mm (MNIST-M) <ref type="bibr" target="#b9">[10]</ref>, sv(SVHN) <ref type="bibr" target="#b31">[32]</ref>, up (USPS) and sy (Synthetic Digits) <ref type="bibr" target="#b9">[10]</ref>, respectively. There are 25000 for training and 9000 for testing in mt, mm, sv, sy, while the entire USPS is chosen as a domain set up. Office-31 <ref type="bibr" target="#b34">[35]</ref> is a famous visual recognition benchmark comprising 31 categories and totally 4652 images in three separated visual domains A (Amazon), D (DSLR), W (Webcam), which indicate images taken by web camera and digital camera in distinct environments. Office-Home <ref type="bibr" target="#b39">[40]</ref> consists of four visual domain sets, i.e., Artistic (Ar), Clip Art (Cl), Product (Pr) and Real-world (Rw) with 65 categories and around 15, 500 images in total.</p><p>Baselines. Since MSDA, MTDA approaches require domain remarks, they obviously do not suit the BTDA setup. Therefore we compare our AMEAN with existing (singlesource-single-target) DA baselines and evaluate their classification accuracies by transferring class information from source S to mixed target T . State-of-the-art DA baselines include: Deep Adaptation Network (DAN) <ref type="bibr" target="#b24">[25]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b26">[27]</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b27">[28]</ref>, Generate To Adapt (GTA) <ref type="bibr" target="#b36">[37]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b38">[39]</ref>, Reverse Gradient (RevGred) <ref type="bibr" target="#b8">[9]</ref> [10], Virtual Adversarial Domain Adaptation (VADA) <ref type="bibr" target="#b37">[38]</ref> and its variant DIRT-T <ref type="bibr" target="#b37">[38]</ref>. DAN, RTN and JAN proposed MMD-based regularizer to pursue cross-domain distribution matching in a feature space; ADDA, RevGred, GTA and VADA are domain adversarial training paradigms encouraging domaininvariant feature learning by "cheating" their domain discriminators. DIRT-T is built upon VADA by introducing a network to guide the dense target feature regions away from the decision boundary. Beyond these approaches, we also report the Source-only results based on F and C that are merely trained on source labeled data to classify target examples.</p><p>Implementation setting. In digit recognition, we evaluate AMEAN on two different backbones. The first is derived from a LeNet architecture with F , D st , D mt , C jointly trained through a reversed gradient layer (Eq.14); the second employs a GAN-based alternating learning scheme <ref type="bibr" target="#b13">[14]</ref> that switches the optimization between Eq.15 , 16 . For a fair comparison, all baselines in Digit-five experiment are based on these backbones. In Office-31 and Office-Home, we evaluate all baselines with AlexNet <ref type="bibr" target="#b21">[22]</ref> and ResNet-50 <ref type="bibr" target="#b18">[19]</ref>, where our AMEANs are trained by Eq.14. Our metalearner employ the same architecture in all experiments, i.e., a four-layered fully-connected DAE. More implementation details are deferred in our Appendix.A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Criteria.</head><p>Our experimental evaluation is aimed to answer two fundamental questions in this paper:</p><p>1. Does BTDA bring more transfer learning risks to existing DA algorithms ? 2. Is our AMEAN able to reduce these transfer risks ? As a primal metric, classification accuracies on a mixed target (Acc BTDA ) are provided to evaluate DA baselines in the BTDA setup, where their adaptations from S to a mixed T are performed to cultivate a classifier that predicts labels on a mixed target test set. To answer the questions above, we also consider two additional metrics, i.e., absolute negative transfer (ANT) and relative negative transfer (RNT):</p><p>• Absolute negative transfer (ANT). Given a DA baseline, if its performance is inferior to its Source-only, it  implies that this DA algorithm not only fails to benefit but also damages the classifier, i.e., suffers from ANT. • Relative negative transfer (RNT). RNT aims to measure how much performance drops if a DA baseline alters from MTDA to BTDA setups. In MTDA, each DA baseline performs adaptation from source S to each explicit target T j (∀j ∈ [k]). It results in k targetspecific domain-adaptive models with their accuracies {Acc j } k j=1 on the target test sets respectively. Towards this end, we compute the MTDA weighted averaged accuracy by Acc MTDA = k j=1 αj Accj where α j indicates the ratio of sub-target T j in a mixed target T . Hence we have RNT = Acc BTDA − Acc MTDA . More details of the metrics can be found in our Appendix.B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results.</head><p>The evaluations based on classification accuracy (ACC, i.e., Acc BTDA ), ANT and RNT have been conducted in Tables.11 -14 (we highlight ANT, RNT in BLUE, RED).</p><p>In Digit-five <ref type="table" target="#tab_1">(Table.11</ref>), ANT frequently occurs in BTDA setups. In sv→mm,mt,up,sy and sy→mm,mt,up,sv, few DA algorithms are exempt for these performance degradation under backbone-1 (i.e., -26.8 for ADDA). This situation is ameliorated on backbone-2. However, all DA baselines suffer from RNT. Especially when an adaptation process starts from a simple source to complex targets, i.e., mt→mm,sv,up,sy, the accuracy drop from MTDA to BTDA is significant. In Office-31. <ref type="table">(Table.</ref>12), due to D and W sharing similar visual appearances, most of their categories are aligned well in the mixed target. Therefore ANT and RNT are suppressed in A→ D,W. But in the other transfer tasks, ANT and RNT still haunt the performances of DA baselines. In Office-Home <ref type="table">(Table.</ref>14), all DA baselines get rid of ANT, whereas they remain inferior to their performances in MTDA setup (suffer RNT). Observe that, deeper models, e.g., ResNet-50, may encourage the baselines to resist ANT. But the deeper models do not help reduce RNTs across DA algorithms. These evidences sufficiently verify the hardness of BTDA and answer the first question.</p><p>Though BTDA is a challenging transfer setup, AMEAN presents as a ideal solver. As shown in <ref type="table">Table.</ref>11 -14 , AMEAN achieved the state-of-the-art in 29 out of 30 BTDA transfer cases, and its average accuracy exceeds the second best by 1.0 ∼ 7.7%. AMEAN almost achieve positive transfer in all transfer cases, and has reaped huge transfer gains in some of them, e.g., +37.7% in mt→mm,sv,up,sy, +21.5% in A→ D,W, +31.3% in Ar→Cl,Pr,Rw,etc.More importantly, AMEAN obtains more impressive performances from deeper architectures, which demonstrates its superiority to address BTDA problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis.</head><p>Adaptation visualization. For the BTDA transfer task mt → mm,sv,up,sy in Digit-five, we visualize the classifi-  cation activations from Source-only, RevGred, VADA and AMEAN in BTDA setup. As can be seen in <ref type="figure" target="#fig_0">Fig.3</ref> , Sourceonly barely captures any classification patterns. In a comparison, RevGred and VADA show better classifiable visualization patterns than Source-only's. But their activations remain pretty messy and most of them are misaligned in their classes. It demonstrates that BTDA is a very challenging scenario for existing DA algorithms. Finally, the activations from AMEAN show clear classification margins. It illustrates the superior transferability of our AMEAN.</p><p>Ablation study. The crucial component of our AMEAN is the meta-learner for auto-designing V mt . Hence our ablation focuses on this model-driven auto-learning technique. In particular, we evaluate three adaptation manners derived from our AMEAN: adaptation without meta-learner (w/o meta learner); adaptation without meta-learner but using explicit sub-target ({T j } k j=1 ) to guide the transfer in BTDA (explicit sub-targets); adaptation with meta-learner (w meta-learner, AMEAN). As illustrated in <ref type="figure" target="#fig_2">Fig.4</ref> , explicit sub-target information is not persistently helpful to BTDA. In sy-to-others, explicit sub-target information even draws back the source-to-target transfer gain. By contrast, metalearner plays a key role to enhance the adaptation towards digit and real-world visual domains and obtain state-of-theart in BTDA. Surprisingly, the dynamical meta-sub-targets even drive the adaptation model exceed those trained with the explicit sub-target domains.</p><p>To further investigate the meta-adaptation dynamic provided by AMEAN, we ablate the multi-target DA by following different target separation strategies: 1). explicit-subtarget (EST); 2).static deep k-clustering (k-C); 3) AMEAN. Note that, 2) ablates the auto-loss-design dynamic in V mt ,  as it solely uses the initial clusters to divide the mixed target and keep a static V mt along model training. The comparison of 2) and 3) helps to unveil whether our auto-loss strategy facilitates AMEAN. As shown in <ref type="table" target="#tab_4">Table 4</ref> , 1) and 2) disregard label information given by a source and their V mt still suffer from a risk of class mismatching. Our auto-loss manner adaptively changes V mt by receiving label information from the features previously learned by Eq.1 and thus, achieve better performance in BTDA. More importantly, it also encourages a fast and more stable adaptation during the minimax optimization process (see <ref type="figure" target="#fig_3">Fig 5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>In this paper, we concern a realistic adaptation scenario, where our target domain is comprised of multiple hidden sub-targets and learners could not identify which sub-target each unlabeled example comes from. This Blending-target domain adaptation (BTDA) conceives category mismatching risk and if we apply existing DA algorithm in BTDA, it will lead to negative transfer effect. To take on this uprising challenge, we propose Adversarial MEta-Adaptation Network (AMEAN). AMEAN starts from the popular adversarial adaptation methods, while newly employs a meta-learner network to dynamically devise a multi-target DA loss along with learning domain-invariant features. The AutoML merits are inherited by AMEAN to reduce the class mismatching in the mixed target domain. Our experiments focus on verifying the threat of BTDA and the efficacy of AMEAN in BTDA. Our evaluations show the significance of BTDA problem as well as the superiority of our AMEAN. Our meta-learner U is trained as deep embedding clustering ( <ref type="bibr" target="#b41">[42]</ref>) by receiving data and its feature-level feedbacks (the concatenation of the feature input to the discriminators and its classification result, for brevity, we mark F (x (t) ) in our paper). It is very important to note that, distinguished from the original version solely using a DAE to initiate data embeddings, our meta-learner leverage the improved DEC <ref type="bibr" target="#b17">[18]</ref> as our implementation, where the clustering embeddings are updated by reconstruction loss as well as the clustering objective w.r.t. centroids {µ j } k j=1 . Therefore, U 1 , U 2 and {µ j } k j=1 are alternatively updated by</p><formula xml:id="formula_14">U2 =U2 − α m m i=1 ∂Lrec(xi; F ) ∂U2 U1 =U1 − α m m i=1 [ ∂Lrec(xi; F ) − k j=1 pi,j log p i,j q i,j ∂U1 ] µj =µj − 2 α m m i=1 K j=1 (1 + ||U1(x (t) i ) − µj||) −1 (pij − qij)(U1(x (t) i ) − µj)<label>(11)</label></formula><p>where α and m denote the learning rate and mini-batch size for optimizing meta-learner. We set initial learning rate as 0.001 and batch size is 256. The auto-encoder architecture implemented in our experiments has been shown in <ref type="table">Table.</ref>5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Entropy Penalty</head><p>In our implementation, we leverage a well-known cluster assumption <ref type="bibr" target="#b15">[16]</ref> to regulate the classifier C learning with unlabeled target data. It can be interpreted as the minimization of the conditional entropy term with respect to the output of C F (x)</p><formula xml:id="formula_15">L ent (F, C) = −E x∼T C F (x) T log C F (x)<label>(12)</label></formula><p>. The objective forces the classification to be confident on the unlabeled target example, which drives the classifier's decision boundaries away from the target unlabeled examples. It has been applied in wide range of domain adaptation researches <ref type="bibr" target="#b36">[37]</ref> [27] <ref type="bibr" target="#b10">[11]</ref>. However, while using available data to empirically estimate the expected loss, <ref type="bibr" target="#b15">[16]</ref> demonstrates that such approximation provably breaks down if C F (·) does not satisfy local Lipschitz condition. Specifically, the classifier without local Lipschitz constraint can abruptly changes its prediction, which allows placement of the classifier decision boundaries close to target training examples while the empirical conditional entropy is still minimized. To prevent this issue, we follow the technique in <ref type="bibr" target="#b37">[38]</ref> where virtual adversarial perturbation term <ref type="bibr" target="#b30">[31]</ref> is incorporated to regulate the classifier and feature extractor:</p><formula xml:id="formula_16">L vir (F,C) = E x (s) ∼S max ||r||≤ D KL (C F (x (s) ) ||C F (x (s) + r) ) +ρE x (t) ∼T max ||r||≤ D KL (C F (x (t) ) ||C F (x (t) + r) )<label>(13)</label></formula><p>where D KL indicates KL divergence. r indicates the virtual adversarial perturbation upper bounded by a magnitude &gt; 0 on source and target images (x (s) ∈ S and x (t) ∈ T ), which are obtained by maximizing the classification differences between C F (x (s) ) and C F (x (s) +r) . This restrictions are simultaneously proposed on source and target and ρ is the balance factor between them. In this way, the collaborative meta-adversarial adaptation objectives (Eq.8-10 in our paper) are reformulated as: </p><p>and max Dst,Dmt</p><formula xml:id="formula_18">V alter (D st , D mt ) = V st (F, D st , C) + V mt (F, D mt ) (15) min F,C V alter (F, C) =V st (F, D st , C) + γ V mt (F, D mt ) + βL ent (F, C) + L vir (F, C)<label>(16)</label></formula><p>. We provide the ablation study of the entropy penalty in <ref type="table">Table 9</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">The Selection of k</head><p>In AMEAN, k denotes the number of sub-target domains and is pre-given. <ref type="table" target="#tab_1">Table 10</ref> demonstrates that choosing k as the number of sub-targets leads to the superior performance of AMEAN. However, whether AMEAN would achieve the better performance if k is adaptively determined, remains an open and interesting question. We would like to investigate this topic in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Architectures</head><p>The architectures for digit recognition in Digit-five have been illustrated in <ref type="table">Table.</ref>6 , 7 . The first backbone is based on LeNet and the second is derived from <ref type="bibr" target="#b37">[38]</ref> for comparing their state-of-the-art models VADA and DIRT-T. The architectures for object recognition in Office-31 and Office-Home are based on AlexNet and ResNet-50, which are consistent with the previous studies <ref type="bibr" target="#b24">[25]</ref> [26] <ref type="bibr" target="#b26">[27]</ref> [9] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Training Details</head><p>We evenly separate the proportion of the source and target examples in each mini-batch. Concretely, we promise that a half of examples in a mini-batch are drawn from S and the rest belong to the mixed target domain training set T train : In digit-five, we randomly drew target exam-ples from the mixed target set T train to construct our minibatches; In Office-31 and Office-Home, we promise the number of target examples from different meta-sub-target are the same by repeat sampling.</p><p>In the Digit-five experiment, we add a confusion loss <ref type="bibr" target="#b19">[20]</ref> w.r.t. S to train the backbone-2. It stabilizes the alternating adaptation since the mixed target in Digit-five is more diverse than the other benchmarks' and the alternating learning manner is quite instable in these scenarios. The implementation can be found in our code.</p><p>The hyper-parameters are shown in <ref type="table">Table 8</ref> . </p><formula xml:id="formula_19">Acc BTDA = k j=1 α j Acc (j) BTDA<label>(18)</label></formula><p>where Acc  In reality, we can obtain Acc BTDA by directly evaluating the DA models on the mixed test set T test j , which leads to the same results in <ref type="bibr" target="#b17">(18)</ref>.</p><p>Based on <ref type="formula" target="#formula_12">(18)</ref>, we also define the RNT metric</p><formula xml:id="formula_20">RN T = Acc BTDA − k j=1 α j Acc j<label>(19)</label></formula><p>where Acc j denotes the j th -target test classification accuracy with respect to a single-target DA classifier trained on the source labeled set S and the j th sub-target unlabeled set T train j . Note that,</p><p>• Acc j is derived from a DA model trained with datasets S and T train j . It means that Acc i , Acc j (i = j) are derived from different DA models, which employ the same DA algorithms yet are trained on T train   Equal-weight ANT, RNT. It worth noting that, though ANT/RNT in (17), <ref type="bibr" target="#b18">(19)</ref> are able to reflect BTDA models' performances on a mixed target domain set, it is not     enough to demonstrate the comprehensive performances of the models over multi-sub-target domains, since it does not equally weight hidden sub-target domains. More specifically, imagine that we have a small set of target images belonging to a hidden sub-target, which the model performs poorly on. Then the RNT metric would shield the model's incapacity on that domain.</p><p>In order to thoroughly reflect the capacities of evaluated models, we additionally report the results when the proportion {α j } k j=1 is equally set. In particular, we tend to con-sider the equal-weight classification accuracy (Acc Acc j <ref type="bibr" target="#b19">(20)</ref> . The metrics developed from <ref type="bibr">(17 18 19)</ref> could be viewed as the complementary of what we report in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Evaluated Baselines in BTDA setup.</head><p>Beyond our AMEAN model, we also reported the BTDA performances from state-of-the-art DA baselines in Digitfive, Office-31, Office-Home. The baselines include Deep Adaptation Network (DAN) <ref type="bibr" target="#b24">[25]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b26">[27]</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b27">[28]</ref>, Generate To Adapt (GTA) <ref type="bibr" target="#b36">[37]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b38">[39]</ref>, Reverse Gradient (RevGrad) <ref type="bibr" target="#b8">[9]</ref> [10], Virtual Adversarial Domain Adaptation (VADA) <ref type="bibr" target="#b37">[38]</ref> and its variant DIRT-T <ref type="bibr" target="#b37">[38]</ref>.</p><p>In the Digit-five experiment, DAN, ADDA, GTA, Re-Grad are all derived from their official codes. To promise a fair comparison, we standardize the backbones by LeNet to report Acc BTDA , Acc (EW) BTDA and the negative transfer effects. VADA and DIRT-T are evaluated by their official codes to provide the results. Their model architectures are consistent with our backbone-2.</p><p>In the Office-31 and Office-Home experiments, we employ the official codes of DAN, RTN, JAN, ReGrad to report Acc BTDA , Acc The codes of all evaluated baselines can be found in their literatures. For a fair comparison, Acc j mainly originates from the reported results in their papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">BTDA experiments by equal-weight evaluation metrics</head><p>The equal-weight versions of the classification accuracy (Acc (EW) BTDA ), absolute negative transfer (AN T EW ) and relative negative transfer RN T EW over all the evaluated baselines in Digit-five, Office-31 and Office-Home are reported in <ref type="table" target="#tab_1">Table 11</ref> <ref type="bibr">-15.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>Blending-target DA (BTDA): Problem Setup Preliminaries. Let's consider an m-class visual recognition problem. Suppose a source dataset includes n s labeled examples S = {(x d + denotes the i th source image lying on a d-dimensional data space and y (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>t-SNE visualizations of the features learned by Source-only, RevGred, VADA and AMEAN on Digit-five in BTDA setup. Shapes and colors indicate different domains and categories, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Ablation studies of our meta-learner across three transfer tasks in Digit-five (Backbone-2) and in Office-31 (AlexNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The ablation of Vmt based on mt→mm,sv,up,sy in Digitfive, where Vmt is constructed in different manners over iterations. AMEAN performs a faster and more stable optimization process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>V</head><label></label><figDesc>joint (D st , D mt , F, C) = V st (F, D st , C) + γV mt (F, D mt ) + βL ent (F, C) + L vir (F, C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>is derived from a DA model trained with S and T train =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>j) are derived from the same DA model, which employ the same DA algorithms and is trained on (S ∪ T train ) and then, tested on T test i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>and quantify the corresponding negative transfer AN T EW and RN T EW in this setup:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>in the Office-31 and Office-Home experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 AMEAN (Stochastic version)Input: Source S; Mixed target T ; feature extractor F ; classifier C; domain discriminators Dst,Dmt; meta-learner U = {U1, U2}. Output: well-trained F * , D * st ,D * mt , C * . 1: while not converged do Xt from T ; Construct {qi,j} k by Eq.2 and {pi,j} k by Eq.3 for each xi in the mini-batch.</figDesc><table><row><cell>2:</cell><cell>Meta-sub-target Discovery (Meta-update):</cell></row><row><cell>3:</cell><cell>Initiate {µj} k j=1 and U ;</cell></row><row><cell>4:</cell><cell>while not converged do</cell></row><row><cell cols="2">5: Sample a mini-batch 6: Update U and {µj} k j=1 in Eq.4 with SGD solver.</cell></row><row><cell>7:</cell><cell>end while</cell></row><row><cell>8:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy (ACC %)on Digit-five in BTDA setup. BLUE, RED indicate the baseline suffer from absolute negative transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.</figDesc><table><row><cell>Models</cell><cell cols="2">mt→mm,sv,up,sy</cell><cell cols="2">mm→mt,sv,up,sy</cell><cell cols="2">sv→mm,mt,up,sy</cell><cell cols="2">sy→mm,mt,sv,up</cell><cell cols="2">up→mm,mt,sv,sy</cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell></row><row><cell>Backbone-1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>26.9</cell><cell>0</cell><cell>56.0</cell><cell>0</cell><cell>67.2</cell><cell>0</cell><cell>73.8</cell><cell>0</cell><cell>36.9</cell><cell>0</cell><cell>52.2</cell><cell>0</cell></row><row><cell>ADDA</cell><cell>43.7</cell><cell>-8.0</cell><cell>55.9 (−0.1)</cell><cell>-3.3</cell><cell>40.4 (−26.8)</cell><cell>-21.7</cell><cell>66.1 (−6.7)</cell><cell>-6.5</cell><cell>34.8 (−0.1)</cell><cell>-13.3</cell><cell>48.2 (−4.0)</cell><cell>-10.5</cell></row><row><cell>DAN</cell><cell>31.3</cell><cell>-7.5</cell><cell>53.1 (−2.9)</cell><cell>-3.1</cell><cell>48.7 (−18.5)</cell><cell>-9.5</cell><cell>63.3 (−10.5)</cell><cell>-3.9</cell><cell>27.0 (−9.9)</cell><cell>-11.0</cell><cell>44.7 (−7.5)</cell><cell>-7.0</cell></row><row><cell>GTA</cell><cell>44.6</cell><cell>-9.2</cell><cell>54.5 (−1.5)</cell><cell>-2.1</cell><cell>60.3 (−6.9)</cell><cell>-3.9</cell><cell>74.5 (+0.7)</cell><cell>-1.1</cell><cell>41.3</cell><cell>-2.0</cell><cell>55.0</cell><cell>-3.7</cell></row><row><cell>RevGrad</cell><cell>52.4</cell><cell>-8.9</cell><cell>64.0</cell><cell>-4.1</cell><cell>65.3 (−1.9)</cell><cell>-4.1</cell><cell>66.6 (−6.2)</cell><cell>-7.5</cell><cell>44.3</cell><cell>-6.3</cell><cell>58.5</cell><cell>-6.2</cell></row><row><cell>AMEAN</cell><cell>56.2 (+3.8)</cell><cell>-</cell><cell>65.2 (+1.2)</cell><cell>-</cell><cell>67.3 (+0.1)</cell><cell>-</cell><cell>71.3 (−2.5)</cell><cell>-</cell><cell>47.5 (+3.2)</cell><cell>-</cell><cell>61.5 (+3.0)</cell><cell>-</cell></row><row><cell>Backbone-2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>47.4</cell><cell>0</cell><cell>58.1</cell><cell>0</cell><cell>73.8</cell><cell>0</cell><cell>74.5</cell><cell>0</cell><cell>50.6</cell><cell>0</cell><cell>60.8</cell><cell>0</cell></row><row><cell>VADA</cell><cell>76.0</cell><cell>-5.3</cell><cell>72.3</cell><cell>-2.3</cell><cell>75.6</cell><cell>-2.5</cell><cell>81.3</cell><cell>-3.8</cell><cell>56.4</cell><cell>-8.7</cell><cell>72.3</cell><cell>-4.5</cell></row><row><cell>DIRT-T</cell><cell>73.5</cell><cell>-7.1</cell><cell>76.1</cell><cell>-1.5</cell><cell>75.9</cell><cell>-5.5</cell><cell>78.5</cell><cell>-3.1</cell><cell>47.0</cell><cell>-7.5</cell><cell>70.2</cell><cell>-5.0</cell></row><row><cell>AMEAN</cell><cell>85.1 (+9.1)</cell><cell>-</cell><cell>77.6 (+1.5)</cell><cell>-</cell><cell>77.4 (+1.5)</cell><cell>-</cell><cell>84.1 (+2.8)</cell><cell>-</cell><cell>75.5 (+19.1)</cell><cell>-</cell><cell>80.0 (+7.7)</cell><cell>-</cell></row><row><cell cols="5">switching the optimization objectives between</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>max Dst,Dmt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy (ACC %) on Office-31 in BTDA setup. BLUE, RED indicate the baseline suffer from absolute negative transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell>A→D,W ACC ANT</cell><cell>RNT</cell><cell>D→A,W ACC ANT</cell><cell>RNT</cell><cell>W→A,D ACC ANT</cell><cell>RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>62.4</cell><cell>0</cell><cell>60.8</cell><cell>0</cell><cell>57.2</cell><cell>0</cell><cell>60.1</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>68.2</cell><cell>-0.2</cell><cell>58.7 (−2.1)</cell><cell>-5.0</cell><cell>55.6 (−1.6)</cell><cell>-4.9</cell><cell>60.8</cell><cell>-3.4</cell></row><row><cell>AlexNet</cell><cell>RTN JAN</cell><cell>71.6 73.7</cell><cell>-1.1 -0.3</cell><cell>56.3 (−4.5) 62.1</cell><cell>-4.6 -4.9</cell><cell>52.2 (−5.0) 58.4</cell><cell>-6.1 -3.6</cell><cell>59.9 (−0.2) 64.7</cell><cell>-4.1 -3.0</cell></row><row><cell></cell><cell>RevGrad</cell><cell>74.1</cell><cell>+0.9</cell><cell>58.6 (−2.2)</cell><cell>-4.3</cell><cell>55.0 (−2.2)</cell><cell>-3.4</cell><cell>62.6</cell><cell>-2.2</cell></row><row><cell></cell><cell cols="2">AMEAN (ours) 74.5 (+0.4)</cell><cell>-</cell><cell>62.8 (+0.7)</cell><cell>-</cell><cell>59.7 (+1.3)</cell><cell>-</cell><cell>65.7 (+1.0)</cell><cell>-</cell></row><row><cell></cell><cell>Source only</cell><cell>68.6</cell><cell>0</cell><cell>70.0</cell><cell>0</cell><cell>66.5</cell><cell>0</cell><cell>68.4</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>78.0</cell><cell>-2.1</cell><cell>64.4 (−5.6)</cell><cell>-6.8</cell><cell>66.7</cell><cell>-1.8</cell><cell>69.7</cell><cell>-3.6</cell></row><row><cell>ResNet-50</cell><cell>RTN JAN</cell><cell>84.3 84.2</cell><cell>+2.3 -1.2</cell><cell>67.5 (−2.5) 74.4</cell><cell>-5.5 -0.8</cell><cell>64.8 (−0.2) 72.0</cell><cell>-5.5 -2.8</cell><cell>72.2 76.9</cell><cell>-2.9 -1.6</cell></row><row><cell></cell><cell>RevGrad</cell><cell>78.2</cell><cell>-3.3</cell><cell>72.2</cell><cell>-2.7</cell><cell>69.8</cell><cell>-2.8</cell><cell>73.4</cell><cell>-2.9</cell></row><row><cell></cell><cell cols="2">AMEAN (ours) 90.1 (+5.8)</cell><cell>-</cell><cell>77.0 (+2.6)</cell><cell>-</cell><cell>73.4 (+1.4)</cell><cell>-</cell><cell>80.2 (+3.4)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy (ACC %) on Office-Home in BTDA setup. BLUE, RED indicate the baseline suffer from absolute negative transfer (ANT%) or relative negative transfer (RNT%), respectively. Best viewed in color.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell cols="2">Ar→Cl,Pr,Rw ACC ANT RNT</cell><cell cols="2">Cl→Ar,Pr,Rw ACC ANT RNT</cell><cell cols="2">Pr→Ar,Cl,Rw ACC ANT RNT</cell><cell cols="2">Rw→Ar,Cl,Pr ACC ANT RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>33.4</cell><cell>0</cell><cell>37.6</cell><cell>0</cell><cell>32.4</cell><cell>0</cell><cell>39.3</cell><cell>0</cell><cell>35.7</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>39.7</cell><cell>-3.7</cell><cell>43.2</cell><cell>-3.0</cell><cell>39.4</cell><cell>-3.4</cell><cell>47.8</cell><cell>-2.2</cell><cell>42.5</cell><cell>-3.1</cell></row><row><cell>AlexNet</cell><cell>RTN JAN</cell><cell>42.8 43.5</cell><cell>-2.0 -2.9</cell><cell>45.2 46.5</cell><cell>-2.5 -3.6</cell><cell>40.6 40.9</cell><cell>-2.4 -6.6</cell><cell>49.6 49.1</cell><cell>-2.7 -2.1</cell><cell>44.6 45.0</cell><cell>-2.3 -4.6</cell></row><row><cell></cell><cell>RevGrad</cell><cell>42.1</cell><cell>-3.3</cell><cell>45.1</cell><cell>-4.4</cell><cell>41.1</cell><cell>-4.5</cell><cell>48.4</cell><cell>-5.6</cell><cell>44.2</cell><cell>-4.4</cell></row><row><cell></cell><cell cols="2">AMEAN (ours) 44.6 (+1.1)</cell><cell>-</cell><cell>47.6 (+1.1)</cell><cell>-</cell><cell>42.8 (+1.7)</cell><cell>-</cell><cell>50.2 (+1.1)</cell><cell></cell><cell>46.3 (+1.3 )</cell><cell>-</cell></row><row><cell></cell><cell>Source only</cell><cell>47.6</cell><cell>0</cell><cell>42.6</cell><cell>0</cell><cell>44.2</cell><cell>0</cell><cell>51.3</cell><cell>0</cell><cell>46.4</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>55.6</cell><cell>-0.8</cell><cell>56.6</cell><cell>+0.9</cell><cell>48.5</cell><cell>-5.1</cell><cell>56.7</cell><cell>-6.3</cell><cell>54.4</cell><cell>-2.6</cell></row><row><cell>ResNet-50</cell><cell>RTN JAN</cell><cell>53.9 58.3</cell><cell>-1.8 -0.4</cell><cell>56.7 60.5</cell><cell>-1.3 +2.3</cell><cell>47.3 52.2</cell><cell>-3.8 -2.2</cell><cell>51.6 57.5</cell><cell>-2.8 -7.0</cell><cell>52.4 57.1</cell><cell>-2.6 -1.9</cell></row><row><cell></cell><cell>RevGrad</cell><cell>58.4</cell><cell>-3.1</cell><cell>58.1</cell><cell>-2.2</cell><cell>52.9</cell><cell>-4.5</cell><cell>62.1</cell><cell>-3.0</cell><cell>57.9</cell><cell>-3.2</cell></row><row><cell></cell><cell cols="2">AMEAN (ours) 64.3 (+5.9)</cell><cell>-</cell><cell>65.5 (+5.0)</cell><cell>-</cell><cell>59.5 (+6.1)</cell><cell>-</cell><cell>66.7 (+4.6)</cell><cell></cell><cell>64.0 (+6.1)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The ablation of Vmt constructed by Explicit Sub-Target (EST), deep k-Clustering (k-C) and AMEAN (ours).</figDesc><table><row><cell></cell><cell cols="4">mt→mm,sv,up,sy mm→mt,sv,up,sy D→A,W W→A,D</cell></row><row><cell>EST</cell><cell>79.9</cell><cell>74.7</cell><cell>62.2</cell><cell>58.4</cell></row><row><cell>k-C</cell><cell>81.6</cell><cell>74.5</cell><cell>61.3</cell><cell>59.1</cell></row><row><cell>ours</cell><cell>85.1</cell><cell>77.6</cell><cell>62.8</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The architecture of our unsupervsied meta-learner.</figDesc><table><row><cell cols="2">Input size</cell><cell cols="2">Output size Activator</cell></row><row><cell>Encoder:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>En Fc 1</cell><cell>image size</cell><cell>500</cell><cell>ReLU</cell></row><row><cell>En Fc 2</cell><cell>500</cell><cell>1000</cell><cell>ReLU</cell></row><row><cell>En Fc 3</cell><cell>1000</cell><cell>k</cell><cell>ReLU</cell></row><row><cell>Decoder:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>De Fc 1</cell><cell>k</cell><cell>1000</cell><cell>ReLU</cell></row><row><cell>De Fc 2</cell><cell>1000</cell><cell>1000</cell><cell>ReLU</cell></row><row><cell>De Fc 3</cell><cell>1000</cell><cell>image-size</cell><cell>ReLU</cell></row><row><cell>7. Appendix.A</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">7.1. Unsupervised Meta-learner</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Backbone-1 in digit-five experiment.</figDesc><table><row><cell></cell><cell cols="2">Kernel size</cell><cell>Output dimension</cell><cell cols="2">BN/IN</cell><cell>Activation</cell><cell>Dropout</cell></row><row><cell>Feature extractor:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv1 1</cell><cell>5*5</cell><cell></cell><cell>64*24*24</cell><cell>BN</cell><cell></cell><cell>ReLU</cell><cell>0</cell></row><row><cell>Maxpool</cell><cell>2*2</cell><cell></cell><cell>64*12*12</cell><cell></cell><cell></cell><cell>0.5</cell></row><row><cell>Conv1 2</cell><cell>5*5</cell><cell></cell><cell>50*8*8</cell><cell>BN</cell><cell></cell><cell>ReLU</cell><cell>0</cell></row><row><cell>Maxpool</cell><cell>2*2</cell><cell></cell><cell>50*4*4</cell><cell></cell><cell></cell><cell>0.5</cell></row><row><cell>Classifier:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fc 1</cell><cell>50*4*4</cell><cell></cell><cell>100</cell><cell>BN</cell><cell></cell><cell>ReLU</cell><cell>0.5</cell></row><row><cell>Fc 2</cell><cell>100</cell><cell></cell><cell>100</cell><cell>BN</cell><cell></cell><cell>ReLU</cell><cell>0</cell></row><row><cell>Fc 3</cell><cell>100</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>0</cell></row><row><cell>Discriminator:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reversed gradient layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fc</cell><cell>50*4*4</cell><cell></cell><cell>100</cell><cell>BN</cell><cell></cell><cell>ReLU</cell><cell>0</cell></row><row><cell>Fc (Dst)</cell><cell>100</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>0</cell></row><row><cell>Fc (Dmt)</cell><cell>100</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>0</cell></row><row><cell></cell><cell cols="5">Table 7. Backbone-2 in digit-five experiment.</cell></row><row><cell></cell><cell>Kernel size</cell><cell cols="2">Output dimension</cell><cell>BN/IN</cell><cell></cell><cell>Activation</cell><cell>Dropout</cell></row><row><cell>Feature extractor:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv1 1</cell><cell>3*3</cell><cell></cell><cell>64*32*32</cell><cell>IN/BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv1 2</cell><cell>3*3</cell><cell></cell><cell>64*32*32</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv1 3</cell><cell>3*3</cell><cell></cell><cell>64*32*32</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0.5</cell></row><row><cell>Maxpool</cell><cell>2*2</cell><cell></cell><cell>64*16*16</cell><cell></cell><cell></cell></row><row><cell>Conv2 1</cell><cell>3*3</cell><cell></cell><cell>64*16*16</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv2 2</cell><cell>3*3</cell><cell></cell><cell>64*16*16</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv2 3</cell><cell>3*3</cell><cell></cell><cell>64*16*16</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0.5</cell></row><row><cell>Maxpool</cell><cell>2*2</cell><cell></cell><cell>64*8*8</cell><cell></cell><cell></cell></row><row><cell>Classifier:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv2 1</cell><cell>3*3</cell><cell></cell><cell>64*8*8</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv2 2</cell><cell>3*3</cell><cell></cell><cell>64*8*8</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Conv2 3</cell><cell>3*3</cell><cell></cell><cell>64*8*8</cell><cell>BN</cell><cell cols="2">LeakyReLU(0.1)</cell><cell>0</cell></row><row><cell>Averagepool</cell><cell></cell><cell></cell><cell>64*1*1</cell><cell></cell><cell></cell></row><row><cell>Fc</cell><cell>64*10</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>0</cell></row><row><cell>Discriminator:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fc</cell><cell>64*8*8+10</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell>ReLU</cell><cell>0</cell></row><row><cell>Fc (Dst)</cell><cell>100*1</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>Sigmoid</cell><cell>0</cell></row><row><cell>Fc (Dmt)</cell><cell>100*4</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .Table 10 .</head><label>810</label><figDesc>The hyper-parameters setting in our experiment. Acc is the average accuracy over five sub-transfer tasks in Digit-five. The number of sub-targets in Digit-five is 4. Acc BTDA − Acc Source−only }(17)where Acc Source−only denotes the classification accuracy about the model trained on the source labeled dataset S and tested on the mixed target set T test =</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Office-31, Office-HOME</cell><cell>Digit-five</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AlextNet</cell><cell></cell><cell>ResNet-50</cell><cell>Backbone-1</cell><cell>Backbone-2</cell></row><row><cell></cell><cell></cell><cell cols="3">mini-batch size</cell><cell>32</cell><cell></cell><cell>32</cell><cell>128</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>λ</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1 (update Dst) / 0.01 (update F )</cell></row><row><cell></cell><cell></cell><cell>γ</cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell>iter max i ter</cell><cell>0.1</cell><cell>iter max i ter</cell></row><row><cell></cell><cell></cell><cell>β</cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell>0.1</cell><cell>0</cell><cell>0.01</cell></row><row><cell></cell><cell></cell><cell>ρ</cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0.01</cell></row><row><cell></cell><cell></cell><cell>M</cell><cell></cell><cell></cell><cell>2000</cell><cell></cell><cell>2000</cell><cell>20000</cell><cell>10000</cell></row><row><cell></cell><cell></cell><cell cols="2">image size</cell><cell></cell><cell>227×227</cell><cell></cell><cell>227×227</cell><cell>28×28</cell><cell>28×28</cell></row><row><cell></cell><cell cols="6">Table 9. Some ablation of entropy term.</cell><cell></cell></row><row><cell></cell><cell cols="7">mt→mm,sv,up,sy mm→mt,sv,up,sy D→A,W W→A,D</cell></row><row><cell>w</cell><cell>85.1</cell><cell></cell><cell></cell><cell>77.6</cell><cell cols="2">62.8</cell><cell>59.7</cell></row><row><cell>w/o</cell><cell>83.7</cell><cell></cell><cell></cell><cell>76.9</cell><cell cols="2">62.6</cell><cell>59.2</cell></row><row><cell>k</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell cols="4">Acc 79.3 83.2 83.7</cell><cell cols="4">82.1 83.2 82.0 78.6</cell></row><row><cell cols="2">8. Appendix.B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">8.1. Evalutation Metrics for BTDA</cell><cell></cell><cell></cell></row><row><cell cols="8">We elaborate how to calculate ANT and RNT in our ex-</cell></row><row><cell cols="3">periment in details:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">AN T = max{0, k ∪ j=1 T test j</cell><cell>; Acc BTDA</cell></row><row><cell cols="8">denotes the multi-target-weighted classification accuracy of</cell></row><row><cell cols="6">the evaluated DA model under BTDA setup:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>BTDA denotes the DA model classification accuracy on the j th sub-target domain test set T test j when the evaluated DA models (e.g., JAN, DAN, AMEAN, etc) is trained with the source labeled set S and the mixed target unlabeled set T train = j } k j=1 denotes the proportion of the multi-target mixture. It is derived from the domain-set proportion in benchmarks, which are valued by {0.236, 0.236, 0.236, 0.236, 0.056}, {0.686, 0.121, 0.193} and {0.155, 0.280, 0.285, 0.280} in Digit-five, Office-31 and Office-Home. When we draw the subset of domains to construct the mixed target, {α j } k j=1 is obtained by normalizing these corresponding benchmarkspecific domain-set proportion 2 .</figDesc><table><row><cell>j)</cell><cell></cell></row><row><cell>k ∪ j=1 T train j</cell><cell>(BTDA setup).</cell></row></table><note>{α</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>The equal-weight (EW) Classification accuracy (ACC %), absolute negative transfer (ANT%) and relative negative transfer (RNT%) on Digit-five in BTDA setup. BLUE, RED indicate ANT and RNT, respectively. More viewed in<ref type="bibr" target="#b19">(20)</ref>.</figDesc><table><row><cell>Models</cell><cell cols="2">mt→mm,sv,up,sy</cell><cell cols="2">mm→mt,sv,up,sy</cell><cell cols="2">sv→mm,mt,up,sy</cell><cell cols="2">sy→mm,mt,sv,up</cell><cell cols="2">up→mm,mt,sv,sy</cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell><cell>ACC ANT</cell><cell>RNT</cell></row><row><cell>Backbone-1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>36.6</cell><cell>0</cell><cell>57.3</cell><cell>0</cell><cell>67.1</cell><cell>0</cell><cell>74.9</cell><cell>0</cell><cell>36.9</cell><cell>0</cell><cell>54.6</cell><cell>0</cell></row><row><cell>ADDA</cell><cell>52.5</cell><cell>-7.4</cell><cell>58.9</cell><cell>-1.2</cell><cell>46.4 (−20.7)</cell><cell>-16.0</cell><cell>67.0 (−7.9)</cell><cell>-7.0</cell><cell>34.8 (−2.1)</cell><cell>-13.3</cell><cell>51.9 (−2.7)</cell><cell>-9.0</cell></row><row><cell>DAN</cell><cell>38.8</cell><cell>-8.6</cell><cell>53.5 (−3.8)</cell><cell>-4.5</cell><cell>55.1 (−12.0)</cell><cell>-3.0</cell><cell>65.8 (−9.1)</cell><cell>-2.8</cell><cell>27.0 (−9.9)</cell><cell>-11.0</cell><cell>48.0 (−6.6)</cell><cell>-6.0</cell></row><row><cell>GTA</cell><cell>51.4</cell><cell>-9.0</cell><cell>54.2 (−3.1)</cell><cell>-2.1</cell><cell>59.8 (−7.3)</cell><cell>-3.6</cell><cell>76.2(+1.3)</cell><cell>-0.6</cell><cell>41.3</cell><cell>-2.0</cell><cell>56.6</cell><cell>-3.6</cell></row><row><cell>RevGrad</cell><cell>60.2</cell><cell>-6.2</cell><cell>66.0</cell><cell>-4.6</cell><cell>64.7 (−2.3)</cell><cell>-6.0</cell><cell>69.2 (−5.7)</cell><cell>-7.1</cell><cell>44.3</cell><cell>-6.3</cell><cell>60.9</cell><cell>-6.0</cell></row><row><cell>AMEAN</cell><cell>61.2 (+1.0)</cell><cell>-</cell><cell>66.9 (+0.9)</cell><cell>-</cell><cell>67.2 (+0.1)</cell><cell>-</cell><cell>73.3 (−1.6)</cell><cell>-</cell><cell>47.5 (+3.2)</cell><cell>-</cell><cell>63.2 (+2.3)</cell><cell>-</cell></row><row><cell>Backbone-2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>55.8</cell><cell>0</cell><cell>55.2</cell><cell>0</cell><cell>74.3</cell><cell>0</cell><cell>76.4</cell><cell>0</cell><cell>50.6</cell><cell>0</cell><cell>62.5</cell><cell>0</cell></row><row><cell>VADA</cell><cell>79.4</cell><cell>-4.9</cell><cell>72.5</cell><cell>-3.1</cell><cell>76.4</cell><cell>-2.2</cell><cell>82.8</cell><cell>-3.8</cell><cell>56.4</cell><cell>-8.7</cell><cell>73.5</cell><cell>-4.5</cell></row><row><cell>DIRT-T</cell><cell>77.5</cell><cell>-6.5</cell><cell>76.8</cell><cell>-4.4</cell><cell>79.7 (+1.8)</cell><cell>-4.9</cell><cell>80.9</cell><cell>-3.9</cell><cell>47.0</cell><cell>-7.5</cell><cell>72.4</cell><cell>-5.5</cell></row><row><cell>AMEAN</cell><cell>86.9 (+7.5)</cell><cell>-</cell><cell>78.5 (+1.7)</cell><cell>-</cell><cell>77.9</cell><cell>-</cell><cell>85.6 (+2.8)</cell><cell>-</cell><cell>75.5 (+19.1)</cell><cell>-</cell><cell>80.9 (+7.4)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>The equal-weight (EW) Classification accuracy (ACC %), absolute negative transfer (ANT%) and relative negative transfer (RNT%) on Office31 in BTDA setup. BLUE, RED indicate ANT and RNT, respectively. More viewed in<ref type="bibr" target="#b19">(20)</ref>.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell>A→D,W ACC ANT</cell><cell>RNT</cell><cell>D→A,W ACC ANT</cell><cell>RNT</cell><cell>W→A,D ACC ANT</cell><cell>RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>62.7</cell><cell>0</cell><cell>73.3</cell><cell>0</cell><cell>74.4</cell><cell>0</cell><cell>70.1</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>68.2</cell><cell>0.0</cell><cell>71.4 (−1.9)</cell><cell>-4.0</cell><cell>73.2 (−1.2)</cell><cell>-3.3</cell><cell>70.9</cell><cell>-2.4</cell></row><row><cell>AlexNet</cell><cell>RTN JAN</cell><cell>70.7 73.5</cell><cell>-1.7 -0.1</cell><cell>69.8 (−3.5) 73.6</cell><cell>-4.1 -4.1</cell><cell>71.5 (−2.9) 75.0</cell><cell>-3.9 -2.5</cell><cell>70.7 74.0</cell><cell>-3.2 -2.2</cell></row><row><cell></cell><cell>RevGrad</cell><cell>74.1</cell><cell>1.0</cell><cell>72.1 (−1.2)</cell><cell>-2.8</cell><cell>73.4 (−1.0)</cell><cell>-1.8</cell><cell>73.2</cell><cell>-1.2</cell></row><row><cell></cell><cell>AMEAN (ours)</cell><cell>74.9 (+0.8)</cell><cell>-</cell><cell>74.9 (+1.3)</cell><cell>-</cell><cell>76.2 (+1.2)</cell><cell>-</cell><cell>75.3 (+1.3)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .</head><label>13</label><figDesc>The equal-weight (EW) Classification accuracy (ACC %), absolute negative transfer (ANT%) and relative negative transfer (RNT%) on Office31 in BTDA setup. BLUE, RED indicate ANT and RNT, respectively. More viewed in<ref type="bibr" target="#b19">(20)</ref>.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell>A→D,W ACC ANT</cell><cell>RNT</cell><cell>D→A,W ACC ANT</cell><cell>RNT</cell><cell>W→A,D ACC ANT</cell><cell>RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>68.7</cell><cell>0</cell><cell>79.6</cell><cell>0</cell><cell>80.0</cell><cell>0</cell><cell>76.1</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>77.9</cell><cell>-2.0</cell><cell>75.0 (−4.6)</cell><cell>-5.0</cell><cell>80.0</cell><cell>-1.3</cell><cell>77.6</cell><cell>-3.0</cell></row><row><cell>ResNet-50</cell><cell>RTN JAN</cell><cell>84.1 84.6</cell><cell>+2.9 -0.8</cell><cell>77.2 (−2.4) 82.7</cell><cell>-4.4 -0.6</cell><cell>79.0 (−1.0) 83.4</cell><cell>-3.3 -1.8</cell><cell>80.1 83.6</cell><cell>-1.6 -1.0</cell></row><row><cell></cell><cell>RevGrad</cell><cell>79.0</cell><cell>-2.3</cell><cell>81.4</cell><cell>-1.5</cell><cell>82.3</cell><cell>-1.3</cell><cell>80.9</cell><cell>-1.7</cell></row><row><cell></cell><cell>AMEAN (ours)</cell><cell>89.8 (+5.2)</cell><cell>-</cell><cell>84.6 (+1.9)</cell><cell>-</cell><cell>84.3 (+0.9)</cell><cell>-</cell><cell>86.2 (+2.6)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>The equal-weight (EW) Classification accuracy (ACC %), absolute negative transfer (ANT%) and relative negative transfer (RNT%) on OfficeHome in BTDA setup. BLUE, RED indicate ANT and RNT, respectively. More viewed in<ref type="bibr" target="#b19">(20)</ref>.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell cols="2">Ar→Cl,Pr,Rw ACC ANT RNT</cell><cell cols="2">Cl→Ar,Pr,Rw ACC ANT RNT</cell><cell cols="2">Pr→Ar,Cl,Rw ACC ANT RNT</cell><cell cols="2">Rw→Ar,Cl,Pr ACC ANT RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>33.4</cell><cell>0</cell><cell>35.3</cell><cell>0</cell><cell>30.6</cell><cell>0</cell><cell>37.9</cell><cell>0</cell><cell>34.3</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>39.7</cell><cell>-3.6</cell><cell>41.6</cell><cell>-2.8</cell><cell>37.8</cell><cell>-3.1</cell><cell>46.8</cell><cell>-2.4</cell><cell>41.5</cell><cell>-3.0</cell></row><row><cell>AlexNet</cell><cell>RTN JAN</cell><cell>42.8 43.5</cell><cell>-2.0 -2.9</cell><cell>43.4 44.6</cell><cell>-2.4 -3.5</cell><cell>39.1 39.4</cell><cell>-2.1 -5.2</cell><cell>48.8 48.5</cell><cell>-2.5 -5.2</cell><cell>43.5 44.0</cell><cell>-2.2 -4.2</cell></row><row><cell></cell><cell>RevGrad</cell><cell>42.2</cell><cell>-3.3</cell><cell>43.8</cell><cell>-3.5</cell><cell>39.9</cell><cell>-3.6</cell><cell>47.7</cell><cell>-5.0</cell><cell>43.4</cell><cell>-3.9</cell></row><row><cell></cell><cell cols="2">AMEAN (ours) 44.6 (+1.1)</cell><cell>-</cell><cell>45.6 (+1.0)</cell><cell>-</cell><cell>41.4 (+1.5)</cell><cell>-</cell><cell>49.3 (+0.5)</cell><cell></cell><cell>45.2 (+1.2)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>The equal-weight (EW) Classification accuracy (ACC %), absolute negative transfer (ANT%) and relative negative transfer (RNT%) on OfficeHome in BTDA setup. BLUE, RED indicate ANT and RNT, respectively. More viewed in<ref type="bibr" target="#b19">(20)</ref>.</figDesc><table><row><cell>Backbones</cell><cell>Models</cell><cell cols="2">Ar→Cl,Pr,Rw ACC ANT RNT</cell><cell cols="2">Cl→Ar,Pr,Rw ACC ANT RNT</cell><cell cols="2">Pr→Ar,Cl,Rw ACC ANT RNT</cell><cell cols="2">Rw→Ar,Cl,Pr ACC ANT RNT</cell><cell>Avg ACC ANT</cell><cell>RNT</cell></row><row><cell></cell><cell>Source only</cell><cell>47.6</cell><cell>0</cell><cell>41.8</cell><cell>0</cell><cell>43.4</cell><cell>0</cell><cell>51.7</cell><cell>0</cell><cell>46.1</cell><cell>0</cell></row><row><cell></cell><cell>DAN</cell><cell>55.6</cell><cell>-0.5</cell><cell>55.1</cell><cell>+0.9</cell><cell>47.8</cell><cell>-4.0</cell><cell>56.6</cell><cell>-6.3</cell><cell>53.8</cell><cell>-2.5</cell></row><row><cell>ResNet-50</cell><cell>RTN JAN</cell><cell>53.9 58.3</cell><cell>-1.8 -0.4</cell><cell>55.4 59.2</cell><cell>-0.7 +2.1</cell><cell>47.2 51.9</cell><cell>-3.3 -1.2</cell><cell>51.8 57.8</cell><cell>-3.0 -6.1</cell><cell>52.1 56.8</cell><cell>-2.2 -1.5</cell></row><row><cell></cell><cell>RevGrad</cell><cell>58.4</cell><cell>-3.0</cell><cell>57.0</cell><cell>-2.2</cell><cell>52.2</cell><cell>-4.6</cell><cell>62.0</cell><cell>-3.2</cell><cell>57.4</cell><cell>-3.2</cell></row><row><cell></cell><cell>AMEAN (ours)</cell><cell>64.3 (+5.9)</cell><cell>-</cell><cell>64.2 (+5.0)</cell><cell>-</cell><cell>59.0 (+6.8)</cell><cell>-</cell><cell>66.4 (+4.4)</cell><cell></cell><cell>63.5 (+6.1)</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The numbers are based on the hidden sub-target test set proportions in a mixed target</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation: An information theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unbiasing semantic segmentation for robot perception using synthetic data feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Balloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<title level="m">Unsupervised domain adaptation by backpropagation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02476</idno>
		<title level="m">Fine-grained recognition in the wild: A multi-task domain adaptation approach</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dowlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wernsing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Covariate shift by kernel mean matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI-17)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06636</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01386</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rothörl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04286</idno>
		<title level="m">Sim-to-real robot learning from pixels with progressive nets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Asymmetric tritraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1704.01705</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05464</idno>
		<title level="m">Adversarial discriminative domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Autoloss: Learning discrete schedules for alternate optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
		<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Real-to-virtual domain unification for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Virtual to real reinforcement learning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-target unsupervised domain adaptation without exactly shared categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multiple source domain adaptation with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
