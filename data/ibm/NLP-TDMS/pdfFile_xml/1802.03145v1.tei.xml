<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Autoencoder for Feature Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxue</forename><surname>Meng</surname></persName>
							<email>qinxue.meng@uts.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Catchpoole</surname></persName>
							<email>daniel.catchpoole@health.nsw.gov.au</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Children&apos;s Cancer Research Unit</orgName>
								<orgName type="institution">The Children&apos;s Hospital at Westmead</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Skillicorn</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Queen&apos;s University at Kingston</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
							<email>paul.kennedy@uts.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Autoencoder for Feature Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature extraction becomes increasingly important as data grows high dimensional. Autoencoder as a neural network based feature extraction method achieves great success in generating abstract features of high dimensional data. However, it fails to consider the relationships of data samples which may affect experimental results of using original and new features. In this paper, we propose a Relation Autoencoder model considering both data features and their relationships. We also extend it to work with other major autoencoder models including Sparse Autoencoder, Denoising Autoencoder and Variational Autoencoder. The proposed relational autoencoder models are evaluated on a set of benchmark datasets and the experimental results show that considering data relationships can generate more robust features which achieve lower construction loss and then lower error rate in further classification compared to the other variants of autoencoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As data becomes increasingly high-dimensional such as genomic information, images, videos and text, reducing dimensionality to generate a high-level representation is considered not only as an important but also necessary data preprocessing step. This is because although machine learning models should, theoretically, be able to work on any number of features, high-dimensional datasets always bring about a series of problems including over-fitting, high-computational complexity and overly complicated models, which gives rise to a well known issue -curse of dimensionality <ref type="bibr" target="#b0">[1]</ref>. Another reason for reducing dimensionality is that high-level representations can help people better understand the intrinsic structure of data.</p><p>Various methods of dimensionality reduction <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> have been proposed and they can be roughly divided into two categories: feature selection and feature extraction. The major difference between them lies in using part or all of input features. For example, considering a given dataset X with a feature set F , feature selection finds a subset of features D s from all features F ( D s âŠ‚ F ) and the number of selected features is smaller than the original ( |D s | |F |) while feature extraction generates a new set of features D e which are combinations of the original ones F . Generally new features are different from original features ( D e F ) and the number of new features, in most cases, is smaller than original features ( |D e | |F |). Although feature selection methods, such as Subset Selection <ref type="bibr" target="#b6">[7]</ref> and Random Forest <ref type="bibr" target="#b7">[8]</ref>, are effective in filtering out unnecessary features, if all input features contribute to final results to some extent, feature selection is more likely to give rise to information loss than feature extraction because it fails to use all of them. Therefore, the research focus of dimensionality reduction has been moving towards feature extraction, though not totally.</p><p>The initial feature extraction methods <ref type="bibr" target="#b8">[9]</ref> are proposed based on projection, mapping input features in the original high-dimensional space to a new low-dimensional space by minimizing information loss. The most famous projection methods are Principal Component Analysis (PCA) <ref type="bibr" target="#b9">[10]</ref> and Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b10">[11]</ref>. The former one is an unsupervised method, projecting original data into its principal directions by maximizing variance. The latter one is a supervised method to find a linear subspace by optimizing discriminating data from classes. The major drawback of these methods is that they do projection linearly. Subsequent studies <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> overcome this issue by employing non-linear kernel functions.</p><p>However, projecting from a high-dimensional space to a low-dimensional one is hard to maintain relationships among data samples which gives rise to change the relationship among data samples so as to generate different results of using original and new features. Therefore, recently, exploiting data relationships has been a major focus of dimensionality reduction research. Multidimensional Scaling (MDS) <ref type="bibr" target="#b16">[17]</ref> considers the relationship of data samples by transforming distances into similarities for visualization. ISOMAP <ref type="bibr" target="#b17">[18]</ref> learns low-dimensional features by retaining the geodesic distances among data samples. Locally Linear Embedding (LLE) <ref type="bibr" target="#b18">[19]</ref> preserves data relationships by embedding local neighbourhood when mapping to low-dimensional space. Laplacian Eigenmaps (LE) <ref type="bibr" target="#b19">[20]</ref> minimizes the pairwise distances in the projected space by weighting the corresponding distances in the original space. One issue of these methods is that they generally have a fixed or predefined way of capturing local data relationships in the high-dimensional space which may not be accurate and valid in a low-dimensional space. Another issue is that the major work of these studies maps data from high-dimensional space to low-dimensional space by extracting features once instead of stacking them to gradually generate deeper levels of representation.</p><p>Autoencoders <ref type="bibr" target="#b20">[21]</ref> use artificial neural networks to reduce dimensionality by minimizing reconstruction loss. Thus, it is easy to stack by adding hidden layers. Because of this, au-toencoders and extensions such as Sparse Autoencoders <ref type="bibr" target="#b21">[22]</ref>, Denoising Autoencoders <ref type="bibr" target="#b22">[23]</ref>, Contractive Autoencoders <ref type="bibr" target="#b23">[24]</ref> and Variational Autoencoders <ref type="bibr" target="#b24">[25]</ref>, demonstrate a promising ability to extract meaningful features, especially in image processing and natural language processing. Yet, these methods only consider data reconstruction and ignore to explicitly model its relationship. A recent study <ref type="bibr" target="#b25">[26]</ref> proposes a Generalized Autoencoder (GAE) which focuses on modifying autoencoder to consider data relationship by adding a weighted relational function. Specifically, it minimizes the weighted distances between reconstructed instances and the original ones. Although subsequent applications <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> confirm that considering data relationship can improve the performance of autoencoder in dimensionality reduction, the Generalized Autoencoder model has some drawbacks. Firstly, determining the weights for each pair-wise distance is very challenging and this practice is risky in converting GAE into a supervised model when some relationshps are over emphasized and others are neglected. Meanwhile focusing on minimizing the loss of data relationships but ignoring the loss of data itself is very likely to lose information. For example, some data attributes contribute more to data relationships than the other. Focusing on reconstructing data relationships may emphasize part of attributes and ignores the others.</p><p>To deal with above issues, we proposed a Relation Autoencoder (RAE) for dimensionality reduction and the contributions are summarized as 1) RAE considers both data features and their relationships by minimizing the loss of them. 2) To alleviate the increased computational complex of considering data relationships, the proposed Relational Autoencoder model can filter out weak and trivial relationships by adding activation functions.</p><p>3) The relationship model has been extended to work with other baseline autoencoder models including Sparse Autoencoder (SAE), Denoising Autoencoder (DAE) and Variational Autoencoder (VAE). In this paper, we comprehensively evaluate the proposed Relational Autoencoder on a set of benchmark datasets. Specifically, in the experiment, we firstly compare the performance of our proposed model with another recently published relationship-based autoencoder model (GAE). Then we compare the performance of these baseline autoencoder models with their extended versions.</p><p>The rest of the paper starts from reviewing related work of autoencoders in Section II followed by problem definition and basic autoencoders in Section III. Section IV presents the proposed Relational Autoencoder model and its extensions of the other baseline autoencoder models. The experimental datasets and results are covered in Section V and Section VI respectively. Finally, we discuss the proposed method and conclude the paper in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head><p>Autoencoder was initially introduced in the later 1980s <ref type="bibr" target="#b32">[33]</ref> as a linear feature extraction method. Unlike latent space approaches which map data into a high dimensional space, autoencoder aims to learn a simpler representation of data by mapping the original data into a low-dimensional space. The main principle of autoencoder follows from the name. "Auto" presents that this method is unsupervised and "encoder" means it learns another representation of data. Specifically, autoencoder learns a encoded representation by minimizing the loss between the original data and the data decoded from this representation. In 1989, Baldi and Hornik <ref type="bibr" target="#b33">[34]</ref> investigated the relationship between a one-layer autoencoder and principal component analysis (PCA). They found that the new compressed features learnt by linear autoencoder is similar to principal components. However, the computational complexity of training an autoencoder is much higher than PCA because the major computational complexity of PCA is based on matrix decomposition. This limits the application of linear autoencoders.</p><p>Later, with the involvement of non-linear activation functions, autoencoder becomes non-linear and is capable of learning more useful features <ref type="bibr" target="#b34">[35]</ref> than linear feature extraction methods. Non-linear autoencoders are not advantaged than the other non-linear feature extraction methods as it takes long time to train them.</p><p>The recent revival of interest in autoencoders is due to the success of effectively training deep architectures because traditional gradient-based optimization strategies are not effective when hidden layers are stacked multiple times with non-linearities. In 2006, Hinton <ref type="bibr" target="#b35">[36]</ref> empirically trained a deep network architecture by sequentially and greedily optimizing weights of each layer in a Restricted Boltzmann Machine (RBM) model to achieve global gradient-based optimization. Bengio <ref type="bibr" target="#b36">[37]</ref> achieved success in training a stacked autoencoder on the MNIST dataset with a similar greedy layerwise approach. This training approach overcomes the problematic non-convex optimization which prevents deep network structure. Subsequent studies show that stacked autoencoder model can learn meaningful, abstract features and thus achieve better classification results in high-dimensional data, such as images and texts <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. The training efficiency of stacked autoencoder is further improved by changing layer weight initialization <ref type="bibr" target="#b41">[42]</ref>.</p><p>As the training efficiency improves, autoencoder becomes increasingly popular. Soon it was found that as layers are stacked, the weights of deeper layers increase sharply because of matrix multiplication and then the importances of these weights become larger than the initial input features. This overfitting issue gives rise to the fact that representations of deep layers are more likely dependent on the network structure instead of the initial input features. Poultney et al. <ref type="bibr" target="#b42">[43]</ref> presented an idea to increase the sparsity of network structure so as to limit the increasing of weights. Goodfellow et al. <ref type="bibr" target="#b43">[44]</ref> added a regularization term in the loss function of autoencoder to impose a penalty on large weights. Vincent et al. <ref type="bibr" target="#b44">[45]</ref> proposed a Denoising Autoencoder (DAE) to solve this issue by adding noises in the input. The proposed DAE model aims not to reconstruct the original input but to reconstruct a corrupted input which is typically corrupted by adding Gaussian noise. The previous studies have no control of the distribution of hidden-layer representations. So Variational Autoencoder (VAE) <ref type="bibr" target="#b24">[25]</ref> is proposed to generate desired distributions of representations in hidden layers.</p><p>Autoencoders are an unsupervised dimension reduction process and the reduced high-level features generally contain the major information of original data. This makes autoencoders not sensitive to slight variations. To make them sensitive to slight variations, Rifai et al. <ref type="bibr" target="#b23">[24]</ref> proposed a Contractive Autoencoder (CAE). In fact, maintaining mutual relationships among data samples works as an effective way of making autoencoders sensitive to slight variations. Because of this, Wang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a Generalized Autoencoder (GAE) targeting at reconstructing the data relationships instead of the data features. A series of applications <ref type="bibr">[27-29, 31, 32]</ref> of GAE confirm that maintaining data relationships can achieve better results but results are highly dependent on how to define and choose distance weights. The practice of having sophisticated, pre-defined distance weights is risky in converting GAE into a supervised model because of assigning high weights to selected relationships is very subjective and may be biased. As a result, we propose a Relation Autoencoder (RAE) for dimensionality reduction by minimising both the loss of data features and relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>This section begins with a definition of feature extraction followed by a description of basic autoencoder models before presenting our proposed Relational Autoencoder (RAE) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>Feature extraction transforms data from the original, highdimensional space to a relatively low-dimensional space. This transformation can be linear or nonlinear. Specifically, considering a given dataset X with n samples and m features, the original feature set is denoted as F O and a feature extraction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic Autoencoder</head><p>Simply, an autoencoder (AE) is composed of two parts, an encoder and a decoder. Considering a data sample X with n samples and m features, the output of encoder Y represents the reduced representation of X and the decoder is tuned to reconstruct the original dataset X from the encoder's representation Y by minimizing the difference between X and X as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, the encoder is a function f that maps an input X to hidden representation Y . The process is formulated as</p><formula xml:id="formula_0">Y = f (X) = s f (W X + b X )<label>(1)</label></formula><p>where s f is a nonlinear activation function and if it is an identity function, the autoencoder will do linear projection. The encoder is parameterized by a weight matrix W and a bias vector b âˆˆ R n . The decoder function g maps hidden representation Y back to a reconstruction X :</p><formula xml:id="formula_1">X = g(Y ) = s g (W Y + b Y )<label>(2)</label></formula><p>where s g is the decoder's activation function, typically either the identity (yielding linear reconstruction) or a sigmoid. The decoder's parameters are a bias vector b y and matrix W . In this paper we only explore the tied weights case where W = W T . Training an autoencoder involves finding parameters Î¸ = (W, b X , b Y ) that minimize the reconstruction loss on the given dataset X and the objective function is given as</p><formula xml:id="formula_2">Î˜ = min Î¸ L(X, X ) = min Î¸ L(X, g(f (X)))<label>(3)</label></formula><p>For linear reconstruction, the reconstruction loss (L 1 ) is generally from the squared error:</p><formula xml:id="formula_3">L 1 (Î¸) = n i=1 ||x i âˆ’ x i || 2 = n i=1 ||x i âˆ’ g(f (x i ))|| 2 (4)</formula><p>For nonlinear reconstruction, the reconstruction loss (L 2 ) is generally from cross-entropy:</p><formula xml:id="formula_4">L 2 (Î¸) = âˆ’ n i=1 [x i log(y i ) + (1 âˆ’ x i ) log(1 âˆ’ y i )] (5)</formula><p>where x i âˆˆ X, x i âˆˆ X and y i âˆˆ Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stacked Autoencoder</head><p>As a neural network model based feature extraction method, a major advantage of Autoencoder is that it is easy to stack for generating different levels of new features to represent original ones by adding hidden layers. For an l-layer stacked autoencoder, the process of encoding is</p><formula xml:id="formula_5">Y = f l (. . . f i (. . . f 1 (X)))<label>(6)</label></formula><p>where f i is the encoding function of layer i. The corresponding decoding function is</p><formula xml:id="formula_6">X = g l (. . . g i (. . . g 1 (Y )))<label>(7)</label></formula><p>where g i is the decoding function of layer i and the Stacked Autoencoder can be trained by greedy layerwise feed-forward approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATIONAL AUTOENCODER (RAE)</head><p>The traditional autoencoder generates new features by minimizing the reconstruction loss of the data. This motivates us to propose a Relational Autoencoder (RAE) to minimize the reconstruction loss of both data and their relationships. The objective function of RAE is defined as</p><formula xml:id="formula_7">Î˜ = (1 âˆ’ Î±) min Î¸ L(X, X ) + Î± min Î¸ L(R(X), R(X )) (8)</formula><p>where R(X) represents the relationship among data samples in X and R(X ) represents the relationship among data samples in X as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Parameter Î± is a scale parameter to control the weights of the data reconstruction loss and the relationship reconstruction loss and Î¸ is the neural network parameter of the autoencoder.</p><p>Data relationship can be modelled in multiple ways. In this paper, we present data relationship by their similarities where R(X) is the multiplication of X and X T . Then the objective function is</p><formula xml:id="formula_8">Î˜ = (1 âˆ’ Î±) min Î¸ L(X, X ) + Î± min Î¸ L(XX T , X X T ) (9)</formula><p>In order to improve computational efficiency and filter out unnecessary relationships, we use activation functions to control weights of similarities. In this paper, we use the rectifier function <ref type="bibr" target="#b45">[46]</ref> to achieve this.</p><formula xml:id="formula_9">Ï„ t (r ij ) = r ij , if r ij â‰¥ t, 0, otherwise,<label>(10)</label></formula><p>Algorithm 1 Iterative learning procedure of RAE while True do <ref type="bibr">10:</ref> loss c = Model.train(L, SGD); <ref type="bibr">11:</ref> if (loss c -loss p Îµ) then <ref type="bibr">12:</ref> Break; <ref type="bibr">13:</ref> else <ref type="bibr">14:</ref> loss p = loss c; <ref type="bibr">15:</ref> end if <ref type="bibr">16:</ref> end while 17: return Model 18: end function where t is a threshold to filter out weak and trivial relationships. Then the objective function of RAE is defined as</p><formula xml:id="formula_10">Î˜ = (1 âˆ’ Î±) min Î¸ L(X, X ) + Î± min Î¸ L(Ï„ t (XX T ), Ï„ t (X X T ))<label>(11)</label></formula><p>In this paper, we choose the loss function L as squared error.</p><p>The pseudo-code of the proposed Relational Autoencoder (RAE) is described in Algorithm 1 where the input parameters are the input dataset (X), parameter of the rectifier function (t), regularization weight (Î»), the number of hidden neurons in layers (N ), activation function (s f ) and a threshold (Îµ) to determine whether the loss has converged. Among them, N is a vector and n i is the number of neurons of ith layer where i is from 1 to |N |. The proposed RAE model starts with defining a loss function (L). Then it initializes a neural network model and iteratively add layers into the network model based on N and s f . The network is trained by stochastic gradient descent (SGD) and updates Î¸ in the loss function (L) until the difference between current-loop loss (loss c) and the previous-loop loss (loss p) is smaller than a predefined threshold (Îµ) which means it has converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extension to Sparse Autoencoder (SAE)</head><p>The objective function of autoencoder reconstructs the input. During the training process, the weights of hidden neurons are the combination of previous layers and these weights increase as layers get deep. High weights of hidden layers make the generated features more dependent on the network structure rather than the input. In order to avoid this, Sparse Autoencoder (SAE) imposes weight-decay regularization so as to keep neuron weights small. The objective function of SAE is</p><formula xml:id="formula_11">Î˜ = Î± min Î¸ L(X, X ) + Î»||W || 2<label>(12)</label></formula><p>where ||W || 2 is a weight-decay regularization term to guarantee weight matrix W having small elements. Parameter Î» is a hyper-parameter to control the strength of the regularization. We extend Sparse Autoencoder (SAE) to a Relational Sparse Autoencoder (RSAE) model by considering the data relationships. The objective function is defined as</p><formula xml:id="formula_12">Î˜ = (1 âˆ’ Î±) min Î¸ L(X, X ) + Î± min Î¸ L(Ï„ t (XX T ), Ï„ t (X X T )) + Î»||W || 2<label>(13)</label></formula><p>B. Extension to Denoising Autoencoder (DAE)</p><p>Denoising Autoencoder (DAE) is proposed to generate better features by imposing an alternative form of regularization. The main principle of DAE is to corrupt a part of the input features of a given dataset X before sending it to an autoencoder model, train the network to reconstruct the destroyed inputX and then minimize the loss between the reconstructedX and original X. The objective function of DAE is</p><formula xml:id="formula_13">Î˜ = min Î¸ [L(X, g(f (X)))] s.t.X âˆ¼ q(X|X)<label>(14)</label></formula><p>whereX is corrupted X from a stochastic corruption process q(X|X) and the objective function is optimized by stochastic gradient descent. Here we extend the proposed Relation Autoencoder model to a Relational Denoising Autoencoder (RDAE) model by considering data relationships. The model minimizes the loss between data X and corrupted dataX and the loss between data relationship XX T and corrupted data relationshipXX T . The objective function is defined as</p><formula xml:id="formula_14">Î˜ = (1 âˆ’ Î±) min Î¸ L(X, g(f (X)) + Î± min Î¸ L(Ï„ t (XX T ), Ï„ t (XX T )) s. t.X âˆ¼ q(X|X)<label>(15)</label></formula><p>In this paper, we consider corruptions as additive isotropic Gaussian noise:X = X + âˆ† whereX âˆ¼ N (0, Î´ 2 ) and Î´ is the standard deviation of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extension to Variational Autoencoder (VAE)</head><p>Variational Autoencoder (VAE) is a different to the other types of autoencoder model. It makes a strong assumption concerning the distribution of latent neurons and tries to minimize the difference between a posterior distribution and the distribution of latent neurons with difference measured by the Kullback-Leibler divergence <ref type="bibr" target="#b46">[47]</ref>. The objective function of VAE is</p><formula xml:id="formula_15">Î˜ = min D KL (q Ï† (Y |X)||p Î¸ (X|Y ))<label>(16)</label></formula><p>where q Ï† (Y |X) is the encoding process to calculate the probability of Y based on the input X while p Î¸ (X|Y ) is the decoding process to reconstruct X. Generally Y is a predefined Gaussian distribution, such as N (0, 1). Therefore the extended Relational Variational Autoencoder (RVAE) is defined as</p><formula xml:id="formula_16">Î˜ = (1 âˆ’ Î±) min D KL (q Ï† (Y |X)||p Î¸ (X|Y )) + Î± min D KL (q Ï† (Y |XX T )||p Î¸ (XX T |Y ))<label>(17)</label></formula><p>V. DATASETS</p><p>The datasets used in this paper to evaluate the proposed Relational Autoencoder are two image datasets, MNIST 1 and CIFAR-10 2 . The MNIST dataset is a well known database of handwritten digits which contains a training set of 60,000 examples and a test set of 10,000 samples. The CIFAR-10 dataset contains 60,000 images which are labelled with 10 classes with each class having 6,000 images. The training set of CIFAR-10 has 50,000 samples while the test set has 10,000 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head><p>The experiment firstly compares the performance of the proposed Relational Autoencoder (RAE) model against basic autoencoder (BAE) and Generative Autoencoder (GAE) in reconstruction loss and classification accuracy <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. Then we compare the performance of the extended Relational Sparse Autoencoder (RSAE), Relational Denoising Autoencoder (RDAE) and Relational Variational Autoencoder (RVAE) with their corresponding original versions to estimate the effects of considering relationship in the depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setting</head><p>All autoencoder models were tested with the same configuration on the same dataset. Specifically, we use tied weights (W = W T ) in building network structure. The activation function of each layer is sigmoid for both encoder and decoder. The optimization functions of reconstruction loss are listed and described in Section IV and they are trained by stochastic gradient descent (SGD) for 400 epochs. The number of neurons of each layer is determined as</p><formula xml:id="formula_17">n i+1 = log(n i ), if n i+1 â‰¥ l t , l t , otherwise,<label>(18)</label></formula><p>where n i is the number of neurons in layer i and n i+1 is the number of neurons in layer i + 1. As the network structure goes deeper, the number of neurons gradually decreases to a predefined threshold l t . To increase the training efficiency, we use Xavier <ref type="bibr" target="#b49">[50]</ref> method to initialize layer weights</p><formula xml:id="formula_18">w ij = U âˆ’ 1 âˆš n iâˆ’1 , 1 âˆš n iâˆ’1<label>(19)</label></formula><p>where w ij is weight of jth neuron in layer i and U [âˆ’ 1 âˆš n , 1</p><p>âˆš n ] is the uniform distribution in the interval (âˆ’ 1 âˆš n , 1 âˆš n ]) and n iâˆ’1 is the number of neurons in the previous layer.</p><p>All autoencoders are trained based on their loss function respectively. But in order to compare the performance of them in feature extraction, the reconstruction loss is measured by Mean Squared Error (MSE). Classification is done by softmax regression based on the extracted features from autoencoder models. To estimate the generation of models, we use 10-fold cross validation in both unsupervised feature extraction and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparing to BAE and GAE</head><p>We firstly compare the performance of our proposed Relational Autoencoder (RAE) with basic autoencoder (BAE) and Generative Autoencoder (GAE) in terms of reconstruction on MNIST and CIFAR-10. For GAE, we set similarity to be the weight of each pairwise relationship. In the experiment, we explore different values of scaling parameter Î± which determines the weights of reconstructing data and relationship (Equation 11). The value of Î± ranges from 0 to 1 in step of 0.02. Meanwhile, because BAE and GAE has no such parameter, their reconstruction loss is not changed as Î± changes and the results are illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>We observe that generally the reconstruction loss of GAE is less than BAE which confirms that considering data relationship is able to reduce information loss in the process of encoding and decoding. The performance of the proposed RAE autoenocder model changes as the scaling parameter Î± changes. Generally, it starts to generate similar results with BAE because Î± is small focusing on reconstructing data rather than relationships. As Î± increases, the performance of GAE continuously decreases until to a tough. Then as Î± keeps increasing, the information loss increases as well because the model begins to over-emphasize relationship reconstruction. It is interesting to see that even if the proposed RAE model considers relationship only, the performance of RAE is better than GAE as RAE uses the activation function to filter out weak relationships. Thus the performance of the proposed RAE autoencoder model is determined by scaling parameter Î± and the value of Î± depends on the dataset.</p><p>Another interesting finding is that the proposed RAE model achieves better results in CIFAR-10 than MNIST. This may be because the CIFAR-10 contains more complex images than MNIST and for complex datasets, maintaining data relationship is of much importance. For classification, RAE achieves the lowest error rate (3.8%) followed by GAE (5.7%) and BAE (8.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing extend autoencoder variations to original ones</head><p>In this experiment, we compare the performance of the extended variants of autoencoders to their original versions and detailed results are listed in <ref type="table" target="#tab_1">Table I</ref>. We observe that considering data relationships contributes to decreasing reconstruction loss suggesting that autoencoders can generate more robust and meaningful features with less information loss and these features are the key to achieve good classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we propose a Relation Autoencoder model which can extract high-level features based on both data itself and their relationships. We extend this principle to other major autoencoder models including Sparse Autoencoder, Denoising Autoencoder and Variational Autoencoder so as to enable them consider data relationships as well. The proposed relational autoencoder models are evaluated on MNIST and CIFAR-10 datasets and the experimental results show that considering data relationships can decrease reconstruction loss and therefore generate more robust features. Better features contribute to improved classification results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Traditional autoencoder generates new feature set Y by minimizing the reconstruction loss between X and X .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Relational autoencoder generates new feature set Y by minimizing the loss of reconstructing X and maintaining relationships among X.function T generates new feature set F N where |F N | &lt; |F O |.Generally, the objective functions of feature extraction methods minimize the difference between the original space F O (X) and the new space F N (X) and changing the objective functions can convert feature extraction from unsupervised methods to supervised methods such as Linear Discriminant Analysis (LDA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The comparative results of reconstruction loss on the MNIST and CIFAR-10 dataset among Relational Autoencoder (RAE), basic autoencoder (BAE) and Generative Autoencoder (GAE). It describes how information loss changes as the scale parameter Î± changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1: function RAE(X, t, Î», N l , s f , Îµ)</figDesc><table><row><cell>2:</cell><cell>L = Equ. 11;</cell><cell></cell></row><row><cell>3:</cell><cell>loss p = 0;</cell><cell></cell></row><row><cell>4:</cell><cell>loss c = 0;</cell><cell></cell></row><row><cell>5:</cell><cell>Model = NN();</cell><cell>initialize a neural network model</cell></row><row><cell>6:</cell><cell>for i = 1 to |N | do</cell><cell></cell></row><row><cell>7:</cell><cell cols="2">Model.addLayer(n i , s f )</cell></row><row><cell>8:</cell><cell>end for</cell><cell></cell></row><row><cell>9:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EXPERIMENTAL</head><label>I</label><figDesc>RESULTS OF AUTOENCODER MODELS</figDesc><table><row><cell>Model</cell><cell cols="2">MNIST</cell><cell cols="2">CIFAR-10</cell></row><row><cell></cell><cell>Loss</cell><cell>Error</cell><cell>Loss</cell><cell>Error</cell></row><row><cell>RAE</cell><cell>0.677</cell><cell>3.8%</cell><cell>0.281</cell><cell>12.7%</cell></row><row><cell>BAE</cell><cell>0.813</cell><cell>8.9%</cell><cell>0.682</cell><cell>15.6%</cell></row><row><cell>GAE</cell><cell>0.782</cell><cell>5.7%</cell><cell>0.574</cell><cell>14.9%</cell></row><row><cell>RSAE</cell><cell>0.296</cell><cell>1.8%</cell><cell>0.292</cell><cell>13.4%</cell></row><row><cell>SAE</cell><cell>0.312</cell><cell>2.2%</cell><cell>0.331</cell><cell>14.2%</cell></row><row><cell>RDAE</cell><cell>0.217</cell><cell>1.1%</cell><cell>0.216</cell><cell>10.5%</cell></row><row><cell>DAE</cell><cell>0.269</cell><cell>1.6%</cell><cell>0.229</cell><cell>11.7%</cell></row><row><cell>RVAE</cell><cell>0.183</cell><cell>0.9%</cell><cell>0.417</cell><cell>17.3%</cell></row><row><cell>VAE</cell><cell>0.201</cell><cell>1.2%</cell><cell>0.552</cell><cell>21.2%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://yann.lecun.com/exdb/mnist/ 2 https://www.cs.toronto.edu/ kriz/cifar.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Survey on distance metric learning and dimensionality reduction in data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="534" to="564" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linear dimensionality reduction: Survey, insights, and generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2859" to="2900" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimensionality reduction techniques for text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akkarapatty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collaborative Filtering Using Data Mining and Analysis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-graph-view learning for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial immune system for attribute weighted naive bayes classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: proceedings of the eleventh international conference</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification and regression by randomforest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R news</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of feature selection and feature extraction techniques in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nasreen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science and Information Conference (SAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal component analysis on spatial data: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>DemÅ¡ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brunsdon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fotheringham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcloone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Association of American Geographers</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linear discriminant analysis for the small sample size problem: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear process monitoring using kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Vanrolleghem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="234" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online kernel principal component analysis: A reduced-order model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1814" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correspondence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lebart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science, Classification, and Related Methods: Proceedings of the Fifth Conference of the International Federation of Classification Societies (IFCS-96)</title>
		<meeting><address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page">423</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Randomized nonlinear component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1359" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<title level="m">Multidimensional scaling: History, theory, and applications</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse autoencoder-based feature transfer learning for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized autoencoder: a neural network framework for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="490" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoencoding the retrieval relevance of medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Camlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tizhoosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khalvati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing Theory, Tools and Applications (IPTA), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="550" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep feature learning for eeg recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sternin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Grahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04306</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning in high-dimensional multimedia data: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A naive bayes probability estimation model based on self-adaptive differential evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="671" to="694" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dimensionality reduction strategy based on auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the 7th International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Research on denoising sparse autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagation errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumerhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlinear autoassociation is not equivalent to pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">International study of the prevalence and outcomes of infection in intensive care units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anzueto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gomersall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2323" to="2329" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dual autoencoders features for imbalance classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="875" to="889" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4d patient data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-adaptive probability estimation for naive bayes classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiple structure-view learning for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
