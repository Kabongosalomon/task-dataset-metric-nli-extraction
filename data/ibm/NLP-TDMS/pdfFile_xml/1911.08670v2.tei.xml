<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MMTM: Multimodal Transfer Module for CNN Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Reza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaezi</forename><surname>Joze</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
							<email>amirreza@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Iuzzolino</surname></persName>
							<email>michael.iuzzolino@colorado.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><forename type="middle">Koishida</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Boulder</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MMTM: Multimodal Transfer Module for CNN Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In late fusion, each modality is processed in a separate unimodal Convolutional Neural Network (CNN) stream and the scores of each modality are fused at the end. Due to its simplicity, late fusion is still the predominant approach in many state-of-the-art multimodal applications. In this paper, we present a simple neural network module for leveraging the knowledge from multiple modalities in convolutional neural networks. The proposed unit, named Multimodal Transfer Module (MMTM), can be added at different levels of the feature hierarchy, enabling slow modality fusion. Using squeeze and excitation operations, MMTM utilizes the knowledge of multiple modalities to recalibrate the channel-wise features in each CNN stream. Unlike other intermediate fusion methods, the proposed module could be used for feature modality fusion in convolution layers with different spatial dimensions. Another advantage of the proposed method is that it could be added among unimodal branches with minimum changes in the their network architectures, allowing each branch to be initialized with existing pretrained weights. Experimental results show that our framework improves the recognition accuracy of well-known multimodal networks. We demonstrate stateof-the-art or competitive performance on four datasets that span the task domains of dynamic hand gesture recognition, speech enhancement, and action recognition with RGB and body joints. * Equal contribution. â€ </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Different sensors can provide complementary information about the same context. Multimodal fusion is the act of extracting and combining relevant information from the different modalities that leads to improved performance over using only one modality. This technique is widely used in various machine learning tasks, such as video classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, action recognition <ref type="bibr" target="#b2">[3]</ref>, emotion recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and audio visual speech enhancement <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. In general, fusion can be achieved at the input level (i.e. early fusion), decision level (i.e. late fusion), or intermediately <ref type="bibr" target="#b8">[8]</ref>. Although studies in neuroscience <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref> and machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> suggest that mid-level feature fusion could benefit learning, late fusion is still the predominant method utilized for mulitmodal learning <ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref>. This is mostly due to practical reasons. For example, a simple pooling operator <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> or an attention mechanism <ref type="bibr" target="#b16">[16]</ref> can be used to fuse 1-dimensional prediction scores of each stream. However, intermediate level features of different modalities have different or unaligned spatial dimensions making the intermediate fusing more challenging. Another reason for the popularity of late fusion is that the architecture of each unimodal stream is carefully designed over years to achieve state-of-the-art performance for each modality. This also enables the CNN streams of a multimodal framework to be initialized by weights that have been pretrained with a large number of unimodal training samples. However, intermediate fusion requires major changes in the base network architecture, which complicates the use of pretrained weights in most cases and requires the network to be retrained from randomly initialized states <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates three common multimodal fusion techniques.</p><p>The goal of the proposed method is to overcome the aforementioned problems of intermediate fusion. Inspired by the squeeze and excitation (SE) module <ref type="bibr" target="#b19">[19]</ref> for unimodal convolutional neural networks, we propose a multimodal transfer module to recalibrate the channel-wise fea-tures of different CNN streams. MMTMs can be inserted into intermediate levels of any late fusion backbone architecture. Each MMTM has two units: a) a multimodal squeeze unit that receives the features from all modalities at a given level of representation across the branches, generating a global joint representation of these features, and b) an excitation unit that uses this joint representation to adaptively emphasize on more important features and suppress less important ones in all modalities. The squeeze unit aggregates spatial dimensions, allowing information with global receptive fields from all modalities to be used in the global representation. It also enables learning a joint representation from modalities with different spatial dimensions.</p><p>Although the module design is generic and could potentially be added at any level in the network hierarchy, the optimal locations and number of modules are different for each application. We design application specific networks for gesture recognition, audio-visual speech enhancement, and action recognition tasks and study the benefit of adding MMTM in their architectures. We make the following empirical observations from these applications. Firstly, adding MMTM to intermediate and high-level features is beneficial, whereas the same is not true about low-level features. We believe that is because intera-modality correlation in low-level features is lower compared to intermediate and high-level features. This is also highlighted in previous research <ref type="bibr" target="#b20">[20]</ref>. Secondly, even in gesture recognition where RGB and depth modalities are spatially aligned and fusion can be done without the squeeze operation, squeezing considerably improves the performance by providing information with a global receptive field. Lastly, excitation by gating operation outperforms the sum operation that is usually used in residual learning, highlighting the importance of the emphasis and suppression mechanisms.</p><p>In summary, this paper makes the following contributions: First, we propose a new neural network module called MMTM to fuse knowledge from intermediate features of unimodal CNNs. Second, we design different network architectures for three different multimodal fusion applications: gesture recognition using multiple visual modalities, audio-visual speech enhancement, and action recognition with RGB and body joints. We demonstrate through experiment on these tasks that MMTM improves the performance beyond the late fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In late fusion, the prediction of each unimodal stream are fused to make the final prediction. Fusion can be via element-wise summation, a weighted average <ref type="bibr" target="#b15">[15]</ref>, a bilinear product <ref type="bibr" target="#b21">[21]</ref>, or a more sophisticated rank minimization <ref type="bibr" target="#b22">[22]</ref> method. Another approach to late fusion utilizes attention to pick the best expert for each input signal <ref type="bibr" target="#b16">[16]</ref>. The gated multimodal units <ref type="bibr" target="#b23">[23]</ref> extends this method by en-abling gating at intermediate feature levels. More recently, Hu et al. <ref type="bibr" target="#b24">[24]</ref> propose a dense multimodal intermediate fusion network for hierarchical joint feature learning. Similar to <ref type="bibr" target="#b23">[23]</ref>, the dense fusion operator in <ref type="bibr" target="#b24">[24]</ref> assumes identical spatial dimensions for different streams. Despite the similarity of these approaches to our work, their applicability is limited to layers where the multimodal features' spatial dimensions are the same, or at the very end of the network where spatial dimensions are already aggregated. The squeeze operation proposed in this work allows the fusion of modalities with different spatial dimensions at any level of the feature hierarchy.</p><p>In a related multimodal learning topic, called crossmodal learning, information from multiple modalities are used to improve the learning performance within any individual modality. It is assumed that data from all the modalities are present during training but the performance is tested on only one modality <ref type="bibr" target="#b25">[25]</ref>. MTUT <ref type="bibr" target="#b12">[12]</ref> uses spatiotemporal semantic alignment loss to improve the performance of each stream in gesture recognition. We believe cross-modal learning approaches are orthogonal to our work since the improved unimodal networks learned by these methods can initialize weights of the CNN streams in our model. Multimodal Action Recognition in Videos Video <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b26">26]</ref> and skeleton <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref> modalities have been extensively used for the action recognition task. Each of these approaches have their own drawbacks. With the lack of explicit human body model, video based action recognition methods deal poorly with background clutter and nonaction movements <ref type="bibr" target="#b11">[11]</ref>. On the other hand, by solely relying on body pose most of contextual and global cues present in the video will be lost. Recent methods develop architectures to fuse these modalities to further improve the performance of action recognition. In <ref type="bibr" target="#b28">[28]</ref>, an end-to-end trainable multitask network for joint pose estimation and action recognition is proposed. PoseMap <ref type="bibr" target="#b11">[11]</ref> utilizes a two stream network to process spatiotemporal pose heatmaps and skeleton separately, and uses late fusion for the final prediction. A bilinear pooling block that separately pools input features in modality and time directions is employed in <ref type="bibr" target="#b29">[29]</ref>. Audio-Visual Speech Enhancement (AVSE) Work in AVSE is strongly motivated by the cocktail party effect, which refers to humans' ability to selectively attend to auditory signals of interest within a noisy environment. Experiments in neuroscience have demonstrated that cross-modal integration of audio-visual signals may improve the perceptual quality of the targeted acoustic signal <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref>. Inspired by the results from biological research, recent studies focus on augmenting audio only speech enhancement methods with visual information, such as lip movement. Stateof-the-art results have been achieved by recent AVSE models that use deep neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>. The predominant approach taken for AV fusion is late fusion <ref type="bibr" target="#b13">[13]</ref>,</p><p>where the audio and visual information is processed separately then integrated at a singular point via channel-wise concatenation.</p><p>Hand Gesture Recognition Interpreting hand gestures via machine learning algorithms is significantly important in human-computer interaction. We review the 3D convolutional neural network based gesture recognition algorithms <ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref> that are designed for processing time series data among other branches <ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref>. In <ref type="bibr" target="#b35">[35]</ref>, a novel 3D CNN is proposed to integrate depth and image gradient values to recognize dynamic hand gestures. Molchanov et al. <ref type="bibr" target="#b36">[36]</ref> employ a multistream 3D CNN to fuse streams of data from multiple sensors including short-range radar, color, and depth sensors for recognition. A real-time method is presented in <ref type="bibr" target="#b37">[37]</ref> to simultaneously detect and classify gestures in videos. Camgoz et al. <ref type="bibr" target="#b38">[38]</ref> present a late fusion approach for fusing the scores of unimodal 3D CNN streams. Miao et al. propose ResC3D <ref type="bibr" target="#b39">[39]</ref>, a 3D CNN architecture that combines multimodal data using an attention model. MFFs <ref type="bibr" target="#b44">[44]</ref> develops a data level fusion method for RGB and optical flow. FOANet <ref type="bibr" target="#b45">[45]</ref> proposes a sparse fusion technique for hand gesture recognition. FOANet decomposes each input modality (RGB, depth, and 2 types of optical flow) into separate focus channels (global, right hand, left hand) and processes each of these 12 focus channels in an independent unimodal network. Finally, it learns a sparsely connected late fusion network to avoid overfitting. Unlike our method, FOANet relies on the output of a detector to find the focus areas in the video. Squeeze and Excitation (SE) Network <ref type="bibr" target="#b19">[19]</ref> Our proposed method can be seen as a generalization to the SE module, which is proposed for unimodal deep neural networks. The SE modules uses self excitation to adaptively recalibrate channel-wise feature responses. Our work adopts the SE module for multimodal feature recalibrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multimodal Transfer Module</head><p>In this section, we discuss the simplest case of fusion between two disjoint CNN streams, CNN 1 and CNN 2 . Let A âˆˆ R N1Ã—Â·Â·Â·Ã—N K Ã—C and B âˆˆ R M1Ã—Â·Â·Â·Ã—M L Ã—C represent the features at a given layer of CNN 1 and CNN 2 , respectively. Here, N i and M i represent the spatial dimensions 1 , and C and C represent the number of channels of the corresponding features in CNN 1 and CNN 2 respectively. MMTM receives features A and B as input, learns a global multimodal embedding from them, and uses this embedding to recalibrate the input features. This is done in a two-step multimodal squeeze and excitation process described below. <ref type="figure">Figure 2</ref>. Architecture of MMTM for two modalities. A and B, that represent the features at a given layer of two unimodal CNNs, are the inputs to the module. For better visualization we limit the number of their spatial dimensions to 2. MMTM uses squeeze operations to generate global feature descriptor from each tensor. Both tensors are map into a joint representation Z by using concatenation and fully-connected layer. The excitation signals E A and E B are generated using the joint representation. Finally the excitation signals are used to gate the channel-wise features in each modality.</p><formula xml:id="formula_0">! " ! # $ $ $' % " % # $â€² Squeeze Squeeze % " % # $â€² ' ( ) ( * ' * ( + , ' , ( ! " ! # $ ' ) -(. ) -(. )</formula><p>Squeeze The information in the output features of convolution layers are limited by the size of their receptive fields and lacks global context. As suggested by <ref type="bibr" target="#b19">[19]</ref>, we first squeeze the spatial information into the channel descriptors via a global average pooling over spatial dimensions of the input features:</p><formula xml:id="formula_1">S A (c) = 1 K i=1 N i n1,...,n K A(n 1 , . . . , n K , c) (1) S B (c) = 1 L i=1 M i m1,...,m L B(m 1 , . . . , m L , c).<label>(2)</label></formula><p>Importantly, the squeeze operation enables fusion between modalities with features of arbitrary spatial dimension. Note that while we use simple average pooling, more sophisticated pooling methods could be used at this step.</p><p>Multimodal Excitation The function of this unit is to generate the excitation signals, E A âˆˆ R C and E B âˆˆ R C , which can be used to recalibrate the input features, A and B, by a simple gating mechanism:</p><formula xml:id="formula_2">A = 2 Ã— Ïƒ(E A ) Ãƒ B = 2 Ã— Ïƒ(E B ) B,</formula><p>where Ïƒ(.) is the sigmoid function and is the channelwise product operation. This allows the suppression or excitation of different filters in each stream. Note that the MMTM weights are regularized in order to control the proximity of E A and E B to zero. Specifically, increasing the regularization weight of E A pushes the gating signal 2 Ã— Ïƒ(E A ) closer to the identity vector, limiting the effect of gating on feature A. The gating signals must apply different calibration weights to different modalities based on the same input representation. We achieve this by first predicting a joint representation Z âˆˆ R C Z from the squeezed signals</p><formula xml:id="formula_3">Z = W[S A , S B ] + b,<label>(3)</label></formula><p>and then predicting excitation signals for each modality through two independent fully-connected layers</p><formula xml:id="formula_4">E A = W A Z + b A , E B = W B Z + b B .<label>(4)</label></formula><p>Here, [Â·, Â·] represents the concatenation operation,</p><formula xml:id="formula_5">W âˆˆ R C Z Ã—(C+C ) , W A âˆˆ R CÃ—C Z , W B âˆˆ R C Ã—C Z are the weights, and b âˆˆ R C Z , b A âˆˆ R C , b B âˆˆ R C</formula><p>are the biases of the fully connected layers. As suggested in <ref type="bibr" target="#b19">[19]</ref>, we use C Z = (C + C )/4 to limit the model capacity and increase the generalization power. For fusing more than two modalities, we simply generalize this approach by concatenating squeezed features from all the modalities in Equation 3 and predict excitation signals for each modality with an independent fully-connected layer like in Equation <ref type="bibr" target="#b3">4</ref>.</p><p>Learning the joint representation in this way allows the features of one modality to recalibrate the features of another modality. For instance, in gesture recognition when a gesture is blurry in RGB camera and more apparent in depth modality, MMTM cross-modal recalibration affords more efficient processing in the RGB stream. <ref type="figure">Figure 2</ref> summarizes the overall architecture of the proposed MMTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>The MMTM is generic and can be easily integrated to any multimodal CNN architectures. In this section, we explore a few applications that can benefit from MMTM and describe the architecture changes necessary to support multimodal fusion. We evaluate the performance of the proposed multimodal models in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hand Gesture Recognition</head><p>Hand gesture recognition is a video classification task. It is shown that complementary sensory information, such as depth and optical flow, improves the performance of the gesture recognition <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44]</ref>. There are multiple multimodal datasets available for this task <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref> and several previous fusion methods have reported their results on these datasets <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref>.</p><p>We design a gesture recognition network for fusing RGB, depth, and optical flow video streams via MMTM. To process the temporal inputs, we use I3D network architecture <ref type="bibr" target="#b48">[48]</ref> with an inflated inception-v1 <ref type="bibr" target="#b49">[49]</ref> backbone for all the streams. In I3D network, convolution and pooling kernels of the backbone network are expanded into 3D, enabling efficient spatial-temporal feature processing. We apply MMTM after the last 6 inception modules (the connectivity is similar to figure 1). Note that the output of 3D convolutions has a time dimension in addition to height, width, and channel dimensions. We empirically find that the best performance is achieved when the squeeze operation is applied over all the dimensions except for the channel dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Audio-Visual Speech Enhancement</head><p>The predominant method for AV speech enhancement combines audio and visual signals via channel-wise concatenation (CWC) using the late fusion approach. As an application of MMTM, we explore AV fusion for speech enhancement tasks using MMTM instead of the CWC-based late fusion. Model details are provided below, and an overview of our AVSE architecture can be found in <ref type="figure" target="#fig_1">Figure 3</ref>. Visual Network We use the spatio-temporal residual network proposed by <ref type="bibr" target="#b50">[50]</ref>, which consists of a 3D spatiotemporal convolution followed by a 2D ResNet-18 <ref type="bibr" target="#b51">[51]</ref>. Processing 3D features in a 2D convolution operation is achieved by packing the temporal dimension, t, into the batch dimension. The network is randomly initialized and trained concurrently with the AVSE task. Audio Network Our audio network is an autoencoder with skip connections; we follow the design detailed in <ref type="bibr" target="#b52">[52]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> (top) depicts the audio processing strategy, which follows the audio processing procedures of <ref type="bibr" target="#b5">[6]</ref> and is detailed in Section 5.2. The network takes a log-mel mixture magnitude spectrogram, log-mel(X mix ), as input and outputs the predicted ideal ratio mask, M . The enhanced magnitude spectrogram, X enh , is obtained via X enh = M X mix , where denotes element-wise multiplication. The network is trained by minimizing the reconstruction loss between the enhanced magnitude, X enh , and the target magnitude, X spec , where X spec is obtained via short-time Fourier transform (STFT) from the target waveform. The optimization objective is given by L = ||X enh âˆ’ X spec || 1 . Audio-Visual Fusion via MMTM Let F j a denote the audio feature at layer j of the autoencoder with F j a âˆˆ R bÃ—tÃ—f Ã—ca , where b, t, f , and c a are the batch, temporal, frequency, and audio channel dimensions, respectively. Let F i v denote visual feature at layer i of the visual network's ResNet-18 with F i v âˆˆ R bÂ·tÃ—hÃ—wÃ—cv , where h, w are spatial dimensions and b, t, c v are the batch, temporal, and visual channel dimensions, respectively. We unpack t from the batch dimension of F i v via reshaping such that F i v âˆˆ R bÃ—tÃ—hÃ—wÃ—cv . The MMTM takes F a and F v as input and carries out the fusion procedure detailed in Section 3. For AVSE, the final output is from audio tower; consequently, MMTM does not gate on visual network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human Action Recognition</head><p>Recent methods in human activity recognition combine video and 3D pose information to further improve the performance of action recognition <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>. Following the same approach, we utilize MMTM for intermediate fusion between a visual and a skeleton based network. Similar to the gesture recognition application, we use I3D for the RGB video stream and HCN, as suggested by <ref type="bibr" target="#b53">[53]</ref>, for the skeletal stream. Although HCN is not the sate-of-the-art for skeleton-based action recognition, the simplicity of its design makes it suitable for our approach.</p><p>As it is shown in <ref type="figure">Figure 4</ref>, HCN is comprised of two 2D convolution subnetworks: one branch processes the raw skeleton data, and the other branch processes the motionthe temporal gradients of the skeletal data. The two subnetworks are fused via channel-wise concatenation and followed by two convolution operations (conv5 and conv6), and finally, a fully connected layer (fc7). <ref type="figure">Figure 4</ref> illustrates the complete network we are proposing. We add 3 MMTMs that receive inputs from last three inception modules of the I3D and conv5, conv6, and fc7 of HCN network. Let A âˆˆ R tÃ—wÃ—hÃ—C represent an I3D feature, where t represents temporal dimension and w, h are the spatial dimensions. Let B âˆˆ R tÃ—nÃ—C represent HCN features after conv5 and conv6 layers, where t is the temporal dimension and n is the body-joints dimension. The output of the fully connected layer (fc7) in HCN network is a 1dimensional vector with no spatial dimension. In MMTMs, we aggregate all the dimensions of the inputs A and B except the channels. The dimensions of the I3D and HCN features sent to the MMTMs (A and B) do not match, but MMTM's squeezing operation makes the fusion possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we evaluate the performance of the proposed method in gesture recognition, speech enhancement, Conv6 FC7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I3D Network</head><p>Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inc.</head><p>Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-Pool</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv5</head><p>Avg-Pool</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HCN Network</head><formula xml:id="formula_6">+ Conv 1x1x1 MMTM FC8 SoftMax Conv1 Conv1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2 Conv2</head><p>Perm. Perm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3 Conv3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv4 Conv4</head><p>Concat.</p><p>Diff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 7x7x7</head><p>Avg-Pool</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 1x1x1</head><p>Conv 3x3x3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg-Pool</head><p>Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inc.</head><p>MMTM MMTM <ref type="figure">Figure 4</ref>. Proposed multimodal architecture for action recognition. Each "Inc." block represents an inception module described in <ref type="bibr" target="#b48">[48]</ref>. and action recognition tasks. Due to the large number of experiments, we use a simple rule to decide the number of MMTMs in each architecture without an extensive architecture tuning scheme. We use MMTMs after each module in the second half of the network with minimum depth. This is 6 MMTMs for hand gesture recognition experiments, 2 in speech enhancement, and 3 in action recognition experiment. Refer to Section 5.4 for the study of the number of MMTMs in hand gesture recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hand Gesture Recognition</head><p>In this section, we evaluate our method against state-ofthe-art dynamic hand gesture methods. We conduct experiments on two recent publicly available multimodal dynamic hand gesture datasets: EgoGesture <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b46">46]</ref> and NVGestures <ref type="bibr" target="#b37">[37]</ref> datasets. <ref type="figure">Figure 5</ref> (a), (b) shows sample frames from the different modalities of these datasets. Implementation Details: In the design of our method, we adopt the architecture of I3D network <ref type="bibr" target="#b48">[48]</ref> as the backbone network for each modality. The architecture details can be found in Section 4.1. We start with the publicly available ImageNet [56] + Kinetics <ref type="bibr" target="#b57">[57]</ref> pretrained networks for all of our experiments on I3D. We optimize the objective function with the standard SGD optimizer using a momentum of 0.9. We start with the base learning rate of 10 âˆ’2 and reduce it 10Ã— when the loss is saturated. We use a batch size of 4 containing 64-frames (32-frames for EgoGesture) snippets in the training stage. We employ the following spatial and temporal data augmentations during the training stage. For spatial augmentation, videos are resized to 256 Ã— 256 pixels, and then randomly cropped with a 224 Ã— 224 patch. The resulting video is randomly flipped horizontally. For temporal augmentation, 64 consecutive frames are picked randomly from the videos. Shorter videos are zero-padded on both sides to obtain 64 frames. During testing, we use 224Ã—224 center crops, apply the models over the full video, and average the predictions.  <ref type="figure">Figure 5</ref>. Sample sequences from multimodal datasets: (a) EgoGesture <ref type="bibr" target="#b41">[41]</ref> (b) NVGesture <ref type="bibr" target="#b37">[37]</ref> (c) VoxCeleb2 <ref type="bibr" target="#b54">[54]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">EgoGesture Dataset</head><p>EgoGesture dataset <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b46">46]</ref> is a large multimodal hand gesture dataset collected for the task of egocentric gesture recognition. This dataset contains 24, 161 hand gesture clips with 83 gesture classes being performed by 50 subjects. Videos in this dataset include both static and dynamic gestures captured with an Intel RealSense SR300 device in RGB-D modalities across multiple indoor/outdoor scenes. We assess the performance of our method along with various hand gesture recognition methods published. <ref type="table">Table 1</ref> compares unimodal test accuracies for I3D on separate modalities and test accuracies of different hand gesture methods by fusion of RGB and depth. VGG16 <ref type="bibr" target="#b58">[58]</ref> processes each frame independently and VGG16+LSTM <ref type="bibr" target="#b59">[59]</ref> combines this method with a recurrent architecture to leverage the temporal information. As can be seen, the 3D CNNbased methods, C3D <ref type="bibr" target="#b60">[60]</ref>, C3D+LSTM+RSTMM <ref type="bibr" target="#b41">[41]</ref>, and I3D <ref type="bibr" target="#b48">[48]</ref> outperform the VGG16-based methods. However, among the 3D CNN architectures, our method outperforms the top performers I3D late fusion by 0.73%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">NVGesture Dataset</head><p>NVGestures dataset <ref type="bibr" target="#b37">[37]</ref> was captured with multiple sensors for studying human-computer interfaces. It contains 1532 dynamic hand gestures recorded from 20 subjects inside a car simulator with artificial lighting conditions. This dataset includes 25 classes of hand gestures. The gestures were recorded with SoftKinetic DS325 device as the RGB-D sensor and DUO-3D for the infrared streams. In addition, the optical flow and infrared disparity map modalities are usually used to enhance the prediction results. Following the previous works <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b44">44]</ref>, we only use RGB, depth, and optical flow modalities in our experiments. The optical flow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Modalities Accuracy I3D <ref type="bibr" target="#b48">[48]</ref> RGB 78.42 I3D <ref type="bibr" target="#b48">[48]</ref> Opt. flow 83.19 I3D <ref type="bibr" target="#b48">[48]</ref> Depth 82.28 HOG+HOG2 <ref type="bibr" target="#b64">[64]</ref> RGB+Depth 36.9 I3D late fusion <ref type="bibr" target="#b48">[48]</ref> RGB+Depth 84.43 Ours RGB+Depth 86.31 Two Stream CNNs <ref type="bibr" target="#b14">[14]</ref> RGB+Opt. flow 65.6 iDT <ref type="bibr" target="#b62">[62]</ref> RGB+Opt. flow 73.4 R3DCNN <ref type="bibr" target="#b37">[37]</ref> RGB+Opt. flow 79.3 MFFs <ref type="bibr" target="#b44">[44]</ref> RGB+Opt. flow 84.7 I3D late fusion <ref type="bibr" target="#b48">[48]</ref> RGB+Opt. flow 84.43 Ours</p><p>RGB+Opt. flow 84.85 R3DCNN <ref type="bibr" target="#b37">[37]</ref> RGB+Depth+Opt. flow 83.8 I3D late fusion <ref type="bibr" target="#b48">[48]</ref> RGB+Depth+Opt. flow 85.68 Ours</p><p>RGB+Depth+Opt. flow 86.93 Human <ref type="bibr" target="#b37">[37]</ref> 88.4 <ref type="table">Table 2</ref>. Accuracies of different multimodal fusion hand gesture methods on the NVGesture dataset <ref type="bibr" target="#b37">[37]</ref>.</p><p>is calculated using the method presented in <ref type="bibr" target="#b61">[61]</ref>. The RGB and optical flow modalities are well-aligned in this dataset, however, the depth map includes a larger field of view. <ref type="table">Table 2</ref> presents the results of our method in comparison with the recent state-of-the-art methods: HOG+HOG2, improved dense trajectories (iDT) <ref type="bibr" target="#b62">[62]</ref>, R3DCNN <ref type="bibr" target="#b37">[37]</ref>, twostream CNNs <ref type="bibr" target="#b14">[14]</ref> and MFFs <ref type="bibr" target="#b44">[44]</ref>. We also report human labeling accuracy for comparison. The iDT <ref type="bibr" target="#b62">[62]</ref> method is often recognized as the best performing method with handengineered features <ref type="bibr" target="#b63">[63]</ref>. Similar to the previous experiment, we observe that the 3D-CNN-based methods outperform other hand gesture recognition methods, and among them, Our method provides the top performance in all the modalities. FOANet <ref type="bibr" target="#b45">[45]</ref> method achieves 91.28% on this dataset using a sparse fusion method. However, this result is not comparable with the methods in <ref type="table">Table 2</ref> since FOANet relies on a separate pre-trained network to detect the hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Audio-Visual Speech Enhancement</head><p>In this section, we evaluate our MMTM method on audio-visual speech enhancement. Using PESQ and STOI objective measures, we demonstrate that our slow fusion MMTM method outperforms state-of-the-art late fusion, channel-wise concatenation AVSE approaches. We use Vox-Celeb2 <ref type="bibr" target="#b54">[54]</ref>, a large audio-visual dataset obtained from YouTube that contains over 1 million utterances for 6,112 celebrities. The training, validation, and test datasets are split by celebrity ID (CID) such that the sets are disjoint over CIDs. In addition, CHiME-1/3 <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b66">66]</ref>, NonStation-aryNoise <ref type="bibr" target="#b67">[67]</ref>, ESC50 <ref type="bibr" target="#b68">[68]</ref>, HuCorpus <ref type="bibr" target="#b69">[69]</ref>, and private datasets are used for additive noise. Video frames are extracted at 25 FPS and S 3 FD [70] performs face detection. Following <ref type="bibr" target="#b50">[50]</ref>, we discard redundant visual information by cropping the mouth region via facial landmarks obtained from Facial Alignment Network <ref type="bibr" target="#b71">[71]</ref>. Lip frames are resized to 122 Ã— 122, transformed to grayscale, then normalized using the global mean and variance statistics from the training set. The audio waveform is extracted from the video following the methods of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b72">72]</ref>. We specify a window length of 40ms, hop size of 10ms, and sampling rate of 16kHz to align one video frame to four audio steps. Short-time Fourier transform (STFT) with a Hanning window function converts the waveform to spectrogram, X spec âˆˆ R T Ã—F with a frequency resolution of F = 321, representing frequencies from 0 âˆ’ 8kHz.</p><p>Training samples of batch size 4 are generated on-the-fly as lip frame and spectrograms pairs, (X vid , X spec ). Interference spectrograms, X inter , are sampled from the Vox-Celeb2 set. We progressively increase the number of interference speakers during training, beginning with one and incrementing by one every 50 epochs until we reach the max of four. A noise spectrogram, X n , is randomly sampled from the noise datasets. The mixture spectrogram is constructed via X mix = X spec + Î±X inter + Î²X n , where Î±, Î² are mixing coefficients that achieve a specific SNR. Training and test SNRs are sampled from 0-20dB and 2.5-17.5dB ranges, respectively. X mix is transformed to a logmel representation, log X mel âˆˆ R T Ã—F , where T = 116 and F = 80. We augment lip frames, X vid , via random cropping (Â± 5 pixels) and left-right flips. Augmented frames are resized to 112 Ã— 112 and fed into the visual network.</p><p>Objective evaluation results are shown in <ref type="table">Table 3</ref>. We evaluate enhanced speech using the perceptual quality of speech quality (PESQ) <ref type="bibr" target="#b73">[73]</ref> and the short-time objective intelligibility (STOI) <ref type="bibr" target="#b74">[74]</ref>. The audio only (AO) model is trained without the visual network and establishes an AO speech enhancement baseline. The AV baseline model establishes a baseline for predominant AVSE approaches that perform late fusion via CWC of AV features. We closely aligned the fusion mechanism in our AV baseline model architecture to that of <ref type="bibr" target="#b5">[6]</ref>, and we matched the sample gen-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Modalities Accuracy HCNours Pose 77.96 I3D <ref type="bibr" target="#b48">[48]</ref> RGB 89.25 DSSCA -SSLM <ref type="bibr" target="#b75">[75]</ref> RGB+Pose 74.86 Bilinear Learning <ref type="bibr" target="#b29">[29]</ref> RGB+Pose 83.0 2D/3D Multitask <ref type="bibr" target="#b28">[28]</ref> RGB+Pose 85.5 PoseMap <ref type="bibr" target="#b11">[11]</ref> RGB+Pose 91.71 Late Fusion (I3D + HCNours) RGB+Pose 91.56 Ours RGB+Pose 91.99 <ref type="table">Table 4</ref>. Accuracies of different multimodal fusion action recognition methods on the NTU-RGBD dataset <ref type="bibr" target="#b55">[55]</ref>.</p><p>eration and training procedure as best we could given the information available. We report on <ref type="bibr" target="#b5">[6]</ref> for reference only. Our AVSE model outperforms the AO and AV baselines on both objective measures PESQ and STOI. We outperform the AO baseline by 0.3 PESQ and 0.01 in STOI, demonstrating that visual information improves speech enhancement performance. Further, we outperform the AV baseline with CWC fusion by 0.06 PESQ, indicating that MMTM via slow fusion affords the greatest performance improvement. Our model generalizes to speakers unseen during training since CID is disjoint across train/test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action Recognition</head><p>NTU-RGBD dataset <ref type="bibr" target="#b55">[55]</ref> is a well-known large scale multimodal dataset. It contains 56, 880 samples captured from 40 subjects performing 60 classes of activities at 80 view-points. Each action clip includes up to two people on the RGB video as well as 25 body joints on 3D coordinate space. We followed the cross-subject evaluation <ref type="bibr" target="#b55">[55]</ref> that splits the 40 subjects into training and testing sets. To have a fair comparison with previous works, we only use RGB and pose (skeleton) modalities. The architecture details can be found in Section 4.3. We followed section 5.1 for training settings as well as RGB data preparation and augmentation. <ref type="table">Table 5</ref> shows the result of our method in comparison with the recent state-of-the-art methods on NTU-RGBD dataset. The first part of the table shows our unimodal baselines with I3D on RGB and HCN [53] on skeletons. We use 3D skeletons and follow the 32 frame subsampling method from the original paper. For simplicity in the fusion mechanism, we implemented multi-person slow fusion method <ref type="bibr" target="#b53">[53]</ref>. Consequently, our reported accuracy on HCN is lower than the result in <ref type="bibr" target="#b53">[53]</ref>. The second part shows stateof-the-art methods specifically design for action recognition by integrating RGB and skeleton. Our proposed fusion method outperforms all the recent action recognition algorithms. To our knowledge this is a new state-of-the-art result for RGB+Pose on the NTU-RGBD dataset <ref type="bibr" target="#b55">[55]</ref>.</p><p>Next, we use the recently released code of <ref type="bibr" target="#b17">[17]</ref> to compare several general purpose multimodal fusion algorithms on this dataset. We implement and train the proposed method within this framework. To have an identical set-ting with other methods, we use inflated Resnet-50 <ref type="bibr" target="#b76">[76]</ref> for video processing and the implementation of HCN <ref type="bibr" target="#b53">[53]</ref> provided in this framework for skeleton processing. <ref type="table">Table 4</ref> illustrates the performance of these unimodal networks as well as different state-of-the-art multimodal fusion methods. MFAS <ref type="bibr" target="#b17">[17]</ref> is an architecture search algorithm that leverages a sequential architecture exploration method to find an optimal fusion architecture. In addition to the two stream CNN <ref type="bibr" target="#b14">[14]</ref>, which is a late fusion algorithm, we also report the results of two intermediate fusion algorithms Gated Multimodal Units (GMU) <ref type="bibr" target="#b23">[23]</ref> and CentralNet <ref type="bibr" target="#b18">[18]</ref>. Our method outperforms the state-of-the-art MFAS method without an extensive model search on this dataset. We believe this performance could be further improved by a comprehensive architecture tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Modalities Accuracy HCN <ref type="bibr" target="#b53">[53]</ref> Pose 85.24 Infalated Resnet-50 <ref type="bibr" target="#b76">[76]</ref> RGB 83.91 Two Stream <ref type="bibr" target="#b14">[14]</ref> RGB+Pose 88.60 GMU <ref type="bibr" target="#b23">[23]</ref> RGB+Pose 85.80 CentralNet <ref type="bibr" target="#b18">[18]</ref> RGB+Pose 89.36 MFAS <ref type="bibr" target="#b17">[17]</ref> RGB+Pose 90.04 Ours RGB+Pose 90.11 <ref type="table">Table 5</ref>. Comparison of state-of-the-art multimodal fusion algorithms on the NTU-RGBD dataset <ref type="bibr" target="#b55">[55]</ref>. All methods use HCN and Infalated Resnet-50 backbone unimodal architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of the Network</head><p>To understand the effects of some of our model choices, we explore the performance of some variations of our model on the NVGesture dataset <ref type="bibr" target="#b37">[37]</ref>. In particular, we compare our fusion method with different architectures in the transfer layer. We also explore using a different number of transfer layers when all the implementation details are the same as RGB+Depth gesture recognition network described in Section 5.1.2.</p><p>Since the spatial dimensions are aligned in this problem, we can directly concatenate the convolutional features without squeezing them in the MMTM. In order to keep the spatial dimensions of these features across the module, we also need to change all the fully connected layers in MMTM to convolution layers with kernel size 1. This ensures that the number of parameters remains the same. We refer to this approach as convolutional MMTM. In addition, we also use a variation of the convolutional MMTM that utilizes a sum operation instead of the gating operation. This approach is closely related to residual learning <ref type="bibr" target="#b51">[51]</ref> and has been proposed for multimodal fusion with aligned spatial dimensions <ref type="bibr" target="#b77">[77]</ref>. Finally, we evaluate the performance of the original Squeeze and Excitation (SE) approach in which each unimodal stream uses self excitation to recalibate its own channel-wise features. The scores of these unimodal  networks are fused by late fusion at the end. <ref type="table" target="#tab_3">Table 6</ref> compares the accuracy of these variations, as well as their FLOPS and number of parameters with the late fusion and MMTM. Surprisingly, the convolutional MMTM variations do not show any noticeable improvement over the late fusion method. This result highlights the importance of extracting information with global receptive field information in the squeeze unit. We also note that not using the squeeze blocks increase the number of FLOPS by about 5 times. Finally, the result of self excitation approach with no intermediate fusion clearly shows that the most of performance gain in MMTM is due to the slow fusion of the modalities rather than pure squeeze and excitation method.</p><p>As we mentioned in Section 4.1, we use MMTM after the last 6 inception modules. In the last study, we evaluate the performance of the RGB+Depth gesture recognition network with MMTM applied to a different number of inception modules. <ref type="figure">Figure 6</ref> shows how the performance changes with respect to the number of MMTMs. This experiment indicates that the best performance is achieved when the output of half of the last inception modules (6 out of 12) are fused by MMTM. This suggests that mid-level and high-level features benefit more than low-level features from this approach.  <ref type="figure">Figure 6</ref>. Accuracy vs. #MMTMs on the NVGesture dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a simple neural network fusion module for leveraging the knowledge from multiple modalities in convolutional neural networks. The proposed module can be added at different levels of the feature hierarchy, allowing slow modality fusion. A wide range of experiments on applications with different types of modalities show applicability of the proposed module to gesture recognition, audiovisual speech enhancement, and human action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) early fusion (b) late fusion (c) intermediate fusion with Multimodal Transfer Module (MMTM). MMTM operates between CNN streams and uses information from different modalities to recalibrate channel-wise features in each modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An overview of our AVSE architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different MMTM architectures on the NVGesture dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In general, it is possible to have more than two (e.g. time dimension in 3D convolutions could be treated as a spatial dimension) or no (e.g. fully connected layers) spatial dimensions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilayer and multimodal fusion of deep neural networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for robust feature generation in audiovisual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition using multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04121</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<publisher>Avinatan Hassidim, William T Freeman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03619</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanesh</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multisensory contributions to low-level,unisensory processing. Current opinion in neurobiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foxe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multisensory processing in sensoryspecific cortical areas. The neuroscientist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>Macaluso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving the performance of unimodal dynamic handgesture recognition with multimodal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Reza</forename><surname>Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Audiovisual fusion: Challenges and new approaches. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Aggelos K Katsaggelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bahaadini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsang Park, Rohit Prasad, and Premkumar Natarajan. Multimodal feature fusion for robust event detection in web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Tsakalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MFAS: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>PÃ©rez-RÃºa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">StÃ©phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FrÃ©dÃ©ric</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Centralnet: a multilayer approach for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">StÃ©phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FrÃ©dÃ©ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning multi-modal architectures by stochastic regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©mi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust late fusion with rank minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangnan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hong</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y GÃ³mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>GonzÃ¡lez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dense multimodal fusion for hierarchically joint representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DIY human action dataset generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Khodabandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Reza</forename><surname>Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pradeep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep bilinear learning for RGB-D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neuroperception: Facial expressions linked to monkey calls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><forename type="middle">K</forename><surname>Ghazanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logothetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Communication goes multimodal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Partan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Marler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sound improves visual discrimination learning in avian predators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candy</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B: Biological Sciences</title>
		<meeting>the Royal Society of London. Series B: Biological Sciences</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">My lips are concealed: Audio-visual speech enhancement through obstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04975</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual speech enhancement using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Hui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-sensor system for driver&apos;s hand-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kari</forename><surname>Pulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3D convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using convolutional 3D neural networks for user-independent continuous gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition based on the ResC3D network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features using 3DCNN and convolutional LSTM for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Syed Afaq Shah, and Mohammed Bennamoun</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Egocentric gesture recognition using recurrent 3D convolutional neural networks with spatiotemporal transformer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multimodal gesture recognition using 3-D convolution and convolutional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motion fused frames: Data level fusion strategy for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gesture recognition: Focus on the hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradyumna</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">EgoGesture: A new dataset and benchmark for egocentric hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">MS-ASL: A largescale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaezi</forename><surname>Hamid Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Themos Stafylakis and Georgios Tzimiropoulos. Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On training targets and objective functions for deep-learning-based audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigurdur</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJ-CAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<title level="m">Voxceleb2: Deep speaker recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>FarnebÃ¤ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A robust and efficient video representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations. IEEE transactions on intelligent transportation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Heidi Christensen, and Phil Green. The PASCAL CHiME speech separation and recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The third CHIME speech separation and recognition challenge: Analysis and outcomes. Computer Speech and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Online plca for real-time semi-supervised source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gautham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A tandem algorithm for pitch estimation and voiced speech segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">S3FD: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08789</idno>
		<title level="m">Asaph Shamir, and Shmuel Peleg. Visual speech enhancement</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-T Recommendation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Rec. ITU-T P. 862</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Deep multimodal feature analysis for action recognition in RGB+D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
