<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
							<email>yun.tang@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
							<email>jing.huang@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
							<email>guangtao.wang@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<email>xiaodong.he@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>JD</roleName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>bowen.zhou@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graph is a multi-relational graph whose nodes represent entities and edges denote relationships between entities. Knowledge graphs store facts about people, places and world from various sources. Those facts are kept as triples (head entity, relation, tail entity) and denoted as <ref type="bibr">(h, r, t)</ref>. A large number of knowledge graphs, such as Freebase <ref type="bibr" target="#b4">(Bollacker et al., 2008)</ref>, DBpedia <ref type="bibr" target="#b0">(Auer et al., 2007)</ref>, NELL <ref type="bibr" target="#b7">(Carlson et al., 2010)</ref> and YAGO3 <ref type="bibr" target="#b15">(Mahdisoltani et al., 2013)</ref>, have been built over the years and successfully applied to many domains such as recommendation and question answering <ref type="bibr" target="#b5">(Bordes et al., 2014;</ref><ref type="bibr" target="#b32">Zhang et al., 2016)</ref>. However, these knowledge graphs need to be updated with new facts periodically. Therefore many knowledge graph embedding methods have been proposed for link prediction that is used for knowledge graph completion.</p><p>Knowledge graph embedding represents entities and relations in continuous vector spaces. Started from a simple and effective approach called TransE <ref type="bibr" target="#b6">(Bordes et al., 2013)</ref>, many knowledge graph embedding methods have been proposed, such as TransH <ref type="bibr" target="#b30">(Wang et al., 2014)</ref>, DistMult <ref type="bibr" target="#b31">(Yang et al., 2014)</ref>, ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref> to the latest RotatE <ref type="bibr" target="#b24">(Sun et al., 2019)</ref> and QuatE <ref type="bibr" target="#b33">(Zhang et al., 2019)</ref>.</p><p>Though much progress has been made, 1-to-N, <ref type="bibr">N-to-1, and N-to-N relation predictions (Bordes et al., 2013;</ref><ref type="bibr" target="#b30">Wang et al., 2014)</ref> still remain challenging. In <ref type="figure" target="#fig_0">Figure 1</ref>, relation "profession" demonstrates an N-to-N example and the corresponding edges are highlighted as green. Assuming the triple (SergeiRachmaninoff, Profession, Pianist) is unknown. The link prediction model takes "SergeiRachmaninoff" and relation "Profession" and rank all entities in the knowledge graph to predict "Pianist". Entity "SergeiRachmaninoff" connected to multiple entities as head entity via relation "profession", while "Pianist" as a tail entity also reaches to multiple entities through relation "profession". It makes the N-to-N prediction hard because the mapping from certain entity-relation pair could lead to multiple different entities. Same issue happens with the case of 1-to-N and N-to-1 predictions.</p><p>The recently proposed RotatE <ref type="bibr" target="#b24">(Sun et al., 2019</ref>) models each relation as a 2-D rotation from the source entity to the target entity. The desired properties for relations include symmetry/antisymmery, inversion and composition which have been demonstrated to be useful for link prediction in knowledge graph. Many existing methods model one or a few of these relation patterns, while RotatE naturally handles all these relation patterns. In addition, the entity and relation embeddings are divided into multiple groups (for example, 1000 2-D rotations are used in <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>). Each group is modeled and scored independently. The final score is computed as the summation of all these scores, which can be viewed as an ensemble of different models and further boost the performance of link prediction. However, RotatE is limited to 2-D rotations and thus has limited modeling capacity. In addition, RotatE does not consider graph context, which is helpful in handling 1-to-N, N-to-1, and N-to-N relation prediction.</p><p>In this work, a novel distance-based knowledge graph embedding called orthogonal transform embedding (OTE) with graph context is proposed to alleviate the 1-to-N, N-to-1 and N-to-N issues, while keeps the desired relation patterns as RotatE. First, we employ orthogonal transforms to represent relations in high dimensional space for better modeling capability. The Orthogonal transform embedding also models the symmetry/antisymmery, inversion and compositional relation patterns just as RotatE does. RotatE can be viewed as an orthogonal transform in 2D complex space.</p><p>Second, we integrate graph context directly into the distance scoring, which is helpful to predict 1-to-N, N-to-1 and N-to-N relations. For example, from the incomplete knowledge graph, people find useful context information, such as (SergeiRachmaninoff, role, Piano) and (SergeiRachmaninoff, Profession, Composer) in <ref type="figure" target="#fig_0">Figure 1</ref>. In this work, each node embedding in knowledge graph is augmented with two graph context representations, computed from the neighboring outgoing and incoming nodes respectively. Each context repre-sentation is computed based on the embeddings of the neighbouring nodes and the corresponding relations connecting to these neighbouring nodes. These context representations are used as part of the distance scoring function to measure the plausibility of the triples during training and inference. We show that OTE together with graph context modeling performs consistently better than RotatE on the standard benchmark FB15k-237 and WN18RR datasets.</p><p>In summary, our main contributions include:</p><p>• A new orthogonal transform embedding OTE, is proposed to extend RotatE from 2D space to high dimensional space, which also models symmetry/antisymmery, inversion and compositional relation patterns;</p><p>• A directed graph context modeling method is proposed to integrate knowledge graph context (including both neighboring entity nodes and relation edges) into the distance scoring function;</p><p>• Experimental results of OTE on standard benchmark FB15k-237 and WN18RR datasets show consistent improvements over RotatE, the state of art distance-based embedding model, especially on FB15k-237 with many high in-degree nodes. On WN18RR our results achieve the new state-of-the-art performance.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embedding</head><p>Knowledge graph embedding could be roughly categorized into two classes : distance-based models and semantic matching models. Distance-based model is also known as additive models, since it projects head and tail entities into the same embedding space and the distance scoring between two entity embeddings is used to measure the plausibility of the given triple.</p><p>TransE <ref type="bibr" target="#b6">(Bordes et al., 2013)</ref> is the first and most representative translational distance model. A series of work is conducted along this line such as TransH <ref type="bibr" target="#b30">(Wang et al., 2014)</ref>, TransR <ref type="bibr" target="#b14">(Lin et al., 2015)</ref> and TransD <ref type="bibr" target="#b11">(Ji et al., 2015)</ref> etc. RotatE <ref type="bibr" target="#b24">(Sun et al., 2019)</ref> further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult <ref type="bibr" target="#b31">(Yang et al., 2014)</ref>, Com-plEx <ref type="bibr" target="#b26">(Trouillon et al., 2016)</ref>, ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref> and QuatE <ref type="bibr" target="#b33">(Zhang et al., 2019)</ref>. <ref type="bibr">ConvKB (Nguyen et al., 2017)</ref> and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks <ref type="bibr" target="#b13">(Kipf and Welling, 2016;</ref><ref type="bibr" target="#b27">Veličković et al., 2017)</ref> for graph context modeling. Encoder-decoder framework was adopted in <ref type="bibr" target="#b21">(Schlichtkrull et al., 2017;</ref><ref type="bibr" target="#b22">Shang et al., 2019;</ref><ref type="bibr" target="#b3">Bansal et al., 2019)</ref>. The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding <ref type="bibr" target="#b16">(Nathani et al., 2019)</ref>. We take another approach in this paper: we integrate the graph context directly into the distance scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Orthogonal Transform</head><p>Orthogonal transform is considered to be more stable and efficient for neural networks <ref type="bibr" target="#b20">(Saxe et al., 2013;</ref><ref type="bibr" target="#b28">Vorontsov et al., 2017)</ref>. However, to optimize a linear transform with orthogonal property reserved is not straightforward. Soft constraints could be enforced during optimization to encourage the learnt linear transform close to be orthogonal. <ref type="bibr" target="#b2">Bansal et al. (2018)</ref> extensively compared different orthogonal regularizations and find regularizations make the training faster and more stable in different tasks. On the other hand, some work has been done to achieve strict orthogonal during optimiza-tion by applying special gradient update scheme. <ref type="bibr" target="#b9">Harandi and Fernando (2016)</ref> proposed a Stiefel layer to guarantee fully connected layers to be orthogonal by using Reimannian gradients. <ref type="bibr" target="#b10">Huang et al. (2017)</ref> consider the estimation of orthogonal matrix as an optimization over multiple dependent stiefel manifolds problem and solve it via eigenvalue decomposition on a proxy parameter matrix. <ref type="bibr" target="#b28">Vorontsov et al. (2017)</ref> applied hard constraint on orthogonal transform update via Cayley transform. In this work, we construct the orthogonal matrix via Gram Schmidt process and the gradient is calculated automatically through autograd mechanism in PyTorch <ref type="bibr" target="#b19">(Paszke et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Method</head><p>We consider knowledge graph as a collection of triples D = {(h, r, t)} with V as the graph node set, and R as the graph edge set. Each triple has a head entity h and tail entity t, where h, t ∈ V . Relation r ∈ R connects two entities with direction from head to tail. As discussed in the introduction section, 1-to-N, N-to-1 and N-to-N relation prediction <ref type="bibr" target="#b6">(Bordes et al., 2013;</ref><ref type="bibr" target="#b30">Wang et al., 2014)</ref> are difficult to deal with. They are addressed in our proposed approach by: 1) orthogonal relation transforms that operate on groups of embedding space. Each group is modeled and scored independently, and the final score is the sum of all group scores. Hence, each group could address different aspects of entity-relation pair and alleviate the 1-to-N and N-to-N relation mapping issues; and 2) directed graph context to integrate knowledge graph structure information to reduce the ambiguity.</p><p>Next, we first briefly review RotatE that motivates our orthogonal transform embedding (OTE), and then describe the proposed method in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RotatE</head><p>OTE is inspired by RotatE <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>. In RotatE, the distance scoring is done via Hadamard production (element-wise) defined on the complex domain. Given a triple (h, r, t), the corresponding embedding are e h , θ r , e t , where e h and e t ∈ R 2d , θ r ∈ R d , and d is the embedding dimension. For each dimension i, e[2i] and e[2i + 1] are corresponding real and imaginary components. The projectionẽ t of t from corresponding relation and head entities is conducted as an orthogonal transform as below:</p><formula xml:id="formula_0">ẽ t [2i] e t [2i+1] = M r (i) e h [2i] e h [2i+1] = cos θ r (i) − sin θ r (i) sin θ r (i) cos θ r (i) e h [2i] e h [2i+1] where M r (i) is a 2D orthogonal matrix derived from θ r .</formula><p>Though RotatE is simple and effective for knowledge graph link prediction, it is defined in 2D complex domain and thus has limited modeling capability. A natural extension is to apply similar operation on a higher dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Orthogonal Transform Embedding (OTE)</head><p>We use e h , M r , e t to represent embeddings of head, relation and tail entity, where e h , e t ∈ R d , and d is the dimension of the entity embedding. The entity embedding e x , where x = {h, t}, is further divided into K sub-embeddings, e.g.,</p><formula xml:id="formula_1">e x = [e x (1); ⋯; e x (K)], where e x (i) ∈ R d s and d = K ⋅ d s . M r is a collection of K lin- ear transform matrix M r = {M r (1), ⋯, M r (K)}, and M r (i) ∈ R d s ×d s .</formula><p>For each sub-embedding e t (i) of tail t, we define the projection from h and r to t as below:</p><formula xml:id="formula_2">e t (i) = f i (h, r) = φ(M r (i))e h (i)<label>(1)</label></formula><p>where φ is the Gram Schmidt process (see details in Section 3.3) applied to square matrix M r (i).</p><p>The output transform φ(M r (i)) is an orthogonal matrix derived from M r (i).ẽ t is the concatenation of all sub-vectorẽ t (i) from Eq. 1, e.g.,</p><formula xml:id="formula_3">e t = f (h, r) = [ẽ t (1); ⋯;ẽ t (K)].</formula><p>The L 2 norm of e h (i) is preserved after the orthogonal transform. We further use a scalar tensor s r (i) ∈ R d s to scale the L 2 norm of each group of embedding separately. Eq. 1 is re-written as</p><formula xml:id="formula_4">e t (i) = diag(exp(s r (i)))φ(M r (i))e h (i) (2)</formula><p>Then, the corresponding distance scoring function is defined as</p><formula xml:id="formula_5">d((h, r), t) = K i=1 (||ẽ t (i) − e t (i)||)<label>(3)</label></formula><p>For each sub-embedding e h (i) of head h, we define the projection from r and t to h as below:</p><formula xml:id="formula_6">e h (i) = diag(exp(−s r (i)))φ(M r (i)) T e t (i) (4)</formula><p>where the reverse project from tail to head is simply transposing the φ(M r (i)) and reversing the sign of s r . Then, the corresponding distance scoring function is defined as</p><formula xml:id="formula_7">d(h, (r, t)) = K i=1 (||ẽ h (i) − e h (i)||). (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gram Schmidt Process</head><p>We employ Gram-Schmidt process to orthogonalize a linear transform into an orthogonal transform (i.e., φ(M r (i)) in Section 3.2). The Gram-Schmidt process takes a set of tensor S = {v 1 , ⋯, v k } for k ≤ d s and generates an orthogonal set S ′ = {u 1 , ⋯, u k } that spans the same k−dimensional subspace of R d s as S.</p><formula xml:id="formula_8">t i = v k − k−1 j=1 ⟨v k , t j ⟩ ⟨t j , t j ⟩ t j (6) u i = t i ||t i ||<label>(7)</label></formula><p>where t 1 = v 1 , ||t|| is the L 2 norm of vector t and ⟨v, t⟩ denotes the inner product of v and t.</p><p>Orthogonal transform has many desired properties, for example, the inverse matrix is obtained by simply transposing itself. It also preserves the L 2 norm of a vector after the transform. For our work, we are just interested in its property to obtain inverse matrix by simple transposing. This saves the number of model parameters (see <ref type="table" target="#tab_5">Table 3</ref>).</p><p>It can be easily proved that OTE has the ability to model and infer all three types of relation patterns: symmetry/antisymmetry, inversion, and composition as RotatE does. The proof is listed in Appendix A.</p><p>It should be noted that, M r (i) is calculated every time in the neural networks forward computation to get orthogonal matrix φ(M r (i)), while the corresponding gradient is calculated and propagated back to M r (i) via autograd computation within PyTorch during the backward computation. It eliminates the need of special gradient update schemes employed in previous hard constraint based orthogonal transform estimations <ref type="bibr" target="#b9">(Harandi and Fernando, 2016;</ref><ref type="bibr" target="#b28">Vorontsov et al., 2017)</ref>. In our experiments, we initialize M r (i) to make sure they are with full rank 1 . During training, we also keep checking the determinant of M r (i). We find the update is fairly stable that we don't observe any issues with subembedding dimensions varied from 5 to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Directed Graph Context</head><p>The knowledge graph is a directed graph: valid triple (h, r, t) does not mean (t, r, h) is also valid. Therefore, for a given entity in knowledge graph, there are two kinds of context information: nodes that come into it and nodes that go out of it. Specially, in our paper, for each entity e, we consider the following two context settings:</p><p>1. If e is a tail, all the (head, relation) pairs in the training triples whose tail is e are defined as Head Relation Pair Context. 2. If e is a head, all the (relation, tail) pairs in the training triples whose head is e are defined as Relation Tail Pair Context. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the computation of graph context for a testing triple (SergeiRachmaninoff, profession, Pianist). Edges for relation "profession" are colored as green. Entities marked with • are head entities to entity "Pianist", and these entities and corresponding relations to connect "Pianist" form the head relation pair context of "Pianist". While entities with ⭒ are tail entities for entity "SergeiRachmaninoff". Those entities and corresponding relations are the relation tail graph context of entity "SergeiRachmaninoff".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Head Relation Pair Context</head><p>For a given tail t, all head-relation pairs (h ′ , r ′ ) of the triples with tail as t are considered as its graph context and denoted as N g(t).</p><p>First, we compute the head-relation context representationẽ c t as the average from all these pairs in N g(t) as below:</p><formula xml:id="formula_9">e c t = ∑ (h ′ ,r ′ )∈N g(t) f (h ′ , r ′ ) + e t |N g(t)| + 1<label>(8)</label></formula><p>where e t is the embedding of the tail t, f (h ′ , r ′ ) is the representation of (h ′ , r ′ ) induced from Eq. 2.</p><p>We use e t in Eq. 8 to make the computation of context representation possible when N g(t) is empty. This can be viewed as a kind of additive smoothing for context representation computation. Then, we compute the distance of the headrelation context of t and the corresponding orthogonal transform based representation of a triple (h, r, t) as follow.</p><formula xml:id="formula_10">d c ((h, r), t) = K i=1 (||ẽ t (i) −ẽ c t (i)||) (9) whereẽ t (i) is computed from Eq. 2.</formula><p>There is no new parameter introduced for the graph context modeling, since the message passing is done via OTE entity-relation project f (h ′ , r ′ ).</p><p>The graph context can be easily applied to other translational embedding algorithms, such as RotatE and TransE etc, by replacing OTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Relation Tail Pair Context</head><p>For a given head h, all relation-tail pairs (r ′ , t ′ )</p><p>of the triples with head as h are considered as its graph context and denoted as N g(h). First, we compute the relation-tail context representationẽ c h as the average from all these pairs in N g(h) as below:</p><formula xml:id="formula_11">e c h = ∑ (r ′ ,t ′ )∈N g(h) f (r ′ , t ′ ) + e h |N g(h)| + 1 (10) where f (r ′ , t ′ ) is computed from Eq. 4.</formula><p>Then, we compute the distance of the relationtail context of h and the corresponding orthogonal transform based representation of a triple (h, r, t) as follow.</p><formula xml:id="formula_12">d c (h, (r, t)) = K i=1 (||ẽ h (i) −ẽ c h (i)||) (11)</formula><p>whereẽ h (i) is computed from Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Scoring Function</head><p>We further combine all four distance scores (Eq. 3, Eq. 5, Eq. 9 and Eq. 11) discussed above as the final distance score of the graph contextual orthogonal transform embedding (GC-OTE) for training and inference</p><formula xml:id="formula_13">d all (h, r, t) = d((h, r), t) + d c (h, (r, t)) +d(h, (r, t)) + d c ((h, r), t). (12)</formula><p>Therefore the full GC-OTE model can be seen as an ensemble of K local GC-OTE models. This view provides an intuitive explanation for the success of GC-OTE. Optimization Self-adversarial negative sampling loss <ref type="bibr" target="#b24">(Sun et al., 2019)</ref> is used to optimize the embedding in this work,</p><formula xml:id="formula_14">L = − p(h ′ , r, t ′ ) log σ(d all (h ′ , r, t ′ ) − γ) − log σ(γ − d all (h, r, t))<label>(13)</label></formula><p>where γ is a fixed margin, σ is sigmoid function, (h ′ , r, t ′ ) is negative triple, and p(h ′ , r, t ′ ) is the negative sampling weight defined in <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>.  Each dataset is split into three sets for: training, validation and testing, which is same with the setting of <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>. The statistics of two data sets are summarized at Table 1. Only triples in the training set are used to compute graph context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Protocol</head><p>Following the evaluation protocol in <ref type="bibr" target="#b8">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b24">Sun et al., 2019)</ref>, each test triple (h, r, t) is measured under two scenarios: head focused (?, r, t) and tail focused (h, r, ?). For each case, the test triple is ranked among all triples with masked entity replaced by entities in knowledge graph. Those true triples observed in either train/validation/test set except the test triple will be excluded during evaluation. Top 1, 3, 10 (Hits@1, Hits@3 and Hits@10), and the Mean Reciprocal Rank (MRR) are reported in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>Hyper-parameter settings The hyper-parameters of our model are tuned by grid search during training process, including learning rate, embedding dimension d and sub-embedding dimension d s . In our setting, the embedding dimension is defined as the number of parameters in each entity embedding. Each entity embedding consists of K subembeddings with dimension d s , i.e., d = K × d s . There are two steps in our model training: 1) the model is trained with OTE or RotatE models, and 2) graph context based models are fine tuned on these pre-trained models. The parameter settings are selected by the highest MRR with early stopping on the validation set. We use the adaptive moment (Adam) algorithm <ref type="bibr" target="#b12">(Kingma and Ba, 2014)</ref> to train the models.</p><p>Specially, for FB15k-237, we set embedding dimension d = 400, sub-embedding dimension d s = 20, and the learning rates to 2e-3 and 2e-4 for pre-training and fine-tuning stages respectively; for WN18RR dataset, we set d = 400, d s = 4, and the learning rates to 1e-4 and 3e-5 for pre-training and fine-tuning stages. Implementation Our models are implemented by PyTorch and run on NVIDIA Tesla P40 Graphics Processing Units. The pre-training OTE takes 5 hours with 240,000 steps and fine-tuning GC-OTE takes 23 hours with 60,000 steps. Though, it takes more computation for graph context based model training, the inference could be efficient if both head and tail context representations are precomputed and saved for each entity in the knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>In this section, we first present the results of link prediction, followed by the ablation study and error analysis of our models. <ref type="table" target="#tab_3">Table 2</ref> compares the proposed models (OTE and graph context based GC-OTE) to several stateof-the-art models: including translational distance based TransE <ref type="bibr" target="#b6">(Bordes et al., 2013)</ref>, Ro-tatE <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>; semantic matching based DistMult <ref type="bibr" target="#b31">(Yang et al., 2014</ref><ref type="bibr">), ComplEx (Trouillon et al., 2016</ref>, ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref> and QuatE <ref type="bibr" target="#b33">(Zhang et al., 2019)</ref>, and graph context information based R- <ref type="bibr">GCN+ (Schlichtkrull et al., 2017)</ref>, SACN <ref type="bibr" target="#b22">(Shang et al., 2019)</ref> and A2N <ref type="bibr" target="#b3">(Bansal et al., 2019)</ref>. These  baseline numbers are quoted directly from published papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Results of Link Prediction</head><p>From <ref type="table" target="#tab_3">Table 2</ref>, we observe that: 1) on FB15k-237, OTE outperforms RotatE, and GC-OTE outperforms all other models on all metrics. Specifically MRR is improved from 0.338 in RotatE, to 0.361, about 7% relative performance improvement. OTE which increases sub-embedding dimension from 2 to 20, and graph context each contributes about half the improvement; 2) on WN18RR, OTE outperforms RotatE and GC-OTE achieves the new state-of-the-art results (as far as we know from published papers). These results show the effectiveness of the proposed OTE and graph context for the task of predicting missing links in knowledge graph.</p><p>Moreover, GC-OTE improves more on FB15k-237 than on WN18RR. This is because FB15k-237 has richer graph structure context compared to WN18RR: an average of 19 edges per node v.s. 2 edges per node in WN18RR. These results indicate that the proposed method GC-OTE is more effective on data set with rich context structure information.   <ref type="table" target="#tab_5">Table 3</ref> shows the results of ablation study of the proposed models and compares the number of model parameters with RotatE on FB15k-237 validation set. We perform the ablation study with embedding dimension of 400. The entity embedding dimension for RotatE-S and RotatE-L are 400 and 2000, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ablation Study</head><p>First we notice that increasing embedding size from 400 to 2000 makes RotatE model size more than quadrupled while the performance gain is very limited (Row 1 and 2 in <ref type="table" target="#tab_5">Table 3</ref>); increasing group embedding size from 2 to 20 does not increase the model size of OTE much, but with nice performance gain (Row 3 and 4 in <ref type="table" target="#tab_5">Table 3</ref>). The model size of OTE is less than one-third of the size of RotatE-L but with better performance. This shows the effectiveness of the OTE.</p><p>We examine the proposed model in terms of the following aspects: Impact of sub-embedding dimension: we fix the embedding dimension as 400, and increase the subembedding dimension d s from 2 to 20, the MRR of OTE is improved from 0.327 to 0.355 (See Row 3 and Row 4). For RotatE, the entity is embedded in complex vector space, this is similar to our setting with sub-embedding dimension = 2. Our results show that increasing the sub-dimension with OTE is beneficial to link prediction. Impact of orthogonal transform: we replace the orthogonal transform operation in OTE with two different settings, 1) removing the diagonal scalar tensor as Eq. 1 (See OTE-scalar) and 2) using normal linear transform rather than orthogonal transform (See LNE). Both settings lead to MRR degradation. This indicates the proposed orthogonal transform is effective in modeling the relation patterns which are helpful for link prediction. Impact of graph context: we add the graph context based model to both OTE (See GC-OTE) and RotatE-L (See GC-RotatE-L). We observe that MRRs are improved for both RotatE-L and OTE. This shows the importance of modeling context information for the task of link prediction. Sub-embedding dimension size: in <ref type="table" target="#tab_5">Table 3</ref> we show that increasing sub-embedding dimension brings a nice improvement on MRR. Is the larger size always better? <ref type="figure" target="#fig_1">Figure 2</ref> shows the impact of d s on the OTE performance with the changing of sub-embedding size. We fix the entity embedding dimension as 400, and vary the sub-embedding size from 2, 5, 10, 20, 50, all the way to 100. The blue line and green bar represent MRR and H@10 value, respectively.</p><p>From <ref type="figure" target="#fig_1">Figure 2</ref> we observe that, both MRR and Hit@10 are improved and slowly saturated around d s = 20 The similar experiments are also conducted on WN18RR data set and we find the best subembedding dimension is 4 on WN18RR.  <ref type="table">Table 4</ref>: H@10 from FB15-237 validation set by categories (1-to-N, N-to-1 and N-to-N).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Error Analysis</head><p>We present error analysis of the proposed model on 1-to-N, N-to-1 and N-to-N relation predictions on FB15k-237. <ref type="table">Table 4</ref> shows results in terms of Hit@10, where "Num." is the number of triples in the validation set belonging to the corresponding category, "H"/"T" represents the experiment to predict head entity /tail entity, and "A" denotes average result for both "H" and "T". Assume c(h, r) and c(r, t) are the number of (h, r) and (r, t) pairs appeared in triples from the training set respectively. A triple (h, r, t) from the validation set is considered as one of the categories in the following: From <ref type="table">Table 4</ref> we observe that, comparing to Ro-tatE large model, the proposed model get better Hit@10 on all cases, especially for the difficult cases when we attempt to predicting the head entity for 1-to-N/N-to-N relation type, and tail entity in N-to-1/N-to-N relation type. The reason is because that in the proposed model, the groupings of sub-embedding relation pairs in OTE and graph context modeling both are helpful to distinguish N different tails/heads when they share the same (head, rel)/(rel, tail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we propose a new distance-based knowledge graph embedding for link prediction. It includes two-folds. First, OTE extends the modeling of RotatE from 2D complex domain to high dimensional space with orthogonal relation transforms. Second, graph context is proposed to integrate graph structure information into the distance scoring function to measure the plausibility of the triples during training and inference.</p><p>The proposed approach effectively improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N link predictions. Experimental results on standard benchmark FB15k-237 and WN18RR show that OTE improves consistently over Ro-tatE, the state-of-the-art distance-based embedding model, especially on FB15k-237 with many high in-degree nodes. On WN18RR our model achieves the new state-of-the-art results. This work is partially supported by Beijing Academy of Artificial Intelligence (BAAI).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Snapshot of knowledge graph in FB15k-237. Entities are represented as golden blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>FB15k-237 for OTE with different subembedding dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(h, r, t)= ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ N-to-1, if c(h, r) &gt; 1 and c(r, t) ≤ 1 1-to-N, if c(h, r) ≤ 1 and c(r, t) &gt; 1 N-to-N,if c(h, r) &gt; 1 and c(r, t) &gt; 1 other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Link prediction for FB15k-237 and WN18RR on test sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on FB15k-237 validation set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A real random matrix has full rank with probability 1<ref type="bibr" target="#b23">(Slinko, 2000)</ref>. We use different random seeds to make sure the generated matrix is full rank.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion on the Ability of Pattern Modeling and Inference</head><p>It can be proved that OTE can infer all three types of relation patterns, e.g., symmetry/antisymmetry, inversion and composition patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Symmetry/antisymmetry</head><p>If e t = f (r, h) and e h = f (r, t) hold, we have</p><p>In other words, if φ(M r ) is a symmetry matrix and no scale is applied, the relation is symmetry relation.</p><p>If the relation is antisymmetry, e.g., e t = f (r, h) and e h ≠ f (r, t), we just need to one of the φ(M r (i)) is not symmetry matrix or s r (i) ≠ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Inversion</head><p>If e 2 = f (r 1 , e 1 ) and e 1 = f (r 2 , e 2 ) hold, we have</p><p>In other words, if diag(exp(s r 1 ))φ(M r 1 ) = φ(M r 2 ) T diag(exp(−s r 2 )), the relation r 2 is inverse relation of r 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Composition</head><p>If e 2 = f (r 1 , e 1 ), e 3 = f (r 2 , e 2 ) and e 3 = f (r 3 , e 1 ) hold, we have</p><p>is equal to diag(exp(s r 2 ))φ(M 2 )diag(exp(s r 1 ))φ(M 1 ) then relation r 3 is composition of relation r 1 and r 2 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TuckER: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can we gain more from orthogonality regularizations in training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A2N: Attending to neighbors for knowledge graph inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrividya</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generalized backpropagation,étude de cas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Orthogonality. ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao Qin</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning entity and relation embedings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<title level="m">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A generalization of Komlós theorem on random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadii</forename><surname>Slinko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ., Department of Mathematics, School of Mathematical and Information</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On orthogonality and learning recurrent networks with long term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Joseph</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
