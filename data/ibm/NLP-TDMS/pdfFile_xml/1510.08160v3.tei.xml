<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-aware Fast R-CNN for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
						</author>
						<title level="a" type="main">Scale-aware Fast R-CNN for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Pedestrian Detection</term>
					<term>Scale-aware</term>
					<term>Deep Learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech [8], INRIA [5], and ETH [9], and obtains competitive results on KITTI <ref type="bibr" target="#b10">[11]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pedestrian detection aims to predict bounding boxes of all the pedestrian instances in an image. It has attracted much attention within the computer vision community in recent years <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref> as an important component for many human-centric applications, such as self-driving vehicles, person re-identification, video surveillance and robotics <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p><p>Recently, many research works <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref> have been devoted to pedestrian detection. However, they generally leave a critical issue caused by various scales 1 of pedestrians in an image unsolved, which is shown to considerably affect the performance of pedestrian detection in natural scenes. We provide an illustration of the motivation of the paper in <ref type="figure" target="#fig_0">Figure 1</ref>. Pedestrian instances in the video surveillance images (e.g., Caltech dataset <ref type="bibr" target="#b7">[8]</ref>) often have very small sizes. Statistically, over 60% of the instances from the Caltech training set have a height smaller than 100 pixels. Accurately localizing these small-size pedestrian instances is quite challenging due to the following difficulties. Firstly, most of the small-size instances appear with blurred boundaries and obscure appearance. It is difficult to distinguish them from the background clutters and other overlapped instances. Secondly, the large-size pedestrian <ref type="bibr">Jianan</ref> Li and Tingfa Xu are with School of Optical Engineering, Beijing Institute of Technology University, China. Xiaodan Liang is from Sun Yat-Sen University, China. ShengMei Shen is from Panasonic R&amp;D Center, Singapore. Jiashi Feng and Shuicheng Yan are from Department of Electrical and Computer Engineering, National University of Singapore. <ref type="bibr" target="#b0">1</ref> Here by "scale" we mean the "size" of the pedestrian in an image. We use these two terms interchangeably here when no confusion is caused. instances typically exhibit dramatically different visual characteristics from the small-size ones. For instance, body skeletons of the large-size instances can provide rich information for pedestrian detection while skeletons of the small-size instances cannot be recognized so easily. Such differences can also be verified by comparing the generated feature maps for largesize and small-size pedestrians, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The high feature responses for detailed body skeletons are shown for the large-size instances while only coarse feature maps are obtained for small-size instances.</p><p>Existing works address the scale-variance problem mainly from two aspects. First, the brute-force data augmentation (e.g., multi-scaling <ref type="bibr" target="#b11">[12]</ref> or resizing <ref type="bibr" target="#b12">[13]</ref>) is used to improve the scale-invariance capability. Second, a single model <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b41">[42]</ref> with multi-scale filters is employed on all instances with various sizes. However, due to the intra-class variance of large-size and small-size instances, it is difficult to handle their considerably different feature responses with a single model. To exploit the dramatically different characteristics of instances with various scales, we adopt the divide-and-conquer philosophy to address this critical scale-variance problem. Based on this philosophy, a unified framework can comprise multiple single models, each of which specializes in detecting instances with scales of a particular range by capturing scalespecific visual patterns.</p><p>Motivated by the above idea, we develop a novel Scale-Aware Fast R-CNN (SAF R-CNN) framework, which is built on the Fast R-CNN pipeline <ref type="bibr" target="#b11">[12]</ref>. The proposed SAF R- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-aware weighting</head><p>Large-size Proposal Small-size Proposal <ref type="figure">Fig. 2</ref>. Illustration of the scale-aware weighting mechanism of our SAF R-CNN. A large-size and a small-size sub-network are learned specifically to detect instances with different sizes. The final result is obtained by fusing the outputs of the two sub-networks according to the object proposal size. Given a large-size object proposal, the weight for the large-size sub-network is high while that for the small-size sub-network is low. In this way, the final result is mainly decided by the large-size network. The situation is the opposite given a small-size object proposal.</p><p>CNN integrates a large-size sub-network and a small-size subnetwork 2 into a unified architecture. As shown in <ref type="figure">Figure 2</ref>, given an input image with object proposals in it, the SAF R-CNN first passes the raw image through the bottom shared convolutional layers to extract its whole feature maps. Taking these feature maps and the locations of object proposals as inputs, two sub-networks offer different category-level confidence scores and bounding box regressions for each proposal, which are then combined to generate the final detection results using two scale-aware weights predicted by a scale-aware weighting layer that performs a gate function defined over the proposal size. SAF R-CNN employs the gate function in a following way to achieve robustness to various scales: it assigns a higher weight for the large-size sub-network when the input has a large size; otherwise, it gives a higher weight for the small-size sub-network. Such a scale-aware weighting mechanism can be deemed as the soft activations for the two sub-networks, and the final results can always be boosted by the sub-network proper for the current input of certain scales. Therefore, SAF R-CNN can achieve outperforming detection performance in a wide range of input scales. Moreover, since the SAF R-CNN shares convolutional features for the whole image with different object proposals, it is very efficient in terms of both training and testing time.</p><p>To sum up, this work makes the following contributions. Firstly, we propose a novel Scale-Aware Fast R-CNN model for pedestrian detection by incorporating a large-size subnetwork and a small-size sub-network into a unified architecture following the divide-and-conquer philosophy. Secondly, a scale-aware weighting mechanism is proposed to lift the contribution of the sub-network specialized for the current input scales and boost the final detection performance in a wide input scale range. Thirdly, extensive experiments on several challenging pedestrian datasets demonstrate that SAF R-CNN delivers new state-of-the-art performance on three out of four challenging pedestrian benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Hand-crafted Model: The models based on hand-crafted features have been widely used for object detection <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Deformable part-based models <ref type="bibr" target="#b9">[10]</ref> consider the appearance of each part and the deformation among parts for detection. The Integral Channel Features (ICF) <ref type="bibr" target="#b6">[7]</ref> and Aggregated Channel Features (ACF) <ref type="bibr" target="#b5">[6]</ref> efficiently extract features such as local sums, histograms, and Haar features using integral images. Wang et al. <ref type="bibr" target="#b39">[40]</ref> combined Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set to handle partial occlusion. Nam et al. <ref type="bibr" target="#b21">[22]</ref> introduced an efficient feature transform that removes correlations in local image neighborhoods by extending the features of <ref type="bibr" target="#b14">[15]</ref> to ACF. A multi-order context representation was used in <ref type="bibr" target="#b2">[3]</ref> to exploit co-occurrence contexts of different objects. Cai et al. <ref type="bibr" target="#b45">[46]</ref> combined features of different complexities to seek an optimal trade-off between accuracy and complexity. In addition, some approaches aim to be scale-invariant. Park et al. <ref type="bibr" target="#b27">[28]</ref> adopted a multi-resolution model that acts as a deformable part-based model when scoring large instances and a rigid template when scoring small instances. Yan et al. <ref type="bibr" target="#b42">[43]</ref> proposed to map the pedestrians in different resolutions to a common subspace to reduce the differences of local features. Then a shared detector is learned on the mapped features to distinguish pedestrians from background.</p><p>Deep Learning Model: Convolutional neural networks (CNNs) have recently been successfully applied in generic object recognition <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Some recent works focus on improving the performance of pedestrian detection using deep learning methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Sermanet et al. <ref type="bibr" target="#b31">[32]</ref> used an unsupervised method based on convolutional sparse coding to pre-train CNN for pedestrian detection. Tian et al. <ref type="bibr" target="#b34">[35]</ref> jointly optimized pedestrian detection with semantic tasks. In addition, several approaches have also been proposed to improve the scale-invariance of CNN. Gong et al. <ref type="bibr" target="#b13">[14]</ref> extracted CNN activations for local patches at three different scales and produced a concatenated feature of the patch by performing orderless VLAD pooling of these activations at each level separately. Xu et al. <ref type="bibr" target="#b41">[42]</ref> detected the input pattern at different scales in multiple columns simultaneously and concatenated the top-layer feature maps from all the columns for final classification. Previous methods often employ the same filter on the object proposals with various sizes, but the difference of intrinsic characteristics of the large-size and the small-size object proposals have not been fully explored. We explore a simple yet effective framework that consists of a large-size and a small-size sub-network, and fuses their results using the scale-aware weights with respect to the proposal sizes. The two sub-networks are learned specifically to be experts on different input scales, thus achieving high robustness to the scale-variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SCALE-AWARE FAST R-CNN (SAF R-CNN)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of Proposed Model</head><p>The proposed Scale-Aware Fast R-CNN (SAF R-CNN) framework is an ensemble of two scale specific sub-networks which detect the pedestrians of large and small sizes, respectively. The detection results of the two sub-networks are then passed through a gate function -defined over the input scale -for fusion. Such scale-aware collaboration of two subnetworks enables the proposed SAF R-CNN to accurately capture unique characteristics of objects at different scales, and meanwhile the shared convolutional filters in its early layers also incorporate the common characteristics shared by all instances. The architecture of SAF R-CNN is developed based on the popular Fast R-CNN detection framework <ref type="bibr" target="#b11">[12]</ref> due to its superior performance and computation efficiency in detecting general objects. The SAF R-CNN takes the whole image and a number of object proposals as input, and then outputs the detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pedestrian Proposals Extraction</head><p>In this paper, we utilize the ACF detector <ref type="bibr" target="#b5">[6]</ref> to generate object proposals. The ACF detector is a fast and effective sliding window based detector that performs quite well on rigid object detection. Unlike other proposal methods for detecting generic objects <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b47">[48]</ref>, the ACF detector can be trained to detect objects of a specific category, which thus can be used for extracting and mining high-quality object proposals. For fair comparison with the state-of-the-arts <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, we also use the object proposals from ACF detector as inputs. Following the standard setting on the Caltech dataset <ref type="bibr" target="#b7">[8]</ref>, we train an ACF pedestrian detector on the Caltech training set and apply the ACF detector on training and testing images with a low detection threshold of −70 to generate object proposals. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the architecture of SAF R-CNN in details. The SAF R-CNN passes the input image into several convolutional layers and max pooling layers to extract feature maps. Then the proposed network branches into two subnetworks, which are learned specifically to detect large-size and small-size instances respectively. Each of the two subnetworks takes as input the feature maps produced from the previous convolutional layers, and further extracts features through several convolutional layers to produce feature maps specialized for a specific range of input scales. The region of interest (RoI) pooling layer as proposed in <ref type="bibr" target="#b11">[12]</ref> is utilized to pool the feature maps of each input object proposal into a fixed-length feature vector which is fed into a sequence of fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture of SAF R-CNN</head><p>Each sub-network ends up with two output layers which produce two output vectors per object proposal. Particularly, one layer outputs classification scores over K object classes plus a "background" class. The other one is the boundingbox regressor which outputs refined bounding-box positions for each of the K object classes. Finally, the outputs from the two sub-networks are weightedly combined to obtain the final result for each input object proposal. Two weights for each object proposal for fusing the outputs from the two sub-networks are given by a scale-aware weighting layer that performs a gate function defined over the object proposal sizes, which will be detailed in Section III-D. The classification scores from the two sub-networks are weightedly combined by the computed weights to obtain the final classification score which is fed into a softmax layer to produce softmax probabilities over K + 1 classes for each input object proposal. Similarly, the bounding box regressions are accordingly combined by the weights to produce the final result for each proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scale-aware Weighting</head><p>As the extracted features from large-size and small-size pedestrians exhibit significant differences, SAF R-CNN incorporates two sub-networks, focusing on the detection of largesize and small-size pedestrians, respectively. A scale-aware weighting layer is designed to perform a gate function which is defined over the sizes of object proposals and used to combine the detection results from two sub-networks. Intuitively, the weights for the two sub-networks should satisfy the following constraints. As illustrated in <ref type="figure">Figure 2</ref>, given a large-size object proposal, the weight for the large-size network is supposed to be high while that for the small-size sub-network is supposed to be low. The situation is the opposite for a small-size object proposal. Thus by fusing the outputs from the two sub-networks with the weights, SAF R-CNN can be robust to diverse sizes of pedestrian instances. Such a scale-aware weighting mechanism can be deemed as the soft activations for the two sub-networks, and the final results can always be boosted by the proper sub-network for the current input of a certain size.</p><p>Note that the size of an object proposal can be measured by either its width or its height. However, for a pedestrian standing with a constant distance to the camera, the height of his bounding box varies little while the width may vary considerably with different poses of the pedestrian. This fact is also described in the Caltech benchmark <ref type="bibr" target="#b7">[8]</ref>. Thus, the height of the bounding box is more stable for measuring the size of a pedestrian.  In our proposed method, we define a scale-aware weighting layer which performs a gate function over the height of the proposal to adaptively weight the outputs from the two subnetworks. More specifically, let ω l and ω s denote the output weights computed through the scale-aware weighting layer for the large-size and the small-size sub-network respectively. Given an input object proposal with height h, ω l is calculated as</p><formula xml:id="formula_0">ω l = 1 1 + α exp − h−h β ,<label>(1)</label></formula><p>whereh denotes the average height of the pedestrians from the training set and α and β are two learnable scaling coefficients. We optimize the two parameters via back propagation.</p><p>The backwards function of the scale-aware weighting layer computes partial derivative of the loss function L which will be discussed later with respect to α and β as</p><formula xml:id="formula_1">∂L ∂α = − exp − h−h β (1 + α exp − h−h β ) 2 ∂L ∂ω l ∂L ∂β = − α(h −h) exp − h−h β β 2 (1 + α exp − h−h β ) 2 ∂L ∂ω l .<label>(2)</label></formula><p>Because the final results are obtained by fusing the outputs from the large-size and the small-size sub-network, we fix the sum of the weights for the two sub-networks as one to avoid improper domination of either model. Thus the weight for the output of the small-size sub-network ω s can be simply calculated as ω s = 1 − ω l .</p><p>Given a large-size object proposal with a high height, the value of ω l goes to 1 while ω s is close to 0. Then the final prediction is mainly contributed from the large-size subnetwork. On the contrary, given a small-size object proposal with a low height, the final results are mostly determined by the small-size sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optimization</head><p>Each sub-network in our SAF R-CNN has two sibling output layers. The first sibling layer outputs a discrete confidence score distribution s = (s 0 , ..., s k ) for each object proposal over K + 1 categories. The second sibling layer outputs the bounding-box regression offsets for each of the K object classes. The bounding-box regression offsets for the class k can be denoted as t k = (t k x , t k y , t k w , t k h ). Following the parameterization scheme in <ref type="bibr" target="#b12">[13]</ref>, t k specifies the location translation and the bounding box size shift relative to the original location and size of the object proposal. The outputs from the two sub-networks are combined according to the weights computed from the size of the input object proposal as above described. Recall that ω l and ω s are the weights for the outputs of the large-size and the small-size subnetwork respectively. A final predicted discrete confidence score distribution can be computed as</p><formula xml:id="formula_2">s f = ω l × s l + ω s × s s ,<label>(3)</label></formula><p>where s l and s s denote the discrete confidence score distribution output by the first sibling layer of the large-size and the small-size sub-network respectively. Similarly, a final weighted bounding-box regression offset is computed as</p><formula xml:id="formula_3">t f = ω l × t l + ω s × t s ,<label>(4)</label></formula><p>where t l and t s denote the bounding-box regression offsets output by the second sibling layer of the large-size and the small-size sub-network respectively. Each training proposal is labeled with a ground-truth class g and a ground-truth bounding-box regression target t * . The following multi-task loss L on each object proposal is utilized to jointly train the network parameters of two sub-networks:</p><formula xml:id="formula_4">L = L cls (s f , g) + 1[g ≥ 1]L loc (t g f , t * ),<label>(5)</label></formula><p>where L cls and L loc are the losses for the classification and the bounding-box regression, respectively. In particular, L cls is the log loss and L loc is the smooth L 1 loss <ref type="bibr" target="#b11">[12]</ref>. The Iverson bracket indicator function 1[g ≥ 1] equals 1 when g ≥ 1 and 0 otherwise. For background proposals (i.e. g = 0), the L loc is ignored. By jointly training two specialized subnetworks connected by the scale-aware weights with respect to the sizes of object proposals, SAF R-CNN is capable of outputting accurate detection results in a wide range of input scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate the effectiveness of the proposed SAF R-CNN on several popular pedestrian detection datasets including Caltech <ref type="bibr" target="#b7">[8]</ref>, INRIA <ref type="bibr" target="#b4">[5]</ref>, ETH <ref type="bibr" target="#b8">[9]</ref>, and KITTI <ref type="bibr" target="#b10">[11]</ref>. More experimental analyses on the effectiveness of each component in our network are further given on the challenging Caltech dataset <ref type="bibr" target="#b7">[8]</ref>.</p><p>A. Datasets 1) Caltech: The Caltech dataset and its associated benchmark <ref type="bibr" target="#b7">[8]</ref> are among the most popular pedestrian detection datasets. It consists of about 10 hours of videos (30 frames per second) collected from a vehicle driving through urban traffic. Every frame in the raw Caltech dataset has been densely annotated with the bounding boxes of pedestrian instances. There are totally 350,000 bounding boxes of about 2,300 unique pedestrians labeled in 250,000 frames. In the reasonable evaluation setting <ref type="bibr" target="#b7">[8]</ref>, the performance is evaluated on pedestrians over 50 pixels tall with no or partial occlusion. We use dense sampling of the training data (every 4th frame) as adopted in <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b21">[22]</ref> for evaluating all variants of SAF R-CNN. During training and testing, the scale of the input image is set as 800 pixels on the shortest side.</p><p>2) INRIA and ETH: The INRIA pedestrian dataset <ref type="bibr" target="#b4">[5]</ref> is split into a training and a testing set. The training set consists of 614 positive images and 1,218 negative images. The testing set consists of 288 images. The ETH testing set <ref type="bibr" target="#b8">[9]</ref> contains 1,804 images in three video clips. Following the training setting commonly adopted by the best performing approaches <ref type="bibr" target="#b22">[23]</ref> [10] <ref type="bibr" target="#b31">[32]</ref>, we train our SAF R-CNN model using the INRIA training set and test it on both the INRIA and the ETH testing sets, in order to evaluate the generalization capacity of our model. We use the 614 positive images in the INRIA training set as training data. Many studies (e.g., <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b28">[29]</ref>) have found that using more training data is beneficial for training deep models. Since the INRIA training set has fewer positive training samples than Caltech training set, we implement Gaussian blurring and motion blurring on the original training images for data augmentation. During training, the scale of the input image is set as 600 pixels on the shortest side. During testing, we resize the images to make the shortest side as 600 and 1200 pixels for the INRIA and the ETH testing set, respectively.</p><p>3) KITTI: The challenging KITTI dataset <ref type="bibr" target="#b10">[11]</ref> consists of 7,481 training and 7,518 test images, which are captured from an autonomous driving platform. Evaluation is done at three levels of difficulty: easy, moderate and hard, where the difficulty is measured by the minimal scale of the pedestrians to be considered and the occlusion and truncation of the pedestrians. For the moderate setting which is used to rank the competing methods in the benchmark, the pedestrians over 25 pixels tall with no or low partial occlusion and truncation are considered. Since the annotations of the testing set are not available, we split the KITTI training set into train and validation subsets as suggested by <ref type="bibr" target="#b3">[4]</ref>. The images are resized as 800 pixels on the shortest side during the training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We use the pre-trained VGG16 model <ref type="bibr" target="#b32">[33]</ref> to initialize SAF R-CNN, which is used in the most recent state-of-the-art method <ref type="bibr" target="#b45">[46]</ref>. The first seven convolutional layers and three max pooling layers of the VGG16 network are used as the shared convolutional layers before the two sub-networks to produce feature maps from the entire input image. The rest layers of the VGG16 network are used to initialize both the large-size and the small-size sub-network. The fourth max pooling layer is removed to produce larger feature maps in both sub-networks. We observe that this operation improves the detection performance. Following Fast R-CNN <ref type="bibr" target="#b11">[12]</ref>, the last max pooling layer of the VGG16 network is replaced by the RoI pooling layer to pool the feature maps of each object proposal into fixed resolution, i.e. 7×7. The final fullyconnected layer and softmax are replaced with two sibling fully-connected layers.</p><p>The SAF R-CNN is trained with Stochastic Gradient Descent (SGD) with momentum of 0.9, and weight decay of 0.0005. Each mini-batch consists of 80 randomly sampled object proposals in one randomly selected image, where 20 positive object proposals are with intersection over union (IoU) with the ground truth box larger than 0.5, and the rest 60 object proposals which have IoU with the ground-truth bounding box less than 0.5 act as negative training instances. To compute the scale-aware weights of each proposal for two sub-networks, the initial values of parameter α and β in Eqn. (1) are set as 1 and 10, respectively. For data augmentation, images are horizontally flipped with a probability of 0.5.</p><p>SAF R-CNN is implemented based on the publicly available Caffe platform <ref type="bibr" target="#b17">[18]</ref>. The whole network is trained on a single NVIDIA GeForce GTX TITAN X GPU with 12GB memory. For training, the first four convolutional layers in the network keep constant parameters initialized from the pretrained VGG16 model. The other layers update parameters with an initial learning rate of 0.001 which is lowered to 1/10 of the current rate after every 4 epochs. We fine-tune the networks for about 7 epochs on the training set.  experimental results are reported in <ref type="figure" target="#fig_3">Figure 4</ref>. We compare the result of SAF R-CNN with all the existing methods that achieved best performance on the Caltech testing set, including VJ <ref type="bibr" target="#b36">[37]</ref>, HOG <ref type="bibr" target="#b4">[5]</ref>, LDCF <ref type="bibr" target="#b21">[22]</ref>, Katamari <ref type="bibr" target="#b1">[2]</ref>, SpatialPool-ing+ <ref type="bibr" target="#b24">[25]</ref>, TA-CNN <ref type="bibr" target="#b34">[35]</ref>, Checkerboards <ref type="bibr" target="#b43">[44]</ref>, and CompACT-Deep <ref type="bibr" target="#b45">[46]</ref>. It can be observed that SAF R-CNN outperforms other methods by a large margin and achieves the lowest logaverage miss rate of 9.32%, which is significantly lower than the current state-of-the-art approach CompACT-Deep <ref type="bibr" target="#b45">[46]</ref>, by 2.43%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with</head><p>2) INRIA and ETH: We train our model using the INRIA training set and evaluate the model on both INRIA and ETH testing sets. <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="figure" target="#fig_7">Figure 6</ref> provide the comparisons of the proposed method with several best-performing methods <ref type="bibr" target="#b24">[25]</ref> [35] <ref type="bibr" target="#b5">[6]</ref>. It can be observed that our model achieves the lowest miss rate on both datasets. For the INRIA dataset, our method obtains the miss rate of 8.04%, which outperforms the second best method <ref type="bibr" target="#b24">[25]</ref> by 3.18%. For the ETH dataset, the miss rate of our model is 34.64% compared with 34.98% of Tian et al. <ref type="bibr" target="#b34">[35]</ref> and 37.37% of Paisitkriangkrai et al. <ref type="bibr" target="#b24">[25]</ref>. In general, the proposed method outperforms other bestperforming methods and achieves state-of-the-art performance on both datasets, which not only validates its superiority in accurate pedestrian detection after tuning in scene (INRIA), but also verifies its generalization capacity to other scenarios (ETH).</p><p>3) KITTI: We train our model using the KITTI training set and evaluate the model on the testing set of the KITTI benchmark. The detection results and performance comparisons of the proposed method with several best-performing methods <ref type="bibr">[</ref>  <ref type="table" target="#tab_5">Table I</ref> and <ref type="figure" target="#fig_8">Figure 7</ref>. It can be observed that our model achieves promising results, i.e., 77.93%, 65.01%, and 60.42% in terms of AP on easy, moderate, and hard subsets respectively, which outperforms most of the previous methods tested on this benchmark by a large margin. Overall, the approach competitive with our model is the 3DOP method of <ref type="bibr" target="#b3">[4]</ref>. However, this work adopts stereo information by using the left and right images of the KITTI training set while our model is trained with only the left images.         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablations Studies</head><p>This subsection is devoted to investigating the effectiveness of different components of SAF R-CNN. The performance achieved by different variants of the SAF R-CNN and parameter settings are reported in the following. All experiments are performed on the challenging Caltech dataset.</p><p>1) Feature Map Size: To generate larger feature maps for small-size object proposals, we remove the fourth max pooling layer in the VGG16 model for training the large-size and the small-size sub-network. To analyze the effectiveness of this strategy, the results of one variant of SAF R-CNN that preserves the fourth max pooling layer in both sub-networks are reported, i.e. "SAF R-CNN Pooling" in <ref type="figure" target="#fig_9">Figure 8</ref>. Compared to "SAF R-CNN", the smaller feature maps for the input object proposals are generated by "SAF R-CNN Pooling". It can be observed that the SAF R-CNN decreases the miss rate by 1.79% compared to "SAF R-CNN Pooling", verifying that larger feature maps are beneficial for the performance improvement in detecting small pedestrian instances. 2) Shared Convolutional Layers: In the SAF R-CNN, we use the first seven convolutional layers and three max pooling layers of the VGG16 network as the shared convolutional layers before the two sub-networks to extract feature maps from the entire input image. To verify the advantage of using these convolutional layers to generate shared features, we evaluate the performance of the variants where shared features are produced by different convolutional layers of the VGG16 network. In <ref type="figure" target="#fig_10">Figure 9</ref>, "SAF R-CNN Conv2" denotes the variant in which the first four convolutional layers and two max pooling layers are used as the shared convolutional layers before the two sub-networks. "SAF R-CNN Conv4" denotes the variant in which the first ten convolutional layers and three max pooling layers act as the shared convolutional layers. "SAF R-CNN Conv5" represents the variant where shared features are extracted from the first thirteen convolutional layers and three max pooling layers. Compared with "SAF R-CNN Conv2", "SAF R-CNN Conv4", and "SAF R-CNN Conv5", SAF R-CNN improves the performance by 1.97%, 0.17%, and 0.77%, respectively, which verifies that better shared features can be provided for the two sub-networks using the first seven convolutional layers and three max pooling layers of the VGG16 network.</p><p>3) Scale-aware Weighting: In the SAF R-CNN, the weights for combining the outputs of two sub-networks are computed by the scale-aware weighting layer which performs the gate function defined over the input height of the object proposal. To analyze the effectiveness of our scale-aware weighting strategy, the variant "SAF R-CNN Average Weighting" is compared, which combines the outputs from the two subnetworks using the equal weights (i.e. 0.5). We also compare SAF R-CNN with the variant "SAF R-CNN Hard 0-1 Weighting", which assigns hard 0-1 weights to the two sub-networks according to the height of the object proposal.  <ref type="figure" target="#fig_0">Fig. 10</ref>. The comparison of scale-aware weighting with averaging and hard 0-1 weighting strategies. The SAF R-CNN adaptively combines the outputs of the two sub-networks using the scale-aware weighing strategy. To verify its effectiveness, the result of SAF R-CNN is compared with the network which obtains the final results by averaging the outputs of the two sub-networks, denoted as "SAF R-CNN Average Weighting", and the network which assigns hard 0-1 weights to the two sub-networks, denoted as "SAF R-CNN Hard 0-1 Weighting". This is equivalent to the variant using the gate function with very small β. Both "SAF R-CNN Average Weighting" and "SAF R-CNN Hard 0-1 Weighting" follow the same finetuning step as SAF R-CNN. <ref type="figure" target="#fig_0">Figure 10</ref> shows that although these networks are with the same network architecture, the SAF R-CNN decreases the miss rate by 1.61% compared to the "SAF R-CNN Average Weighting" and 0.57% compared to the "SAF R-CNN Hard 0-1 Weighting". It demonstrates that adaptively weighting the outputs of the two sub-networks according to the object proposal size is beneficial for the final performance improvement, which makes SAF R-CNN robust to various sizes of the pedestrian instances. 4) Input Image Scale: Several experiments have been conducted to investigate the effect of the input image scale on the detection performance. The results for SAF R-CNN with input image scales of 500, 600, 700 and 800 are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. It can be observed that the miss rate decreases along with the increase of the input image scale. This verifies that a larger scale of the whole image can help produce more informative feature maps for each proposal to assist in detecting smallsize pedestrian instances. From our experiments, only minor improvement is observed when using larger image scales (such as 1000) but higher computation complexity is required. To balance the computation cost and detection accuracy, we select 800 as the scale of the input image in all other experiments. 5) Comparisons with R-CNN, Fast R-CNN, and Faster R-CNN: We also compare SAF R-CNN with R-CNN <ref type="bibr" target="#b12">[13]</ref>, Fast R-CNN <ref type="bibr" target="#b11">[12]</ref>, and Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> for pedestrian detection, shown in <ref type="table" target="#tab_5">Table II</ref>. R-CNN <ref type="bibr" target="#b12">[13]</ref> addresses the scale-variance problem by resizing the proposals into a fixed image scale while Fast R-CNN <ref type="bibr" target="#b11">[12]</ref> uses two ways to deal with the scale problem. One is the brute-force approach in which the input image is resized into a pre-defined size on the shortest side, denoted as "Fast R-CNN single-scale". For fair comparison with our SAF R-CNN, the input image of "Fast R-CNN   <ref type="bibr" target="#b12">[13]</ref>, "FAST R-CNN SINGLE-SCALE" <ref type="bibr" target="#b11">[12]</ref> SHORTEN AS "FAST R-CNN-S", "FAST R-CNN MULTI-SCALE" <ref type="bibr" target="#b11">[12]</ref> SHORTEN AS "FAST R-CNN-M", AND "FASTER R-CNN" <ref type="bibr" target="#b29">[30]</ref>. single-scale" is resized to 800 pixels on the shortest side. The other one is the multi-scale approach which utilizes multiscale image pyramids for each image, denoted as "Fast R-CNN multi-scale". The same five scales of 480, 576, 688, 864 and 1200 are adopted to construct the input image pyramid as specified in <ref type="bibr" target="#b15">[16]</ref>. For Faster R-CNN, we resize the input image to 800 pixels on the shortest side to make fair comparison with SAF R-CNN. It can be seen that SAF R-CNN significantly outperforms all four baselines. It verifies the superiority of using our scale-aware weighting technique in SAF R-CNN to detect the pedestrian instances with various sizes. We also compare the testing time of SAF R-CNN with other baselines. SAF R-CNN is only a little slower than "Fast R-CNN singlescale" and Faster R-CNN, and 9.0× faster than R-CNN and 5.2× faster than "Fast R-CNN multi-scale". This observation further demonstrates the advantage of SAF R-CNN which yields a large improvement in miss rate with low computation cost. 6) Visualization of Detection Results: Several detection results of our SAF R-CNN, TA-CNN <ref type="bibr" target="#b34">[35]</ref> and CompACT-Deep <ref type="bibr" target="#b45">[46]</ref> are visualized in <ref type="figure" target="#fig_0">Figure 12</ref> to further demonstrate the superiority of SAF R-CNN in detecting small-size instances. The first column shows the input images and the rest three columns sequentially show the detection results by TA-CNN <ref type="bibr" target="#b34">[35]</ref>, CompACT-Deep <ref type="bibr" target="#b45">[46]</ref> and our SAF R-CNN. The ground-truth bounding boxes of pedestrians are annotated with red rectangles, and the green rectangles represent the detected instances by our SAF R-CNN and the two baselines. One can observe that SAF R-CNN can successfully detect most of the small-size pedestrian instances that TA-CNN <ref type="figure" target="#fig_0">Fig. 12</ref>. Comparison of pedestrian detection results with other state-of-the-art methods. The first column shows the input images with ground-truths annotated with red rectangles. The rest columns show the detection results (green rectangles) of TA-CNN <ref type="bibr" target="#b34">[35]</ref>, CompACT-Deep <ref type="bibr" target="#b45">[46]</ref> and SAF R-CNN respectively. Our SAF R-CNN can successfully detect most small-size instances which the other two state-of-the-art methods have missed. For better viewing, please see original PDF file. and CompACT-Deep have missed, especially for those with obscured boundaries. The last row in <ref type="figure" target="#fig_0">Figure 12</ref> shows that our SAF R-CNN is robust to heavy occlusion of pedestrians and large background clutters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed a novel Scale-Aware Fast R-CNN (SAF R-CNN) model which incorporates a large-size sub-network and a small-size sub-network into a unified architecture to deal with various sizes of pedestrian instances in the image. By sharing convolutional filters in early layers for extracting common features and combining the outputs of the two sub-networks using the designed scale-aware weighing mechanism, SAF R-CNN is capable of training the specialized sub-networks for large-size and small-size pedestrian instances in order to capture their unique characteristics. Extensive experiments have demonstrated that the proposed SAF R-CNN is superior in detecting small-size pedestrian instances and achieves state-of-the-art performance on several challenging benchmarks. In future, we will extend the proposed SAF R-CNN to general object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the motivation of our SAF R-CNN model. (a) shows some example pedestrian images. (b) shows the distribution of pedestrians heights on the Caltech training set. One can observe that small-size (i.e. small height) instances indeed dominate the distribution. (c) and (d) demonstrate that the visual appearance and the extracted feature maps of the large-size and small-size instances are significantly different. In particular, large background clutters, obscured boundaries, body skeletons and heavy occlusion make the small-size pedestrians very difficult to detect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of our SAF R-CNN. The features of the whole input image are first extracted by a sequence of convolutional layers and max pooling layers, and then fed into two sub-networks. Each sub-network first utilizes several convolutional layers to further extract scale-specific features. Then, an RoI pooling layer pools the produced feature maps into a fixed-length feature vector and then a sequence of fully connected layers ending up with two output layers are performed to generate scale-specific detection results: one outputs classification scores over K object classes plus a "background" class and the other outputs refined bounding-box positions for each of the K object classes. Finally, the outputs of the two sub-networks are weighted to obtain the final results with weights from the scale-aware weighting layer which performs a gate function defined over the object proposal sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The comparison of SAF R-CNN on pedestrian detection with all recent state-of-the-art methods on the Caltech dataset. The SAF R-CNN outperforms other methods with the lowest log-average miss rate of 9.32%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The comparison of SAF R-CNN on pedestrian detection with all recent state-of-the-art methods on the INRIA dataset. The SAF R-CNN outperforms other methods with the lowest log-average miss rate of 8.04%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The comparison of SAF R-CNN on pedestrian detection with all recent state-of-the-art methods on the ETH dataset. The SAF R-CNN outperforms other methods with the lowest log-average miss rate of 34.64%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The comparison of SAF R-CNN on pedestrian detection with recent state-of-the-art methods on the KITTI dataset (moderate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The comparison of using different sizes of feature maps in our SAF R-CNN. The result of SAF R-CNN is compared with its variant in which the fourth max pooling layer is preserved in the sub-networks, in order to demonstrate the effectiveness of using larger feature maps to detect small pedestrian instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>The comparison of using different convolutional layers as the shared convolutional layers before the two sub-networks. The result of SAF R-CNN is compared with the variant in which the first four convolutional layers and two max pooling layers act as the shared convolutional layers, denoted as "SAF R-CNN Conv2", the variant where the first ten convolutional layers and three max pooling layers are used as the shared convolutional layers, denoted as "SAF R-CNN Conv4", and the variant where shared features are extracted from the first thirteen convolutional layers and three max pooling layers, denoted as "SAF R-CNN Conv5".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>SAF R-CNN Average Weighting 9.89% SAF R-CNN Hard 0-1 Weighting 9.32% SAF R-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Performance comparison of using different scales of the input image as the input of SAF R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We use the Caltech training set to train our model and evaluate it on the Caltech testing set. The overall</figDesc><table><row><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>.80</cell><cell></cell></row><row><cell></cell><cell>.64</cell><cell></cell></row><row><cell></cell><cell>.50</cell><cell></cell></row><row><cell></cell><cell>.40</cell><cell></cell></row><row><cell></cell><cell>.30</cell><cell></cell></row><row><cell>miss rate</cell><cell>.20</cell><cell>94.73% VJ</cell></row><row><cell></cell><cell></cell><cell>68.46% HOG</cell></row><row><cell></cell><cell>.10</cell><cell>24.80% LDCF</cell></row><row><cell></cell><cell></cell><cell>22.49% Katamari</cell></row><row><cell></cell><cell></cell><cell>21.89% SpatialPooling+</cell></row><row><cell></cell><cell></cell><cell>20.86% TA-CNN</cell></row><row><cell></cell><cell>.05</cell><cell>18.47% Checkerboards 11.75% CompACT-Deep</cell></row><row><cell></cell><cell></cell><cell>9.32% SAF R-CNN</cell></row><row><cell></cell><cell></cell><cell>-3 1) Caltech: 10 10 -2 10 -1 10 0 10 1 false positives per image</cell><cell>State-of-the-arts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I AVERAGE</head><label>I</label><figDesc>PRECISION (AP) (IN %) ON THE TESTING SET OF THE KITTI DATASET.</figDesc><table><row><cell>Methods</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>R-CNN</cell><cell>61.61</cell><cell>50.13</cell><cell>44.79</cell></row><row><cell>pAUCEnsT</cell><cell>65.26</cell><cell>54.49</cell><cell>48.60</cell></row><row><cell>FilteredICF</cell><cell>67.65</cell><cell>56.75</cell><cell>51.12</cell></row><row><cell>DeepParts</cell><cell>70.49</cell><cell>58.67</cell><cell>52.78</cell></row><row><cell cols="2">CompACT-Deep 70.69</cell><cell>58.74</cell><cell>52.71</cell></row><row><cell>Regionlets</cell><cell>73.14</cell><cell>61.15</cell><cell>55.21</cell></row><row><cell>3DOP</cell><cell>81.78</cell><cell>67.47</cell><cell>64.70</cell></row><row><cell>SAF R-CNN</cell><cell>77.93</cell><cell>65.01</cell><cell>60.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF THE MISS RATES AND TESTING TIME WITH OTHER FOUR METHODS FOR SCALE-VARIANCE PROBLEM, INCLUDING "R-CNN"</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Throughout the paper, we use "large-size network"/"small-size network" to refer to a network trained specifically for detecting objects of large/small sizes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagannath</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="613" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection evolution with multi-order contextual co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1798" to="1805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagannath</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiscale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="459" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4073" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Occlusion handling via random subspace classifiers for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaume</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludmila I</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCyb</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="354" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3258" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="546" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning sampling distributions for efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TCyb</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed object detection with linear svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kongqiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCyb</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2122" to="2133" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiresolution models for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="241" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc Aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rob Fergus, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhya</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene-specific pedestrian detection for static video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="374" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An hog-lbp human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6369</idno>
		<title level="m">Scale-invariant convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust multi-resolution pedestrian detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3033" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Strip features for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCyb</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1898" to="1912" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
