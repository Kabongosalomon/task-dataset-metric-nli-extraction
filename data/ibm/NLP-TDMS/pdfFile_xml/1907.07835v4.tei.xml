<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EEG-Based Emotion Recognition Using Regularized Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixiang</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Di</forename><forename type="middle">Wang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
						</author>
						<title level="a" type="main">EEG-Based Emotion Recognition Using Regularized Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Affective Computing</term>
					<term>EEG</term>
					<term>Graph Neural Network</term>
					<term>SEED !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this paper, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E MOTION recognition focuses on the recognition of human emotions based on a variety of modalities, such as audio-visual expressions, body language, physiological signals, etc. Compared to other modalities, physiological signals, such as electroencephalography (EEG), electrocardiogram (ECG), electromyography (EMG), etc., have the advantage of being difficult to hide or disguise. In recent years, due to the rapid development of noninvasive, easyto-use and inexpensive EEG recording devices, EEG-based emotion recognition has received an increasing amount of attention in both research <ref type="bibr" target="#b0">[1]</ref> and applications <ref type="bibr" target="#b1">[2]</ref>.</p><p>Emotion models can be broadly categorized into discrete models and dimensional models. The former categorizes emotions into discrete entities, e.g., anger, disgust, fear, happiness, sadness, and surprise in Ekman's theory <ref type="bibr" target="#b2">[3]</ref>. The latter describes emotions using their underlying dimensions, e.g., valence, arousal and dominance <ref type="bibr" target="#b3">[4]</ref>, which measures emotions from unpleasant to pleasant, passive to active, and submissive to dominant, respectively. EEG signals measure voltage fluctuations from the cortex in the brain and have been shown to reveal important information about human emotional states <ref type="bibr" target="#b4">[5]</ref>. For example, greater relative left frontal EEG activity has been observed when experiencing positive emotions <ref type="bibr" target="#b4">[5]</ref>. The voltage fluctuations in different brain regions are measured by electrodes attached to the scalp. Each electrode collects EEG signals in one channel. The collected EEG signals are often analyzed in specific frequency bands, namely delta (1-4 Hz), theta (4-7 Hz), alpha <ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref>, beta <ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref><ref type="bibr" target="#b25">(26)</ref><ref type="bibr" target="#b26">(27)</ref><ref type="bibr" target="#b27">(28)</ref><ref type="bibr" target="#b28">(29)</ref><ref type="bibr" target="#b29">(30)</ref>, and gamma (&gt;30 Hz).</p><p>Many existing EEG-based emotion recognition methods are primarily based on the supervised machine learning approach, wherein features are often extracted from preprocessed EEG signals in each channel over a time window. Then, a classifier is trained on the extracted features to recognize emotions. Wang et al. <ref type="bibr" target="#b5">[6]</ref> compared power spectral density features (PSD), wavelet features and nonlinear dynamical features with a Support Vector Machine (SVM) classifier. Zheng and Lu <ref type="bibr" target="#b6">[7]</ref> investigated critical frequency bands and channels using PSD, differential entropy (DE) <ref type="bibr" target="#b7">[8]</ref> and PSD asymmetry features, and obtained robust accuracy using deep belief networks (DBN). However, most existing EEG-based emotion recognition approaches do not address the following three challenges: 1) the topological structure of EEG channels are not effectively exploited to learn more discriminative EEG representations; 2) EEG signals vary significantly across different subjects, which hinders the generalizability of the trained classifiers in subject-independent classification settings; and 3) participants may not always generate the intended emotions when watching emotioneliciting stimuli. Consequently, the emotion labels in the collected EEG data may be noisy and inconsistent with the actual elicited emotions.</p><p>There have been several attempts to address the first challenge. Zhang et al. <ref type="bibr" target="#b8">[9]</ref> and Zhang et al. <ref type="bibr" target="#b9">[10]</ref> incorporated spatial relations in EEG signals using convolutional neural networks (CNN) and recurrent neural networks (RNN), respectively. However, their approaches require a 2D representation of EEG channels on the scalp, which may cause information loss during flattening because channels are actually arranged in the 3D space. In addition, their approach of using CNNs and RNNs to capture inter-channel relations has difficulty in learning long-range dependencies <ref type="bibr" target="#b10">[11]</ref>. Graph neural networks (GNN) has been applied in <ref type="bibr" target="#b11">[12]</ref> to capture inter-channel relations using an adjacency matrix. However, similar to CNNs and RNNs, the GNN approach <ref type="bibr" target="#b11">[12]</ref> only considers relations between the nearest channels, which thus may lose valuable information between distant channels, such as the PSD asymmetry between channels on the left and right hemispheres in the frontal region, which has been shown to be informative in valence prediction <ref type="bibr" target="#b4">[5]</ref>.</p><p>In recent years, several studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> attempted to tackle the second challenge by investigating the transferability of EEG-based emotion recognition models across subjects. Lan et al. <ref type="bibr" target="#b14">[15]</ref> compared several domain adaptation techniques such as maximum independence domain adaptation (MIDA), transfer component analysis (TCA), subspace alignment (SA), etc. They found that the subjectindependent classification accuracy can be improved by around 10%. Li et al. <ref type="bibr" target="#b15">[16]</ref> applied domain adversarial training to lower the influence of individual subject on EEG data and obtained improved performance as well. However, their adversarial training does not exploit any graph structure of the EEG signals and only leads to small performance improvement in our experiment (see Section 7.1).</p><p>To the best of our knowledge, no attempt has been made to address the third challenge, i.e., noisy emotion labels, in EEG-based emotion recognition.</p><p>In this paper, we propose a regularized graph neural network (RGNN) aiming to address all the three aforementioned challenges. Graph analysis for human brain has been studied extensively in the neuroscience literature <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, making an accurate connectome is still an open question and subject to different scales <ref type="bibr" target="#b17">[18]</ref>. Inspired by <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we consider each EEG channel as a node in our graph. Our RGNN model extends the simple graph convolution network (SGC) <ref type="bibr" target="#b19">[20]</ref> and leverages the topological structure of EEG channels. Specifically, we propose a sparse adjacency matrix to capture both local and global inter-channel relations based on the biological topology of human brain <ref type="bibr" target="#b18">[19]</ref>. Local inter-channel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion-related functional connectivity <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>In addition, we propose a node-wise domain adversarial training (NodeDAT) method to regularize RGNN for better generalization in subject-independent classification scenarios. Different from the domain adversarial training in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, our NodeDAT method provides a finer-grained regularization by minimizing the domain discrepancies between features in the source and target domains for each channel/node. Moreover, we propose an emotion-aware distribution learning (EmotionDL) method to address the problem of noisy labels in the datasets. Prior studies have shown that noisy labels can adversely impact classification accuracy <ref type="bibr" target="#b22">[23]</ref>. Instead of learning the traditional single-label classification, our EmotionDL method learns a distribution of labels of the training data and thus acts as a regularizer to improve the robustness of our model against noisy labels. Finally, we conduct extensive experiments to validate the effectiveness of our RGNN model and investigate emotion-related informative neuronal activities.</p><p>In summary, the main contributions of this paper are as follows:</p><p>1) We propose a regularized graph neural network (RGNN) model to recognize emotions based on EEG signals. Our biologically inspired model captures both local and global inter-channel relations. 2) We propose two regularizers: node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to improve the robustness of our model against cross-subject variations and noisy labels, respectively. 3) We conduct extensive experiments in both subjectdependent and subject-independent classification settings on two public EEG datasets, namely SEED <ref type="bibr" target="#b6">[7]</ref> and SEED-IV <ref type="bibr" target="#b23">[24]</ref>. Experimental results demonstrate the effectiveness of our proposed model and regularizers. In addition, our RGNN model achieves superior performance over the state-of-the-art models in most experimental settings. 4) We investigate the emotional neuronal activities and the results reveal that pre-frontal, parietal and occipital regions may be the most informative regions for emotion recognition. In addition, global inter-channel relations between the left and right hemispheres are important, and local inter-channel relations between (FP1, AF3), (F6, F8) and (FP2, AF4) may also provide useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work in the fields of EEGbased emotion recognition, graph neural network, unsupervised domain adaptation, and learning with noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">EEG-Based Emotion Recognition</head><p>EEG feature extractors and classifiers are the two fundamental components in the machine learning approach of EEGbased emotion recognition. EEG features can be broadly divided into single-channel features and multi-channel ones <ref type="bibr" target="#b24">[25]</ref>. The majority of existing features are computed on a single channel, e.g., statistical features <ref type="bibr" target="#b25">[26]</ref>, PSD <ref type="bibr" target="#b26">[27]</ref>, differential entropy (DE) <ref type="bibr" target="#b7">[8]</ref>, and wavelet features <ref type="bibr" target="#b27">[28]</ref>. A few number of features are computed on multiple channels to capture the inter-channel relations, e.g., the asymmetry of PSD between two hemispheres <ref type="bibr" target="#b6">[7]</ref> and functional connectivity <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, where common indices such as correlation, coherence and phase synchronization were used estimate brain functional connectivity between channels. Another line of research in multi-channel features is to use common spatial filters <ref type="bibr" target="#b30">[31]</ref> and spatial-temporal filters <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> to extract class-discriminative EEG features. In contrast, our model is deigned to operate on single-channel features and learn to effectively combine them using a graph neural network. EEG classifiers can be broadly divided into topologyinvariant classifiers and topology-aware ones. The majority of existing classifiers are topology-invariant classifiers such as SVM, k-Nearest Neighbors (KNN), DBN <ref type="bibr" target="#b33">[34]</ref> and RNN <ref type="bibr" target="#b34">[35]</ref>, which do not take the topological structure of EEG features into account when learning the EEG representations. In contrast, topology-aware classifiers such as CNN <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> and GNN <ref type="bibr" target="#b11">[12]</ref> consider the inter-channel topological relations and learn EEG representations for each channel by aggregating features from nearby channels using convolutional operations either in the Euclidean space or in the non-Euclidean space. However, as discussed in Section 1, existing CNNs and GNNs have difficulty in learning the dependencies between distant channels, which may reveal important emotion-related information. Recently, Zhang et al. <ref type="bibr" target="#b9">[10]</ref> and Li et al. <ref type="bibr" target="#b37">[38]</ref> proposed to use RNNs to learn spatial topological relations between channels by scanning electrodes in both vertical and horizontal directions. However, their approaches do not fully exploit the topological structure of EEG channels. For example, two topologically close channels may be far away from each other in their scanning sequence. In contrast, our model is able to learn relations between distant channels using global connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Network</head><p>Graph neural network (GNN) is a type of neural network dealing with data in the graph domain, e.g., molecular structures, social networks and knowledge graphs <ref type="bibr" target="#b38">[39]</ref>. One early work on GNN <ref type="bibr" target="#b39">[40]</ref> aimed to learn a converged static state embedding for each node in the graph using a transition function applied to its neighborhood. Later, inspired by the convolutional operation of CNN in the Euclidean domain, Bruna et al. <ref type="bibr" target="#b40">[41]</ref> combined spectral graph theory <ref type="bibr" target="#b41">[42]</ref> with neural network and defined convolutional operations in the graph domain using the spectral filters computed from the normalized graph Laplacian. Following this line of research, Defferrard et al. <ref type="bibr" target="#b42">[43]</ref> proposed fast localized convolutions by using a recursive formulation of the K-order Chebyshev polynomials to approximate the filters. The resulting representation for each node is an aggregation of its K thorder neighborhood. Kipf and Welling <ref type="bibr" target="#b43">[44]</ref> further limited K = 1 and proposed the standard graph convolutional network (GCN) with a faster localized graph convolutional operation. The convolutional layers in GCN can be stacked K times to effectively convolve the K th -order neighborhood of a node. Recently, Wu et al. <ref type="bibr" target="#b19">[20]</ref> simplified GCN by removing the nonlinearities between convolutional layers in GCN and proposed the simple graph convolution network (SGC), which effectively behaves like a linear feature transformation followed by a logistic regression. Apart from the convolution operation used in GCNs, there are other types of operations used in GNNs, such as attention <ref type="bibr" target="#b44">[45]</ref>. However, they are often trained significantly slower than SGC <ref type="bibr" target="#b19">[20]</ref>. In this paper, we extend SGC to model EEG signals because it performs orders of magnitude faster than other networks with comparable classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation aims to mitigate the domain shift in knowledge transfer from a supervised source domain to an unsupervised target domain. The most common approaches are instance re-weighting and domaininvariant feature learning. Instance re-weighting methods <ref type="bibr" target="#b45">[46]</ref> aim to infer the resampling weight directly by feature distribution matching across source and target domains in a non-parametric manner. Domain-invariant feature leaning methods align features from both source and target domains to a common feature space. The alignment can be achieved by minimizing divergence <ref type="bibr" target="#b46">[47]</ref>, maximizing reconstruction <ref type="bibr" target="#b47">[48]</ref>, or adversarial training <ref type="bibr" target="#b21">[22]</ref>. Our proposed NodeDAT regularizer extends the domain adversarial training <ref type="bibr" target="#b21">[22]</ref> to graph neural networks and achieves finer-grained regularization by minimizing the discrepancies between features in source and target domains for each node individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning with Noisy Labels</head><p>Commonly adopted approaches to learning with noisy labels are based on the noise transition matrix and robust loss functions. The noise transition matrix specifies the probabilities of transition from each ground truth label to each noisy label and is often applied to modify the crossentropy loss. The matrix can be pre-computed as a prior <ref type="bibr" target="#b48">[49]</ref> or estimated from noisy data <ref type="bibr" target="#b49">[50]</ref>. A few studies tackle noisy labels by using noise-tolerant robust loss functions, such as unhinged loss <ref type="bibr" target="#b50">[51]</ref> and ramp loss <ref type="bibr" target="#b51">[52]</ref>. Our proposed EmotionDL regularizer is inspired by <ref type="bibr" target="#b52">[53]</ref>, which applies distribution learning to classify ambiguous images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we introduce the preliminaries of the simple graph convolution network (SGC) <ref type="bibr" target="#b19">[20]</ref> and spectral graph convolution, which are the basis of our RGNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simple Graph Convolution Network (SGC)</head><p>Given a graph G = (V, E), where V denotes a set of nodes and E denotes a set of edges between nodes in V. Data on V can be represented by a feature matrix X ∈ R n×d , where n = |V| and d denotes the input feature dimension. The edge set E can be represented by a weighted adjacency matrix A ∈ R n×n with self-loops, i.e., A ii = 1, i = 1, 2, ..., n. In general, GNNs learn a feature transformation function for X and produces output Z ∈ R n×d , where d denotes the output feature dimension.</p><p>Between adjacent layers in GNNs, the feature transformation can be written as</p><formula xml:id="formula_0">H l+1 = f (H l , A),<label>(1)</label></formula><p>where l = 0, 1, ..., L − 1, L denotes the number of layers, H 0 = X, H L = Z, and f denotes the function we want to learn. A simple definition of f would be</p><formula xml:id="formula_1">H l+1 = σ(AH l W l ),<label>(2)</label></formula><p>where σ denotes a non-linear function and W l denotes a weight matrix at layer l. For each node x, function f simply computes the weighted sum of all the node features in its neighborhood including x itself, followed by a non-linear transformation. However, one major limitation of the f in <ref type="formula" target="#formula_1">(2)</ref> is that repeatedly applying f along multiple layers may lead to H l with overly large values due to summation. Kipf and Welling <ref type="bibr" target="#b43">[44]</ref> alleviated this limitation by proposing the graph convolution network (GCN) as follows</p><formula xml:id="formula_2">H l+1 = σ(D − 1 2 AD − 1 2 H l W l ),<label>(3)</label></formula><p>where D denotes the diagonal degree matrix of A, i.e., D ii = j A ij . The normalized adjacency matrix D − 1 2 AD − 1 2 prevents H from growing overly large. If we ignore σ and W l temporarily and expand (3), the hidden state H l+1 i for node x i , i = 1, 2, ..., n, can be computed via</p><formula xml:id="formula_3">H l+1 i ← A ii D ii + 1 H l i + n j=1 A ij (D ii + 1)(D jj + 1) H j l . (4)</formula><p>Note that each neighboring H l j is now normalized by the degrees of both x i and x j . Successively applying L layers aggregates node features within a neighborhood of size L.</p><p>To further accelerate training while keeping comparable performance, Wu et al. <ref type="bibr" target="#b19">[20]</ref> proposed SGC by removing the non-linear function σ in (3) and reparameterizing all linear transformations W l across all layers into one linear transformation W as follows</p><formula xml:id="formula_4">Z = H L = SH L−1 W L−1 = ... = S L XW,<label>(5)</label></formula><formula xml:id="formula_5">where S = D − 1 2 AD − 1 2 and W = W 0 W 1 ...W L−1 .</formula><p>Essentially, SGC computes a topology-aware linear transforma-tionX = S L X, followed by a final linear transformation Z =XW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spectral Graph Convolution</head><p>We analyze GCN from the perspective of spectral graph theory <ref type="bibr" target="#b41">[42]</ref>. Graph Fourier analysis relies on the graph Laplacian L = D − A or the normalized graph Laplacian</p><formula xml:id="formula_6">L = I − D − 1 2 AD − 1 2 .</formula><p>SinceL is a symmetric positive semidefinite matrix, it can be decomposed asL = UΛU T , where U is the orthonormal eigenvector matrix ofL and Λ = diag(λ 1 , ..., λ N ) is the diagonal matrix of corresponding eigenvalues. Given graph data X, the graph Fourier transform of X isX = U T X, and the inverse Fourier transform ofX is X = UX. Hence, the graph convolution between X and a filter G is computed as follows</p><formula xml:id="formula_7">X * G = U((U T G) (U T X)) = UĜU T X,<label>(6)</label></formula><p>where denotes element-wise multiplication andĜ = diag(ĝ 1 , ...,ĝ n ) denotes a diagonal matrix with n spectral filter coefficients.</p><p>To reduce the current learning complexity of O(n) to that of conventional CNN, i.e., O(K), (6) can be approximated using the Kth order polynomials as follows</p><formula xml:id="formula_8">UĜU T X ≈ U( K i=0 θ i Λ i )U T X = K i=0 θ iL i X,<label>(7)</label></formula><p>where θ i denotes learnable parameters. To further reduce computational cost, Defferrard et al. <ref type="bibr" target="#b42">[43]</ref> proposed to use Chebyshev polynomials to approximate the filtering operation as follows</p><formula xml:id="formula_9">UĜU T X = K i=0 θ i T i (L )X,<label>(8)</label></formula><p>whereL = 2 λmaxL − I denotes the scaled normalized Laplacian with its eigenvalues lying within [−1, 1], λ max denotes the maximum eigenvalue ofL, and T i (x) denotes the Chebyshev polynomials recursively defined as</p><formula xml:id="formula_10">T i (x) = 2xT i−1 (x) − T i−2 (x) with T 0 (x) = 1 and T 1 (x) = x.</formula><p>The GCN proposed in <ref type="bibr" target="#b43">[44]</ref> made a few approximations to simplify the filtering operation in (8): 1) use K = 1; 2) set λ max = 2; and 3) set θ 1 = −θ 0 . The resulted GCN arrives at <ref type="bibr" target="#b2">(3)</ref>. Essentially, the graph convolutional operations defined in <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_4">(5)</ref> behave like a low-pass filter by smoothing the features of each node on the graph using node features in its neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REGULARIZED GRAPH NEURAL NETWORK</head><p>In this section, we present our regularized graph neural network (RGNN), specifically, the biologically inspired adjacency matrix, the dynamics of RGNN, and two regularizers, i.e., node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adjacency Matrix in RGNN</head><p>The adjacency matrix A ∈ R n×n in RGNN represents the topological structure of EEG channels and is essential to graph representation learning, where n denotes the number of channels in EEG signals. Each entry A ij is learnable and indicates the weight of connection between channels i and j. Note that A contains self-loops. To reduce overfitting, we model A as a symmetric matrix by using only n(n+1) 2 number of parameters instead of n 2 . Salvador et al. <ref type="bibr" target="#b54">[55]</ref> observed that the strength of connection between brain regions decays as an inverse square function of physical distance. Hence, we initialize the local inter-channel relations in our adjacency matrix as follows</p><formula xml:id="formula_11">A ij = min(1, δ d 2 ij ),<label>(9)</label></formula><p>where d ij , i, j = 1, 2, ..., n, denotes the physical distance between channels i and j, which is computed from their 3D coordinates obtained from the data sheet of the recording device, and δ &gt; 0 denotes a calibration constant. Achard and Bullmore <ref type="bibr" target="#b55">[56]</ref> observed that sparse fMRI networks, comprising around 20% of all possible connections, typically maximize the efficiency of the network topology. Therefore, we choose δ = 5 such that around 20% of the entries in A are non-negligible. We empirically regard entries having values larger than 0.1 as non-negligible connections. Bullmore and Sporns <ref type="bibr" target="#b18">[19]</ref> suggested that the brain organization is shaped by an economic trade-off between minimizing wiring costs and network running costs. Minimizing wiring costs encourages local inter-channel connections as modelled in <ref type="bibr" target="#b8">(9)</ref>. However, minimizing network running costs encourages certain global inter-channel connections for high efficiency of information transfer across the network as a whole. To this end, we add several global connections to our adjacency matrix to improve the network efficiency. The global connections depend on specific electrode placement adopted in experiments. <ref type="figure" target="#fig_0">Fig. 2</ref> depicts the global connections in both SEED <ref type="bibr" target="#b6">[7]</ref> and SEED-IV <ref type="bibr" target="#b23">[24]</ref>. The selection of global channels is supported by prior studies showing that the asymmetry in neuronal activities between the left and right hemispheres is informative in valence and arousal predictions <ref type="bibr" target="#b4">[5]</ref>. To leverage the differential asymmetry information, we initialize the global inter-channel relations in A to [−1, 0] as follows  <ref type="bibr" target="#b53">[54]</ref>. GRL denotes gradient reversal layer <ref type="bibr" target="#b21">[22]</ref>. where (i, j) denotes the indices of global channel pairs: (FP1, FP2), (AF3, AF4), (F5, F6), (FC5, FC6), (C5, C6), (CP5, CP6), (P5, P6), (PO5, PO6) and (O1, O2). Note that we select these indices because 1) they are connected to a large number of nodes in their immediate neighborhood, which maximizes the effects of EEG asymmetry; and 2) they empirically perform slightly better than alternative sets of indices (see Section 7.1). Our adjacency matrix A obtained in (9) and (10) aims to represent the brain network which combines both local anatomical connectivity and emotionrelated global functional connectivity.</p><formula xml:id="formula_12">A ij = A ij − 1,<label>(10)</label></formula><formula xml:id="formula_13">FPZ F7 F3 FZ CZ CPZ PZ POZ OZ FCZ F1 F2 F4 F8 FT7 FC3 FC1 FC2 FC4 FT8 T7 C3 C1 C2 C4 T8 TP7 CP3 CP1 CP2 CP4 TP8 P7 P3 P1 P2 P4 P8 PO3 PO4 PO8 CB1 CB2 PO7 PO5 O2 O1 FP1 FP2 AF3 AF4 F5 F6 FC5 FC6 C5 C6 CP5 CP6 P5 P6 PO6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamics of RGNN</head><p>Our RGNN model extends the SGC model <ref type="bibr" target="#b19">[20]</ref>. The architecture of RGNN is illustrated in <ref type="figure">Fig. 1</ref>. Given EEG features X ∈ R N ×n×d and labels Y ∈ Z N , where N denotes the number of training samples, Y i ∈ {0, 1, ..., C − 1} denotes the class index, and C denotes the number of classes. Our model aims to minimize the following cross-entropy loss:</p><formula xml:id="formula_14">Φ = − N i=1 log(p(Y i |X i , θ)) + α||A|| 1 ,<label>(11)</label></formula><p>where θ denotes the model parameters we want to optimize, and α denotes the strength of L1 sparse regularization for our adjacency matrix A. By passing each feature matrix X i into our RGNN, the output probability of class Y i can be computed as follows</p><formula xml:id="formula_15">Z i = S L X i W, p(Y i |X i , θ) = softmax Yi (pool(σ(Z i ))W O ),<label>(12)</label></formula><p>where S ∈ R n×n , W ∈ R d×d and L follow the definitions in (5), σ(x) = max(0, x) denotes a non-linear transformation, W O ∈ R d ×C denotes the output weight matrix, and pool(·) denotes the sum pooling across all nodes on the graph. We choose sum pooling because it demonstrated more expressive power than mean pooling and max pooling <ref type="bibr" target="#b56">[57]</ref>. Note that we use the absolute values of A to compute the degree matrix D (see <ref type="formula" target="#formula_2">(3)</ref>) because A has negative entries, e.g., global connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Node-wise Domain Adversarial Training</head><p>EEG signals vary significantly across different subjects, which hinders the generalizability of trained classifiers in subject-independent classification settings. To improve the robustness of our model across subjects, we extend the domain adversarial training <ref type="bibr" target="#b21">[22]</ref> by proposing a node-wise domain adversarial training (NodeDAT) method to reduce the discrepancies between source and target domains, i.e., training and testing sets, respectively. Specifically, a domain classifier is proposed to classify each node representation into either source domain or target domain. During optimization, our model aims to confuse the domain classifier by learning domain-invariant representations. Compared to <ref type="bibr" target="#b21">[22]</ref>, which only regularizes the pooled representation in the last layer, our NodeDAT method has finer-grained regularization because it explicitly regularizes each node representation before pooling. Specifically, given labelled source/training data X S ∈ R N ×n×d (in this subsection, we denote X by X S for better clarity) and unlabelled target/testing data X T ∈ R N ×n×d , where in practice X T can be either oversampled or donwsampled to have the same number of samples as X S <ref type="bibr" target="#b21">[22]</ref>, the domain classifier aims to minimize the sum of the following two binary cross-entropy losses:</p><formula xml:id="formula_16">Φ D = − N i=1 n j=1 (log(p j (0|X S i , θ D )) + log(p j (1|X T i , θ D ))),<label>(13)</label></formula><p>where θ D denotes the parameters of the domain classifier, 0 and 1 denote source and target domains, respectively. Intuitively, the domain classifier is learned to classify source data as 0 and target data as 1. The domain probabilities p j (·) for the jth node on the ith example are computed as</p><formula xml:id="formula_17">p j (0|X S i , θ D ) = softmax 0 (σ(Z S ij )W D ), p j (1|X T i , θ D ) = softmax 1 (σ(Z T ij )W D ),<label>(14)</label></formula><p>where Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{S,T } ij</head><p>denotes the jth node representation in Z {S,T } i , and W D ∈ R d ×2 denotes the matrix parameter in the domain classifier, i.e., θ D .</p><p>In order to confuse the domain classifier and learn domain invariant node presentation Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{S,T } ij</head><p>, we implement a gradient reversal layer (GRL) <ref type="bibr" target="#b21">[22]</ref> that acts like an identity layer in the forward propagation and reverses the gradients of the domain classifier during backpropagation. Consequently, the parameters in the feature extractor essentially perform gradient ascent with respect to the gradients from the domain classifier. The reversed gradients are further scaled by a GRL scaling factor β which gradually increases from 0 to 1 as the training progresses. The gradually increasing β allows our domain classifier to be less sensitive to noisy inputs at the early stages of the training process. Specifically, as suggested in <ref type="bibr" target="#b21">[22]</ref>, we let β = 2 1+e −10p − 1, where p ∈ [0, 1] denotes the progression of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Emotion-aware Distribution Learning</head><p>Participants may not always generate the intended emotions when watching emotion-eliciting stimuli, which may have negative impact on model performance <ref type="bibr" target="#b22">[23]</ref>. To this end, we propose an emotion-aware distribution learning (Emo-tionDL) method to learn a distribution of classes instead of one single class for each training sample. Specifically, we convert each training label Y i ∈ {0, 1, ..., C − 1} into a prior probability distribution of all classesŶ i ∈ R C , whereŶ ic denotes the probability of class c inŶ i . The conversion is dataset-dependent. SEED has three classes: negative, neutral, and positive with corresponding class indices 0, 1, and 2, respectively. We convert Y as followŝ</p><formula xml:id="formula_18">Y i =      (1 − 2 3 , 2 3 , 0), Y i = 0, ( 3 , 1 − 2 3 , 3 ), Y i = 1, (0, 2 3 , 1 − 2 3 ), Y i = 2,<label>(15)</label></formula><p>where ∈ [0, 1] denotes a hyper-parameter controlling the noise level in the training labels. This conversion mechanism is based on our assumption that participants are unlikely to generate opposite emotions when watching emotioneliciting stimuli. Therefore, for each class, the converted class distribution centers on the original class and has zero probabilities at its opposite classes. SEED-IV has four classes: neutral, sad, fear, and happy with corresponding class indices 0, 1, 2, and 3, respectively. We convert Y as followŝ</p><formula xml:id="formula_19">Y i =          (1 − 3 4 , 4 , 4 , 4 ), Y i = 0, ( 3 , 1 − 2 3 , 3 , 0), Y i = 1, ( 4 , 4 , 1 − 3 4 , 4 ), Y i = 2, ( 3 , 0, 3 , 1 − 2 3 ), Y i = 3.<label>(16)</label></formula><p>This conversion is based on the distances between the four emotion classes on the valence-arousal plane. Specifically, in the self-reported ratings <ref type="bibr" target="#b23">[24]</ref> for SEED-IV, neutral, sad, fear, and happy movie ratings cluster in the zero valence zero arousal, low valence low arousal, low valence high arousal, and high valence high arousal regions, respectively. We assume that participants are likely to generate emotions that have similar ratings in either valence or arousal dimensions, e.g., both angry and happy have high arousal, but unlikely to generate emotions that are far away in both dimensions, e.g., sad and happy are different in both valence and arousal dimensions.</p><p>After obtaining the converted class distributionsŶ, our model can be optimized by minimizing the following Kullback-Leibler (KL) divergence <ref type="bibr" target="#b53">[54]</ref> instead of (11):</p><formula xml:id="formula_20">Φ = N i=1 KL(p(Y|X i , θ),Ŷ i ) + α||A|| 1 ,<label>(17)</label></formula><p>where p(Y|X i , θ) denotes the output probability distribution computed via <ref type="bibr" target="#b11">(12)</ref>. Note that EmotionDL incorporates more prior knowledge than label smoothing, which simply adds uniform noise to other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Optimization of RGNN</head><p>Combining both NodeDAT and EmotionDL, the overall loss function Φ of RGNN is computed as follows</p><formula xml:id="formula_21">Φ = Φ + Φ D .<label>(18)</label></formula><p>The detailed algorithm for training RGNN is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETTINGS</head><p>In this section, we present the datasets, classification settings and model settings in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conduct experiments on two public datasets, namely SEED and SEED-IV. The SEED dataset <ref type="bibr" target="#b6">[7]</ref> comprises EEG data of 15 subjects (7 males) recorded in 62 channels using the ESI NeuroScan System 1 . The data were collected when participants watch emotion-eliciting movies in three types of emotions, namely negative, neutral and positive. Each movie lasts around 4 minutes. Three sessions of data are collected and each session comprises 15 trials/movies for  <ref type="formula" target="#formula_11">(9)</ref> and (10); <ref type="bibr">3:</ref> for i = 1: T do <ref type="bibr">4:</ref> repeat <ref type="bibr">5:</ref> Draw one batch of training samples X B andŶ B from X andŶ, respectively; <ref type="bibr">6:</ref> Draw one batch of testing samples X T B from X T ; <ref type="bibr">7:</ref> Compute degree matrix D based on (3); <ref type="bibr">8:</ref> Compute normalized adjacency matrix S based on (5); <ref type="bibr">9:</ref> Compute output representation Z based on (12); <ref type="bibr">10:</ref> Use X B andŶ B to compute KL loss Φ based on (17); <ref type="bibr">11:</ref> Use X B and X T B to compute domain loss Φ D based on (13); <ref type="bibr">12:</ref> Compute GRL scaling factor β; <ref type="bibr">13:</ref> Update</p><formula xml:id="formula_22">W D ← W D − η ∂Φ D ∂W D ; 14: Update W O ← W O − η ∂Φ ∂W O ; 15: Update W ← W − η( ∂Φ ∂W − β ∂Φ D ∂W ); 16: Update A ← A − η( ∂Φ ∂A − β ∂Φ D ∂A ); 17:</formula><p>until all samples in X have been drawn; each subject. To make a fair comparison with existing studies, we directly use the pre-computed differential entropy (DE) features smoothed by linear dynamic systems (LDS) <ref type="bibr" target="#b6">[7]</ref> in SEED. DE extends the idea of Shannon entropy and measures the complexity of a continuous random variable. In SEED, DE features are pre-computed over five frequency bands (delta, theta, alpha, beta and gamma) for each second of EEG signals (without overlapping) in each channel. The SEED-IV dataset <ref type="bibr" target="#b23">[24]</ref> comprises EEG data of 15 subjects (7 males) recorded in 62 channels 2 . The recording device is the same as the one used in SEED. The data were collected when participants watch emotion-eliciting movies in four types of emotions, namely neutral, sad, fear, and happy. Each movie lasts around 2 minutes. Three sessions of data are collected and each session comprises 24 trials/movies for each subject. Similar to SEED, we adopt the pre-computed DE features from SEED-IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Settings</head><p>We closely follow prior studies to conduct both subjectdependent and subject-independent classifications on both SEED and SEED-IV to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Subject-Dependent Classification</head><p>For SEED, we follow the experimental settings in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> to evaluate our RGNN model using subject-dependent classification. Specifically, for each subject, we train our 2. SEED-IV also contains eye movement data, which we do not use in our experiments. model using the first 9 trials as the training set and the remaining 6 trials as the testing set. We evaluate the model performance by using the accuracy averaged across all subjects over two sessions of EEG data <ref type="bibr" target="#b6">[7]</ref>. Similarly, for subject-dependent classification on SEED-IV, we follow the experimental settings in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b37">[38]</ref> to use the first 16 trials for training and the remaining 8 trials containing all emotions (two trials per emotion class) for testing. We evaluate our model using data from all three sessions <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Subject-Independent Classification</head><p>For SEED, we follow the experimental settings in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref> to evaluate our RGNN model using subjectindependent classification. Specifically, we adopt leave-onesubject-out cross-validation, i.e, during each fold, we train our model on 14 subjects and test on the remaining subject. We evaluate the model performance using the accuracy averaged cross all test subjects over one session of EEG data <ref type="bibr" target="#b12">[13]</ref>. Similarly, for SEED-IV, we follow the experimental settings in <ref type="bibr" target="#b37">[38]</ref> to evaluate our RGNN model using subjectindependent classification. We evaluate our model using data from all three sessions <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Settings in RGNN</head><p>For hyper-parameters of RGNN in all experiments, we empirically set the number of convolutional layers L = 2, dropout rate of 0.7 at the output fully-connected layer <ref type="bibr" target="#b63">[64]</ref>, and batch size of 16. We use Adam <ref type="bibr" target="#b64">[65]</ref> to optimize model parameters using gradient descent. We only tune the output feature dimension d , label noise level , learning rate η, L1 regularization factor α, and L2 regularization for each experiment. Note that we only adopt NodeDAT in subject-independent classification experiments. Our model is publicly available <ref type="bibr" target="#b2">3</ref> . We compare our model with several baselines, which are all cited from published results <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PERFORMANCE EVALUATIONS</head><p>In this section, we present model evaluation results and investigate the critical frequency bands and confusion matrices of our RGNN model. <ref type="table" target="#tab_1">Table 1</ref> presents the subject-dependent classification accuracy of our RGNN model and all baselines on both SEED and SEED-IV. The performance on SEED in the individual delta, theta, alpha, beta, and gamma bands is reported as well. It is encouraging to see that our model achieves better performance than all baselines including the state-of-the-art BiHDM on both datasets when features from all frequency bands are used. In particular, our model performs better than DGCNN, another GNN-based model that leverages the topological structure of EEG channels. Besides the proposed two regularizers (see <ref type="table" target="#tab_5">Table 3</ref>), the main performance improvement can be attributed to two factors: 1) our adjacency matrix incorporates the emotion-discriminative global interchannel asymmetry relation between the left and right hemispheres; and 2) our model has less concern of overfitting by 3. https://github.com/zhongpeixiang/RGNN  extending SGC, which is much simpler than ChebNet <ref type="bibr" target="#b42">[43]</ref> used in DGCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Subject-Dependent Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Subject-Independent Classification</head><p>Similar to <ref type="table" target="#tab_1">Table 1</ref>, <ref type="table" target="#tab_3">Table 2</ref> presents the subject-independent classification results. When using features from all frequency bands, our model performs marginally worse than BiHDM on SEED but much better than BiHDM on SEED-IV (nearly 5% improvement). In addition, our model achieves the lowest standard deviation in accuracy compared to all baselines on both datasets, showing the robustness of our model against cross-subject variations.</p><p>Comparing the results shown in Tables 1 and 2, we find that the accuracy obtained in subject-independent settings is consistently worse than the accuracy obtained in subject-dependent settings by around 5% to 30% for every model. This finding is unsurprising because the variability of EEG signals across subjects makes subject-independent classification more challenging. However, an interesting observation is that the performance gap between these two settings is gradually decreasing from around 27% on SEED and 19% on SEED-IV using SVM to around 9% on SEED and 6% on SEED-IV using our model. One possible reason for the diminishing performance gap is that recent deep learning models in subject-independent classification settings are becoming better at leveraging a large amount of data and learning subject-invariant EEG representations. This observation seems to indicate that transfer learning may be a necessary tool for emotion recognition in crosssubject settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance Comparison of Frequency Bands</head><p>We further compare the performance of our model and all baselines on SEED using features from different frequency bands, as reported in <ref type="table" target="#tab_1">Tables 1 and 2</ref>. In subject-dependent experiments, STRNN achieves the highest accuracy in delta, theta and alpha bands, BiDANN performs best in beta band, and our model performs best in gamma band. In subjectindependent experiments, BiDANN-S achieves the highest accuracy in theta and alpha bands, and our model performs best in delta, beta and gamma bands.</p><p>We investigate the critical frequency bands for emotion recognition. For both subject-dependent and subjectindependent settings on SEED, we compare the performance of each model across different frequency bands. In general, most models including ours achieve better performance on beta and gamma bands than delta, theta and alpha bands, with one exception of STRNN, which performs the worst on gamma band. This observation is consistent with the literature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b65">[66]</ref>. One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band, whereas other models perform comparably in both bands, indicating that gamma band may be the most discriminative band for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Confusion Matrix</head><p>We present the confusion matrices of our model in <ref type="figure" target="#fig_2">Fig. 3</ref>. For SEED, our model can recognize positive and neutral emotions better than negative emotion in both classification settings. Comparing subject-independent classification (see <ref type="figure" target="#fig_2">Fig. 3(b)</ref>) to subject-dependent classification (see <ref type="figure" target="#fig_2">Fig. 3(a)</ref>), the performance of our model gets relatively much worse at detecting negative emotion, indicating that participants are likely to generate distinct EEG patterns when experiencing negative emotion.</p><p>For SEED-IV, our model performs significantly better on sad emotion than all other emotions in both classification settings. Comparing subject-independent classification (see <ref type="figure" target="#fig_2">Fig. 3(d)</ref>) to subject-dependent classification (see <ref type="figure" target="#fig_2">Fig. 3(c)</ref>), the performance of our model gets relatively much worse    at detecting sad emotion, which is similar to SEED. We note that fear is the only emotion that performs better in subjectindependent classification than in subject-dependent classification. This finding indicates that participants watching horror movies may generate similar EEG patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this section, we conduct ablation study and sensitivity analysis for our RGNN model. We also analyze important brain regions and inter-channel relations for emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Ablation Study</head><p>We conduct ablation study to investigate the contribution of each key component in our model. <ref type="table" target="#tab_5">Table 3</ref> reports the subject-independent classification results on both datasets. We compared different initialization methods of the adjacency matrix and found that our distance-based method (see <ref type="bibr" target="#b8">(9)</ref>) obtains slightly better performance than functional connectivity-based methods, i.e., correlation and coherence computed from the training dataset. The uniformly randomly initialized adjacency matrix in [0, 1] performs worst,  <ref type="formula" target="#formula_0">(15)</ref> and <ref type="bibr" target="#b15">(16)</ref>.</p><p>indicating that properly initializing the adjacency matrix is beneficial to model performance. Our symmetric adjacency matrix design also proves to be useful in reducing overfitting and improving accuracy.</p><p>Removing the global connection causes noticeable performance drop on both datasets, demonstrating the importance of global connections in modelling the EEG differential asymmetry. Moreover, we compared the performance of alternative sets of global connections. Alternative 1 has global indices that are nearer to the central region, i.e., (FP1, FP2), (AF3, AF4), (F3, F4), (FC3, FC4), (C3, C4), (CP3, CP4), (P3, P4), (PO5, PO6) and (O1, O2). Alternative 2 has global indices that are further from the central region, i.e., (FP1, FP2), (AF3, AF4), (F7, F8), (FT7, FT8), (T7, T8), (TP7, TP8), (P7, P8), (PO7, PO8) and (O1, O2). Both alternatives perform slightly worse than our model but much better than no global connection, indicating that they are able to model EEG asymmetry to a certain extent.</p><p>Our NodeDAT regularizer has a noticeable positive impact on the performance of our model, suggesting that domain adaptation is helpful in cross-subject classification. To further investigate the impact of our node-level domain classifier, we experimented with replacing NodeDAT with a generic domain classifier DAT <ref type="bibr" target="#b21">[22]</ref>. The clear performance gap between DAT and our RGNN model indicates that NodeDAT can better regularize the model by learning subject-invariant representation at node level than graph level. In addition, if NodeDAT is removed, the performance of our model has a greater variance, validating the importance of our NodeDAT regularizer in improving the robustness of RGNN against cross-subject variations.</p><p>Our EmotionDL regularizer improves the performance of our model by around 3% in accuracy on both datasets. This performance gain validates our assumption that participants are not always generating the intended emotions when watching emotion-eliciting stimuli. In addition, our EmotionDL regularizer can be easily adopted by other deep learning based emotion recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Sensitivity Analysis</head><p>We analyze the performance of our model across varying L1 sparsity coefficient α (see <ref type="bibr" target="#b10">(11)</ref>) and noise coefficient in EmotionDL (see <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_0">(16)</ref>), as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. For subject-dependent classification, increasing α from 0 to 0.1 generally increases the model performance. However,  for subject-independent classification, increasing α beyond a certain threshold, i.e, 0.01 in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>, decreases the model performance. One possible explanation for the difference in model behaviors is that there is much less training data in subject-dependent classification, which thus requires a stronger regularization to reduce overfitting, whereas for subject-independent classification where the amount of training data is less of a concern, adding stronger regularization may introduce bias and hinder the learning efficacy.</p><p>As illustrated in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, our model behaves consistently across different experimental settings with varying noise coefficient . Specifically, by increasing , the performance of our model first increases and then decreases. In particular, our model usually performs best when is set to 0.2, demonstrating the existence of label noises and the necessity of addressing them on both datasets. Introducing excessive noise in EmotionDL causes performance drop, which is expected because excessive noise weakens the true learning signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Analysis of Important Brain Regions and Interchannel Relations</head><p>We identify important brain regions for emotion recognition. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the heatmaps of the diagonal elements in our learned adjacency matrix A in subject-dependent classification on SEED-IV for each frequency band. The values are scaled to the [0, 1] interval for better visualization. Conceptually, as shown in (4), the diagonal values in A represents the contribution of each channel in computing the final EEG representation. It is clear from 5 that there is strong activation on the pre-frontal, parietal and occipital regions for all frequency bands, indicating that these regions may be strongly related to the emotion processing in the brain. Our finding is consistent with existing studies, which observed that asymmetrical frontal and parietal EEG activity may reflect changes on both valence and arousal <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The synchronization between frontal and occipital regions has also been reported to be related to positive emotions <ref type="bibr" target="#b66">[67]</ref>. In addition, there is strong activation on the temporal regions for beta and gamma bands, which is consistent with <ref type="bibr" target="#b6">[7]</ref>. The symmetry pattern on the activation maps of channels also indicates that the asymmetry in EEG activity between the left and right hemispheres is critical for emotion recognition.</p><p>We identify important inter-channel relations for emotion recognition. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the top 10 connections between channels having the largest edge weights in our adjacency matrix A. Note that all global connections remain among the strongest connections after A is learned, demonstrating again that global inter-channel relations are essential for emotion recognition. It is clear from <ref type="figure" target="#fig_5">Fig. 6(b)</ref> that the connection between the channel pair (FP1, AF3) is the strongest, followed by (F6, F8), (FP2, AF4) and (PO8, CB2), indicating that local inter-channel relations in the frontal region may be important for emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we propose a regularized graph neural network for EEG-based emotion recognition. Our model is inspired by neuroscience theories on human brain organization and captures both local and global inter-channel relations in EEG signals. In addition, we propose two regularizers, namely NodeDAT and EmotionDL, to improve the robustness of our model against cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets demonstrate the superior performance of our model than several competitive baselines and the state-of-the-art BiHDM in most experimental settings. Our model analysis shows that our proposed biologically inspired adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our model. Investigations on the brain regions reveal that prefrontal, parietal and occipital regions may be the most informative regions for emotion recognition. In addition, global inter-channel relations between the left and right hemispheres are important, and local inter-channel relations between (FP1, AF3), (F6, F8) and (FP2, AF4) may also provide useful information.</p><p>In the future, we plan to explore: 1) training a more discriminative domain classifier, e.g., by using more advanced classifiers or applying more sophisticated techniques to handle imbalanced samples between training and test sets, to help our model learn more domain-invariant EEG representations; 2) applying our model to EEG signals that have a smaller number of channels. A simpler version of our model and more advanced regularizations may be necessary to avoid over-smoothing on these small graphs. In addition, data processing techniques that can improve the spatial resolution of EEG signals, e.g., spatial filtering, may be worth exploring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The 62-channel EEG placement used to collect data in SEED and SEED-IV. Gray symmetric channels are connected globally via red dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Confusion matrices of RGNN. (a) Subject-dependent classification on SEED. (b) Subject-independent classification on SEED. (c) Subject-dependent classification on SEED-IV. (d) Subject-independent classification on SEED-IV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Classification accuracy of RGNN with varying hyperparameters. (a) L1 sparsity coefficient α in (11). (b) Noise coefficient in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Activation maps learned from subject-dependent classification on SEED-IV. (a) Delta band. (b) Theta band. (c) Alpha band. (d) Beta band. (e) Gamma band.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Top 10 connections between channels in the adjacency matrix A, excluding global connections in (10) for better clarity. (a) Initialized A according to<ref type="bibr" target="#b8">(9)</ref>. (b) Learned and averaged A across five frequency bands in subject-dependent classification on both SEED and SEED-IV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1</head><label>1</label><figDesc>The Training Algorithm of RGNN Input: Training samples X andŶ, unlabelled testing samples X T , learning rate η, number of epochs T , batch size B, other regularization hyper-parameters; Output: The learned model parameters in RGNN; 1: Randomly initialize model parameters in RGNN using Xavier initialization [58]; 2: Initialize adjacency matrix A based on</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :80.90/12.27 83.35/09.15 82.69/12.99</head><label>1</label><figDesc>Subject-dependent classification accuracy (mean/std) on SEED and SEED-IV /14.14 60.95/10.20 66.64/14.41 80.76/11.56 79.56/11.38 83.99/09.92 56.61/20.05 GSCCA [59] 63.92/11.16 64.64/10.33 70.10/14.76 76.93/11.00 77.98/10.72 82.96/09.95 69.08/16.66 DBN [7] 64.32/12.45 60.77/10.42 64.01/15.97 78.92/12.48 79.19/14.58 86.08/08.34 66.77/07.38 STRNN [10] 83.41/10.16 69.61/15.65 89.50/07.63 -DGCNN [12] 74.25/11.42 71.52/05.99 74.43/12.16 83.65/10.17 85.73/10.64 90.40/08.49 69.88/16.29 BiDANN [16] 76.97/10.95 75.56/07.88 81.03/11.74 89.65/09.59 88.64/09.46 92.38/07.04 70.29/12.63</figDesc><table><row><cell>SEED</cell><cell>SEED-IV</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Subject-independent classification accuracy (mean/std) on SEED and SEED-IV</figDesc><table><row><cell>SEED</cell><cell>SEED-IV</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Ablation study for subject-independent classifi-</cell></row><row><cell cols="3">cation accuracy (mean/std) on SEED and SEED-IV. Symbol</cell></row><row><cell cols="3">"−" indicates the following component is removed.</cell></row><row><cell>Model</cell><cell>SEED</cell><cell>SEED-IV</cell></row><row><cell>RGNN</cell><cell cols="2">85.30/06.72 73.84/08.02</cell></row><row><cell cols="3">correlation-based adjacency matrix 84.41/06.94 72.73/08.36</cell></row><row><cell>coherence-based adjacency matrix</cell><cell cols="2">84.02/07.05 72.26/08.48</cell></row><row><cell>random adjacency matrix</cell><cell cols="2">83.57/07.34 71.78/08.64</cell></row><row><cell>− symmetric adjacency matrix</cell><cell cols="2">83.69/07.92 72.02/08.66</cell></row><row><cell>− global connection</cell><cell cols="2">82.42/08.24 71.13/08.78</cell></row><row><cell>global connection alternative 1</cell><cell cols="2">84.52/06.87 73.29/08.18</cell></row><row><cell>global connection alternative 2</cell><cell cols="2">84.23/07.04 73.08/08.35</cell></row><row><cell>− NodeDAT</cell><cell cols="2">81.92/09.35 71.65/09.43</cell></row><row><cell>DAT</cell><cell cols="2">83.51/08.11 72.40/08.54</cell></row><row><cell>− EmotionDL</cell><cell cols="2">82.27/08.81 70.76/09.22</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotions recognition using EEG signals: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Alarcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="393" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis of depression using EEG signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Sudarshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santhosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Neurology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="329" to="336" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonverbal Communication: Where nature meets culture</title>
		<editor>Segerstrale U, P. Molnar P</editor>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="27" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frontal brain electrical activity (EEG) distinguishes valence and intensity of musical emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Trainor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="500" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emotional state classification from EEG data using machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differential entropy feature for EEG-based vigilance estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6627" to="6630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Benatallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1703" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial-temporal recurrent neural network for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EEG emotion recognition using dynamical graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Personalizing EEG-based affective models with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2732" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast, efficient domain adaptation technique for cross-domain electroencephalography (EEG)-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1014</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation techniques for EEG-based emotion recognition: a comparative study on two public datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sourina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Müller-Putz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bihemisphere domain adversarial neural network model for EEG emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complex brain networks: graph theoretical analysis of structural and functional systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph analysis of the human connectome: promise, progress, and pitfalls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zalesky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="426" to="444" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The economy of brain network organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">336</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imaging human connectomes at the macroscale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Craddock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jbabdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Di</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heberlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Milham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class noise vs. attribute noise: A quantitative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotionmeter: A multimodal framework for recognizing human emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature extraction and selection for emotion recognition from EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition via fast and robust feature smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Brain Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition in music listening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Duann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1798" to="1806" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparison of wavelet transform and FFT methods in the analysis of EEG signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="247" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identifying functional brain connectivity patterns for EEG-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 9th International IEEE/EMBS Conference on Neural Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="235" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EEG based emotion recognition by combining functional connectivity network and local activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2869" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic common spatial patterns for multichannel eeg analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian machine learning: EEG/MEG signal processing measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rstfc: A novel algorithm for spatiotemporal filtering and classification of single-trial eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3070" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">EEG-based emotion classification using deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep physiological affect network for the recognition of human emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Emotion recognition from multi-channel EEG data through convolutional recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional neural networks for EEG-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="368" to="380" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A novel bi-hemispheric discrepancy model for EEG emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01704</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Support vector machines with the ramp loss and the hard margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neurophysiological architecture of functional magnetic resonance images of human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suckling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficiency and cost of economical brain functional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Achard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multichannel EEG-based emotion recognition via group sparse canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large scale transductive svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1687" to="1712" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-subject emotion recognition using deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">EEG alpha activity reflects attentional demands, and beta activity reflects emotional and cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">4700</biblScope>
			<biblScope unit="page" from="750" to="752" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">EEG phase synchronization during emotional response to positive and negative film stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rognoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience Letters</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="164" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
