<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>real-time</term>
					<term>deep convo- lutional neural networks</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is a critical technology for autonomous vehicles to understand surrounding scenes. For practical autonomous vehicles, it is undesirable to spend a considerable amount of inference time to achieve high-accuracy segmentation results. Using light-weight architectures (encoderdecoder or two-pathway) or reasoning on low-resolution images, recent methods realize very fast scene parsing which even run at more than 100 FPS on single 1080Ti GPU. However, there are still evident gaps in performance between these real-time methods and models based on dilation backbones. To tackle this problem, we propose novel deep dual-resolution networks (DDRNets) for real-time semantic segmentation of road scenes. Besides, we design a new contextual information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to enlarge effective receptive fields and fuse multi-scale context. Our method achieves new state-of-the-art trade-off between accuracy and speed on both Cityscapes and CamVid dataset. Specially, on single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 109 FPS on Cityscapes test set and 74.4% mIoU at 230 FPS on CamVid test set. Without utilizing attention mechanism, pretraining on larger semantic segmentation dataset or inference acceleration, DDRNet-39 attains 80.4% test mIoU at 23 FPS on Cityscapes. With widely used test augmentation, our method is still superior to most state-of-the-art models, requiring much less computation. Codes and trained models will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation is a basic task in which each pixel of input images should be assigned to the corresponding label <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It plays a vital role in many practical applications such as medical image segmentation, navigation of autonomous vehicles and robots <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. With the rise of deep learning technologies, convolutional neural networks are applied to image segmentation and greatly outperform traditional methods. A series of novel networks have been devised to promote effectiveness since fully convolutional network (FCN) <ref type="bibr" target="#b5">[6]</ref> was proposed to handle semantic segmentation problems. Since semantic segmentation is a kind of dense prediction task, neural networks need to output high-resolution feature maps of large receptive fields to produce good results, which is computationally dependent. Such problem is especially serious for scene parsing of autonomous driving because it requires enforcement on very large images to cover a wide field of view. DeepLab <ref type="bibr" target="#b6">[7]</ref> eliminates some of downsampling in ResNet to maintain high resolution and utilizes convolutions with large dilations <ref type="bibr" target="#b7">[8]</ref> to enlarge receptive fields. Since then ImageNet pre-trained backbones with dilated convolutions have become the standard layout widely used in various methods, including DeepLabV2 <ref type="bibr" target="#b8">[9]</ref>, DeepLabV3 <ref type="bibr" target="#b9">[10]</ref>, PSPNet <ref type="bibr" target="#b10">[11]</ref> and DenseASPP <ref type="bibr" target="#b11">[12]</ref>. However, these methods are very time-consuming during inference which can not be deployed on practical self-driving vehicles. In fact, they even can not process single image in one second because of utilizing multiscale test to improve accuracy. Without modifying general ImageNet classification architectures, authors of HRNet <ref type="bibr" target="#b12">[13]</ref> directly realize deep highresolution representation and implement parallel high-to-low resolution subnetworks to enlarge receptive fields. HRNet outperforms previous exemplars including DeepLabV3 and PSPNet on Cityscapes dataset using less computation <ref type="bibr" target="#b13">[14]</ref>. But due to its deep multiple branches and multiple repeated fusion of multi-scale feature <ref type="bibr" target="#b12">[13]</ref>, HRNet seems not to be an ideal choice for real-time semantic segmentation.</p><p>With ever-increasing demand of deployment on mobile devices, real-time segmentation algorithms <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref> draw more and more attention. Most of them utilize the lightweight encoder-decoder architectures. DFANet <ref type="bibr" target="#b19">[20]</ref> employs deeply multi-scale feature aggregation and achieves 71.3% test mIoU with 100 FPS using lightweight depthwise separable convolutions. Different from encoder-decoder paradigm, authors in <ref type="bibr" target="#b20">[21]</ref> propose a novel bilateral network consisted arXiv:2101.06085v1 [cs.CV] 15 Jan 2021 of a spatial path and a context path. Specially, the spatial path utilizes three relatively wide 3×3 convolutional layers to extract spatial details and the context path is a compact pre-trained backbone to learn contextual information. Such bilateral methods including <ref type="bibr" target="#b21">[22]</ref> achieve higher accuracy than encoder-decoder structures within real-time inference time.</p><p>When it comes to semantic segmentation of road scenes for autonomous driving which is a safety-critical application, we require high-quality semantic segmentation as much as possible and low computation load for deployment. Recently, some competitive methods aiming at semantic segmentation of road scenes were proposed. SwiftNet <ref type="bibr" target="#b22">[23]</ref> defends the advantage of pre-training encoder on ImageNet and leverages light-weight lateral connections to upsample. Authors in <ref type="bibr" target="#b23">[24]</ref> propose a strategy of multiply spatial fusion and class boundary supervision. FANet <ref type="bibr" target="#b24">[25]</ref> achieves a trade-off between speed and accuracy with fast attention module and extra downsampling throughout the network. BiSeNetV2 <ref type="bibr" target="#b25">[26]</ref> achieving 72.6% test mIOU at 156 FPS on Cityscapes hits a new peak for bilateral methods in real-time applications. But these works do not show the potential towards more highquality results, some of these methods may not be easily extended due to deliberately devised architectures and tuned hyper-parameters.</p><p>In this paper, inspired by HRNet, we propose a deep dualresolution network with deep high-resolution representation ability for real-time semantic segmentation of high-resolution images, specially for road-driving images. Our DDRNet starts with one trunk and then is split into two parallel deep branches with different resolution. One deep branch generates relatively high-resolution feature maps and the other extracts rich contextual information by multiple downsampling operations. Multiple bilateral connections are bridged between two branches for efficient information fusion. Besides, we propose a novel module named DAPPM which greatly increases the receptive fields and extracts context information more sufficiently than normal PPM. Before training on semantic segmentation dataset, the dual-resolution network is first trained on ImageNet following common paradigms.</p><p>According to extensive experimental results on two popular benchmarks, DDRNet attains an excellent balance between segmentation accuracy and inference speed, and takes up less GPU memory than HRNet during training. Our method achieves new state-of-the-art mIoU on both Cityscapes and CamVid compared to other real-time algorithms without attention mechanism and any extra bells or whistles. With standard test augmentation technology, DDRNet is comparable to stateof-the-art models but requires much less computing resources.</p><p>The main contributions are summarized as follows:</p><p>• A novel bilateral network with deeply dual resolution is proposed for real-time semantic segmentation. Our network attains new state-of-the-art performance considering inference speed without any extra bells or whistles. • A novel module is designed to harvest rich context information by combining feature aggregation with pyramid pooling. When integrating it with low-resolution feature maps, it leads to little increase in inference time.</p><p>• By simply increasing the width and depth of network, DDRNet achieves a top trade-off between mIoU and FPS among existing methods, from 77.4% mIoU at 109 FPS to 80.4% mIoU at 23 FPS on Cityscapes test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, advanced methods on the strength of dilated backbones have boosted the performance of semantic segmentation under many challenging scenes. Contextual information representation is proved to be a key component for scene parsing tasks. However, with more and more attention drawn to real-world applications, many works explore the potential of more light-weight architectures such as encoder-decoder methods and two-pathway methods. <ref type="figure" target="#fig_1">Fig. 2</ref> shows overall architectures of popular methods and our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. High-performance Semantic Segmentation</head><p>Capacity of high-resolution representation is very important for acquiring state-of-the-art results because semantic segmentation is a kind of dense prediction tasks. But receptive fields of neural networks will be too small to learning high-level semantic information if only getting rid of pooling layers of ImageNet classification backbones. It is an acceptable strategy to utilize dilated convolutions to set up long-range connection between pixels while removing the last two downsampling layers <ref type="bibr" target="#b6">[7]</ref>. However, it also brings new challenges to realtime inference due to the exponential growth of high-resolution feature-map dimension and insufficient optimization of dilated convolutions. There is a fact that most state-of-the-art models are built on dilation backbones and thus have very low practical value on self-driving scene parsing.</p><p>Some works try to explore the substitute of the standard dilation backbone. Authors of DeepLabv3plus <ref type="bibr" target="#b26">[27]</ref> propose a simple decoder and fuse upsampled feature maps with lowlevel feature maps. Such improvement alleviates the requirement of high-resolution representation generated by dilated convolutions. DeepLabv3plus can achieve competitive results though the output stride of encoder is set to 16. HRNet highlights deep high-resolution representations and embodies clear advantages over dilation backbones on semantic segmentation. We argue that higher computational efficiency and inference speed of HRNet owe to its much thinner high-resolution information flows. Taking HRNetV2-W48 <ref type="bibr" target="#b13">[14]</ref> for example, dimensions of 1/4-resolution features and 1/8resolution features are 48 and 96 which are much smaller than the dimensions of pre-trained ResNets <ref type="bibr" target="#b27">[28]</ref> with dilation convolutions. Though high-resolution branches of HRNet are much thinner, they can be greatly enhanced by parallel lowresolution branches and repeated multi-scale fusion.</p><p>Our work starts from the core concept of HRNet and moves forward more compact architectures, maintaining highresolution representations and extracting high-level contextual information through two concise trunks. Experimental results demonstrate the great potential of DDRNets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-time Semantic Segmentation</head><p>Almost all the real-time semantic segmentation methods adopt two basic architectures: encoder-decoder architectures and two-pathway architectures.</p><p>1) Encoder-decoder Architecture: Compared to those methods with dilated convolutions, encoder-decoder architectures intuitively cost less computation and inference time. An encoder is usually a deep network with repeated spatial reduction to extract contextual information and the decoder restores the resolution to accomplish dense prediction by interpolation or transposed convolution <ref type="bibr" target="#b28">[29]</ref>. Specially, an encoder can be a light-weight backbone pre-trained on ImageNet or an efficient variant designed based on it like ERFNet <ref type="bibr" target="#b4">[5]</ref> and ESPNet <ref type="bibr" target="#b17">[18]</ref>. Thus, the typical output resolution of an encoder is 1/32 of the input resolution. After processed by an encoder, resolution is gradually restored to 1/4 or 1/8 by upsampling modules and combining low-level information of the encoder.</p><p>2) Two-pathway Architecture: While the encoder-decoder architecture greatly reduces computation, it damages the performance of semantic segmentation because partial information is lost during the process of repeated downsampling and can not be restored by unsampling. Two-pathway architecture is proposed in order to alleviate this problem <ref type="bibr" target="#b20">[21]</ref>. Besides one pathway of common encoders obtaining semantic information, the other shallow pathway of high resolution provides rich spatial details as a supplement. In order to achieve a good balance between accuracy and speed, the two pathways can be a light-weight encoder of sufficient depth and a shallow but wide branch made up of a few convolutions <ref type="bibr" target="#b25">[26]</ref>  <ref type="bibr" target="#b29">[30]</ref>. The two pathways in BiSeNet are separated at the begining while two branches in Fast-SCNN <ref type="bibr" target="#b21">[22]</ref> share the learning to downsample module. Different from existing two-pathway methods, our DDRNet is implemented with two deep branches which share the early stages and exchange information with each other. The detailed differences are discussed in Section IV.</p><p>3) Light-weight Encoder: There are many computationally efficient backbones can be used as the encoder such as MobileNet <ref type="bibr" target="#b30">[31]</ref>, ShuffleNet <ref type="bibr" target="#b31">[32]</ref> and small version of Xception <ref type="bibr" target="#b32">[33]</ref>. MobileNet replaces standard convolutions with depth-wise separable convolutions to low the number of parameters and computation. Strong regularization effect of depthwise separable convolutions is alleviated by inverted residual blocks in MobileNetV2 <ref type="bibr" target="#b33">[34]</ref>. ShuffleNet utilizes the compactness of grouped convolutions and proposes a channel shuffle operation to promote information fusion between different groups. But all of these networks contain numerous depthwise separable convolutions which can not be implemented efficiently with existing GPU architecture. For this reason, though the FLOPs of ResNet-18 <ref type="bibr" target="#b27">[28]</ref> is about six times of MobileNetV2 1.0×, inference speed of ResNet-18 is higher than MobileNetV2 1.0× on single 1080Ti GPU <ref type="bibr" target="#b22">[23]</ref>. Thus, we employ numerous basic residual modules which comprise two sequential 3×3 convolutions and residual learning to build the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Context Extraction Module</head><p>Another key point of semantic segmentation is how to capture more abundant contextual information. Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b8">[9]</ref> consists of parallel atrous convolution layers with different rates which can attend to multi-scale context information. Compared to ASPP, Pyramid Pooling Module(PPM) <ref type="bibr" target="#b10">[11]</ref> in PSPNet is more computationally efficient by implementing pyramid pooling ahead of convolutional layers. Different from the local nature of convolutional kernels, self-attention mechanism is good at capturing global dependencies. In this way, Dual Attention Network (DANet) <ref type="bibr" target="#b34">[35]</ref> takes advantage of both position attention and channel attention to further improve feature representation. Object Context Network (OCNet) <ref type="bibr" target="#b35">[36]</ref> utilizes self-attention mechanism to explore object context which is defined as a set of pixels that belong to the same object category. Authors in CCNet <ref type="bibr" target="#b36">[37]</ref> propose criss-cross attention to improve the efficiency of GPU memory and computation for non-local attention and apply it to semantic segmentation. In this paper, we do not make use of self-attention mechanism to capture context prior in view of its influence on inference speed. In contrast, we strengthen the PPM module with more scales and deep feature aggregation, and append it to the end of the lowresolution branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Firstly, we rethink the HRNet and analyze the key point of its superiority. Next, the whole pipeline was described, which consists of two parts: pre-training a dual-resolution classification backbone and applying it on the semantic segmentation dataset. We will introduce how to construct a dual-resolution network for classification and how to modify it to achieve superior performance on semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rethinking HRNet</head><p>HRNet is a general architecture which maintains highresolution representation throughout the whole network. In order to enhance model capacity, it starts from a high-resolution subnetwork as the first stage and high-to-low resolution subnetworks are added one by one to form more stages. So for the HRNet, there are actually several parallel multi-resolution branches and each branch is interconnected. Such complicate architecture enables HRNet to better extract features under different resolutions. From the results in <ref type="bibr" target="#b13">[14]</ref>, HRNet outperforms FPN-based ResNet on object detection, showing itself nature of multi-scale representation. HRNet also achieves better performance with much lower computation complexity and costs less inference time compared to DeepLabv3plus.</p><p>Learning from previous works, semantic segmentation demands high-resolution feature maps to be competent for dense prediction and large receptive fields to parse the scenes. By contrast, multi-scale representation ability is more significant for object detection tasks because neural network is supposed to detect as many multi-scale objects as possible in one image. From this point, the architecture of HRNet can be simplified through only reserving two branches. One branch is responsible for maintaining high-resolution feature maps while the other branch generates large enough receptive fields by repeated downsampling. We prove that such compact architecture can greatly improve the inference speed and reduce the memory consumption which is terrible for HRNet by extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dual-resolution Network for Image Classification</head><p>For convenience, we can add an extra high-resolution branch on the widely used classification backbone such as ResNets.</p><p>To get a trade-off between resolution and inference speed, we let the high-resolution branch create feature maps whose resolution is 1/8 of the input image resolution. Therefore, the extra branch is inserted following the end of the conv3 stage. Note that the extra branch does not contain any downsampling operation and has one-to-one correspondence with the original branch (low-resolution branch) to form deep high-resolution representation. Following HRNet, similar bridges are added between two pathways to perform bilateral feature fusion. The specific architectures of three dual-resolution networks with different parameters and GFLOPs are shown in <ref type="table" target="#tab_0">Table I and  Table II</ref>, where DDRNet-23 is derived from ResNet-18 and DDRNet-39 is derived from ResNet-34.</p><p>We modify the input stem of original ResNet, replacing one 7×7 convolutional layer with two sequential 3×3 convolutional layers. And then, basic residual blocks are utilized to construct two branches. To enhance the representation ability, one bottleneck block is added at the end of each branch. The bilateral fusion includes fusing the high-resolution branch into the low-resolution branch (high-to-low fusion) and fusing the low-resolution into the high-resolution branch (low-to-high fusion). For the high-to-low fusion, high-resolution feature maps are downsampled by a sequence of 3×3 convolutions with a stride of 2 before pointwise summation. For the low-to-high resolution, low-resolution feature maps are firstly compressed by a 1×1 convolution and then upsampled by bilinear interpolation. <ref type="figure" target="#fig_2">Fig. 3</ref> shows how bilateral fusion is implement. The i-th high-resolution feature map X Hi and lowresolution feature map X Li can be written as:</p><formula xml:id="formula_0">X Hi = R(F H (X H(i−1) ) + T L−H (F L (X L(i−1) ))) X Li = R(F L (X L(i−1) ) + T H−L (F H (X H(i−1) )))<label>(1)</label></formula><p>where F H and F L correspond to the sequence of basic residual blocks under high resolution and low resolution, T L−H and T H−L refer to the low-to-high and high-to-low transformer, R denotes the ReLU function.</p><p>The dual-resolution networks are trained on ImageNet <ref type="bibr" target="#b37">[38]</ref> following the same data augmentation strategy as previous works <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>. All the models are trained with input resolution of 224×224, a batch size of 256 and 100 epoches on four 2080Ti GPUs. The initial learning rate is set to 0.1 and is reduced by 10 times at epoch 30, 60 and 90. We train all the networks using SGD with a weight decay of 0.0001 and a Nesterov momentum of 0.9. Top-1 errors on ImageNet validation set are shown in <ref type="table" target="#tab_0">Table III</ref>. Though the efficiency of DDRNets is not superior to many advanced light backbones which are elaborately designed on ImageNet, our method still achieves start-of-the-art results on semantic segmentation dataset considering a speed trade-off. If combined with more powerful modules like <ref type="bibr" target="#b39">[40]</ref> or attention mechanism <ref type="bibr" target="#b40">[41]</ref>, or architecture search <ref type="bibr" target="#b41">[42]</ref>, stronger pre-trained models can further boost performances on semantic segmentation with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Aggregation Pyramid Pooling Module</head><p>Here, a novel context extraction module named DAPPM is proposed, which can be seen as a combination of deep feature aggregation and pyramid pooling. <ref type="figure">Fig. 5</ref> shows the interior structure of a DAPPM. Following MSFNet <ref type="bibr" target="#b23">[24]</ref>, we perform large pooling kernels and exponential strides to generate feature maps of 1/128, 1/256, 1/512 input image resolution. Input feature maps of 1/64 resolution and imagelevel information generated by global average pooling are also utilized. We believe that it is insufficient to blend all the multi-scale contextual information by single 3×3 or 1×1 convolution such as Pyramid Pooling Module. Inspired from Res2Net <ref type="bibr" target="#b39">[40]</ref>, we first upsample the feature maps and then fuse contextual information of different scales in a hierarchialresidual way followed by 3×3 convolutions. Considering an  input x, each scale y i can be written as:</p><formula xml:id="formula_1">y i =      C 1×1 (x), i = 1; C 3×3 (U (C 1×1 (P 2 i +1,2 i−1 (x))) + y i−1 ), 1 &lt; i &lt; n; C 3×3 (U (C 1×1 (P global (x))) + y i−1 ), i = n. (2) where C 1×1 is 1×1 convolution, C 3×3 is 3×3 convolution, U</formula><p>denotes upsampling operation, P j,k denotes the pool layer of which kernel size is j and stride is k, P global denotes the global average pooling. In the end, a 1×1 convolution is performed to compress all the feature maps. Besides, a 1×1 projection shortcut is added for easy optimization. Similar to SPP in SwiftNet <ref type="bibr" target="#b22">[23]</ref>, DAPPM is performed with the sequence BN-ReLU-Conv.</p><p>Inside a DAPPM, context extracted by larger pooling kernels is integrated with deeper information flow and multiscale nature is formed by integrating different depth with different sizes of pooling kernel. <ref type="table" target="#tab_0">Table IV</ref> shows that DAPPM is able to provide much richer context than PPM. Though DAPPM consists of more convolution layers and more complicate fusion, it hardly affects the inference speed because  itself input resolution is only 1/64 of input image resolution. Taking 1024×1024 input for example, the maximum feature resolution of DAPPM is 16×16. <ref type="figure">Fig. 4</ref> shows the overview of our method. Some changes are made on DDRNet to achieve better performance on semantic segmentation task. First, the stride of 3×3 convolution in the RBB of low-resolution branch is set to 2 to further downsample. And then, a DAPPM is added at the output of low-resolution branch, harvesting rich contextual information based on high-level feature maps of 1/64 resolution. Besides, the last high-to-low fusion is replaced with low-to-high fusion which is implemented with bilinear interpolation and summation fusion. At last, we devise a simple segmentation head which consists of one 3×3 convolutional layer followed by one 1×1 convolutional layer. Computational load of the segmentation head can be adjusted by changing output dimension of the 3×3 convolutional layer. We set the value to 64 for DDRNet-23-slim, 128 for DDRNet-23 and 256 for DDRNet-39. Note that except the segmentation head and the DAPPM module, all the modules have been pre-trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall Architecture for Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Deep Supervision</head><p>Extra supervisions during training stage can ease the optimization of deep convolutional neural networks (DCNNs). In PSPNet, an auxiliary loss is added at the output of res4 22 block of ResNet-101 and the corresponding weight is set to 0.4 according to experimental results <ref type="bibr" target="#b10">[11]</ref>. BiSeNetV2 <ref type="bibr" target="#b25">[26]</ref> proposes a booster training strategy in which extra segmentation heads are added at the end of each stage of the semantic branch. However, it needs a number of experiments to find the optimal weights which are used to balance each loss, and leads to a non-negligible increase in training memory. In order to acquire better results, SFNet <ref type="bibr" target="#b42">[43]</ref> utilizes a similar strategy named Cascaded Deeply Supervised Learning. In this paper, we only report results obtained by adopting single extra supervision for fair comparison with most of the methods. We add an auxiliary loss as shown in <ref type="figure">Fig. 4</ref> and set the weight to 0.4 following PSPNet. The auxiliary segmentation head can be discarded in the testing stage. The final loss which is sum of cross-entropy can be expressed as:</p><formula xml:id="formula_2">L f = L n + αL a<label>(3)</label></formula><p>where L f , L n , L a represents the final loss, normal loss, auxiliary loss respectively and α denotes the weight of auxiliary loss, which is 0.4 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DIFFERENCES BETWEEN OUR METHOD AND OTHER BILATERAL NETWORKS</head><p>Bilateral Segmentation Network (BiSeNet) devises two branches to learn spatial information and context information separately <ref type="bibr" target="#b20">[21]</ref>. Its authors claim that the detail branch, namely high-resolution branch ,should be shallow and wide for faster inference speed. According to results in BiSeNetV2 <ref type="bibr" target="#b25">[26]</ref>, though deeper model with less GFLOPs outperforms wider model, performance starts to degrade when network depth is scaled up to critical value. We think that it should be ascribed to the absence of residual learning and efficient supervision because most semantic segmentation datasets equip fewer finely annotated images compared to classification datasets. BiSeNetV2 works well in extremely real-time scenarios but difficult to be applied to high-accuracy segmentation models.</p><p>Fast-SCNN is another excellent method which utilizes two branches to speed semantic segmentation <ref type="bibr" target="#b21">[22]</ref>. Different from BiSeNet, its two branches share the first three convolution blocks for learning downsampling. But similar to BiSeNet, the high-resolution branch of Fast-SCNN is very shallow because it only uses one high-resolution convolutional layer to process the output from learning to downsample module.</p><p>Similar to Fast-SCNN, the two branches share the first several stages in DDRNet. But after that there are one-by-one corresponding relations between the high-resolution branch and the low-resolution branch. DDRNet allows more information exchange and generates high-resolution feature maps with large enough receptive fields which we think have a greatly positive influence on semantic segmentation. Besides, we first pre-train the dual-resolution network on ImageNet dateset before training it on semantic segmentation dataset, while most two-pathway methods do not fully benefit from ImageNet pre-training. Though our method is proposed for real-time segmentation, it is equipped to compete with methods designed for pure pursuit of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Cityscapes <ref type="bibr" target="#b43">[44]</ref> is one of the most famous datasets focusing on urban street scenes parsing. The dataset contains 2975 finely annotated images for training, 500 images for validation and 1525 images for test. We do not use extra 20000 coarsely labeled images during train. There are total 19 classes available for semantic segmentation task. The resolution of images is 2048×1024 which is consistent with self-driving requirements but challenging to achieve the real-time inference.</p><p>CamVid <ref type="bibr" target="#b44">[45]</ref> consists of 701 densely annotated frames and resolution of each frame is 960×720. We spilt it into 367 for training, 101 for validation and 233 for test following previous works. We merge train set and validation set at actual train and evaluate our models on test set using 11 classes.</p><p>B. Train Setting 1) Cityscapes: Following <ref type="bibr" target="#b49">[50]</ref>, we use the SGD optimizer with the initial learning rate of 0.01, the momentum of 0.9 and the weight decay of 0.0005. We adopt the ploy learning policy with the power of 0.9 to drop the learning rate and implement the data augmented method including random cropping images, random scaling in the range of 0.5 to 2.0 and random horizonal flipping. Images are randomly cropped into 1024×1024 for training following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref>. All the models are trained with 484 epoches (about 120K iterations), a batch size of 12 and using syncBN on four 2080Ti GPUs. Before evaluating on test server, we use images from train and val set at the same time for local training. For fair comparison with <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b42">[43]</ref>, online hard example mining (OHEM) <ref type="bibr" target="#b50">[51]</ref> is also used.  <ref type="table" target="#tab_0">V  ACCURACY AND SPEED COMPARISON ON CITYSCAPES. WE REPORT RESULTS ON BOTH VAL SET AND TEST SET. SINCE INFERENCE SPEED OF DIFFERENT  MODELS IS MEASURED UNDER DIFFERENT CONDITIONS, THE CORRESPONDING GPU MODELS AND INPUT RESOLUTIONS ARE REPORTED. OUR GFLOPS  CALCULATION ADOPTS 2048×1024 IMAGE AS INPUT. THE CORRESPONDING SPEED IS MEASURED USING TENSORRT ACCELERATION IF THE METHOD IS  MARKED</ref>  2) CamVid: We set the initial learning rate to 0.001 and train all the models for 968 epoches. Images are randomly cropped into 960×720 for training following <ref type="bibr" target="#b19">[20]</ref>. All the models are trained on single GPU and other training details are identical to those for Cityscapes. When employing Cityscapes pre-train, we fine-tune the models for 200 epoches with the initial learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Measure of Inference Speed</head><p>The inference speed is measured on a single GTX 2080Ti GPU by setting the batch size to 1 and with CUDA 10.0, CUDNN 7.6 and PyTorch 1.3. We follow the test code provided by SwiftNet <ref type="bibr" target="#b22">[23]</ref> for accurate measurement. Similar to MSFNet and SwiftNet, we exclude batch normalization layers after convolutional layers because they can be integrated into convolutions during inference. We run the same network 500 times under input resolution of 2048×1024 for Cityscapes and input resolution of 960×720 for CamVid, and report the average time to eliminate occasionality. D. Speed and Accuracy Comparisons 1) Cityscapes: As can be observed from <ref type="table">Table V</ref> and <ref type="figure" target="#fig_0">Fig. 1</ref>, our method achieves a new state-of-the-art trade-off between real-time and high-accuracy. Specially, DDRNet-23slim (our smallest model) achieves 77.4% mIoU on test set at 109 FPS. It outperforms DFANet A and MSFNet (under 1024×512) by 6.1% test mIoU with similar inference speed, and reasons approximate three times as fast as MSFNet under same resolution. Besides, it runs 50% faster than the smallest SFNet and achieves 2.9% mIoU gain on test set. It is worth noting that our method also towers over those methods based on architecture search for real-time semantic segmentation including CAS <ref type="bibr" target="#b47">[48]</ref> and GAS <ref type="bibr" target="#b48">[49]</ref> at a similar inference speed. For wider models, DDRNet-23 achieves the overall best accuracy among the existing real-time methods, reaching 79.4% mIoU at 39 FPS. DDRNet-23 has the performance gain of 0.5% over SFNet (ResNet-18) but runs much faster than it. We keep going deeper and wider with DDRNets, achieving 80.4% mIoU on Cityscapes test server at 23 FPS, only using fine annotated data. If combined with pre-training on much bigger dataset and TensorRT acceleration like <ref type="bibr" target="#b42">[43]</ref>, our method can build a skyscraping baseline for real-time semantic segmentation of road scenes. On Cityscapes val set, DDRNet-23-slim outperforms all published results of realtime methods with 36.3 GFLOPs and 5.7M parameters. And DDRNet-23 achieves a new overall best result of 79.5% mIoU. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the visilized results of DDRNet-23-slim and DDRNet-23 under different scenes.</p><p>2) CamVid: As shown in <ref type="table" target="#tab_0">Table VI</ref>  Cityscapes pre-train. It obtains the second-highest accuracy and runs faster than all the other methods. In the meanwhile, the performance of DDRNet-23 is better than previous stateof-the-art method MSFNet. DDRNet-23 also has a big performance gain over BiSeNetV2-L and SFNet (ResNet-18) but runs about two times faster than them. Given that training pixels of CamVid are much less than that of Cityscapes, we believe that the outstanding performances of DDRNets partly attribute to appropriate ImageNet pre-training. In addition, our models pre-trained on Cityscapes achieve superior segmentation accuracy at the real-time inference speed. Specially, Cityscapes pre-trained DDRNet-23 realizes 79.9% mIoU at 94 FPS, stronger and much faster than BiSeNetV2-L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons with State-of-the-art Results</head><p>In this part, we further demonstrate the capacity of DDRNet for semantic segmentation by comparing to state-of-the-art models on Cityscapes test set. Such methods frequently perform multi-scale and horizontal flip inference to achieve better results regardless of time cost. For fair comparison with them, we also apply multiple scales including 0.50×, 0.75×, 1×, 1.25×, 1.5×, 1.75×, 2× with left-right flipping during test. As is shown in <ref type="table" target="#tab_0">Table VII</ref>, standard test augmentation improves the accuracy of DDRNet-39 from 80.4% to 81.9%. Besides, DDRNet-39 outperforms numerous powerful models which are integrated with self-attention modules including CCNet, DANet and OCNet. It is noteworthy that our method only requires 11% computation of DANet. DDRNet-39 also gets ahead of SFNet (based on ResNet-101 backbone) which is a state-of-the-art method for real-time semantic segmentation, only requiring 34% computation. DDRNet-39 1.5× of which size is closer to other models in <ref type="table" target="#tab_0">Table VII</ref> is a wider version of DDRNet-39, achieving a very competitive performance of 82.4% mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparisons with HRNet</head><p>The most difference between DDRNet and HRNet is the number of parallel branches. Besides, we apply the multiscale context extraction module to features maps of very low resolution. Experimental results in <ref type="table" target="#tab_0">Table VIII</ref> demonstrate the great improvement of DDRNet over HRNet in both inference time and train memory usage. We get the val results of two small HRNets from official implementation on Github. Training memory is measured by deploying the models with a batch size of 2 on single 2080Ti at a crop size of 1024×512, and excluding the auxiliary segmentation head.  <ref type="bibr" target="#b22">[23]</ref> 73.9 -GTX 1080Ti SwiftNetRN-18 <ref type="bibr" target="#b22">[23]</ref> 72.6 -GTX 1080Ti BiSeNet1 <ref type="bibr" target="#b20">[21]</ref> 65.6 175 GTX 1080Ti BiSeNet2 <ref type="bibr" target="#b20">[21]</ref> 68.7 116 GTX 1080Ti BiSeNetV2 † <ref type="bibr" target="#b25">[26]</ref> 72.4 124 GTX 1080Ti BiSeNetV2-L † <ref type="bibr" target="#b25">[26]</ref> 73.2 33 GTX 1080Ti CAS <ref type="bibr" target="#b47">[48]</ref> 71.2 169 TitanXp GAS <ref type="bibr" target="#b48">[49]</ref> 72.8 153 TitanXp SFNet(DF2) <ref type="bibr" target="#b42">[43]</ref> 70   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablative Experiments on Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Ablative Experiment of Standard Bells and Whistles:</head><p>We analyze the effect of some basic training tricks which are also adopted by recent advanced method SFNet <ref type="bibr" target="#b42">[43]</ref>. As shown in <ref type="table" target="#tab_0">Table IX</ref>, the accuracy is raised from 76.1 to 77.8 with deep supervision, OHEM ,and train at a larger crop size ( the default is 1024×512).</p><p>2) Ablative Experiment of DAPPM: In this part, the effectiveness of DAPPM is demonstrated by comparison with PPM which is a widely used global-context extractor. For fair comparison, we choose the SPP in SwiftNet which is a simplified version of PPM. The results in <ref type="table">Table X</ref> suggest that context extraction modules can greatly improve the performance of scene parsing, from 74.1% mIoU to 77.8% mIoU. Benefited from extracting context on very low-resolution feature maps, inference speed is hardly affected. Besides, DAPPM achieves 1% mIoU gain compared to simplified PPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a novel deep dual-resolution architecture is proposed for real-time semantic segmentation of road scenes and a new module for extracting multi-scale contextual information is presented. To our best knowledge, we are the first to introduce deep high-resolution representation into real-time semantic segmentation and our simple strategy outperforms all previous models on two popular benchmarks without any extra bells or whistles. Most existing real-time networks are elaborately designed or are advanced backbones specially devised for ImageNet, which are very different from dilated backbone widely used for high-accuracy methods. By contrast, DDRNet only utilizes basic residual modules and bottleneck modules, and can provide a wide range of speed and accuracy trade-off by scaling model width and depth. Due to the simplicity and efficiency of our method, it can be seen as a strong baseline towards unifying real-time and high-accuracy semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The authors are with Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin 150001, China (e-mail: hui-huipan@hit.edu.cn) A comparison of speed-accuracy trade-off on Cityscapes test set. The red triangles indicate our methods while blue triangles represent other methods. Green circles represent architecture search methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A comparison about dilation backbone, encoder-decoder backbone, two-pathway backbone and our deep dual-resolution backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The details of bilateral fusion in DDRNet. Summation fusion is implemented before ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>The overview of DDRNets on semantic segmentation. "RB" denotes sequential residual basic blocks. "RBB" denotes single residual bottleneck block. "DAPPM" denotes the Deep Aggregation Pyramid Pooling Module. "Seg. Head" denotes the segmentation head. Black solid lines denote information paths with data processing (including upsampling and downsampling) and black dashed lines denote information path without data processing. "sum" denotes pointwise concatenation. Dashed boxes denote the components which are disregarded in the inference stage. The detailed architecture of Deep Aggregation Pyramid Pooling Module. The number of multi-scale branches can be adjusted according to input resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visualized segmentation results on Cityscapes val set. The four columns left-to-right refer to input image, ground truth, the output of DDRNet-23-slim, and the output of DDRNet-23. The first four rows show the performance of two models while the last two rows represent some segmentation failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>ARCHITECTURES OF DDRNET-23-SLIM AND DDRNET-23 FOR IMAGENET</figDesc><table><row><cell>stage</cell><cell></cell><cell>output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DDRNet-23-slim</cell><cell>DDRNet-23</cell></row><row><cell>conv1</cell><cell cols="2">112 × 112</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3 × 3, 32, stride 2</cell><cell>3 × 3, 64, stride 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3 × 3, 32, stride 2</cell><cell>3 × 3, 64, stride 2</cell></row><row><cell>conv2</cell><cell></cell><cell>56 × 56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 32 3 × 3, 32</cell><cell>× 2</cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell></row><row><cell>conv3</cell><cell></cell><cell>28 × 28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell><cell>3 × 3, 128 3 × 3, 128</cell><cell>× 2</cell></row><row><cell>conv4</cell><cell cols="3">14 × 14, 28 × 28</cell><cell></cell><cell cols="3">3 × 3, 128 3 × 3, 128</cell><cell>× 2</cell><cell></cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell><cell>3 × 3, 256 3 × 3, 256</cell><cell>× 2</cell><cell>3 × 3, 128 3 × 3, 128</cell><cell>× 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Bilateral fusion</cell><cell>Bilateral fusion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3 × 3, 256 3 × 3, 256</cell><cell>× 2</cell><cell></cell><cell>3 × 3, 64 3 × 3, 64</cell><cell>× 2</cell><cell>3 × 3, 512 3 × 3, 512</cell><cell>× 2</cell><cell>3 × 3, 128 3 × 3, 128</cell><cell>× 2</cell></row><row><cell>conv5 1</cell><cell cols="3">7 × 7, 28 × 28</cell><cell></cell><cell cols="3">1 × 1, 256 </cell><cell cols="3">Bilateral fusion  1 × 1, 64</cell><cell></cell><cell></cell><cell>1 × 1, 512</cell><cell></cell><cell>Bilateral fusion  1 × 1, 128 </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell> </cell><cell cols="4">3 × 3, 256   × 1</cell><cell></cell><cell> </cell><cell>3 × 3, 64</cell><cell>  × 1</cell><cell> </cell><cell>3 × 3, 512</cell><cell>  × 1</cell><cell>  3 × 3, 128   × 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">1 × 1, 512</cell><cell></cell><cell></cell><cell>1 × 1, 128</cell><cell>1 × 1, 1024</cell><cell>1 × 1, 256</cell></row><row><cell>conv5 2</cell><cell></cell><cell>7 × 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">High-to-low fusion</cell><cell>High-to-low fusion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">1 × 1, 1024</cell><cell>1 × 1, 2048</cell></row><row><cell></cell><cell></cell><cell>1 × 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">7 × 7 global average pool</cell><cell>7 × 7 global average pool</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">1000-d fc, softmax</cell><cell>1000-d fc, softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">DDRNET-39 ARCHITECTURE FOR IMAGENET</cell><cell></cell></row><row><cell>stage</cell><cell></cell><cell></cell><cell cols="4">DDRNet-39</cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1</cell><cell></cell><cell></cell><cell cols="5">3 × 3, 64, stride2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">3 × 3, 64, stride2</cell><cell></cell><cell></cell></row><row><cell>conv2</cell><cell></cell><cell></cell><cell cols="3">3 × 3, 64 3 × 3, 64</cell><cell cols="2">× 3</cell><cell></cell><cell></cell></row><row><cell>conv3</cell><cell></cell><cell></cell><cell cols="3">3 × 3, 128 3 × 3, 128</cell><cell cols="2">× 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3 × 3, 256 3 × 3, 256</cell><cell>× 3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 128 3 × 3, 128</cell><cell>× 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Bilateral fusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv4</cell><cell></cell><cell>3 × 3, 256 3 × 3, 256</cell><cell>× 3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 128 3 × 3, 128</cell><cell>× 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Bilateral fusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3 × 3, 512 3 × 3, 512</cell><cell>× 3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 × 3, 128 3 × 3, 128</cell><cell>× 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Bilateral fusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv5 1</cell><cell></cell><cell>1 × 1, 512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 128 </cell><cell></cell></row><row><cell></cell><cell> </cell><cell>3 × 3, 512</cell><cell>  × 1</cell><cell></cell><cell></cell><cell> </cell><cell cols="3">3 × 3, 128   × 1</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1, 1024</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 × 1, 256</cell><cell></cell></row><row><cell>conv5 2</cell><cell></cell><cell></cell><cell cols="5">High-to-low fusion</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">1 × 1, 2048</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">7 × 7 global average pool</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">1000-d fc, softmax</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">TOP-1 ERROR RATES, PARAMETER SIZE AND GFLOPS OF THREE</cell></row><row><cell></cell><cell cols="2">SCALED-UP DDRNETS</cell><cell></cell></row><row><cell>Model</cell><cell>top-1 err.</cell><cell>Params.</cell><cell>GFLOPs</cell></row><row><cell>DDRNet-23-slim</cell><cell>29.8</cell><cell>7.57M</cell><cell>0.98G</cell></row><row><cell>DDRNet-23</cell><cell>24.1</cell><cell>28.22M</cell><cell>3.88G</cell></row><row><cell>DDRNet-39</cell><cell>22.7</cell><cell>40.13M</cell><cell>6.95G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV CONSIDERING</head><label>IV</label><figDesc>AN INPUT IMAGE OF 1024×1024, THE GENERATED CONTEXT SIZES OF PPM AND DAPPM ARE LISTED</figDesc><table><row><cell></cell><cell>PPM</cell><cell>DAPPM</cell></row><row><cell></cell><cell></cell><cell>[16]</cell></row><row><cell></cell><cell></cell><cell>[16, 8]</cell></row><row><cell>Output scale</cell><cell>[16, 6, 3, 2, 1]</cell><cell>[16, 8, 4]</cell></row><row><cell></cell><cell></cell><cell>[16, 8, 4, 2]</cell></row><row><cell></cell><cell></cell><cell>[16, 8, 4, 2, 1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, DDRNet-23-slim achieves 74.4% mIoU on CamVid test set at 230 FPS without</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI ACCURACY</head><label>VI</label><figDesc>AND SPEED COMPARISON ON CAMVID TEST SET. MSFNET RUNS AT 1024×768 AND MSFNET* RUNS AT 768×512 WHILE OTHER METHODS RUN AT 960×720. THE CORRESPONDING SPEED IS MEASURED USING TENSORRT ACCELERATION IF THE METHOD IS MARKED WITH †.</figDesc><table><row><cell>Model</cell><cell>MIoU</cell><cell>Speed (FPS)</cell><cell>GPU</cell></row><row><cell></cell><cell cols="2">w/o Cityscapes pre-train</cell><cell></cell></row><row><cell>DFANet A [20]</cell><cell>64.7</cell><cell>120</cell><cell>TitanX</cell></row><row><cell>DFANet B [20]</cell><cell>59.3</cell><cell>160</cell><cell>TitanX</cell></row><row><cell>SwiftNetRN-18 pyr</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII STATE</head><label>VII</label><figDesc>-OF-THE-ART MODELS ON CITYSCAPES TEST SET. OS DENOTES THE FINAL OUTPUT STRIDE. ALL THE METHODS TRAIN MODELS ON BOTH TRAIN AND VAL SET EXCEPT PSPNET MARKED WITH †ONLY USING TRAIN SET. GFLOPS CALCULATION ADOPTS 1024×1024 IMAGE AS INPUT AND MOST OF RESULTS ABOUT GFLOPS AND PARAMS CAN BE FOUND IN<ref type="bibr" target="#b42">[43]</ref> </figDesc><table><row><cell>Model</cell><cell>OS</cell><cell>mIoU</cell><cell>GFLOPs</cell><cell>Params.</cell></row><row><cell>SAC [54]</cell><cell>8</cell><cell>78.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DepthSeg [55]</cell><cell>8</cell><cell>78.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PSPNet † [11]</cell><cell>8</cell><cell>78.4</cell><cell>1065.4</cell><cell>65.7M</cell></row><row><cell>ResNet38 [56]</cell><cell>8</cell><cell>78.4</cell><cell>-</cell><cell>-</cell></row><row><cell>BiSeNet [21]</cell><cell>8</cell><cell>78.9</cell><cell>219.1</cell><cell>51.0M</cell></row><row><cell>DFN [57]</cell><cell>8</cell><cell>79.3</cell><cell>1121.0</cell><cell>90.7M</cell></row><row><cell>PSANet [58]</cell><cell>8</cell><cell>80.1</cell><cell>1182.6</cell><cell>85.6M</cell></row><row><cell>DenseASPP [12]</cell><cell>8</cell><cell>80.6</cell><cell>632.9</cell><cell>35.7M</cell></row><row><cell>CCNet [37]</cell><cell>8</cell><cell>81.4</cell><cell>1153.9</cell><cell>66.5M</cell></row><row><cell>DANet [35]</cell><cell>8</cell><cell>81.5</cell><cell>1298.8</cell><cell>66.6M</cell></row><row><cell>OCNet [36]</cell><cell>8</cell><cell>81.7</cell><cell>-</cell><cell>-</cell></row><row><cell>OCRNet [59]</cell><cell>8</cell><cell>81.8</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNetV2-W48 [50]</cell><cell>4</cell><cell>81.6</cell><cell>348.1</cell><cell>65.9M</cell></row><row><cell>SFNet [43]</cell><cell>4</cell><cell>81.8</cell><cell>417.5</cell><cell>50.3M</cell></row><row><cell>DDRNet-39</cell><cell>8</cell><cell>81.9</cell><cell>140.6</cell><cell>32.3M</cell></row><row><cell>DDRNet-39 1.5×</cell><cell>8</cell><cell>82.4</cell><cell>303.0</cell><cell>70.2M</cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARATIVE EXPERIMENTS BETWEEN DDRNET AND HRNET IN TERMS</cell></row><row><cell cols="4">OF MIOU, FPS AND TRAIN MEMORY</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>mIoU</cell><cell>FPS</cell><cell>Train mem.</cell></row><row><cell cols="2">HRNetV2-W18-Small-v1 [50]</cell><cell>70.3</cell><cell>72.0</cell><cell>1989MiB</cell></row><row><cell cols="2">HRNetV2-W18-Small-v2 [50]</cell><cell>76.2</cell><cell>32.3</cell><cell>2745MiB</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell>76.9</cell><cell>108.8</cell><cell>1629MiB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX INFLUENCES</head><label>IX</label><figDesc>OF STANDARD BELLS AND WHISTLES, INCLUDING DEEP SUPERVISION (DS), OHEM AND TRAIN AT A CROP SIZE OF 1024×1024 ANALYSIS OF PPM AND DAPPM ON ACCURACY AND SPEED.</figDesc><table><row><cell>Model</cell><cell>DS</cell><cell>OHEM</cell><cell>1024×1024</cell><cell>mIoU</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.1</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.1</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.9</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell></row><row><cell></cell><cell></cell><cell>TABLE X</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>PPM</cell><cell>DAPPM</cell><cell>mIoU</cell><cell>Speed</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell>74.1</cell><cell>115.3</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell>76.8</cell><cell>111.4</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell>77.8</cell><cell>108.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1814" to="1828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarse-to-fine semantic segmentation from image-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task decomposition and synchronization for semantic biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7497" to="7510" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Her2net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Small object augmentation of urban scenes for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5175" to="5190" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-integrated and featurerefined network for lightweight object parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5079" to="5093" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of roaddriving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12607" to="12616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation via multiply spatial fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07217</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03815</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cabinet: Efficient context aggregation network for low-latency semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00993</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10120</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML-ITS, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph-guided architecture search for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
