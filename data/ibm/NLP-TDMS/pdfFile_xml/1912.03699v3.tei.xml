<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimum Class Confusion for Versatile Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Minimum Class Confusion for Versatile Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Versatile Domain Adaptation, Minimum Class Confusion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are a variety of Domain Adaptation (DA) scenarios subject to label sets and domain configurations, including closed-set and partial-set DA, as well as multi-source and multi-target DA. It is notable that existing DA methods are generally designed only for a specific scenario, and may underperform for scenarios they are not tailored to. To this end, this paper studies Versatile Domain Adaptation (VDA), where one method can handle several different DA scenarios without any modification. Towards this goal, a more general inductive bias other than the domain alignment should be explored. We delve into a missing piece of existing methods: class confusion, the tendency that a classifier confuses the predictions between the correct and ambiguous classes for target examples, which is common in different DA scenarios. We uncover that reducing such pairwise class confusion leads to significant transfer gains. With this insight, we propose a general loss function: Minimum Class Confusion (MCC). It can be characterized as (1) a non-adversarial DA method without explicitly deploying domain alignment, enjoying faster convergence speed; (2) a versatile approach that can handle four existing scenarios: Closed-Set, Partial-Set, Multi-Source, and Multi-Target DA, outperforming the state-of-the-art methods in these scenarios, especially on one of the largest and hardest datasets to date (7.3% on DomainNet). Its versatility is further justified by two scenarios proposed in this paper: Multi-Source Partial DA and Multi-Target Partial DA. In addition, it can also be used as a general regularizer that is orthogonal and complementary to a variety of existing DA methods, accelerating convergence and pushing these readily competitive methods to stronger ones. Code is available at https://github.com/thuml/Versatile-Domain-Adaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The scarcity of labeled data hinders deep neural networks (DNNs) from use in real applications. This challenge gives rise to Domain Adaptation (DA) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28]</ref>, an important technology that aims to transfer knowledge from a labeled source domain to an unlabeled target domain in the presence of dataset shift. A rich line of DNN-based methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref> have been proposed for Unsupervised DA (UDA), a closed-set scenario with one source domain and Note that scenarios (5)- <ref type="bibr" target="#b5">(6)</ref> are newly proposed in this paper. Our Minimum Class Confusion (MCC) is a versatile method towards all these DA scenarios.</p><p>one target domain sharing the same label set. Recently, several highly practical scenarios were proposed, such as Partial DA (PDA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51]</ref> with the source label set subsuming the target one, Multi-Source DA (MSDA) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b47">48]</ref> with multiple source domains, and Multi-Target DA (MTDA) <ref type="bibr" target="#b31">[32]</ref> with multiple target domains. As existing UDA methods cannot be applied directly to these challenging scenarios, plenty of methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b31">32]</ref> have been designed for each specific scenario, which work quite well in each tailored scenario.</p><p>In practical applications, however, it is difficult to confirm the label sets and domain configurations in the data acquisition process. Therefore, we may be stuck in choosing a proper method tailored to the suitable DA scenario. The most ideal solution to escape from this dilemma is a versatile DA method that can handle various scenarios without any modification. Unfortunately, existing DA methods are generally designed only for a specific scenario and may underperform for scenarios they are not tailored to. For instance, PADA <ref type="bibr" target="#b2">[3]</ref>, a classic PDA method, excels at selecting out outlier classes but suffers from the internal domain shift in MSDA and MTDA, while DADA <ref type="bibr" target="#b31">[32]</ref>, an outstanding method tailored to MTDA, cannot be directly applied to PDA or MSDA. Hence, existing DA methods are not versatile enough to handle practical scenarios of complex variations.</p><p>In this paper, we define Versatile Domain Adaptation (VDA) as a line of versatile approaches able to tackle a variety of scenarios without any modification. Towards VDA, a more general inductive bias other than the domain alignment should be explored. In this paper, we delved into the error matrices of the target domain and found that the classifier trained on the source domain may confuse to distinguish the correct class from a similar class, such as cars and trucks. As shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, the probability that a source-only model misclassifies cars as trucks on the target domain is over 25%. Further, we analyzed the error matrices in other DA scenarios and reached the same conclusion. These findings give us a fresh perspective to enable VDA: class confusion, the tendency that a classifier confuses the predictions between the correct and ambiguous classes for target examples. We uncover that less class confusion leads to more transfer gains for all the domain adaptation scenarios in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Still, we need to address a new challenge that the ground-truth class confusion cannot be calculated if the labels in the target domain are inaccessible. Fortunately, the confusion between different classes can be naturally reflected by an exampleweighted inner-product between the classifier predictions and their transposes. And we can define class confusion from this perspective, enabling it to be computed from well-calibrated classifier predictions. To this end, we propose a novel loss function: Minimum Class Confusion (MCC). It can be characterized as a novel and versatile DA approach without explicitly deploying domain alignment <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>, enjoying fast convergence speed. In addition, it can also be used as a general regularizer that is orthogonal and complementary to existing DA methods, further accelerating and improving those readily competitive methods. Our contributions are summarized as follows:</p><p>-We propose a practical setting, Versatile Domain Adaptation (VDA),</p><p>where one method can tackle many DA scenarios without modification.</p><p>-We uncover that the class confusion is a common missing piece of existing DA methods and that less class confusion leads to more transfer gains.</p><p>-We propose a novel loss function: Minimum Class Confusion (MCC), which is versatile to handle four existing DA scenarios, including closed-set, partial-set, multi-source, and multi-target, as well as two proposed scenarios: multi-source partial DA and multi-target partial DA.</p><p>-We conduct extensive experiments on four standard DA datasets, and show that MCC outperforms the state-of-the-art methods in different DA scenarios, especially on one of the largest and hardest datasets (7.3% on DomainNet), enjoying a faster convergence speed than mainstream DA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Domain Adaptation (UDA). Most of the existing domain adaptation researches focused on UDA, in which numerous UDA methods were proposed based on either Moment Matching or Adversarial Training.</p><p>Moment Matching methods aim at minimizing the distribution discrepancy across domains. Deep Coral <ref type="bibr" target="#b39">[40]</ref> aligns second-order statistics between distributions. DDC <ref type="bibr" target="#b43">[44]</ref> and DAN <ref type="bibr" target="#b20">[21]</ref> utilize Maximum Mean Discrepancy <ref type="bibr" target="#b10">[11]</ref>, JAN <ref type="bibr" target="#b23">[24]</ref> defines Joint Maximum Mean Discrepancy, SWD <ref type="bibr" target="#b17">[18]</ref> introduces Sliced Wasserstein Distance and CAN <ref type="bibr" target="#b16">[17]</ref> leverages Contrastive Domain Discrepancy.</p><p>Adversarial Training methods were inspired by the Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref>, aiming at learning domain invariant features in an adversarial manner. DANN <ref type="bibr" target="#b7">[8]</ref> introduces a domain discriminator to distinguish source and target features, while the feature extractor strives to fool it. ADDA <ref type="bibr" target="#b42">[43]</ref>, MADA <ref type="bibr" target="#b29">[30]</ref> and MCD <ref type="bibr" target="#b35">[36]</ref> extend this architecture to multiple feature extractors and classifiers. Motivated by Conditional GANs <ref type="bibr" target="#b26">[27]</ref>, CDAN <ref type="bibr" target="#b21">[22]</ref> aligns domain features in a class-conditional adversarial game. CyCADA <ref type="bibr" target="#b14">[15]</ref> adapts features in both pixel and feature levels. TADA <ref type="bibr" target="#b46">[47]</ref> introduces the first transferable attention mechanism. SymNet <ref type="bibr" target="#b51">[52]</ref> uses a symmetric classifier, and DTA <ref type="bibr" target="#b18">[19]</ref> learns discriminative features with a new adversarial dropout.</p><p>There are other approaches to domain adaptation. For instance, SE <ref type="bibr" target="#b6">[7]</ref> is based on the teacher-student <ref type="bibr" target="#b40">[41]</ref> architecture. TransNorm <ref type="bibr" target="#b45">[46]</ref> tackles DA with a new transferable backbone. TAT <ref type="bibr" target="#b19">[20]</ref> proposes transferable adversarial training to guarantee the adaptability. BSP <ref type="bibr" target="#b4">[5]</ref> balances between the transferability and discriminability. AFN <ref type="bibr" target="#b48">[49]</ref> enlarges feature norm to enhance feature transferability. Some methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> also utilize the less-reliable self-training or pseudo labeling, e.g. TPN <ref type="bibr" target="#b28">[29]</ref> is based on pseudo class-prototypes.</p><p>Partial Domain Adaptation (PDA). In PDA, the target label set is a subset of the source label set. Representative methods include SAN <ref type="bibr" target="#b1">[2]</ref>, IWAN <ref type="bibr" target="#b50">[51]</ref>, PADA <ref type="bibr" target="#b2">[3]</ref> and ETN <ref type="bibr" target="#b3">[4]</ref>, introducing different weighting mechanisms to select out outlier source classes in the process of domain feature alignment.</p><p>Multi-Source Domain Adaptation (MSDA). In MSDA, there are multiple source domains of different distributions. MDAN <ref type="bibr" target="#b53">[54]</ref> provides theoretical insights for MSDA, while Deep Cocktail Network <ref type="bibr" target="#b47">[48]</ref> (DCTN) and M 3 SDA <ref type="bibr" target="#b30">[31]</ref> extend adversarial training and moment-matching to MSDA, respectively.</p><p>Multi-Target Domain Adaptation (MTDA). In MTDA, we need to transfer a learning model to multiple unlabeled target domains. DADA <ref type="bibr" target="#b31">[32]</ref> is the first approach to MTDA through disentangling domain-invariant representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this paper, we study Versatile Domain Adaptation (VDA) where one method can tackle many scenarios without any modification. We justify the versatility of one method by four existing scenarios: (1) Unsupervised Domain Adaptation (UDA) <ref type="bibr" target="#b7">[8]</ref>, the standard scenario with a labeled source domain Given the shared feature extractor F , MCC is defined on the class predictions Y t given by the source classifier G on the target data. MCC is versatile to address various domain adaptation scenarios standalone, or to be integrated with existing methods (moment matching, adversarial training, etc). (Best viewed in color.)</p><formula xml:id="formula_0">S = {(x i s , y i s )} ns i=1 and an unlabeled target domain T = {x i t } nt i=1</formula><p>, where x i is an example and y i is the associated label; (2) Partial Domain Adaptation (PDA) <ref type="bibr" target="#b2">[3]</ref>, which extends UDA by relaxing the source domain label set to subsume the target domain label set; (3) Multi-Source Domain Adaptation (MSDA) <ref type="bibr" target="#b30">[31]</ref>, which extends UDA by expanding to S labeled source domains {S 1 , S 2 , ..., S S }; (4) Multi-Target Domain Adaptation (MTDA) <ref type="bibr" target="#b31">[32]</ref>, which extends UDA by expanding to T unlabeled target domains {T 1 , T 2 , ..., T T }. We further propose two scenarios to confirm the versatility: (5)/(6) Multi-Source/Multi-Target Partial Domain Adaptation (MSPDA/MTPDA), which extend PDA to multisource/multi-target scenarios. Tailored to a specific scenario, existing methods fail to readily handle these scenarios. We propose Minimum Class Confusion (MCC) as a generic loss function for VDA. Hereafter, we denote by y i· , y ·j and Y ij the i-row, the j-th column and the ij-th entry of matrix Y, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Minimum Class Confusion</head><p>To enable versatile domain adaptation, we need to find out a proper criterion to measure the pairwise class confusion on the target domain. Unlike previous methods such as CORAL <ref type="bibr" target="#b39">[40]</ref> that focus on features, we explore the class predictions. Denote the classifier output on the target domain as Y t = G(F (X t )) ∈ R B×|C| , where B is the batch size of the target data, |C| is the number of source classes, F is the feature extractor and G is the classifier. In our method, we focus on the classifier predictions Y and omit the domain subscript t for clarity.</p><p>Probability Rescaling. According to <ref type="bibr" target="#b11">[12]</ref>, DNNs tend to make overconfident predictions, hindering them from directly reasoning about the class confusion. Therefore, we adopt temperature rescaling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>, a simple yet effective technique, to alleviate the negative effect of overconfident predictions. Using temperature scaling, the probability Y ij that the i-th instance belongs to the j-th class can be recalibrated as</p><formula xml:id="formula_1">Y ij = exp (Z ij /T ) |C| j =1 exp (Z ij /T ) ,<label>(1)</label></formula><p>where Z ij is the logit output of the classifier layer and T is the temperature hyper-parameter for probability rescaling. Obviously, Eq. (1) boils down to the vanilla softmax function when T = 1. Class Correlation. As Y ij reveals the relationship between the i-th instance and the j-th class, we define the class correlation between two classes j and j as</p><formula xml:id="formula_2">C jj = y T ·j y ·j .<label>(2)</label></formula><p>It is a coarse estimation of the class confusion. Lets delve into the definition of the class correlation in Eq. <ref type="bibr" target="#b1">(2)</ref>. Note that y ·j denotes the probabilities that the B examples in each batch come from the j-th class. The class correlation measures the possibility that the classifier simultaneously classifies the B examples into the j-th and the j -th classes. It is noteworthy that such pairwise class correlation is relatively safe: for false predictions with high confidence, the corresponding class correlation is still low. In other words, highly confident false predictions will negligibly impact the class correlation. Uncertainty Reweighting. We note that examples are not equally important for quantifying class confusion. When the prediction is closer to a uniform distribution, showing no obvious peak (obviously larger probabilities for some classes), we consider the classifier as ignorant of this example. On the contrary, when the prediction shows several peaks, it indicates that the classifier is reluctant between several ambiguous classes (such as car and truck). Obviously, these examples that make the classifier ambiguous across classes will be more suitable for embodying class confusion. As defined in Eq. <ref type="formula" target="#formula_2">(2)</ref>, these examples can be naturally highlighted with higher probabilities on the several peaks. Further, we introduce a weighting mechanism based on uncertainty such that we can quantify class confusion more accurately. Here, those examples with higher certainty in class predictions given by the classifier are more reliable and should contribute more to the pairwise class confusion. We use the entropy function H(p) −E p log p in information theory as an uncertainty measure of distribution p. The entropy (uncertainty) H( y i· ) of predicting the i-th example by the classifier is defined as</p><formula xml:id="formula_3">H( y i· ) = − |C| j=1 Y ij log Y ij .<label>(3)</label></formula><p>While the entropy is a measure of uncertainty, what we want is a probability distribution that places larger probabilities on examples with larger certainty of class predictions. A de facto transformation to probability is the softmax function</p><formula xml:id="formula_4">W ii = B (1 + exp(−H( y i· ))) B i =1 (1 + exp(−H( y i · ))) ,<label>(4)</label></formula><p>where W ii is the probability of quantifying the importance of the i-th example for modeling the class confusion, and W is the corresponding diagonal matrix. Note that we take the opposite value of the entropy to reflect the certainty. Laplace Smoothing <ref type="bibr" target="#b37">[38]</ref> (i.e. adding a constant 1 to each addend of the softmax function) is used to form a heavier-tailed weight distribution, which is suitable for highlighting more certain examples as well as avoiding overly penalizing the others. For better scaling, the probability over the examples in each batch of size B is rescaled to sum up to B such that the average weight for each example is 1.</p><p>With this weighting mechanism, the preliminary definition of class confusion is</p><formula xml:id="formula_5">C jj = y T ·j W y ·j .<label>(5)</label></formula><p>Category Normalization. The batch-based definition of the class confusion in Eq. <ref type="formula" target="#formula_5">(5)</ref> is native for the mini-batch SGD optimization. However, when the number of classes is large, it will run into a severe class imbalance in each batch. To tackle this problem, we adopt a category normalization technique widely used in Random Walk <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_6">C jj = C jj |C| j =1 C jj .<label>(6)</label></formula><p>Taking the idea of Random Walk, the normalized class confusion in Eq.(6) has a neat interpretation: It is probable to walk from one class to another (resulting in the wrong classification) if the two classes have a high class confusion. Minimum Class Confusion. Given the aforementioned derivations, we can formally define the loss function to enable Versatile Domain Adaptation (VDA). Recall that C jj well measures the confusion between each class pair j and j . We only need to minimize the cross-class confusion, i.e. j = j . Namely, the ideal situation is that no examples are ambiguously classified into two classes at the same time. In this sense, the Minimum Class Confusion (MCC) loss is defined as</p><formula xml:id="formula_7">L MCC ( Y t ) = 1 |C| |C| j=1 |C| j =j C jj .<label>(7)</label></formula><p>Since the class confusion in Eq. (6) has been normalized, minimizing the betweenclass confusion in Eq. <ref type="bibr" target="#b6">(7)</ref> implies that the within-class confusion is maximized. Note that Eq. <ref type="formula" target="#formula_7">(7)</ref> is a general loss that is pluggable to existing approaches. We want to emphasize that the inductive bias of class confusion in this work is more general than that of domain alignment in previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. As discussed in Section 2, many previous methods explicitly align features from the source and target domains, at the risk of deteriorating the feature discriminability and impeding the transferability <ref type="bibr" target="#b4">[5]</ref>. Further, the inductive bias of class confusion is general and applicable to a variety of domain adaptation scenarios, while that of domain alignment will suffer when the domains cannot be aligned naturally (e.g. the partial-set DA scenarios <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Versatile Approach to Domain Adaptation</head><p>The main motivation of this work is to design a versatile approach to a variety of DA scenarios. As the class confusion is a common inductive bias of many DA scenarios, combining the cross-entropy loss on the source labeled data and the MCC loss on the target unlabeled data will enable these DA scenarios.</p><p>Denote by y s = G(F (x s )) the class prediction for a source example x s , and by Y t = G(F (X t )) the class predictions for a batch of B target examples X t . The versatile approach (also termed by MCC for clarity) proposed in this paper for a variety of domain adaptation scenarios is formulated as</p><formula xml:id="formula_8">min F,G E (xs,ys)∈S L CE ( y s , y s ) + µ E Xt⊂T L MCC ( Y t ),<label>(8)</label></formula><p>where L CE is the cross-entropy loss and µ is a hyper-parameter for the MCC loss. With this joint loss, feature extractor F and classifier G of the deep DA model can be trained end-to-end by back-propagation. Note that, Eq. <ref type="formula" target="#formula_8">(8)</ref> is a versatile approach to many DA scenarios without any modifications to the loss.</p><p>-Unsupervised Domain Adaptation (UDA). Since Eq. <ref type="formula" target="#formula_8">(8)</ref> is formulated natively for this vanilla domain adaptation scenario, MCC can be directly applied to this scenario without any modification. -Partial Domain Adaptation (PDA). Without explicit domain alignment, we need not to worry about the misalignment between source outlier classes and target classes, which is the technical bottleneck of PDA <ref type="bibr" target="#b2">[3]</ref>. Meanwhile, compared to the confusion between the target classes, the confusion between the source outlier classes on the target domain is negligible in the MCC loss. Therefore, we can directly apply Eq. (8) to PDA. -Multi-Source Domain Adaptation (MSDA). Prior methods of MSDA consider multiple source domains as different domains, capturing the internal source domain shifts, and a simple merge of source domains proves fragile. However, since MCC is based on class confusion instead of domain alignment, we can safely merge S source domains as S ← S 1 ∪ · · · ∪ S S . -Multi-Target Domain Adaptation (MTDA). Though a simple merge of target domains is risky for existing methods, for MCC applied to MTDA, we can safely merge T target domains as T ← T 1 ∪ · · · ∪ T T . -Multi-Source/Multi-Target Partial Domain Adaptation (MSPDA / MTPDA). As MCC can directly tackle PDA and MSDA/MTDA, it can handle these derived scenarios by simply merging multiple sources or targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularizer to Existing DA Methods</head><p>Since the inductive bias of class confusion is orthogonal to the widely-used domain alignment, our method is well complementary to the previous methods. The MCC loss Eq. (7) can serve as a regularizer pluggable to existing methods.</p><p>We take as an example the standard domain alignment framework <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>  </p><formula xml:id="formula_9">) + µ E Xt⊂T L MCC ( Y t ) − λ E x∈S∪T L CE (D( f ), d),<label>(9)</label></formula><p>where the third term is the domain-adversarial loss for the domain discriminator D striving to distinguish the source from the target, and d is the domain label, f = F (x) is the feature representation learned to confuse the domain discriminator. The overall framework is a minimax game between two players F and D, in which λ and µ are trade-off hyper-parameters between different loss functions. Generally, the MCC loss is also readily pluggable to other representative domain adaptation frameworks, e.g. moment matching <ref type="bibr" target="#b20">[21]</ref> and large norm <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate MCC as a standalone approach with many methods for six domain adaptation scenarios (UDA, MSDA, MTDA, PDA, MSPDA and MTPDA). We also evaluate MCC as a regularizer to existing domain adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We use four standard datasets: Our methods are implemented based on PyTorch. ResNet <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> is used as the network backbone. We use Deep Embedded Validation (DEV) <ref type="bibr" target="#b49">[50]</ref> to select hyper-parameter T and provide parameter sensitivity analysis. A balance between the cross-entropy and MCC, i.e. µ = 1.0 works well for all experiments. We run each experiment for 5 times and report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Multi-Target Domain Adaptation (MTDA). We evaluate the MTDA tasks following the protocol of DADA <ref type="bibr" target="#b31">[32]</ref>, which provides six tasks on DomainNet, the most difficult dataset to date. We adopt the strategy that directly merges multiple target domains. As shown in <ref type="table" target="#tab_0">Table 1</ref>, many competitive methods are not effective in this challenging scenario. However, our simple method outperforms the current state-of-the-art method DADA <ref type="bibr" target="#b31">[32]</ref> by a big margin (7.3%). Note that the source-only accuracy is rather low on this dataset, validating that our method, with well-designed mechanisms, is sufficiently robust to wrong predictions.</p><p>Multi-Source Domain Adaptation (MSDA). When running our method for MSDA, we similarly merge multiple source domains in MCC and compare it to existing DA algorithms that are specifically designed for MSDA on DomainNet. As shown in <ref type="table" target="#tab_0">Table 1</ref>, based on the inductive bias of minimizing the class confusion, MCC significantly outperforms M 3 SDA <ref type="bibr" target="#b30">[31]</ref>, the state-of-the-art method by a big margin (5.0%). Note that these specific methods are of very complex architecture and loss designs and may be hard to use in practical applications. Partial Domain Adaptation (PDA). Due to the existence of source outlier classes, PDA is known as a challenging scenario because of the misalignment between the source and target classes. For a fair comparison, we follow the protocol of PADA <ref type="bibr" target="#b2">[3]</ref> and AFN <ref type="bibr" target="#b48">[49]</ref>, where the first 25 categories (in alphabetic order) of the Office-Home dataset are taken as the target domain. As shown in <ref type="table">Table 2</ref>, on this dataset, MCC outperforms AFN <ref type="bibr" target="#b48">[49]</ref>, the ICCV'19 honorable-mention entry and the state-of-the-art method for PDA, by a big margin (3.3%). <ref type="table">Table 2</ref>: Accuracy (%) on Office-Home for PDA (ResNet-50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (S:T) A:C A:P A:R C:A C:P C:R P:A P:C P:R R:A R:C R:P Avg</head><p>ResNet <ref type="bibr" target="#b12">[13]</ref> 38 Unsupervised Domain Adaptation (UDA). We evaluate MCC for the most common UDA scenario on several datasets. (1) VisDA-2017. As reported in <ref type="table" target="#tab_2">Table 3</ref>, MCC surpasses state-of-the-art UDA algorithms and yields the highest accuracy to date among methods of no complex architecture and loss designs. (2) Office-31. As shown in <ref type="table" target="#tab_3">Table 4</ref>, MCC performs the best. (3) Two Moon <ref type="bibr" target="#b19">[20]</ref>. We train a shallow MLP from scratch and plot the decision boundaries of MCC and Minimum Entropy (MinEnt) <ref type="bibr" target="#b9">[10]</ref>. MCC yields much better boundaries in <ref type="figure" target="#fig_5">Fig. 4</ref>.   Multi-Source/Multi-Target Partial Domain Adaptation (MSPDA / MTPDA). <ref type="table" target="#tab_4">Table 5</ref> shows that MCC is versatile to handle these hard scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Empirical Analyses</head><p>General Regularizer. MCC can be used as a general regularizer for existing DA methods. We compare its performance with entropy minimization (MinEnt) <ref type="bibr" target="#b9">[10]</ref> and Batch Spectral Penalization (BSP) <ref type="bibr" target="#b4">[5]</ref>. As shown in Tables 6 and 7, MCC yields larger improvements than MinEnt and BSP to a variety of DA methods.  Ablation Study. It is interesting to investigate the contribution of each part of the MCC loss: Class Correlation (CC), Probability Rescaling (PR), and Uncertainty Reweighting (UR). Results in <ref type="table" target="#tab_7">Table 8</ref> justify that each part has its indispensable contribution. To enable ease of use, we seamlessly integrate these parts into a coherent loss and reduce the number of hyper-parameters.</p><p>Further, we analyze how the specially designed Uncertainty Reweighting (UR) mechanism works. <ref type="figure" target="#fig_6">Fig. 5</ref> shows three typical examples as well as their weights and the confusion values before and after reweighting. The classifier prediction on the first image shows no obvious peak, while the one on the third image shows two obvious peaks on classes calculator and phone. The third image is more suitable for embodying class confusion. Naturally, its confusion value is higher than the first one, and our reweighting mechanism further highlights the suitable one. On the other hand, as the reweighting mechanism is defined with entropy, we recognize that it will improperly assign high weights to examples with highly confident predictions, including the wrong ones. As shown in the second image, its ground truth label is a lamp, but it is classified as a bike. In our method, the confusion value of such an example is so low that the influence of higher weight can be neglected. Therefore, our reweighting mechanism is effective and reliable.  <ref type="figure" target="#fig_7">Fig. 6</ref>, MCC has the lowest A-distance <ref type="bibr" target="#b0">[1]</ref>, which is close to the oracle one (i.e. supervised learning on both domains). In <ref type="figure" target="#fig_8">Fig. 7</ref>, the ideal value of MCC is also lower than that of mainstream DA methods. Both imply better generalization.</p><p>Parameter Sensitivity. Temperature factor T and MCC coefficient µ are the two hyper-parameters of MCC and MinEnt <ref type="bibr" target="#b9">[10]</ref> when applying them standalone or with existing methods. We traverse hyper-parameters around their optimal values [T * , µ * ], as shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, MCC is much less sensitive to its hyper-parameters.</p><p>(a) A-Distance <ref type="bibr" target="#b0">[1]</ref> (b) MinEnt <ref type="bibr" target="#b9">[10]</ref> (c) MCC Convergence Speed. We show the training curves throughout iterations in <ref type="figure" target="#fig_8">Fig. 7</ref>. Impressively, MCC takes only 1000 iterations to reach an accuracy of 95%, while at this point the accuracies of CDAN and DANN are below 85%. When used as a regularizer for existing domain adaptation methods, MCC largely accelerates convergence. In general, MCC is 3× faster than the existing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper studies a more practical paradigm, Versatile Domain Adaptation (VDA), where one method tackles many scenarios. We uncover that less class confusion implies more transferability, which is the key insight to enable VDA. Based on this, we propose a new loss function: Minimum Class Confusion (MCC). MCC can be applied as a versatile domain adaptation approach to a variety of DA scenarios. Extensive results justify that our method, without any modification, outperforms state-of-the-art scenario-specific domain adaptation methods with much faster convergence. Further, MCC can also be used as a general regularizer for existing DA methods, further improving accuracy and accelerating training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(MCC), a general loss function for Versatile Domain Adaptation (VDA) Versatile Domain Adaptation (VDA) subsumes typical domain adaptation scenarios: (1) Unsupervised Domain Adaptation (UDA); (2) Partial Domain Adaptation (PDA); (3) Multi-Source Domain Adaptation (MSDA); (4) Multi-Target Domain Adaptation (MTDA); (5) Multi-Source Partial Domain Adaptation (MSPDA); (6) Multi-Target Partial Domain Adaptation (MTPDA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The error matrices of several models on VisDA-2017<ref type="bibr" target="#b32">[33]</ref>.(a)-(b): Sourceonly model tested on the source and target domains, showing severe class confusion on target domain examples. (c)-(d): Models trained with entropy minimization (MinEnt) [10] and Minimum Class Confusion (MCC) on target domain examples, respectively. The proposed MCC loss substantially diminishes the class confusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The schematic of the Minimum Class Confusion (MCC) loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>based on domain-adversarial training. Integrating the MCC loss as a regularizer yields min F,G max D E (xs,ys)∈S L CE ( y s , y s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>Office-31 [35]: a classic domain adaptation dataset with 31 categories and 3 domains: Amazon (A), Webcam (W) and DSLR (D); (2) Office-Home [45]: a more difficult dataset (larger domain shift) with 65 categories and 4 domains: Art (A), Clip Art (C), Product (P) and Real World (R); (3) VisDA-2017 [33]: a dataset with 12 categories and over 280,000 images; (4) DomainNet [31]: the largest and hardest domain adaptation dataset, with approximately 0.6 million images from 345 categories and 6 domains: Clipart (c), Infograph (i), Painting (p), Quickdraw (q), Real (r) and Sketch (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Decision boundaries on the Two Moon dataset. Blue points indicate target data, and different classes of the source data are depicted in purple and yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Three typical samples and the corresponding weights and confusion values. Theoretical Insight. Ben-David et al. [1] derived the expected error E T (h) of a hypothesis h on the target domain E T (h) ≤ E S (h) + 1 2 d H∆H (S, T ) + ideal by: (a) expected error of h on the source domain, E S (h); (b) the A-distance d H∆H (S, T ), a measure of domain discrepancy; and (c) the error ideal of the ideal joint hypothesis h * on both source and target domains. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>(a): A-Distance of the last fc-layer features of task A → W on Office-31 (UDA); (b)-(c): Hyper-parameter sensitivity of task A → W on Office-31 (UDA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>The ideal error values (%) and training curves throughout iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on DomainNet for MTDA and MSDA (ResNet-101).</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(a) MTDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) MSDA</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>c:</cell><cell>i:</cell><cell>p:</cell><cell>q:</cell><cell>r:</cell><cell>s: Avg</cell><cell>Method</cell><cell>:c</cell><cell>:i</cell><cell>:p</cell><cell>:q</cell><cell>:r</cell><cell>:s Avg</cell></row><row><cell cols="7">ResNet [13] 25.6 16.8 25.8 9.2 20.6 22.3 20.1</cell><cell cols="7">ResNet [13] 47.6 13.0 38.1 13.3 51.9 33.7 32.9</cell></row><row><cell>SE [7]</cell><cell cols="6">21.3 8.5 14.5 13.8 16.0 19.7 15.6</cell><cell>MCD [36]</cell><cell cols="6">54.3 22.1 45.7 7.6 58.4 43.5 38.5</cell></row><row><cell cols="7">MCD [36] 25.1 19.1 27.0 10.4 20.2 22.5 20.7</cell><cell cols="7">DCTN [48] 48.6 23.5 48.8 7.2 53.5 47.3 38.2</cell></row><row><cell cols="7">DADA [32] 26.1 20.0 26.5 12.9 20.7 22.8 21.5</cell><cell cols="7">M 3 SDA [31] 58.6 26.0 52.3 6.3 62.7 49.5 42.6</cell></row><row><cell>MCC</cell><cell cols="6">33.6 30.0 32.4 13.5 28.0 35.3 28.8</cell><cell>MCC</cell><cell cols="6">65.5 26.0 56.6 16.5 68.0 52.7 47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on VisDA-2017 for UDA (ResNet-101).</figDesc><table><row><cell>Method</cell><cell cols="2">plane bcybl bus car horse knife mcyle persn plant sktb train truck mean</cell></row><row><cell cols="2">ResNet [13] 55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5</cell><cell>52.4</cell></row><row><cell cols="3">MinEnt [10] 80.3 75.5 75.8 48.3 77.9 27.3 69.7 40.2 46.5 46.6 79.3 16.0 57.0</cell></row><row><cell>DANN [8]</cell><cell>81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8</cell><cell>57.4</cell></row><row><cell>DAN [21]</cell><cell cols="2">87.1 63.0 76.5 42.0 90.3 42.9 85.9 53.1 49.7 36.3 85.8 20.7 61.1</cell></row><row><cell>MCD [36]</cell><cell cols="2">87.0 60.9 83.7 64.0 88.9 79.6 84.7 76.9 88.6 40.3 83.0 25.8 71.9</cell></row><row><cell cols="3">CDAN [22] 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9</cell></row><row><cell>AFN [49]</cell><cell cols="2">93.6 61.3 84.1 70.6 94.1 79.0 91.8 79.6 89.9 55.6 89.0 24.4 76.1</cell></row><row><cell>MCC</cell><cell cols="2">88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) on Office-31 for UDA (ResNet-50).</figDesc><table><row><cell>Method</cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell>Avg</cell></row><row><cell cols="8">ResNet [13] 68.4±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3 76.1</cell></row><row><cell>DAN [21]</cell><cell cols="7">80.5±0.4 97.1±0.2 99.6±0.1 78.6±0.2 63.6±0.3 62.8±0.2 80.4</cell></row><row><cell>RTN [23]</cell><cell cols="7">84.5±0.2 96.8±0.1 99.4±0.1 77.5±0.3 66.2±0.2 64.8±0.3 81.6</cell></row><row><cell>DANN [8]</cell><cell cols="7">82.0±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5 82.2</cell></row><row><cell>JAN [24]</cell><cell cols="7">85.4±0.3 97.4±0.2 99.8±0.2 84.7±0.3 68.6±0.3 70.0±0.4 84.3</cell></row><row><cell>GTA [37]</cell><cell cols="7">89.5±0.5 97.9±0.3 99.8±0.4 87.7±0.5 72.8±0.3 71.4±0.4 86.5</cell></row><row><cell cols="8">CDAN [22] 94.1±0.1 98.6±0.1 100.0±0.0 92.9±0.2 71.0±0.3 69.3±0.3 87.7</cell></row><row><cell>AFN [49]</cell><cell cols="7">88.8±0.5 98.4±0.3 99.8±0.1 87.7±0.6 69.8±0.4 69.7±0.4 85.7</cell></row><row><cell>MDD [53]</cell><cell cols="7">94.5±0.3 98.4±0.3 100.0±0.0 93.5±0.2 74.6±0.3 72.2±0.1 88.9</cell></row><row><cell>MCC</cell><cell cols="7">95.5±0.2 98.6±0.1 100.0±0.0 94.4±0.3 72.9±0.2 74.9±0.3 89.4</cell></row><row><cell>(a) MinEnt [10]</cell><cell></cell><cell>(b) MCC</cell><cell></cell><cell cols="2">(c) DANN+MinEnt</cell><cell cols="2">(d) DANN+MCC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) on Office-Home for MSPDA and MTPDA.</figDesc><table><row><cell></cell><cell cols="3">(a) MSPDA</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) MTPDA</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>:A</cell><cell>:C</cell><cell>:P</cell><cell>:R</cell><cell>Avg</cell><cell>Method</cell><cell>A:</cell><cell>C:</cell><cell>P:</cell><cell>R:</cell><cell>Avg</cell></row><row><cell>DANN [8]</cell><cell cols="5">58.3 43.6 60.7 71.2 58.5</cell><cell>DANN [8]</cell><cell cols="5">44.6 44.8 39.1 44.1 43.1</cell></row><row><cell>PADA [3]</cell><cell cols="5">62.8 51.8 71.7 79.2 66.4</cell><cell>PADA [3]</cell><cell cols="5">59.9 53.7 51.1 61.4 56.5</cell></row><row><cell cols="6">M 3 SDA [31] 67.4 55.3 72.2 80.4 68.8</cell><cell cols="6">DADA [32] 65.1 63.0 60.4 63.0 62.9</cell></row><row><cell>AFN [49]</cell><cell cols="5">77.1 61.2 79.3 82.5 75.0</cell><cell>AFN [49]</cell><cell cols="5">68.7 65.6 63.4 67.5 66.3</cell></row><row><cell>MCC</cell><cell cols="5">79.6 67.5 80.6 85.1 78.2</cell><cell>MCC</cell><cell cols="5">73.1 72.1 69.4 68.3 70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Accuracy (%) on VisDA-2017 as regularizer for UDA (ResNet-101).</figDesc><table><row><cell>Method</cell><cell>plane bcybl bus car horse knife mcyle persn plant sktb train truck mean</cell></row><row><cell>DANN [8]</cell><cell>81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8 57.4</cell></row><row><cell cols="2">DANN + MinEnt [10] 87.4 55.0 75.3 63.8 87.4 43.6 89.3 72.5 82.9 78.6 85.6 27.4 70.7</cell></row><row><cell>DANN + BSP [5]</cell><cell>92.2 72.5 83.8 47.5 87.0 54.0 86.8 72.4 80.6 66.9 84.5 37.1 72.1</cell></row><row><cell>DANN + MCC</cell><cell>90.4 79.8 72.3 55.1 90.5 86.8 86.6 80.0 94.2 76.9 90.0 49.6 79.4</cell></row><row><cell>CDAN [22]</cell><cell>85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38 73.9</cell></row><row><cell cols="2">CDAN + MinEnt [10] 90.5 65.8 79.1 62.2 89.8 28.7 92.8 75.4 86.8 65.3 85.2 35.3 71.4</cell></row><row><cell>CDAN + BSP [5]</cell><cell>92.4 61.0 81.0 57.5 89.0 80.6 90.1 77.0 84.2 77.9 82.1 38.4 75.9</cell></row><row><cell>CDAN + MCC</cell><cell>94.5 80.8 78.4 65.3 90.6 79.4 87.5 82.2 94.7 81.0 86.0 44.6 80.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Accuracy (%) on Office-31 as regularizer for UDA (ResNet-50). 0±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5 82.2 DANN + MinEnt [10] 91.7±0.3 98.3±0.1 100.0±0.0 87.9±0.3 68.8±0.3 68.1±0.3 85.8 DANN + BSP [5] 93.0±0.2 98.0±0.2 100.0±0.0 90.0±0.4 71.9±0.3 73.0±0.3 87.7 DANN + MCC 95.6±0.3 98.6±0.1 99.3±0.0 93.8±0.4 74.0±0.3 75.0±0.4 89.4 CDAN [22] 94.1±0.1 98.6±0.1 100.0±0.0 92.9±0.2 71.0±0.3 69.3±0.3 87.7 CDAN + MinEnt [10] 91.7±0.2 98.5±0.1 100.0±0.0 90.4±0.3 72.3±0.2 69.5±0.2 87.1 CDAN + BSP [5] 93.3±0.2 98.2±0.2 100.0±0.0 93.0±0.2 73.6±0.3 72.6±0.3 88.5 CDAN + MCC 94.7±0.2 98.6±0.1 100.0±0.0 95.0±0.1 73.0±0.2 73.6±0.3 89.2 AFN [49] 88.8±0.5 98.4±0.3 99.8±0.1 87.7±0.6 69.8±0.4 69.7±0.4 85.7 AFN + MinEnt [10] 90.3±0.4 98.7±0.2 100.0±0.0 92.1±0.5 73.4±0.3 71.2±0.3 87.6 AFN + BSP [5] 89.7±0.4 98.0±0.2 99.8±0.1 91.0±0.4 71.4±0.3 71.4±0.2 86.9 AFN + MCC 95.4±0.3 98.6±0.2 100.0±0.0 96.0±0.2 74.6±0.3 75.2±0.2 90.0</figDesc><table><row><cell>Method</cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell>Avg</cell></row><row><cell>DANN [8]</cell><cell>82.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of MCC on Office-31 for UDA (ResNet-50).</figDesc><table><row><cell>Method</cell><cell cols="7">A→W D→W W→D A→D D→A W→A Avg</cell></row><row><cell>MCC (CC Only)</cell><cell>92.2</cell><cell>96.9</cell><cell>100.0</cell><cell>88.6</cell><cell>73.2</cell><cell>64.5</cell><cell>85.9</cell></row><row><cell>MCC (CC + PR)</cell><cell>93.1</cell><cell>98.5</cell><cell>100.0</cell><cell>91.6</cell><cell>70.9</cell><cell>69.0</cell><cell>87.2</cell></row><row><cell>MCC (CC + PR + UR)</cell><cell>93.7</cell><cell>98.6</cell><cell>100.0</cell><cell>93.2</cell><cell>72.1</cell><cell>73.7</cell><cell>88.4</cell></row><row><cell>MCC (All)</cell><cell>95.5</cell><cell>98.6</cell><cell>100.0</cell><cell>94.4</cell><cell>72.9</cell><cell>74.9</cell><cell>89.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the Natural Science Foundation of China (61772299, 71690231), and China University S&amp;T Innovation Plan by Ministry of Education.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulbricht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Drop to adapt: Learning discriminative features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I J</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">Visda: The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international communication of association for computing machinery conference</title>
		<meeting>the international communication of association for computing machinery conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A DIRT-T Approach to Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep Hashing Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1953" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Larger norm more transferable: An adaptive feature norm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Towards accurate model selection in deep unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Domain-symmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
