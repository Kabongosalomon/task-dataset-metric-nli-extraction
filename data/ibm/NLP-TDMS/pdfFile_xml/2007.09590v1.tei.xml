<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiting</forename><surname>Huang</surname></persName>
							<email>huangweiting@ebupt.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">EBUPT Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Ren</surname></persName>
							<email>renpengfei@ebupt.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">EBUPT Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Wang</surname></persName>
							<email>wangjingyu@ebupt.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">EBUPT Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qi</surname></persName>
							<email>qiqi@ebupt.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">EBUPT Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Sun</surname></persName>
							<email>sunhaifeng@ebupt.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">EBUPT Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an adaptive weighting regression (AWR) method to leverage the advantages of both detectionbased and regression-based method. Hand joint coordinates are estimated as discrete integration of all pixels in dense representation, guided by adaptive weight maps. This learnable aggregation process introduces both dense and joint supervision that allows end-to-end training and brings adaptability to weight maps, making network more accurate and robust. Comprehensive exploration experiments are conducted to validate the effectiveness and generality of AWR under various experimental settings, especially its usefulness for different types of dense representation and input modality. Our method outperforms other state-of-the-art methods on four publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate and robust 3D hand pose estimation is crucial in applications of human-computer interaction, such as virtual reality and augmented reality. The accuracy and speed have improved significantly in recent years thanks to the emergence of deep cameras and deep learning. However, it is still challenging to achieve accurate and robust results due to the low quality of depth images, extreme viewpoints, severe self-occlusion and self-similarity among fingers.</p><p>Recently, deep neural network based methods have shown great performance in 3D hand pose estimation <ref type="bibr" target="#b16">(Tompson et al. 2014;</ref><ref type="bibr" target="#b2">Ge, Ren, and Yuan 2018;</ref><ref type="bibr" target="#b16">Wang et al. 2018;</ref><ref type="bibr" target="#b0">Chen et al. 2018a;</ref><ref type="bibr" target="#b8">Moon, Chang, and Lee 2018b;</ref><ref type="bibr" target="#b16">Wan et al. 2018;</ref><ref type="bibr" target="#b18">Xiong et al. 2019;</ref>. These works can be divided into two categories according to networks' output type regressionbased methods and detection-based methods. Regressionbased methods directly map input depth images to 3D hand pose parameters or hand joint coordinates. They allow different regions on feature maps to be weighted in the fully connected layers to regress joint coordinates. Thus it captures global constraints among hand joints and performs better in extreme viewpoints where severe occlusion occurs ). However, fully connected layers flatten the feature map and destroy the spatial structure in it, making it hard to learn the highly non-linear mapping from image space to joint space. Detection-based methods instead generate dense estimations such as heatmaps <ref type="bibr" target="#b6">(Li and Lee 2019;</ref><ref type="bibr" target="#b17">Xiao, Wu, and Wei 2018)</ref> or offset vector fields <ref type="bibr" target="#b16">(Wan et al. 2018;</ref><ref type="bibr" target="#b4">Ren et al. 2019)</ref>, then the joint coordinates are inferred through a non-learnable information aggregation process such as the taking argmax operation or mean-shift estimation, guided by weight maps. The fully convolutional network architecture keeps the spatial structure of extracted feature maps and better exploits local evidence in depth images. However, the information aggregation is treated as post processing and separated from network training, posing gaps between training and inferencing. Besides, detection-based methods force networks to learn a fixed range of weight distribution and strictly restrict the post processing information aggregation process. Specifically, ground truth heatmaps are generated by a fixed size Gaussian blobs for heatmaps based methods. And offset vector fields based methods must strictly limit the number <ref type="bibr" target="#b16">(Wan et al. 2018)</ref> or the distribution range of candidate points <ref type="bibr" target="#b2">(Ge, Ren, and Yuan 2018)</ref> in order to prevent points which are irrelevant to target joint from affecting estimation accuracy. The above procedure reduces the generalization ability of networks and makes estimations unstable when depth values near the target joint are heavily missing.</p><p>In this paper, we propose a simple yet effective adaptive weighting regression (AWR) method to unify the two complementary lines of work in a single pass. Guided by adaptive weight maps, AWR aggregates different regions of dense representation through discrete integration of all pixels in it. This operation is differentiable so that it can be embedded into the network for end-to-end training and applies direct supervision on joint coordinates, drawing consensus in network's supervision and output. The inferred joint coordinates are continuous and up to arbitrary accuracy. Besides, the weight distribution in weight maps can be adjusted adaptively to achieve more accurate and robust performance under the guidance of joint supervision. As shown in <ref type="figure" target="#fig_0">Fig 1,</ref> when target joint is visible and easy to distinguish, the weight distribution of AWR tends to focus more on pixels around it as standard detection-based methods do, which helps to make full use of local evidence. When depth values around the target joint are heavily missing due to occlusion or under the situation of severe self-similarity among fingers, the weight distribution spreads out to capture information of adjacent joints. This mechanism makes the network more robust to situations where depth values around hand joints are heavily missing. Therefore, AWR greatly improves network's accuracy and robustness through learnable aggregation and shares the merits of both regression-based and detection-based methods.</p><p>The idea of aggregating different regions in dense representations to derive joint coordinates has previously been seen in human <ref type="bibr" target="#b13">(Sun et al. 2018)</ref> or hand pose estimation <ref type="bibr" target="#b18">(Xiong et al. 2019;</ref>. However, <ref type="bibr" target="#b13">(Sun et al. 2018)</ref> for human pose estimation focus on RGB images and this type of representation ignores the 3D geometric properties of essentially 2.5D depth images. And <ref type="bibr" target="#b18">(Xiong et al. 2019;</ref> for hand pose estimation are specialized for certain type of network structure, dense representation and input modality. The effectiveness and generality of this operation are not extensively validated. We present a simple yet effective network to empirically show the improvement in accuracy and robustness brought by adaptive weighting regression. This helps to inspire and pave the way for future works in 3D hand pose estimation.</p><p>We conduct comprehensive explorations to illustrate the effectiveness and generality of our proposed AWR under various experimental settings, including different types of dense representation, input modality, network architecture as well as input and dense representation sizes. Experiments show that AWR brings improvements in both accuracy and robustness. The pipeline of our method and arrangements for Our main contribution is presenting an adaptive weighting regression (AWR) method to aggregate dense representation through discrete integration. AWR unifies the dense representation and hand joint regression to enable direct supervision on joint coordinates, narrowing the gap between training and inferencing. Comprehensive explorations have been done to validate the improvement in network's accuracy and robustness brought by AWR as well as its generality to work under various experimental settings. The overall network is simple yet effective and achieves state-of-theart performance on four publicly available datasets. Code is available at https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We formulate hand pose estimation as a dense representation embedded regression problem to leverage the advantages of both regression-based and detection-based methods. We utilize fully convolutional 2D CNN to generate dense representation, enabling network to maintain the spatial structure of extracted feature maps and better exploit local pattern in depth images. Then a learnable and adaptive weighting operation is applied to aggregate spatial information of different regions in dense representation. It has two advantages, including 1) adding direct supervision on joint coordinates to draw consensus between training and inferencing; 2) enhancing network's robustness and generalization ability by adaptively aggregating spatial information from related regions in dense representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adaptive Weighting Regression</head><p>Adaptive weighting regression (AWR) aggregates different regions in dense representation by taking discrete integration of all pixels on it to derive joint coordinates, guided by adaptive weight maps. Weight maps are crucial for AWR since they help network focus more on regions related to the target joint. To achieve accurate and robust estimation, they need to have the following two properties. Firstly, during aggregation, since points around the target joint give more accurate predictions of joint coordinates (Li and Lee 2019), weights of these points need to be higher. The distance information contained in heatmaps or offsets can be considered as a natural depiction of this property. Secondly, when depth values around hand joints are heavily missing, weight distribution should spread out to more related regions to consider correlations among hand joints and enhance the network's robustness. Most detection-based methods adopt information aggregation during post processing. However, the post processing is not learnable and the generated weight maps cannot adapt to different situations, impairing the network's generalization ability to occlusions or severe self-similarity among fingers.</p><p>We present an adaptive weighting regression operation to transform dense spatial representation into joint coordinates. It is a discrete integral operation and can be described as follows:</p><formula xml:id="formula_0">pj = n i=1 wj(pij) × j(pij)<label>(1)</label></formula><p>where p ij denotes one of n pixels in dense representation of joint j; j(p ij ) denotes hand joint coordinates recovered from point p i for joint j; w j (p ij ) denotes the value of point p ij in weight maps, normalized using softmax function. The operation is differentiable and computationally simple but unifies the dense representation and hand joint coordinates. This operation is learnable and allows end-to-end training, which narrows the gap between training and inferencing. Benefiting from both dense representation and joint coordinates supervision, our method is able to adaptively aggregate spatial information in dense representation. Besides, it is simple, fast and adding negligible overhead in computation and storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comprehensive Explorations</head><p>The main purpose of our work is to explore the effectiveness and generality of adaptive weighting regression (AWR) method. Therefore, we conduct extensive explorations under various experimental settings, including dense representation types, input modalities, network architectures as well as input and dense representation sizes.</p><p>Dense representation types. We research on currently most frequently used dense representation types in pose estimation and select six of them to observe the performance of AWR. Firstly, following (Li and Lee 2019), we propose a simple dense representation by stacking x, y and z coordinate of the target joint. Specifically, each joint corresponds to a pose map with three channels where each channel contains the same value of x, y or z coordinate of the joint. Since pose heatmaps lack the measurement of distance from pixels to target joint, weight maps are learned without supervision to guide the aggregation of local evidence. This type of representation is referred to as "P".</p><p>Heatmaps are the most commonly used representation in human and pose estimation, especially probability heatmaps where each pixel represents the probability of it being a specific joint. Since probability heatmaps only represent 2D properties of joint coordinates and 3D heatmaps are computationally expensive, we introduce depth maps or depth offset maps to encode depth information. Specifically, the depth maps and depth offset maps have the same resolution as probability heatmaps and denotes the depth value of target joint or depth offset from current pixel to target joint respectively. We refer to probability heatmaps combined with depth maps as "H1" and the other as "H2".</p><p>Recent work <ref type="bibr" target="#b18">(Xiong et al. 2019</ref>) uses 2D offsets between anchor points and hand joints to represent 2D positions of joints. Due to the large variance of offsets, we further decompose them into 2D directional unit vector fields and closeness heatmaps, reflecting 2D directions and closeness from each pixel in depth images to target joints. Similarly, 2D offsets lack the prediction in depth. Therefore, we simultaneously predict depth value for each pixel or depth offset from current pixel to target joint and they are referred to as "O1" and "O2".</p><p>The representation of offsets is continuous and up to arbitrary localization accuracy. But separating the prediction of plane and depth coordinate and using only 2D distance to generate closeness heatmaps ignores the 3D property of depth images to some extent. 3D offsets, as used in <ref type="bibr" target="#b16">Wan et al. 2018)</ref>, instead predict 3D directional unit vector fields and closeness heatmaps, reflecting 3D directions and per-pixel closeness to target hand joint. They unify the prediction of three coordinates together and fully exploit the 3D spatial information present in depth images. We refer to 3D offsets as "O3" in the experiment section.</p><p>The</p><formula xml:id="formula_1">formulation of offset representation φ(p i , p j ) is shown in Eq 2. φ(pi, pj) = 1 Hand (pi) × (pi − pj), ||pi − pj|| ≤ k 0, otherwise<label>(2)</label></formula><p>where p i and p j denote depth, 2D or 3D coordinates of a pixel in depth images and target hand joint respectively; k denotes the maximum distance from pixels in depth images to target hand joint and 1 Hand (p i ) is an indicator function. If p i belongs to hand, 1 Hand (p i ) equals to 1 otherwise 0. For 2D and 3D offsets, they are further decomposed into 2D or 3D directional unit vectors V (p i , p j ) and closeness heatmaps S(p i , p j ).</p><formula xml:id="formula_2">S(pi, pj) = 1 Hand (pi) × k−||p i −p j || k , ||pi − pj|| ≤ k 0, otherwise V (pi, pj) = 1 Hand (pi) × p i −p j ||p i −p j || , ||pi − pj|| ≤ k 0, otherwise<label>(3)</label></formula><p>Input modalities. To fully exploit 3D geometric information in depth images, recent researchers tend to transform depth images into 3D voxels <ref type="bibr" target="#b8">(Moon, Chang, and Lee 2018b)</ref> or point clouds <ref type="bibr" target="#b7">(Moon, Chang, and Lee 2018a;</ref><ref type="bibr" target="#b2">Ge, Ren, and Yuan 2018)</ref> and apply a 3D CNN or PointNet <ref type="bibr" target="#b10">(Qi et al. 2017)</ref>. We adopt three different input modalities (depth images, voxels and point clouds) in our experiments to testify the generality of AWR. For voxels, we follow the pipeline in <ref type="bibr" target="#b8">(Moon, Chang, and Lee 2018b)</ref> which transform the depth images into occupancy voxels and apply 3D hourglass network structure, except that our network outputs 3D offsets to hand joints, which are further decomposed into 3D directional unit vectors and 3D closeness heatmaps. We select voxels whose distance to the target joint is within 15 voxels as candidate voxels to recover joint coordinates from. As for point clouds, we adopt a similar network structure as in <ref type="bibr" target="#b2">(Ge, Ren, and Yuan 2018)</ref> and predict 3D offsets, except that to have a fair comparison with other input modalities, we do not pre-process the point clouds, neither transform the point clouds into oriented bounding box (OBB) coordinate or refine fingertips.</p><p>Network architectures. We use simple 2D CNN network which contains only a backbone network, such as ResNet <ref type="bibr" target="#b3">(He et al. 2016)</ref> or Hourglass <ref type="bibr" target="#b9">(Newell, Yang, and Deng 2016)</ref> to extract feature maps. Then several deconvolutional layers are applied to lift the resolution of extracted feature maps, followed by a few convolutional heads to generate dense representation. Since different types of dense representation may consist of independent components with different physical characteristics, we use separated convolutional heads to generate them. Specifically, for O3, two heads are used to output directional unit vector fields and closeness heatmaps respectively. While for O1 and O2, we add another head to generate depth maps or depth offset maps. For H1 and H2, two convolution heads are applied to output probability heatmaps and depth maps or depth offset maps respectively. For P, the two heads output pose and weight maps respectively and weight maps are adaptively learned by the network without supervision.</p><p>We research on several backbone networks to show the effectiveness and generality of AWR. Specifically, network designs of ResNet <ref type="bibr" target="#b3">(He et al. 2016</ref>) and multi-stage Hourglass <ref type="bibr" target="#b9">(Newell, Yang, and Deng 2016)</ref> and network depths of ResNet18, 50, 101 are investigated. Results show that AWR corporates well with various network structures.</p><p>Input and dense representation sizes. The performance of standard detection-based methods is affected by the resolution of inputs and dense representations. To attain higher accuracy, larger input and dense representation sizes are required. We try out different sizes for both input depth images and dense representation to show the robustness of our method to resolution change. We show that even with small input and dense representation resolution, our method still achieves considerable good performance, which makes our method favorable in real-world applications when computational cost is restricted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>There are two supervisions in our method: dense representation supervision and joint coordinates supervision. We first pre-train the network using only dense representation supervision, then finetune the network with joint supervision. We find that this pipeline works better than training with both supervisions simultaneously. We argue the reason is that the former provides weight maps with more adaptability. Following previous works , we adopt smooth L1 loss for joint supervision since it is less sensitive to out-liers than L2 loss. However, dense representation loss varies for different dense representation types. We try out smooth L1 loss as well as L2 loss to find better fitting loss type and it turns out that for offsets, smooth L1 loss performs better, while for heatmaps, L2 loss is more stable.</p><p>Our method is implemented with PyTorch using Adam (Kingma and Ba 2015) optimizer with initial learning rate of 0.001 and weight decay of 0.0005. The batch size is set to 32. We train the network with initial learning rate and then drop as the performance reaches a plateau. Same as , we first train a small separated 2D CNN to attain hand center and extract hand regions from depth images, then adopt data augmentation strategies in world coordinate, to prevent the network from overfitting, including random rotation <ref type="bibr">([-180, 180]</ref>), random translation ([-10, 10]) and random scaling ([0.9, 1.1]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>We conduct experiments on four publicly available hand pose datasets: NYU dataset <ref type="bibr" target="#b16">(Tompson et al. 2014)</ref>, ICVL dataset <ref type="bibr" target="#b14">(Tang et al. 2014a</ref>), MSRA dataset <ref type="bibr" target="#b12">(Sun et al. 2015)</ref> and HANDS 2017 dataset . NYU dataset contains 72K and 8K frames for training and evaluation respectively. Each frame contains 3D ground truth coordinates for 36 joints. Following previous works in <ref type="bibr" target="#b2">(Ge, Ren, and Yuan 2018)</ref>, we apply our method on a subset of 14 hand joints. ICVL dataset consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame. MSRA dataset contains 76.6K frames and there are 21 hand joint annotations in each frame. We adopt the leave-one-subject-out cross-validation strategy for MSRA dataset. HANDS 2017 dataset is currently the largest dataset with 957K training and 295K testing frames. The dataset contains 21 hand joint annotations for each frame.</p><p>Since NYU dataset has a relatively wider coverage of hand poses and its test set labels are publicly available, we conduct exploration experiments on NYU dataset to investigate the effectiveness and generality of our proposed AWR under various experimental settings. The other three datasets are mainly used for comparison with previous state-of-theart methods.</p><p>We evaluate our method using two commonly used metric. The first metric is per-joint and all-joint mean error over all test frames. The joint error is computed as Euclidean distance between predicted and ground truth hand joint. The second metric is the proportion of good frames over all test frames. A frame is considered good only when each joint's error is within a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exploration Studies</head><p>Dense representation types. As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, no matter what the dense representation is, aggregating operation based methods consistently outperform detection-based method, indicating that the operation can improve estimation accuracy through ensemble learning and it works effectively with all the representation types presented.  the proportion of good frames of AWR based methods outperforms detection-based methods at all thresholds, indicating that AWR significantly improves the network's robustness. Besides, we have several other observations. Firstly, aggregating operation based methods with only joint supervision outperforms directly regressing joint coordinates (R) thanks to the intermediate representation and adaptive weight maps. Secondly, since depth offsets can encode 3D geometric information of a 2.5D hand surface, depth offsets based methods consistently perform better than methods that directly predict absolute depth values (H2 &gt; H1, O2 &gt; O1). Finally, among these representations, 3D offsets (O3) performs the best since they encode rich spatial structure information and adopt more reasonable 3D distance measurement. As shown in <ref type="table" target="#tab_0">Table 1</ref>, directly performing dense joint regression is worse than directly regressing joint coordinates (P Dense &lt; R). However, with the help of adaptive weighting, the performance is greatly improved (P Joint &gt; R, P Both &gt; R). We can come to two conclusions: 1) points at different positions possess different estimation accuracy, simply averaging the estimation of all points impairs the accuracy of final estimation (P Dense &lt; R); 2) adaptively aggregating the results of different regions efficiently improves the estimation accuracy (P Joint &gt; R), as both methods share almost the same network architecture except that P Joint generates extra intermediate dense representation and use AWR to ensemble it.</p><p>Input and dense representation sizes. Results in <ref type="table" target="#tab_1">Table  2</ref> indicate that larger input and dense size obtain better accuracy. Method (f,d) and method (g,e) show that when input and dense size increase and decrease the same ratio respectively, the performance of AWR based method (Both) dropped while detection-based method (Dense) improved, reflecting that AWR based method is more sensitive to dense size than input size. We argue that since the accuracy of AWR based methods is related to the number of candidate points during aggregation, as dense size decreases, the number of candidate points drops drastically and impairs the estimation results. But generally, AWR based methods outperform detection-based methods with smaller input and output sizes, making our method favorable in real-world applica- <ref type="figure">Figure 3</ref>: The proportion of good frames of directly regressing method (R), probability heatmaps with depth offset maps based method (H2) and 3D offsets (O3) based method using only dense representation supervision (Dense) or both dense representation and joint supervision (Both). Figure is best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tions.</head><p>Network architectures. In this experiment, we first try out different network designs (ResNet <ref type="bibr" target="#b3">(He et al. 2016)</ref> and Hourglass <ref type="bibr" target="#b9">(Newell, Yang, and Deng 2016)</ref>). As shown in <ref type="table" target="#tab_1">Table 2</ref>, AWR outperforms method with only dense representation supervision and works well with single-stage 2D CNNs as well as cascaded network architecture (Method b, i, j), indicating that it can be easily embedded into different network architectures to boost the performance. For cascaded network, as stage number increases, performance gets better but at the cost of computational complexity.</p><p>Then the depth of ResNet is experimented. Method a, b, c in <ref type="table" target="#tab_1">Table 2</ref> show that with the increase of network depths, mean joint error drops steadily for the two lines of work. Specifically, a Both already achieves better results compared with b Dense and c Dense with far less of the computational and storage cost, indicating that simple network can obtain powerful performance through ensemble learning which further shows the superiority of our method.</p><p>Input modalities. Three commonly used input modalities are experimented, including depth images, voxels and point clouds. As shown in <ref type="table" target="#tab_2">Table 3</ref>, consistent improvements are brought by AWR under all three input modalities, demonstrating the effectiveness and generality of the aggregating operation under different coordinates and spaces.</p><p>We also experiment on the kernel size of 3D offsets, which is k in Equation 3. As shown in <ref type="table" target="#tab_3">Table 4</ref>, after applying AWR, the network is more robust to kernel size change than detection-based methods since introducing joint supervision brings weight maps certain degrees of adaptability. As kernel size increases, the accuracy of AWR method improved slightly. However, since larger kernel size requires more epochs and time to converge, to balance between training time and accuracy, we set kernel size as 1 in most experiments.    <ref type="table" target="#tab_0">Table 1</ref>, 2 and 3 show that no matter what the representation types, network architectures, input and dense representation sizes or input modalities are, AWR based methods consistently outperforms those trained with only dense representation supervision. Besides, AWR introduces robustness to hyperparameter of offsets representation, facilitating hyperparameter searching process. The results fully illustrate the effectiveness and generality of AWR and demonstrate the importance of ensemble learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-art Methods</head><p>We first compare our method with others on HANDS 2017 dataset   <ref type="bibr" target="#b18">(Xiong et al. 2019)</ref>. Results in Table 5 reflect that our ResNet18 based method already exceeds previous state-of-the-art methods by a large margin. And our ResNet50 based method further improves the average mean joint error by 0.36mm. The best result of mean joint error is 7.48mm. For seen and unseen hand objects, mean joint error are 5.21mm and 9.36mm respectively, reflecting the effectiveness and good generalization ability of our proposed method. Note that during comparison, we refer "Ours-ResNet18" and "Ours-ResNet50" as our method using ResNet18 and ResNet50 as backbone respectively. We also compare our method on NYU <ref type="bibr" target="#b16">(Tompson et al. 2014)</ref>, ICVL <ref type="bibr" target="#b14">(Tang et al. 2014a</ref>) and MSRA <ref type="bibr" target="#b12">(Sun et al. 2015)</ref> dataset with most of the state-of-the-art methods, including latent regression forest (LRF) <ref type="bibr" target="#b15">(Tang et al. 2014b</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019).</head><p>As shown in <ref type="figure" target="#fig_4">Fig 4,</ref> our method outperforms all existing methods on the three 3D hand pose estimation datasets using either the per-joint and all-joint mean error or the proportion of good frames. Specifically, on NYU dataset <ref type="bibr" target="#b16">(Tompson et al. 2014)</ref>, our ResNet18 based method already outperforms the superior V2V-PoseNet by 0.67mm with less storage comsumption and computational cost. For the proportion of good frames, all three of our methods outperforms other methods under all the thresholds. On ICVL dataset <ref type="bibr" target="#b14">(Tang et al. 2014a</ref>), our ResNet50 based method achieves better accuracy than other methods. When it comes to the proportion of good frames, it performs best when the threshold is smaller than 12mm and bigger than 20mm, and only slightly worse than V2V-PoseNet in between. Since our method uses cubes with a side length of 250mm to extract hand regions out of depth images, while V2V-PoseNet (Moon, Chang, and Lee 2018b) uses a smaller size of 200mm to locate hands more accurately. Therefore, a slight difference in performance is tolerable. On MSRA dataset <ref type="bibr" target="#b12">(Sun et al. 2015)</ref>, our method achieves comparable accuracy to <ref type="bibr">DenseReg (Wan et al. 2018)</ref> and outperforms other methods. The proportion of good frames of our method is the best under 15mm threshold, but is relatively worse afterwards. However, as stated in <ref type="bibr" target="#b10">(Oberweger et al. 2016)</ref>, part of the ground truth labels in ICVL <ref type="bibr" target="#b14">(Tang et al. 2014a</ref>) and MSRA <ref type="bibr" target="#b12">(Sun et al. 2015)</ref> dataset are mistakenly annotated, which limits the learning capability of our method and leads to little improvement compared to other methods.</p><p>The comparison results show the superiority of our method in both accuracy and robustness over other state-ofthe-art methods and proves that a simple baseline network can achieve powerful performance after introducing AWR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We propose an adaptive weighting regression method for fast and robust 3D hand pose estimation from a single depth image. To overcome the existing defects of detection-based and regression-based methods and leverage the advantages of both, we aggregate different parts of the dense representation through discrete integration of all pixels in them to attain joint coordinates, guided by adaptive weight maps. This operation is learnable and can be easily embedded into different network architectures. It significantly improves the network's estimation accuracy and its robustness to situations where depth values around the target joint are missing and when there is severe self-similarity among fingers. We conduct comprehensive exploration experiments to illustrate the consistent improvement brought by AWR as well as its generality under various experimental settings. Powerful performance is obtained using simple baseline methods, making our method favorable in real-world applications. Our method achieves state-of-the-art performance on NYU, ICVL, MSRA and HANDS 2017 hand pose datasets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Weight maps produced during aggregation for (a) standard detection-based methods and (b) our method, followed by their estimation results and (c) corresponding ground truth labels under situations where the target joint (index fingertip) is visible (top row) or occluded (middle row) and when there exists self-similarity among fingers (bottom row). Figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of our method and arrangements for comprehensive explorations.Figure is best viewed in color. experiments can be seen in Fig 2. Our proposed method outperforms previous state-of-the-art methods on four publicly available datasets, including NYU (Tompson et al. 2014), ICVL (Tang et al. 2014a), MSRA (Sun et al. 2015) and HANDS 2017 (Yuan et al. 2017) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 3 shows that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>), region ensemble network (REN-9×6×6) (Wang et al. 2018), pose guided region ensemble network (Pose-REN) (Chen et al. 2018a), dense 3d regression (DenseReg) (Wan et al. 2018), regresson-based 3d convolutional network (3DCNN) (Ge et al. 2017), voxel-to-voxel prediction network (V2V) (Moon, Chang, and Lee 2018b), dense point-to-point regression pointnet (P2P) (Ge, Ren, and Yuan 2018), pose estimation using point sets (HandPointNet) (Moon, Chang, and Lee 2018a), point clouds based semantic hand pose regression network (SHPR-Net) (Chen et al. 2018b) and multitask information sharing network (CrossInfoNet)(Du et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on NYU, ICVL and MSRA dataset. The all-joint and per-joint mean error (top row) and the proportions of good frames over different thresholds (bottom row)). Left: NYU dataset, middle: ICVL dataset, right: MSRA dataset. Figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of six types of dense representation trained with only dense representation supervision (Dense), only joint supervision (Joint) and both supervisions (Both) on NYU dataset. R represents directly regressing joint coordinates using ResNet50.</figDesc><table><row><cell>Representation</cell><cell cols="3">Mean Error (mm) Dense Joint Both</cell></row><row><cell>R</cell><cell>-</cell><cell>8.82</cell><cell>-</cell></row><row><cell>P</cell><cell>9.58</cell><cell>8.49</cell><cell>8.49</cell></row><row><cell>H1</cell><cell>8.70</cell><cell>8.13</cell><cell>7.95</cell></row><row><cell>H2</cell><cell>8.12</cell><cell>8.04</cell><cell>7.66</cell></row><row><cell>O1</cell><cell>8.19</cell><cell>7.92</cell><cell>7.86</cell></row><row><cell>O2</cell><cell>8.05</cell><cell>8.02</cell><cell>7.58</cell></row><row><cell>O3</cell><cell>7.87</cell><cell>7.75</cell><cell>7.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of networks trained with only dense supervision (Dense) and both joint and dense supervision (Both) under various experimental settings on NYU dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Input Size Dense Size Params (FLOPs)</cell><cell cols="2">Mean Error (mm) Dense Both</cell></row><row><cell>a</cell><cell>ResNet-18</cell><cell>128 × 128</cell><cell>64 × 64</cell><cell>15M (7.6G)</cell><cell>8.53</cell><cell>7.75</cell></row><row><cell>b</cell><cell>ResNet-50</cell><cell>128 × 128</cell><cell>64 × 64</cell><cell>34M (14.4G)</cell><cell>7.87</cell><cell>7.48</cell></row><row><cell>c</cell><cell>ResNet-101</cell><cell>128 × 128</cell><cell>64 × 64</cell><cell>53M (24.1G)</cell><cell>7.81</cell><cell>7.37</cell></row><row><cell>d</cell><cell>ResNet-50</cell><cell>128 × 128</cell><cell>32 × 32</cell><cell>33M (12.1G)</cell><cell>8.07</cell><cell>7.68</cell></row><row><cell>e</cell><cell>ResNet-50</cell><cell>128 × 128</cell><cell>16 × 16</cell><cell>32M (11.6G)</cell><cell>8.24</cell><cell>7.85</cell></row><row><cell>f</cell><cell>ResNet-50</cell><cell>96 × 96</cell><cell>48 × 48</cell><cell>34M (8.1G)</cell><cell>8.14</cell><cell>7.59</cell></row><row><cell>g</cell><cell>ResNet-50</cell><cell>96 × 96</cell><cell>24 × 24</cell><cell>33M (6.8G)</cell><cell>8.32</cell><cell>7.76</cell></row><row><cell>h</cell><cell>ResNet-50</cell><cell>96 × 96</cell><cell>12 × 12</cell><cell>32M (6.5G)</cell><cell>8.37</cell><cell>8.01</cell></row><row><cell>i</cell><cell cols="2">Hourglass-1stage 128 × 128</cell><cell>64 × 64</cell><cell>4.6M(11.6G)</cell><cell>8.38</cell><cell>7.70</cell></row><row><cell>j</cell><cell cols="2">Hourglass-2stage 128 × 128</cell><cell>64 × 64</cell><cell>8.7M(18.2G)</cell><cell>7.73</cell><cell>7.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Results of three types of input modalities trained</cell></row><row><cell cols="3">with only dense representation supervision (Dense) and both</cell></row><row><cell cols="3">joint and dense representation supervision (Both).</cell></row><row><cell>Input Modality</cell><cell cols="2">Mean Error (mm) Dense Both</cell></row><row><cell>Voxels</cell><cell>11.40</cell><cell>8.60</cell></row><row><cell>Point clouds</cell><cell>10.6</cell><cell>10.52</cell></row><row><cell>Depth image</cell><cell>7.87</cell><cell>7.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Impact on hyperparameters of offsets representation</cell></row><row><cell cols="5">trained with only dense representation supervision (Dense)</cell></row><row><cell cols="5">and both dense representation and joint supervisions (Both).</cell></row><row><cell>Kernel Size</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell></row><row><cell>Dense</cell><cell cols="4">8.3mm 7.87mm 7.95mm 8.03mm</cell></row><row><cell>Both</cell><cell cols="4">7.49mm 7.48mm 7.46mm 7.45mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on HANDS 2017 dataset. "SEEN" and "UNSEEN" denotes whether or not that the hand subject has been seen in the training set. "AVG" denotes all-joint mean error in millimeters over all test frames.</figDesc><table><row><cell>Method</cell><cell cols="3">SEEN UNSEEN AVG</cell></row><row><cell>Vanora</cell><cell>9.55</cell><cell>13.89</cell><cell>11.91</cell></row><row><cell>THU VCLab</cell><cell>9.15</cell><cell>13.83</cell><cell>11.70</cell></row><row><cell>oasis</cell><cell>8.86</cell><cell>13.33</cell><cell>11.30</cell></row><row><cell>RCN-3D</cell><cell>7.55</cell><cell>12.00</cell><cell>9.97</cell></row><row><cell>V2V-PoseNet</cell><cell>6.97</cell><cell>12.43</cell><cell>9.95</cell></row><row><cell>A2J</cell><cell>6.92</cell><cell>9.95</cell><cell>8.57</cell></row><row><cell>Ours-ResNet18</cell><cell>5.76</cell><cell>9.57</cell><cell>7.84</cell></row><row><cell>Ours-ResNet50</cell><cell>5.21</cell><cell>9.36</cell><cell>7.48</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation. Neurocomputing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="43425" to="43439" />
		</imprint>
	</monogr>
	<note>Crossinfonet: Multi-task information sharing based hand pose estimation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yuan ; Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Awr: Adaptive weighting regression for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Systems (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Point-to-pose voting based hand pose estimation using residual permutation equivariant layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>and Lee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee ; Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8417" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee ; Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng ; Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">pointnet++deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oberweger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
	<note>Conference and Workshop on Neural Information Processing Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Srn: Stacked regression network for realtime 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ren et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="536" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tompson</surname></persName>
		</author>
		<idno>169:1-169:10. [Wan et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="404" to="414" />
		</imprint>
	</monogr>
	<note>Dense 3d regression for hand pose estimation. Wang et al. 2018. Region ensemble network: Towards good practices for deep 3d hand pose estimation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei ;</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pixel-wise regression: 3d hand pose estimation via spatialform representation and differentiable decoder</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Depth-based 3d hand pose estimation: From current achievements to future goals. CoRR abs/1905.02085</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
