<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Neural Architecture Search using Limited GPU Time in a Dynamic Search Space: A Gene Expression Programming Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeovane</forename><forename type="middle">Honorio</forename><surname>Alves</surname></persName>
							<email>jhalves@inf.ufpr.br</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Imaging (VRI)</orgName>
								<orgName type="laboratory">Laboratory of Vision</orgName>
								<orgName type="institution">Federal University of Parana (UFPR) Curitiba -PR</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">nd Lucas Ferrari de Oliveira Laboratory of Vision, Robotics and Imaging (VRI) Federal University of Parana (UFPR) Curitiba -PR</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Neural Architecture Search using Limited GPU Time in a Dynamic Search Space: A Gene Expression Programming Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-neural architecture search</term>
					<term>automl</term>
					<term>cnn</term>
					<term>deep learning</term>
					<term>gene expression programming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient identification of people and objects, segmentation of regions of interest and extraction of relevant data in images, texts, audios and videos are evolving considerably in these past years, which deep learning methods, combined with recent improvements in computational resources, contributed greatly for this achievement. Although its outstanding potential, development of efficient architectures and modules requires expert knowledge and amount of resource time available. In this paper, we propose an evolutionary-based neural architecture search approach for efficient discovery of convolutional models in a dynamic search space, within only 24 GPU hours. With its efficient search environment and phenotype representation, Gene Expression Programming is adapted for network's cell generation. Despite having limited GPU resource time and broad search space, our proposal achieved similar state-of-the-art to manuallydesigned convolutional networks and also NAS-generated ones, even beating similar constrained evolutionary-based NAS works. The best cells in different runs achieved stable results, with a mean error of 2.82% in CIFAR-10 dataset (which the best model achieved an error of 2.67%) and 18.83% for CIFAR-100 (best model with 18.16%). For ImageNet in the mobile setting, our best model achieved top-1 and top-5 errors of 29.51% and 10.37%, respectively. Although evolutionary-based NAS works were reported to require a considerable amount of GPU time for architecture search, our approach obtained promising results in little time, encouraging further experiments in evolutionarybased NAS, for search and network representation improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In conventional machine learning, feature engineering is employed with an intent to develop feature descriptors for a specific problem -feature which would represent not only a set of samples, but its overall population. Although this proposal is interesting, it is difficult and exhaustive to develop feature descriptors to solve a specific problem. Thus, deep learning would come to remove the need for feature engineeringthe weights of a network would be optimized to a dataset for accurate representation. Following the idea of automatic feature engineering, automation of network design would be a This work was supported by CAPES/CNPq. next step in deep learning -the arduous job of optimizing the structure of a deep network would fall to a computer method, reducing human workload. Not only that, but it may provide efficient networks not followed by human thinking (structures with unconventional patterns). This would drive research to a new area: Neural Architecture Search (NAS), a sub-field of AutoML (Automated Machine Learning). The main idea of NAS is to obtain optimum deep learning models (convnets, recurrent nets and so on) using another method, rather than a human manually designing the model. Thus, this method would search for an architecture that better suits a specific problem. This outstanding idea would be applied for a variety of deep networks to solve computer vision, machine translation, speech recognition problems and so on <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we employ a methodology based on GEP, entitled as Neural Architecture Search using Gene Expression Programming (NASGEP) <ref type="bibr" target="#b0">1</ref> , to assess the likelihood of fast and efficient architecture search with this evolutionary approach. More specifically, we address specific points which can be considered as features for why use this NAS approach:</p><p>• Dynamic search space using the GEP representation, which will increase its space considerably but may provide innovative cells if compared with other search spaces (e.g. NAS, DARTS, etc);</p><p>• Reusable modules -being treated as GEP genes, tracking which combination of convolutional blocks improves individuals' fitness;</p><p>• Weights from the fittest modules, not only from convolutional blocks, are passed on to further generations for cutting search time;</p><p>• Evolutionary search strategy within 24 GPU hours, aiming to obtain efficient models without excessive resource consumption.</p><p>II. RELATED WORK Development of architecture search approaches can be categorized in three dimensions:</p><p>• How to represent candidate networks (search space); • How to find out better candidates (search strategy); • How to evaluate each candidate (parameter estimation strategy). To develop a NAS approach, these dimensions need to be fully determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search Space</head><p>For the first dimension -search space, the possible structure of each candidate network needs to be defined. Two principal points need to be carefully address: a limited search space size, since extensive number of candidates would hinder optimum size in small search time; but also networks would be different between each other as much as possible, since structure variance would introduce the possibility to find out which structure patterns produce fitter results <ref type="bibr" target="#b0">[1]</ref>.</p><p>Currently, there are three categories of search spaces: chainstructure, multi-branch and cell-based ones. Chain-based NAS works focus on developing optimum sequential models, although with possible presence of skip-connections. Generally, these models are simpler and do not present significant differences between manually-designed models like ResNet, WideResNet and so on.</p><p>Unlike them, multi-branch NAS approaches (like <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>) focus on complex optimizations on the entire architecture. It is like a big Inception <ref type="bibr" target="#b3">[4]</ref> block as the entire network. Being extremely different from chain-based models, this type of model introduces too much complexity, increasing exponentially the search space to be traveled.</p><p>As for cell-based ones, they produce superior results in lesser time than the others, as it limits the search space to a part of the network (cells are repeated along the entire network) without reducing considerably the variance between models (a cell has similar structure than multi-branched, although not in size). Cell-based search spaces split between normal and reduction cells: the former aims feature representation, being replicated many times through the network; then the latter aims reduction of feature map size (with poolings or convolutions with stride = 2). Thus, the best model always has two different cells (in some cases, the reduction cell is already fixed or it has a similar structure than the normal cell).</p><p>Two common cell-based subgroups are used: the NASbased <ref type="bibr" target="#b1">[2]</ref> and the Differentiable Architecture Search (DARTS)based cells <ref type="bibr" target="#b4">[5]</ref>, which is basically a directed acyclic graph (DAG). Cells based on the NAS work (also NASNet <ref type="bibr" target="#b5">[6]</ref> and AmoebaNet <ref type="bibr" target="#b6">[7]</ref>) are composed of N blocks (generally, N = 4), in which each block is composed of an addition of two operations (e.g. convolution and/or poolings). Each block's output can be used as an input of another block or concatenated with other blocks to generate the cell's output.</p><p>In the case of DARTS-based cells, operations are combined based on edges and their combinations (like a directed acyclic graph). Assuming there are M sequential edges (E 0 , ..., E M −1 ), each edge is combined with previous edges by some operation. Then, the edge E M −1 combines with all previous edges (E 0 , ..., E M −2 ) to produce the cell's output.</p><p>Architecture search for the former aims to select which inputs and operations each block will have; and for the latter, which operations will have between edges. Several works based on these two cell representations, such as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, surpassed even human-designed models in CIFAR and ImageNet datasets.</p><p>Although this encourages further usage of them, it also presents a problem regarding human bias. These cells present a fixed structure regarding how operations are combined: in the first, always two operations (even if one operation is an identity) are added to generate a block and then these blocks are concatenated to generate the cell's output; in the second, outputs of prior edges are combined in the same way to generate the current edge. It reduces drastically what types of structures can be found, also limiting to structures close to models already experimented (introducing human bias). Thus, for searching for more different patterns, it is crucial to relax how the cells are generated, however not to a point in which search space becomes too big (e.g. multi-branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Search Strategy and Parameter Estimation Strategy</head><p>Search strategies in NAS works are mainly in the areas of reinforcement learning (RL), evolutionary computation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b12">[13]</ref> and gradient-based techniques. RL-based NAS would employ different methods such as REINFORCE policy, proximal policy optimization (PPO) and Q-learning (like <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>). The first NAS approaches that achieved state-of-the-art results were based on this type.</p><p>Genetic algorithms and other evolutionary methods would also be used as NAS search strategies, which generates a population of models and evolves them with mutation and other reproduction techniques. Evolutionary-based methods would produce promising results, although generally using too much GPU computation time (dozens to hundred GPU days), which would encourage research in other types of search strategies.</p><p>Gradient-based (e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>) NAS approaches aim a more continuous optimization, unlike the discrete and conditional from other types. A common idea is to present many possible operations (e.g. convolutions, poolings, skipconnections, etc) to be applied between one edge and another from a DAG representation and optimize weights representing what operation is better in that connection. This would represent not only a more flexible and fluid optimization, since it is more visible on how each operation contributes to the candidate score, but also works based on this approach that presents efficient results in less time.</p><p>As for parameter estimation strategies, it mainly concerns evaluating validation error and training loss. Also, strategies to reduce candidate training time such as fewer epochs, reduced size, weight sharing and cosine annealing learning rate scheduler are employed. Thus, evaluation of candidates can be realized in less computing time <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evolutionary-based NAS</head><p>Evolutionary-based NAS approaches generally consume a considerable amount of time, being infeasible to be employed in small research labs and companies, since it would cost a massive amount of GPU processing. Approaches which consume little GPU resources would be plausible to be employed in small companies and research labs, not only to popularize NAS application, but also to provide better models to specificgroup problems. Although other techniques may be applied, some would be limited to a fixed search space. Not only that, but the already proposed evolutionary-based approaches may have focused too much in search with unlimited resource time, that other alternatives to employ a low cost evolutionary-based NAS may not be extensively studied.</p><p>In virtue of these points, we propose an evolutionary-based NAS approach which focuses on obtaining efficient models with limited GPU time (24 hours in a NVIDIA GTX Titan Xp, a common card used in NAS benchmark) in a dynamic search space represented by GEP phenotype (tree-like structure). A NAS approach using only one GPU day may be used in groups with less resources. While this approach can be used with different types of deep networks, we focus this time only in convolutional networks (also called convnets), due to the broad range of experiments to be carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we defined the methodology and properties of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Workflow</head><p>Before entering in details of why and how each piece of NASGEP is used, we present its entire workflow below (being depicted in <ref type="figure">Figure 1</ref>): 1) Genes for normal and reductions cells are generated (separately) for their initial population (size of P g ); 2) An initial population of P r reduction cells is created;</p><p>3) The first generation of P i models is generated and trained: a) A random genotype (head and tail) is generated, to transform into the normal cell's phenotype; b) A random reduction cell is picked from the initial P r individuals; c) Then, each model is trained (only one epoch) to generated their fitness; 4) Before reproduction of individuals, genes are killed and remaining are reproduced: a) Genes not being used with a fitness below T g are discarded (to remove bad genes from population); b) Tournament selection is used to select two parents; c) At least C g min children are created per generation; d) Children are generated until their size reaches C g max in the generation or if the gene pool size reaches P (max) g ; 5) The children generation is generated with tournament selection: a) First, the new generation of reduction cells is created with reproduction of two parents (chosen by tournament selection, for each child); b) Then, for each individual in the normal population, reproduction is employed to generate their normal cell's genotype; c) For the individual's reduction cell generation, a tournament selection is applied to the population of reduction cells to select one of the alive cells; 6) Children are trained: a) Each child is trained for one epoch; b) If the fitness of a child reached a threshold T c , the model is trained for another epoch (this is employed to reward better individuals in the search process); c) If a child reached the maximum number of epochs configured E max , training is finished for this child and it is marked to be killed; d) In our specific approach, if a individual is marked to be killed after reaching the maximum number of epochs E max , its current reduction cell is also marked to be killed (or not used for reproduction); 7) After training of children population, NASGEP needs to reduce population size (from individuals and also reduction cells) -thus applying survivor selection: a) First, the best individual is always preservedelitism; b) Then, the oldest individual is killed; c) If any, dead individuals (which have already trained for E max epochs) are discarded; d) Finally, individuals in excess for both reduction and normal populations are removed based on their fitness (P i individuals including the best one are preserved); e) Reduction cells that are being used will survive to the next generation; f) Thus, after survivor selection, more than P i individuals can be found in the reduction population; g) Then, generation is finished; 8) For the next generation, each model is trained like step 6; 9) Workflow goes back to step 4 and moves on, until the time-limit of 1 GPU day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Why Gene Expression Programming?</head><p>The phenotype representation of genetic programming (and its variations, like gene expression programming <ref type="bibr" target="#b19">[20]</ref>) consists of a binary tree with many leaves, which can represent the inputs of a convolutional cell and the root as its output (with intermediary nodes being convolutions and binary operations to join their output, like concatenation and addition). Thus, this phenotype provides a representation with little modifications to adapt for convolutional cells.</p><p>Not only that, but this representation provides flexibility in searching different models. For example: one branch is more deeper and the other more shallower <ref type="figure">(Figure 2a</ref>); other with additions and concatenations alternating with each other, even simulating a NAS-cell based representation <ref type="figure">(Figure 2b)</ref>. Another point to be highlighted is the simple genotype representation in GEP. Similar to genetic algorithm, genotype in GEP is a sequence of elements (e.g. string of characters and numbers). Not only modifications in this sequence can be easily done (like mutation and other reproduction methods), but the search space can be transferred to another search strategy. This simple sequence of head and tail may be changed with other techniques, like a Long Short-Term Memory (LSTM) architecture. Although the search strategy is modified, the dynamic search space representation remains intact.</p><p>Also, using Automatically Defined Functions (ADFs, i.e. genes) encourages the optimization of small modules and combinations between them. Rather than optimize a bigger cell, optimizations on smaller modules would facilitate fitness convergence. If one part of a bigger cell interferes negatively on its fitness, the entire cell structure is discarded. But, if many of these small modules are found in different cells, through one cell may have lower fitness because of a bad gene, other good genes will obtain good fitness of other cells. Then, through generations, bad genes will be discarded and good ones will be combined in newer individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Blocks: Functions and Terminals</head><p>In GEP, an individual is composed of both head and tail sequences. Both are combined to generate the individual's phenotype. For the head, functions and terminals (elements of a GEP sequence) are used. As for the tail, only terminals can be employed. The question is: what are functions and terminals in our NAS approach?</p><p>For normal and reduction cells, the following elements are used as functions: addition and concatenation. For terminals, only ADFs (i.e. genes) are used. As for genes, addition and many types of convolutions (with different kernel sizes) are used as functions. For terminals, convolutions and previous cells' outputs. For the latter, only normal cells have these outputs as terminals. This was chosen since, in the reduction cell, additions with them would not be possible as they would have different feature map sizes. These elements are used to compose the cells in our network. Convolution blocks in this case are defined as the sequence of ReLU activation -&gt; Convolution -&gt; Batch Normalization, according <ref type="bibr" target="#b20">[21]</ref> (also followed in <ref type="bibr" target="#b5">[6]</ref>).</p><p>As for the types of convolutions, we used the following:</p><p>• Point-wise convolution (1x1 kernel); • Depth-wise convolution with kernels 3x3, 5x5, 3x5, 5x3, 1x7 or 7x1;</p><p>• Separable depth-wise convolution (a point-wise convolution followed by a depth-wise) or in the inverse other, with kernel sizes of 3x3, 5x5, 3x5, 5x3, 1x7 and 7x1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convolutional Network Peculiarities</head><p>With a diversified environment of convolutions, we can find useful combinations to represent a specific dataset. Also, as we are working with concatenations, the number of channels may increase on different occasions. Thus, a point-wise convolution is applied when a input encounter with another feature map with lower channel size (e.g. these two are inputs of an addition), or when the output of a cell is greater than the current size (only outputs of reduction cells change the number of channels, but also only double it). To reduce training time, whenever a convolution block or gene has validation accuracy greater than before, its current weights are saved to the disk. Then, whenever a new individual is created, these weights are loaded to the model, rather than being randomly initialized with, in our case, Kaiming initialization <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Reproduction</head><p>Generation of new individuals is executed in the reproduction step. The default techniques of GEP are used to generate them, being applied to a pair of parents the following (in order): mutation, transposition, root transposition, one-point and then two-point recombination. Default parameters from the original paper were employed. Also, this reproduction is also applied to genes, to generate and improve these modules for next generations. One different thing in our approach is that, to encourage combination of addition and concatenation in different ways, elements in an individual's head only change to a function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Experiments were conducted in a NVIDIA GTX Titan Xp with 12 GB VRAM, a common card for GPU time evaluation. To assess the capability of our proposed work, experiments on common benchmark datasets were executed. For their popularity in similar experiments, the CIFAR-10, CIFAR-100 and ImageNet datasets were used in this step. Each dataset is composed of common objects and living beings found in images. Also, different numbers of classes, samples per class and resolutions are employed in them. Before entering in more details regarding the experiments executed, hyper-parameters and other details about our architecture search are discussed in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search and full training phases' details</head><p>For the search phase, some parameters need to be defined differently from the full model training phase, since we want to evaluate as many models as possible. Thus, reduction of model size is applied for network candidates. The initial feature map size for the first convolution (before normal and reduction cells) is set to 16. Candidates are evaluated by their fitness. In our experiments the validation accuracy was employed as the fitness score.</p><p>Initial population size P i for each generation is fixed to 10. For normal and reduction cells, the genotype's head size is equal to 4 (thus tail size to 5). For their genes, head size was set to 1 (and tail size to 2). These values were chosen to enable a more flexible combination (e.g. vast combination of different small modules). The initial size of the gene pool is set to P g = 50 and its maximum size P (max) g is set to 100. For each generation, at least C g min = 2 genes are created and at most C g max = 10. T g is updated at the end of each generation to the P i th best individual's fitness (for removal of bad genes). The threshold T c is updated to 75% of this fitness.</p><p>Number of parameters of network candidates was limited to 300 thousand to avoid generating models bigger than memory available.</p><p>Training for each candidate can reach E max = 10 epochs, using Stochastic Gradient Descent (SGD) with cosine annealing learning rate scheduler for optimization <ref type="bibr" target="#b22">[23]</ref>. Also, batch size is set to 512 to focus on rapidly training. The initial learning rate value is set to 0.1 for the search phase, to obtain quickly the validation error of each candidate that is close to the error after full training. In the full training phase, batch size is reduced to 128 and learning rate to 0.025, which focus on over-fitting reduction, besides its slow convergence.</p><p>Number of repetitions for the normal cell (see <ref type="figure" target="#fig_1">Figure 3</ref> for reference) in each stage is set to N = 3. In the full training phase, the initial feature map size is set to 64. The ImageNet's stem has three sequential 3x3 convolution blocks (where the first do not have ReLU activation) with stride = 2. The first convolution block increases the number of channels to half the initial feature map size (size of 32) and the second convolution block to the initial size chosen (in this case, 64). Also, the number of epochs goes from 10 to 300 (for CIFAR datasets). The number of epochs of 90 is used in the ImageNet full training phase (no search phase is employed here). After the last normal cell, batch normalization followed by a ReLU activation is applied. Then, global average pooling is employed to reduce the feature map size of each channel to 1x1. Finally, a fully connected layer reduces the output size to the number of classes, to be inputted in the softmax.</p><p>For data augmentation, mean and standard deviation correction was applied, after application of random crop and horizontal flip, CutOut <ref type="bibr" target="#b23">[24]</ref> (length = 16) and AutoAugment (AA) <ref type="bibr" target="#b24">[25]</ref>. For regularization, we also employed drop-path <ref type="bibr" target="#b25">[26]</ref> with rate = 0.1. Weight decay of 0.0005 and momentum of 0.9 were also chosen values for regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments with CIFAR datasets</head><p>A popular benchmark dataset for image classification is the CIFAR project <ref type="bibr" target="#b26">[27]</ref>, divided into two datasets: CIFAR-10 and CIFAR-100 (where the number is the amount of classes in the dataset). For both datasets, an amount of 50000 images for training is found. Testing subsets contain 10000 images each. Classes for both datasets have the same number of samples (6000 each class for CIFAR-10 and 600 for CIFAR-100).</p><p>Search and training is employed in the CIFAR datasets separately. Not only that, but to assess the reliability of NASGEP, a total of five runs are done for both datasets (higher values would be infeasible, since searching and training deep learning models consume an excessive amount of resources). In the search phase (and also tuning of our method), the testing subset was not touched. Thus, we extract 5000 samples from the training subset to be the validation subset, where we would find which combination of normal and reduction cells achieved superior fitness.</p><p>1) Ablation Study: In this study, we aim to analyze specific components (data augmentation and regularization) used to train our model. Besides, evaluation of the weight sharing approach and training with random sampling is also exposed. The objective is not only to determine which strategies have superior fitness, but also if our search proposal is really obtaining better models in the search space. <ref type="table" target="#tab_0">Table I</ref> shows the experiments employed in this study.</p><p>The first four experiments aim to analyze data augmentation and regularization in the same searched models. The last one focuses on the search analysis using the best data augmentation configuration. All experiments used CutOut. In general, combinations of these strategies improve accuracy satisfactory. Drop-path together with AA was an exception that, although had lower error than using only drop-path, it was worse than using only AA. This was probably caused by the inclusion of noise in the application of drop-path, which would limit the learning of some data variations generated by the data augmentation approach.</p><p>To evaluate the different between random sampling and the searched models, we use the relative improvement (RI) metric in <ref type="bibr" target="#b27">[28]</ref>, RI = 100 × (Acc m − Acc r )/Acc r , with Acc m being the mean of the five runs of our proposal and Acc r the mean of five random samplings. We obtained a RI = 0.3098, similar to DARTS (RI = 0.32). Since the combination of CutOut and AutoAugment achieved the best results, they will be used in remaining experiments. 2) Evaluation on the Best Strategy: In <ref type="table" target="#tab_0">Table II</ref>, state-ofthe-art manually designed and NAS approaches are compared if our own in CIFAR-10 and CIFAR-100 datasets.</p><p>After five runs, we achieved a test error of 2.82 ± 0.13 in CIFAR-10, and 18.83 ± 0.39 in CIFAR-100, with our best models in each dataset obtaining an error of 2.67% and 18.16%, respectively. Normal and reduction cells from the best candidate in CIFAR-10 can be seen in <ref type="figure" target="#fig_3">Figures 4 and  5</ref>, respectively. The structure of both cells are very different, also in the number of convolutional operations. Normal had a quantity similar to AmoebaNet cells. On the other hand, the reduction cell had a greater number of convolutions, mainly inverse separable convolutions. Not only were they different between them, but also different between other approaches, which had more fixed structures, mainly in not combining addition and concatenation dynamically. Even with a flexible search space, our method obtained satisfactory cells for the final convnet. Number of parameters from candidates chosen were around three million, similar to other recent NAS approaches which focused on this reduction without losing their performance. Compared with the best NAS approaches like AmoebaNet and DARTS, NASGEP had only little worse results, showing the capability of the approach even with a diverse search space. Furthermore, NASGEP obtained significant results using only one GPU day (in contrast with the three thousand from Amoe-baNet), and outperformed substantially other evolutionarybased approaches, seeing in <ref type="table" target="#tab_0">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments with ImageNet on Mobile Setting</head><p>Another popular dataset used for image classification (and other problems such as localization and detection) is the Ima-geNet Large Scale Visual Recognition Challenge (ILSVRC)  dataset <ref type="bibr" target="#b33">[34]</ref>. This specific dataset contains more than one million training images from 1000 classes. Its validation and testing subsets contain 50000 images each. Since the ImageNet dataset contains a massive amount of images, only one run was executed for it. Similar to other NAS works (e.g. DARTS and AmoebaNet), we transferred the learned CIFAR cell for evaluation of the ImageNet case. <ref type="table" target="#tab_0">Table III</ref> shows results in both manually-designed models and NAS-generated ones. Also, the result obtained with NASGEP is presented.</p><p>In this case, NASGEP had a top-1 error of 29.51% and top-5 of 10.37%, significantly greater than other approaches shown in <ref type="table" target="#tab_0">Table III</ref>. Besides the fact that lower GPU resource time was used, it is crucial to further investigate alternatives to address this, even if the approaches compared already had slightly better results in CIFAR experiments (although we did achieve satisfactory results compared to AlexNet and SqueezeNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, an evolutionary-based neural architecture search, NASGEP, for limited GPU usage was developed. It mainly consisted of using GEP as its search space and strategy. Not only evaluation of limited GPU usage was in mind, but also to adapt the phenotype representation of GEP to a convolutional cell. Choosing this phenotype was mainly for the introduction of a more dynamic search space in NAS, but also to assess the reliability of employing evolutionarybased approaches in time-limited resources. Although our approach did not surpass state-of-the-art methods in CIFAR and ImageNet mobile settings, promising results were found, besting state-of-the-art manually-designed and NAS models in CIFAR-10 dataset (and similar to them in CIFAR-100 and ImageNet). With this in regard, studies in evolutionary-based approaches with cell-based dynamic search spaces may be further pursued in time-limited GPU execution.</p><p>Further research in how to take advantage of the GEP phenotype and variants may be pursued, aiming for more optimized search spaces. In this study, ADFs were only treated as terminals. Although this stabilizes the fitness for a specific gene (as limited the gene to be only added or concatenated in a cell), it reduces the ways a gene can be treated, such as being the input or output of another gene. Treating genes as functions may present a new environment for study and optimization: the way a gene is treated in a cell can somehow improve the performance of a model? Also, further experiments in different datasets but with the same classes would assess the capability of the best cells found to generalize to similar datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Overall representation of the NASGEP workflow. Some examples of how a GEP phenotype can represent a convolutional cell. Leaves of this tree, i.e. outputs of previous cells (h 0 , ..., h i−2 , h i−1 ), are used as inputs of many operations (like convolutions, additions and concatenations). They are combined to produce the current cell's output h i . This entire representation is repeated many times to generate the full convolutional network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Overall network representation of CIFAR and ImageNet networks, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Fig. 4 :</head><label>14</label><figDesc>Normal cell from the best model found in CIFAR-10, also transferred to ImageNet mobile setting training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Reduction cell from CIFAR-10's best model, which the convolutions following an input (h i−1 or h i−2 ) have stride = 2 for feature map size reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Ablation study in the CIFAR-10 dataset with different data augmentation methods, regularization and search strategies.</figDesc><table><row><cell></cell><cell>Test error (in %)</cell><cell>Params</cell></row><row><cell>CutOut only</cell><cell>3.54 ± 0.15</cell><cell>3.62M±0.18</cell></row><row><cell>CutOut + AA CutOut + Drop-Path CutOut + Drop-Path + AA</cell><cell>2.82 ± 0.13 3.24 ± 0.10 2.99 ± 0.07</cell><cell>3.62M±0.18 3.62M±0.18 3.62M±0.18</cell></row><row><cell>CutOut + AA + No Weight Sharing CutOut + AA + Random Sampling</cell><cell>2.92 ± 0.19 3.12 ± 0.28</cell><cell>3.48M±0.34 2.54M±0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results from CIFAR datasets.</figDesc><table><row><cell>NAS</cell><cell></cell><cell cols="2">Search Space</cell><cell cols="3">Search Strategy GPU Days</cell><cell cols="2">CIFAR-10 Test error (in %)</cell><cell>Params</cell><cell>CIFAR-100 Test error (in %)</cell><cell>Params</cell></row><row><cell cols="2">WideResNet [29] ResNeXt [30] DenseNet-BC [31]</cell><cell cols="2">Chain Multi-branch Chain</cell><cell cols="2">Manual Manual Manual</cell><cell>N/A N/A N/A</cell><cell>3.08 ± 0.16 3.58 3.46</cell><cell>36.5M 34.4M 25.6M</cell><cell>18.41 ± 0.27 17.31 17.18</cell><cell>36.5M 34.4M 25.6M</cell></row><row><cell cols="2">NAS [14] NASNet [6] AmoebaNet [7] EANN-Net [32] CoDeepNEAT [11] Genetic CNN [33] Hill-climbing [2] DARTS [5]</cell><cell cols="2">Chain NAS-cell NAS-cell Chain Multi-branch Multi-branch Multi-branch DAG-cell</cell><cell cols="2">RNN and RL RL Evolution Evolution Evolution Evolution Hill-climbing Gradient</cell><cell>2000 2000 3150 -&gt; 1 20 1 4</cell><cell>4.47 2.65 2.55 7.05 ± 0.02 7.3 7.10 5.2 2.76 ± 0.09</cell><cell>7.1M 3.3M 2.8M ---19.7M 3.3M</cell><cell>-----29.03 23.4 -</cell><cell>------22.3M -</cell></row><row><cell>NASGEP</cell><cell></cell><cell cols="2">GEP-based cell</cell><cell cols="2">Evolution</cell><cell>1</cell><cell>2.82 ± 0.13</cell><cell>3.62M±0.18</cell><cell>18.83 ± 0.39</cell><cell>3.59M±0.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>h i</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cat</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>cat</cell><cell></cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>cat</cell><cell>+</cell><cell></cell><cell>sep 1x7</cell><cell>isep 1x7</cell><cell></cell></row><row><cell></cell><cell>+</cell><cell>isep 3x3</cell><cell>isep 1x7</cell><cell>sep 1x7</cell><cell>isep 5x5</cell><cell>isep 7x1</cell><cell></cell></row><row><cell>isep 3x3</cell><cell>sep 3x3</cell><cell>sep 5x5</cell><cell>h i-1</cell><cell>h i-1</cell><cell>h i-1</cell><cell>h i-1</cell><cell></cell></row><row><cell>h i-1</cell><cell>h i-2</cell><cell>h i-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results from ImageNet dataset in mobile setting.</figDesc><table><row><cell>NAS</cell><cell>Search Space</cell><cell cols="5">Search Strategy GPU Days Top-1 error (in %) Top-5 error (in %) Params</cell></row><row><cell>AlexNet [35] SqueezeNet [36] ShuffleNet [37] MobileNet [38]</cell><cell>Chain Chain Chain Chain</cell><cell>Manual Manual Manual Manual</cell><cell>N/A N/A N/A N/A</cell><cell>37.5 39.6 24.7 29.4</cell><cell>17 17.5 --</cell><cell>60M 0.42M 5M 4.2M</cell></row><row><cell>NASNet [6] AmoebaNet [7] PNAS [39] DARTS [5]</cell><cell>NAS-cell NAS-cell NAS-cell DAG-cell</cell><cell>RL Evolution SMBO Gradient</cell><cell>2000 3150 225 4</cell><cell>26 24.3 25.8 26.7</cell><cell>8.4 7.6 8.1 8.7</cell><cell>5.3M 6.4M 5.1M 4.7M</cell></row><row><cell>NASGEP</cell><cell>GEP-based cell</cell><cell>Evolution</cell><cell>1</cell><cell>29.51</cell><cell>10.37</cell><cell>4.1M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for architecture search and training available at http://github.com/ jeohalves/nasgep.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank NVIDIA Corporation with the donation of the Titan Xp GPU used in our experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1804" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<title level="m">Evolving deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A genetic programming approach to designing convolutional neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural fabrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4053" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic convolutional neural architecture search for image classification under different scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="38" to="495" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pcdarts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gene expression programming: A new adaptive algorithm for solving problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="129" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Esperana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HygrdpVKvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auto-creation of effective neural network architecture by evolutionary algorithm and resnet for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3895" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
