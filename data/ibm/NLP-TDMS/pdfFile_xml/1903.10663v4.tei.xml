<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combination of Multiple Global Descriptors for Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heejae</forename><surname>Jun</surname></persName>
							<email>heejae.jun@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Youngjoon Kim Insik Kim Jongtack Kim NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
							<email>byungsoo.ko@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Youngjoon Kim Insik Kim Jongtack Kim NAVER/LINE Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Combination of Multiple Global Descriptors for Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement. However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory. In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner. The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties. In the benchmark evaluation, the proposed framework achieves the state-of-theart performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks. Our model implementations and pretrained models are publicly available 1 . * Authors contributed equally 1 https://github.com/naver/cgd arXiv:1903.10663v4 [cs.CV] 23 Apr 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head><p>We propose a simple, yet effective framework which we refer to as a CGD framework for image retrieval tasks. It learns a combined descriptor which is generated by concatenating multiple global descriptors in an end-to-end manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the ground-breaking in 2012 ImageNet competition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>, image descriptors based on deep convolutional neural networks (CNNs) have surfaced as generic descriptors in computer vision tasks, including classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">51]</ref>, object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref>, and semantic segmentation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref>. Moreover, recent works leveraging image descriptors based on deep CNNs have emerged for image retrieval task which used to apply conventional methods relying on local descriptor matching <ref type="bibr" target="#b33">[34,</ref><ref type="bibr">23]</ref> and re-ranking with spatial verification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In the case of recent researches on image retrieval <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, fully connected (FC) layers after several convolutional layers are used as global descriptors followed by dimensionality reduction. Other works generate global descriptors from the activations of the convolutional layers. Representative global descriptors generated by global-pooling methods include sum pooling of convolutions (SPoC) <ref type="bibr">[3]</ref>, maximum activation of convolutions (MAC) <ref type="bibr" target="#b52">[53]</ref>, and generalizedmean pooling (GeM) <ref type="bibr" target="#b42">[43]</ref>. The performance of each global descriptor varies by dataset as each descriptor has different properties <ref type="bibr" target="#b4">[5]</ref>. For example, SPoC activates larger regions on the image representation while MAC activates more focused regions <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recent researches have focused on ensemble techniques for image retrieval task. Conventional ensemble techniques which train multiple learners individually and use a combined model lead to an increase in performance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24]</ref>. Many high-ranked approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6]</ref> in the recent Google landmark retrieval challenge <ref type="bibr">[2]</ref> and Zehang et al. <ref type="bibr" target="#b30">[31]</ref> boost the performance by combining different global descriptors which are trained individually. However, explicitly training multiple learners for ensemble could lead to longer training time and higher memory consumption. In order to handle this problem, other ensemble approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref> attempt to train a retrieval model in an end-to-end manner. These approaches can be tricky as they need specifically designed strategy or loss to control diversity among learners, which also cause a more laborious training process.</p><p>In this paper, we focus on how to exploit multiple global descriptors to get an ensemble effect without explicitly training multiple learners and controlling diversity among learners. Our contribution is threefold. <ref type="bibr" target="#b1">(1)</ref> We propose a novel framework, the combination of multiple global descriptors (CGD), that combines multiple global descriptors which can be trained in an end-to-end manner. It achieves an ensemble effect without any explicit ensemble model or diversity control over each global descriptor. Moreover, the proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. To the best of our knowledge, we are the first to leverage multiple types of global descriptors to get a final descriptor in the image retrieval task. (2) We investigate the effect of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments demonstrate that  <ref type="figure">Figure 1</ref>. The combination of multiple global descriptors (CGD) framework. The framework is described with ResNet-50 backbone where Stage 3 downsampling is removed. From the last feature map, each of n global descriptor branch outputs a k-dimensional embedding vector, which is concatenated into the combined descriptor for ranking loss. Exclusively the first global descriptor is used for auxiliary classification loss where M denotes the number of classes.</p><p>using combined descriptor outperforms a single global descriptor because it can use different types of feature properties.</p><p>(3) The proposed framework achieves the state-ofthe-art performance on CARS196 <ref type="bibr" target="#b25">[26]</ref>, CUB200-2011 <ref type="bibr" target="#b53">[54]</ref> (CUB200), Stanford Online Products <ref type="bibr" target="#b38">[39]</ref> (SOP) and Inshop Clothes <ref type="bibr" target="#b31">[32]</ref> (In-shop) by a large margin on image retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In the recent works for image retrieval task, global descriptors based on deep CNNs have been used as off-theshelf feature <ref type="bibr" target="#b46">[47,</ref><ref type="bibr">3]</ref> over the conventional hand-crafted features such as SIFT <ref type="bibr" target="#b33">[34]</ref>. SPoC [3] is sum pooling from the feature map which performs well mainly due to the subsequent descriptor whitening. MAC <ref type="bibr" target="#b52">[53]</ref> by max pooling is another powerful descriptor, while regional MAC <ref type="bibr" target="#b52">[53]</ref> performs max pooling over regions, then sum over the regional MAC descriptors at the end. GeM <ref type="bibr" target="#b42">[43]</ref> generalizes max and average pooling with a pooling parameter. Other global descriptor method includes weighted sum pooling <ref type="bibr" target="#b21">[22]</ref>, weighted GeM <ref type="bibr" target="#b56">[57]</ref>, multiscale RMAC <ref type="bibr" target="#b29">[30]</ref>, etc. Some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref> attempt using the additional strategy or the attention mechanism to maximize the activations of essential features on the feature map. Dai et al. <ref type="bibr" target="#b8">[9]</ref> present a strategy called batch feature erasing (BFE) to force the network to optimize the feature representation of different regions. Li et al. <ref type="bibr" target="#b27">[28]</ref> propose a model that has soft pixel attention and hard regional attention along with simultaneous optimization of feature representations. The downsides of adopting the additional strategy or the attention mechanism are that it can not only lead to an increase network size and training time, but also require additional parameters for training. However, our proposed framework does not need any additional strategy or attention mechanism when it requires only a few additional parameters for training.</p><p>The ensemble is a well-known technique that aims to boost performance by training multiple learners and obtains a combined result from the trained learners. In the last decades, it is widely used in image retrieval tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b30">31]</ref>. Xuan et al. <ref type="bibr" target="#b57">[58]</ref> propose a method where each embedding function is learned by randomly bagging and training labels into small subsets. Kim et al. <ref type="bibr" target="#b23">[24]</ref> suggest an attention-based ensemble, where single feature embedding function is trained while each learner learns different attention modules. The downside of ensemble techniques is that it leads to an increase in computational cost as the model complexity increases <ref type="bibr" target="#b61">[62]</ref>, and requires additional control to yield diversity between learners <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. However, our proposed framework takes advantage of the idea of the ensemble technique when it can be trained in an end-to-end manner with no diversity control.</p><p>Our proposed framework is depicted in <ref type="figure">Figure 1</ref>.</p><p>The proposed framework consists of a CNN backbone network and two modules. The first module is the main module that learns an image representation, which is a combination of multiple global descriptors for a ranking loss. Next, is an auxiliary module to fine-tune a CNN with a classification loss. The proposed framework is trained with a final loss, which is the sum of the ranking loss from the main module and the classification loss from the auxiliary module in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Network</head><p>Our proposed framework can use any CNN backbones such as BN-Inception <ref type="bibr" target="#b20">[21]</ref>, ShuffleNet-v2 <ref type="bibr" target="#b34">[35]</ref>, ResNet <ref type="bibr" target="#b16">[17]</ref> and its variants, etc, while we use ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as a baseline backbone described in <ref type="figure">Figure 1</ref>. To preserve more information in the last feature map, we modify the network by discarding the down-sampling operation between Stage 3 and Stage 4 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55]</ref>. This modification gives a 14×14 sized feature map at the end for input size of 224 × 224, which improves the performance by containing richer information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main Module: Multiple Global Descriptors</head><p>The main module has multiple branches that output each image representation by using different global descriptors on the last convolutional layer. In this paper, we use three types of the most representative global descriptors on each branch, including SPoC, MAC, and GeM.</p><p>Given an image I, the output of the last convolutional layer is a 3D tensor X of C × H × W dimension, where C is the number of feature maps. Let X c be the set of H × W activations for feature maps c ∈ {1 . . . C}. The network output consists of C channels of such 2D feature maps. Global descriptor takes X as input and produces a vector f as output by pooling process. Such pooling methods can be generalized as follows:</p><formula xml:id="formula_0">f = [f 1 . . . f c . . . f C ] , f c = ( 1 |X c | x∈X c x p c ) 1 p c . (1)</formula><p>We define SPoC as f (s) when p c = 1, MAC as f (m) when p c → ∞, and GeM as f (m) for the rest of the cases. For the case of GeM, the parameter p c can be manually set or trained because it is differentiable, while we use fixed p c parameter 3 throughout the experiments. Output feature vector Φ (ai) from the i-th branch is generated by dimensionality reduction through the FC layer and normalization through the l 2 -normalization layer:</p><formula xml:id="formula_1">Φ (ai) = W (i) ·f (a i ) W (i) ·f (a i ) 2 , a i ∈ {s, m, g},<label>(2)</label></formula><p>for i ∈ {1 . . . n}, where n is the number of branches, W i is the weight of the FC layer and the global descriptor f (ai)</p><p>can be SPoC when a i = s, MAC when a i = m, or GeM for</p><formula xml:id="formula_2">a i = g.</formula><p>The final feature vector referred to as combined descriptor ψ CGD of our framework combines output feature vectors of multiple branches and performs l 2 -normalization sequentially:</p><formula xml:id="formula_3">ψ CGD = Φ (a 1 ) ⊕...⊕Φ (a i ) ⊕...⊕Φ (a n ) Φ (a 1 ) ⊕...⊕Φ (a i ) ⊕...⊕Φ (a n ) 2 , (3)</formula><p>for a i ∈ {s, m, g}, where ⊕ denotes concatenation. This combined descriptor can be trained with any ranking loss, while we use batch-hard triplet loss <ref type="bibr" target="#b17">[18]</ref> as a representative.</p><p>In the proposed framework, there are two advantages to combining multiple global descriptors. First, it gives an ensemble effect with only a few additional parameters. To get the ensemble effect while making it trainable in an end-to-end manner, our framework extracts and combines multiple global descriptors within a single CNN backbone. Second, it automatically provides different properties for each branch's output without any diversity control. While <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref> propose specially designed losses to encourage diversity among learners, our framework does not require any specially designed loss to control diversity among branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Auxiliary Module: Classification Loss</head><p>The auxiliary module fine-tunes the CNN backbone based on the first global descriptor of the main module by using a classification loss. It is motivated by the approach <ref type="bibr" target="#b13">[14]</ref>, which consists of two steps: training a CNN backbone with a classification loss and then fine-tuning the network with a triplet loss. However, we refine their approach to have a single step for end-to-end training, while <ref type="bibr" target="#b13">[14]</ref> has to be trained with two steps. Training with auxiliary classification loss helps to maximize inter-class distance which makes the model to train faster and stable.</p><p>Temperature scaling <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref> in softmax cross-entropy loss (softmax loss), and label smoothing [51] are proven to be helpful for the training process. The softmax loss is defined as</p><formula xml:id="formula_4">L Sof tmax = − 1 N N i=1 log exp ((W T y i fi+by i )/τ ) M j=1 exp ((W T j fi+bj )/τ ) ,<label>(4)</label></formula><p>where N , M , and y i are the batch size, the number of classes, and the corresponding identity label of i-th input, respectively. W , and b are trainable weight, and bias, respectively. f is a global descriptor from the first branch, where τ is a temperature parameter with default value 1.</p><p>The temperature scaling with low-temperature parameter τ in the Equation 4, assigns a larger gradient to more challenging examples and is helpful for intra-class compact, and inter-class spread-out embedding. The label smoothing enhances a model, thereby improves generalization by estimating the marginalized effect of a label-dropout during training. Therefore, to prevent over-fitting, and learn better embedding, we add label smoothing and temperature scaling in the auxiliary classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Configurations of Framework</head><p>Configurations Our proposed framework is expandable by the number of global descriptor branches, and it allows different types of networks according to the configuration of global descriptors. As we use SPoC (S), MAC (M), GeM (G), and exclusively the first global descriptor is used for the auxiliary classification loss, we can make twelve possible configurations. First letter in a notation is the first global descriptor to be used for the auxiliary classification loss. For example with a configuration SMG, the first letter which is the first global descriptor S will be used for the auxiliary classification loss and all S, M, and G are concatenated to be combined descriptor for ranking loss. Therefore, the twelve configurations are obtained as follows: S, M, G, SM, MS, SG, GS, MG, GM, SMG, MSG, GSM. How to Choose the Best As each global descriptor has different properties, the performance of each descriptor can vary by datasets <ref type="bibr" target="#b4">[5]</ref>. In order to find the best configuration, we evaluate every single descriptor and choose the highest and the second-highest single descriptors to use them for combination. The number of global descriptors to combine has to be determined by the size of output dimensionality. For a small output dimensionality, a small number of descriptors is recommended. This rule to choose the best configuration is shown with an experiment below in Section 4.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Efficiency of Time and Memory</head><p>Compared to previous methods of feature ensemble <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref>, our proposed framework has better efficiency in terms of time and memory. Because each learner of an ensemble method needs individual training and inference, ensembling N number of learners with different global descriptors requires N number of GPUs, and it requires post-processing step such as concatenation or normaliza- For CUB200 and CARS196, cropped images with bounding box information are used. We follow the same training and test split as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b59">60]</ref> for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>All experiments are implemented using MXNet [8] on a Tesla P40 GPU with 24 GB memory. We use BN-Inception <ref type="bibr" target="#b20">[21]</ref>, ShuffleNet-v2 <ref type="bibr" target="#b34">[35]</ref>, ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, SE-ResNet-50 <ref type="bibr" target="#b19">[20]</ref> with ImageNet ILSVRC-2012 <ref type="bibr" target="#b9">[10]</ref> pretrained weights from MXNet GluonCV <ref type="bibr" target="#b1">[1]</ref>. For every experiment, we use the input size of 224 × 224 and the 1536dimensional embedding, unless otherwise noted in the experiment. In the training phase, the input image is resized to 252 × 252, cropped randomly to 224 × 224, and then flipped randomly to the horizontal. We use an Adam <ref type="bibr" target="#b24">[25]</ref> optimizer with a learning rate of 1e-4, and a step decay is used for scheduling the learning rate. A margin of m for triplet loss is 0.1, and a temperature of τ for softmax loss is 0.5, with a batch size of 128 for every experiment. In the inference phase, we only resize the image by the default input size of 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments for Architecture Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Training Classification and Ranking Loss Jointly</head><p>Auxiliary Classification Loss Our proposed framework is trained by a ranking loss with an auxiliary classification loss from a descriptor of the first branch. We compare the performance between using the ranking loss exclusively, and the ranking loss with the auxiliary classification loss on CARS196 in the <ref type="table">Table 1</ref>. In this experiment, we do not apply label smoothing, and temperature scaling on the auxiliary classification loss in every case. It shows that using both losses provides higher performance than using ranking loss exclusively. Classification loss focuses on cluster-  ing each class into a close embedding space on a categorical level. Ranking loss focuses on gathering samples in the same class and making a distance between samples from the different classes in the instance level. Therefore, training the ranking loss with the auxiliary classification loss jointly gives better optimization for categorical, and fine-grained feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing and Temperature Scaling</head><p>As mentioned in Section 3.3, label smoothing, and temperature scaling are proven to be helpful to learn better embedding for the classification loss. We investigate if it can be applied when a model is trained with both the ranking loss and the auxiliary classification loss. We show the performance appraisal of the 'no tricks', the label smoothing, the temperature scaling with temperature term 0.5, and 'both tricks' on the auxiliary classification loss in <ref type="table">Table 2</ref>. The experiment is performed on the ResNet-50 <ref type="bibr" target="#b16">[17]</ref> backbone with the configuration SM. It shows that each label smoothing and temperature scaling improves the performance compared to the 'no tricks'. Moreover, applying 'both tricks' together stacks up each performance boost, and gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Combining Multiple Global Descriptors</head><p>Position of Combination As our proposed framework uses multiple global descriptors, we perform experiments with different positions of a combination of multiple global descriptors to choose the best architecture. Architecture type A in the <ref type="figure" target="#fig_1">Figure 2a</ref> trains each global descriptor with individual ranking loss, and then combines them at the inference phase as in <ref type="bibr" target="#b23">[24]</ref>, while they use the same global descriptor for every branch and do not use classification loss. Architecture type B in the <ref type="figure" target="#fig_2">Figure 2b</ref> combines the raw output of global descriptors and train it with a single ranking loss, similar to studies of <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>, while they do not use multiple global descriptors. Also, our proposed framework combines multiple global descriptors after the FC layers and l2-normalization as described in <ref type="figure">Figure 1</ref>. As shown in Table 3, the proposed position of the combination presents the best performance over the architecture type A and type B. The reason is that CGD can maintain properties and diversities of each feature vector from multiple branches. In contrast, the final embedding of type A in the training phase is different from that of the inference phase, and the final embedding of type B loses each property of the global descriptors because they are mixed up by FC layer after concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method of Combination</head><p>In terms of the combination method, concatenation, and summation of multiple descriptors are proven to enhance performance in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b8">9]</ref>. Therefore, we compare two combination methods to choose the best. As shown in <ref type="table">Table 4</ref>, concatenation of multiple global descriptors gives better performance compared to their summation. This also indicates the importance of preserving each property, and diversity from multiple global descriptors, as the summation mix activations of each global descriptor up, while the concatenation maintains them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effectiveness of Combined Descriptor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Quantitative Analysis</head><p>The core of our proposed framework is exploiting multiple global descriptors. As we defined in Section 3.4, we conduct experiments with twelve possible configurations on each image retrieval dataset. In the <ref type="figure" target="#fig_5">Figure 3</ref>, majorities of combined descriptors outperform over than single global descriptors. Moreover, the best configuration is a combination of the highest and the second-highest single descriptors, which we will use this pattern to find the best configuration, as mentioned in Section 3.4. While the performance of each descriptor is varied by the properties of datasets, the main   essence is that exploiting multiple global descriptors gives performance boost compared to single global descriptors. <ref type="table">Table 5</ref> shows the performance of individual global descriptors before combining operation and how much performance gain they can produce after the operation. Every combined descriptor have 1536-dimensional embedding vector, while individual descriptor has 1536-dimensional embedding vector for S, M, G, 768-dimensional embedding vector for SM, MS, SG, GS, MG, GM, and 512-dimensional embedding vector for SMG, MSG, GSM. Having a larger embedding dimension usually gives better performances. However, if the performance difference is not much between a large embedding and a small embedding, it may be preferable to use multiple small embeddings from different global descriptors. For example, as individual descriptor GeM from SG with 768 embedding dimensions has similar performance with a single descriptor G with 1536 embedding dimensions, SG gets a significant performance boost by combining different features of SPoC, and GeM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Qualitative Analysis</head><p>A visualization tool proposed in <ref type="bibr" target="#b48">[49]</ref> highlights the regions of images that contribute the most to pairwise similarity. We modify this work for our framework to see how much each region of an image contributes to the similarity for each final embedding. <ref type="figure" target="#fig_7">Figure 4</ref> shows a visualization of topmost (Re-call@1) retrieved image of each configuration on the same query.</p><p>As mentioned in <ref type="bibr" target="#b48">[49]</ref>, the regions of similarity are large in the configuration S, while the configuration M has more  focused regions of similarity. The configuration SM seems to have the similarity regions mixed with the configuration S, and M. SPoC tends to see the overall information when it lacks discriminability because they average the high activated outputs by non-active outputs <ref type="bibr" target="#b18">[19]</ref>. MAC is preferable to retain the high activation when it is only powerful for sparse features <ref type="bibr" target="#b4">[5]</ref>. However, the configuration SM, which has both properties of SPoC, and MAC seems to keep the overall information, and also retain the discriminative regions. This property of the configuration SM pushes up the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Flexibility of CGD Framework</head><p>Ranking Loss <ref type="table" target="#tab_3">Table 6</ref> shows that CGD framework can use various ranking losses, such as soft-margin or batchhard triplet loss <ref type="bibr" target="#b17">[18]</ref>, HAP2S loss <ref type="bibr" target="#b58">[59]</ref>, and weighted sampling margin loss <ref type="bibr" target="#b55">[56]</ref>. We compare the performance of the configuration S as a baseline for single global descriptor, and SM for multiple global descriptors using these losses. Coefficient α for HAP2S P loss is set to 10, coefficient σ for HAP2S E loss is set to 0.5, and margin α and boundary β for margin loss are fixed at 0.1 and 1.2, respectively. In every case, the performance of the configuration SM is better than S, which shows that our framework is flexible in applying various losses. Backbone Our framework can use different types of CNN backbone. We perform experiments on image retrieval datasets with various CNN backbone: BN-Inception <ref type="bibr" target="#b20">[21]</ref>, ShuffleNet-v2 <ref type="bibr" target="#b34">[35]</ref>, ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, and SE-ResNet-50 <ref type="bibr" target="#b19">[20]</ref>. In <ref type="table">Table 7a</ref> and   <ref type="table">Table 7</ref>. Performance comparisons with previous state-of-the-art approaches on image retrieval datasets. For better comparison, values with the same color (purple, blue, green, red) have the same backbone, and embedding dimension (Dim), while bold text indicates the best performance within the same color. † denotes 256 input size for inference phase, while the rest use 224 input size. ‡ refers to non-conventional usage. approaches on four image retrieval datasets in <ref type="table">Table 7a</ref> and <ref type="table" target="#tab_4">Table 7b</ref>. To make a fair comparison, we put an experimental result using the same CNN backbone, input size, and output dimension with other approaches. As each dataset has different properties, we choose the best performing configuration with ResNet-50 on each dataset by following the aforementioned rule in Section 3.4 and perform other experiments with the same configuration. Even though BFE <ref type="bibr" target="#b8">[9]</ref> uses a 256 input size, and ours uses a 224 input size, the CGD framework gets higher performance on every dataset. Overall, the CGD framework outperforms all the major benchmarks in the image retrieval tasks with a high margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have introduced a simple but powerful framework called CGD for image retrieval. The CGD framework exploits multiple global descriptors to get an ensemble effect when it can be trained in an end-to-end manner. Moreover, the proposed framework is flexible and expandable by global descriptors, CNN backbones, losses, and datasets. We analyze the effectiveness of combined descriptor quantitatively and qualitatively. Our extensive experiments show that exploiting multiple global descriptors lead to higher performance over the single global descriptor because combined descriptor can manipulate different types of feature properties. Our framework performs the best on all the major image retrieval benchmarks considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fully connected layer -BN: Batch normalization layer -l2: l2-normalization layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>Architecture type A. (b) Architecture type B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Different architecture types for training multiple global descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Recall@1 (%) on CARS196. (b) Recall@1 (%) on CUB200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(c) Recall@1 (%) on Stanford Online Products.(d) Recall@1 (%) on In-shop Clothes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Performance of different configurations of our proposed framework. For the faster experiments on SOP, we use a mini-test set by sampling a hundred instances per class. Due to the uncertainty of deep learning model, we report results over ten runs with box plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Model S Sedan 2012 (SM_R1, Rank: 1, Cossim: 0.8812) (b) Infiniti G Coupe IPL 2012 (S_R1, Rank: 19, Cossim: 0.8481) (c) Jaguar XK XKR 2012 (M_R1, Rank: 87, Cossim: 0.8287) (d) Infiniti G Coupe IPL 2012 (S_R1, Rank: 1, Cossim: 0.8287) (e) Tesla Model S Sedan 2012 (SM_R1, Rank: 165, Cossim: 0.7627) (f) Jaguar XK XKR 2012 (M_R1, Rank: 468, Cossim: 0.7332) (g) Jaguar XK XKR 2012 (M_R1, Rank: 1, Cossim: 0.8485) (h) Tesla Model S Sedan 2012 (SM_R1, Rank: 26, Cossim: 0.8296) (i) Infiniti G Coupe IPL 2012 (S_R1, Rank: 102, Cossim: 0.8118) Tesla Model S Sedan 2012</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Spatial similarity visualizations on CARS196. Heatmaps show the contribution of each region toward pairwise similarity computation. For each configuration SM, S, and M, we visualize the top 1 retrieved image (a), (d), and (g), respectively, from the same query image. These images are denoted by SM R1, S R1, and M R1. They are used to visualize on different configurations (b, c, e, f, h, i) so that we can see the rank changes among the configurations. The green box indicates that the image is in the same class, while the red box indicates that the image is in a different class. "Cossim" denotes cosine similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 4eat the rank of 165 in the configuration S and Figure 4h at the rank of 26 in the configuration M into the rank of 1 as Figure 4a. Following this experiment, combining multiple global descriptors allows the use of different properties of the global descriptors that can help to compute similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(b) Recall@K (%) on Stanford Online Products and In-shop Clothes. CGD (SG/GS) denotes that the configuration SG is used for Stanford Online Products and GS is used for In-shop Clothes on the proposed CGD framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Rank 86.7 ± 0.3 92.1 ± 0.3 95.3 ± 0.2 97.3 ± 0.1 Both 93.1 ± 0.1 96.0 ± 0.2 97.4 ± 0.2 98.3 ± 0.2 Recall@K ± std. dev. comparison between using only the ranking loss (Rank) and using both the classification and ranking losses (Both) on CARS196. We report results over five runs. ± 0.1 96.4 ± 0.2 97.8 ± 0.1 98.7 ± 0.1 Both 94.4 ± 0.2 96.8 ± 0.0 98.0 ± 0.0 98.8 ± 0.1 Recall@K ± std. dev. among the baseline 'no tricks' (None), label smoothing (LS), temperature scaling (TS), and 'both tricks' (Both) on CARS196. We report results over five runs.</figDesc><table><row><cell>Loss</cell><cell>1</cell><cell>Recall@K (%) 2 4</cell><cell>8</cell></row><row><cell>Trick</cell><cell>1</cell><cell>Recall@K (%) 2 4</cell><cell>8</cell></row><row><cell cols="4">None 93.1 ± 0.1 96.0 ± 0.2 97.4 ± 0.2 98.3 ± 0.2 LS 93.5 ± 0.2 96.1 ± 0.1 97.5 ± 0.1 98.4 ± 0.1 TS 94.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>± 0.4 83.5 ± 0.4 89.8 ± 0.3 94.0 ± 0.2 B 73.7 ± 0.3 82.6 ± 0.3 89.2 ± 0.2 93.5 ± 0.2 CGD 75.3 ± 0.5 83.9 ± 0.3 89.9 ± 0.3 94.0 ± 0.3 Recall@K ± std. dev. among architecture type A, type B, and the proposed framework with the configuration SM on CUB200-2011. We report results over five runs. Recall@K ± std. dev. comparison by combination method with the configuration SM on CUB200-2011. We report results over five runs.</figDesc><table><row><cell>Type</cell><cell>1</cell><cell>Recall@K (%) 2 4</cell><cell>8</cell></row><row><cell>A 74.6 Comb.</cell><cell>1</cell><cell>Recall@K (%) 2 4</cell><cell>8</cell></row><row><cell cols="4">Sum Concat 75.3 ± 0.5 83.9 ± 0.3 89.9 ± 0.3 94.0 ± 0.3 73.8 ± 0.5 82.9 ± 0.4 89.4 ± 0.3 93.7 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Recall@K ± std. dev. of the single global descriptor S as a baseline and the combined descriptor SM with various ranking losses on CARS196. † denotes the batch-hard triplet, and ‡ denotes the soft-margin hard triplet. We report results over five runs.</figDesc><table><row><cell>Config. + Loss</cell><cell>1</cell><cell>Recall@K (%) 2 4</cell><cell>8</cell></row><row><cell cols="4">S + Triplet  † SM + Triplet  † S + Triplet  ‡ SM + Triplet  ‡ S + HAP2S E SM + HAP2S E 94.6 ± 0.1 97.0 ± 0.1 98.2 ± 0.1 98.9 ± 0.0 93.8 ± 0.2 96.5 ± 0.1 97.8 ± 0.1 98.7 ± 0.1 94.3 ± 0.2 96.8 ± 0.2 98.1 ± 0.1 98.9 ± 0.1 89.5 ± 0.1 94.2 ± 0.1 96.7 ± 0.1 98.2 ± 0.1 90.4 ± 0.2 94.8 ± 0.2 97.1 ± 0.1 98.5 ± 0.0 93.8 ± 0.3 96.6 ± 0.1 98.0 ± 0.1 98.8 ± 0.1 S + HAP2S P 94.4 ± 0.1 96.9 ± 0.1 98.2 ± 0.1 98.9 ± 0.0 SM + HAP2S P 95.0 ± 0.1 97.2 ± 0.1 98.2 ± 0.1 98.9 ± 0.1 S + Margin 92.8 ± 0.2 95.7 ± 0.1 97.3 ± 0.1 98.3 ± 0.1 SM + Margin 93.9 ± 0.2 96.4 ± 0.1 97.7 ± 0.1 98.6 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7b</head><label>7b</label><figDesc>, each value of the same color indicates the same CNN backbone and embedding dimension, which demonstrates that the CGD framework outperforms existing models with the same backbone. Additional experiments with ShuffleNet-v2 presents a reasonable performance even though it is a compact network. Other experiments with SE-ResNet-50 provide the best performance among all as the backbone is very powerful.92.2 96.8 99.1 88.4 97.2 98.1 98.4 98.7 98.8 CGD (SG/GS) ResNet-50 ‡ 1536 83.9 93.8 97.5 99.2 90.9 98.0 98.7 99.0 99.1 99.2 CGD (SG/GS) ShuffleNet-v2 1536 78.7 90.9 96.1 98.8 86.1 96.9 97.8 98.4 98.6 98.7 CGD (SG/GS) SE-ResNet-50 ‡ 1536 84.2 93.9 97.4 99.2 91.9 98.1 98.7 99.0 99.1 99.3</figDesc><table><row><cell>Model</cell><cell cols="2">Backbone</cell><cell>Dim</cell><cell cols="2">1</cell><cell cols="2">CUB200 2 4</cell><cell>8</cell><cell>1</cell><cell>CARS196 2 4</cell><cell>8</cell></row><row><cell cols="3">Facility [38] BN-Inception Model Backbone Dim</cell><cell>1</cell><cell>10</cell><cell cols="3">SOP 100 1000</cell><cell>1</cell><cell>10</cell><cell>In-shop 20 30</cell><cell>40</cell><cell>50</cell></row><row><cell>Facility [38]</cell><cell>BN-Inception</cell><cell>64</cell><cell cols="4">67.0 83.7 93.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HTL [11]</cell><cell>BN-Inception</cell><cell cols="6">512 74.8 88.3 94.8 98.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HTL [11]</cell><cell>BN-Inception</cell><cell>128</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="3">80.9 94.3 95.8 97.2 97.4 97.8</cell></row><row><cell>Margin [56]</cell><cell>ResNet-50</cell><cell cols="6">128 72.7 86.2 93.8 98.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ABE-8 [24]</cell><cell>GoogleNet  ‡</cell><cell cols="9">512 76.3 88.4 94.8 98.2 87.3 96.7 97.9 98.2 98.5 98.7</cell></row><row><cell>BFE  † [9]</cell><cell>ResNet-50  ‡</cell><cell cols="9">1536 83.0 93.3 97.3 99.2 89.1 96.3 97.6 98.5 99.1</cell><cell>-</cell></row><row><cell>CGD (SG/GS)</cell><cell>BN-Inception</cell><cell>64</cell><cell cols="8">75.6 89.0 95.5 98.6 86.6 96.3 97.4 97.9 98.2 98.4</cell></row><row><cell>CGD (SG/ -)</cell><cell>BN-Inception</cell><cell cols="6">512 80.5 92.1 96.7 98.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CGD ( -/GS)</cell><cell>BN-Inception</cell><cell>128</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="3">88.5 97.1 98.0 98.5 98.8 98.9</cell></row><row><cell>CGD (SG/GS)</cell><cell>ResNet-50</cell><cell cols="2">128 81.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Dataset: Comparison with State-of-the-Art Finally, we</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">compare our proposed framework with the state-of-the-art</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">tion. Our proposed method needs only one GPU independently of the number of global descriptors without any postprocessing step because of a shared backbone. Given limited memory resources, training and inference of a model in an end-to-end manner is beneficial in terms of time and memory.4. Experiments4.1. DatasetsWe evaluate our proposed framework on image retrieval datasets including CUB200-2011<ref type="bibr" target="#b53">[54]</ref>, CARS196<ref type="bibr" target="#b25">[26]</ref>, Stanford Online Products<ref type="bibr" target="#b38">[39]</ref>, and In-shop Clothes<ref type="bibr" target="#b31">[32]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CGD (MG/SG) denotes that the configuration MG is used for CUB200-2011 and SG is used for CARS196 on the proposed CGD framework</title>
	</analytic>
	<monogr>
		<title level="m">Recall@K (%) on CUB200-2011 (cropped) and CARS196 (cropped)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gluoncv: a deep learning toolkit for computer vision</title>
		<ptr target="https://gluon-cv.mxnet.io.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregating local deep features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">2nd place and 2nd place solution to kaggle landmark recognition andretrieval competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03990</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch feature erasing for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07130</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Attention-aware generalized mean pooling for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selective deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-K. Le</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1600" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crossdimensional weighting for aggregated deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pca-sift: A more distinctive representation for local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pairwise geometric matching for large-scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5153" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ms-rmac: Multiscale regional maximum activation of convolutions for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="613" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2073" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a fine vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikulík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient model averaging for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bierboosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large-scale landmark retrieval/recognition under a noisy and diverse dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yokoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04087</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning feature embedding with strong neural activations for fine-grained retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing deep similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Visual-based product retrieval with multi-task learning and self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image search with selective match kernels: aggregation across single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05879</idno>
		<title level="m">Particular object retrieval with integral max-pooling of cnn activations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weighted generalized mean pooling for deep image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="495" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hardaware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Making classification competitive for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12649</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Heated-up softmax embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04157</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Binary ensemble neural network: More bits per network or more networks per bit?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07550</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
