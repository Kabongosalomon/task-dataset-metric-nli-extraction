<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multivariate Time Series Classi cation with WEASEL+MUSE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07">2016. July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt University of Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Humboldt University of Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multivariate Time Series Classi cation with WEASEL+MUSE</title>
					</analytic>
					<monogr>
						<title level="m">ACM Reference format: Patrick Schäfer and Ulf Leser</title>
						<meeting> <address><addrLine>Washington, DC, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">11</biblScope>
							<date type="published" when="2016-07">2016. July 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time series</term>
					<term>multivariate</term>
					<term>classi cation</term>
					<term>feature selection</term>
					<term>bag-of- pa erns</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series (MTS) arise when multiple interconnected sensors record data over time. Dealing with this high-dimensional data is challenging for every classi er for at least two aspects: First, an MTS is not only characterized by individual feature values, but also by the interplay of features in di erent dimensions. Second, this typically adds large amounts of irrelevant data and noise. We present our novel MTS classi er WEASEL+MUSE which addresses both challenges. WEASEL+MUSE builds a multivariate feature vector, rst using a sliding-window approach applied to each dimension of the MTS, then extracts discrete features per window and dimension. e feature vector is subsequently fed through feature selection, removing non-discriminative features, and analysed by a machine learning classi er. e novelty of WEASEL+MUSE lies in its speci c way of extracting and ltering multivariate features from MTS by encoding context information into each feature. Still the resulting feature set is small, yet very discriminative and useful for MTS classi cation. Based on a popular benchmark of 20 MTS datasets, we found that WEASEL+MUSE is among the most accurate classi ers, when compared to the state of the art. e outstanding robustness of WEASEL+MUSE is further con rmed based on motion gesture recognition data, where it out-of-the-box achieved similar accuracies as domain-speci c methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A time series (TS) is a collection of values sequentially ordered in time. TS emerge in many scienti c and commercial applications, like weather observations, wind energy forecasting, industry automation, mobility tracking, etc. <ref type="bibr" target="#b27">[28]</ref> One driving force behind their rising importance is the sharply increasing use of heterogeneous sensors for automatic and high-resolution monitoring in Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org.   domains like smart homes <ref type="bibr" target="#b9">[10]</ref>, machine surveillance <ref type="bibr" target="#b15">[16]</ref>, or smart grids <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. A multivariate time series (MTS) arises when multiple interconnected streams of data are recorded over time. ese are typically produced by devices with multiple (heterogeneous) sensors like weather observations (humidity, temperature), Earth movement (3 axis), or satellite images (in di erent spectra).</p><p>In this work we study the problem of multivariate time series classi cation (MTSC). Given a concrete MTS, the task of MTSC is to determine which of a set of prede ned classes this MTS belongs to, e.g., labeling a sign language gesture based on a set of prede ned gestures. e high dimensionality introduced by multiple streams of sensors is very challenging for classi ers, as MTS are not only described by individual features but also by their interplay/cooccurrence in di erent dimensions <ref type="bibr" target="#b2">[3]</ref>.</p><p>As a concrete example, consider the problem of gesture recognition of di erent users performing isolated gestures ( <ref type="figure" target="#fig_1">Figure 1</ref>). e dataset was recorded using 8 sensors recording x/y/z coordinates at the le /right hand, le /right elbow, le /right wrist and le /right thumb (24 dimensions in total). e data is high dimensional and characterized by long idle periods with small bursts of characteristic movements in every dimension. Here, the exact time instant of an event, e.g., thumbs up, is irrelevant for classi cation. To e ectively deal with this kind of information, an MTSC has to deal with noise, irrelevant dimension data, and, most importantly, extract relevant features from each dimension.</p><p>In this paper, we introduce our novel domain agnostic MTSC method called WEASEL+MUSE (WEASEL plus Multivariate Unsupervised Symbols and dErivatives). WEASEL+MUSE conceptually builds on the bag-of-pa erns (BOP) model and the WEASEL (Word ExtrAction for time SEries cLassi cation) pipeline for feature selection. e BOP model moves a sliding window over an MTS, extracts discrete features per window, and creates a histogram over feature counts.</p><p>ese histograms are subsequently fed into a machine learning classi er. However, the concrete way of constructing and ltering features in WEASEL+MUSE is di erent from state-of-theart multivariate classi ers:</p><p>(1) Identi ers: WEASEL+MUSE adds a dimension (sensor) identi er to each extracted discrete feature. ereby WEASEL+MUSE can discriminate between the presence of features in di erent dimensions -i.e., a le vs. right hand was raised.</p><p>(2) Derivatives: To improve the accuracy, derivatives are added as features to the MTS. ose are the di erences between neighbouring data points in each dimension. ese derivatives represent the general shape and are invariant to the exact value at a given time stamp. is information can help to increase classi cation accuracy. (3) Noise robust: WEASEL+MUSE derives discrete features from windows extracted from each dimension of the MTS using a truncated Fourier transform and discretization, thereby reducing noise. (4) Interplay of features: e interplay of features along the dimensions is learned by assigning weights to features (using logistic regression), thereby boosting or dampening feature counts. Essentially, when two features from di erent dimensions are characteristic for the class label, these get assigned high weights, and their co-occurrence increases the likelihood of a class. (5) Order invariance: A main advantage of the BOP model is its invariance to the order of the subsequences, as a result of using histograms over feature counts. us, two MTS are similar, if they show a similar number of feature occurrences rather than having the same values at the same time instances. (6) Feature selection: e wide range of features considered by WEASEL+MUSE (dimensions, derivatives, unigrams, bigrams, and varying window lengths) introduces many non-discriminative features. erefore, WEASEL+MUSE applies statistical feature selection and feature weighting to identify those features that best discern between classes. e aim of our feature selection is to prune the feature space to a level that feature weighting can be learned in reasonable time.</p><p>In our experimental evaluation on 20 public benchmark MTS datasets and a use case on motion capture data, WEASEL+MUSE   <ref type="bibr" target="#b22">[23]</ref>).</p><p>is constantly among the most accurate methods. WEASEL+MUSE clearly outperforms all other classi ers except the very recent deeplearning-based method from <ref type="bibr" target="#b10">[11]</ref>. Compared to the la er, WM performs be er for small-sized datasets with less features or samples to use for training, such as sensor readings. e rest of this paper is organized as follows: Section 2 brie y recaps bag-of-pa erns classi ers and de nitions. In Section 3 we present related work. In Section 4 we present WEASEL+MUSE's novel way of feature generation and selection. Section 5 presents evaluation results and Section 6 our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: TIME SERIES AND BAG-OF-PATTERNS</head><p>A univariate time series (TS) T = {t 1 , . . . , t n } is an ordered sequence of n ∈ N real values t i ∈ R. A multivariate time series (MTS) T = {t 1 , . . . , t m } is an ordered sequence of m ∈ N streams (dimensions) with t i = (t i,1 , . . . , t i,n ) ∈ R n . For instance, a stream of m interconnected sensors recording values at each time instant. As we primarily address MTS generated from automatic sensors with a xed and synchronized sampling along all dimensions, we can safely ignore time stamps. A time series dataset D contains N time series. Note, that we consider only MTS with numerical a ributes (not categorical). e derivative of a stream t i = (t i,1 , . . . , t i,n ) is given by the sequence of pairwise di erences t i = (|t i,2 − t i,1 |, . . . , |t i,n − t i,n−1 |). Adding derivatives to an MTS T = {t 1 , . . . , t m } of m streams, e ectively doubles the number of streams: T = {t 1 , . . . , t m , t 1 , . . . , t m }.</p><p>Given a univariate TS T , a window S of length w is a subsequence with w contiguous values starting at o set a in T , i.e., S(a, w) = (t a , . . . , t a+w −1 ) with 1 ≤ a ≤ n − w + 1.</p><p>We associate each TS with a class label ∈ Y from a prede ned set of labels Y . Time series classi cation (TSC) is the task of predicting a class label for a TS whose label is unknown. A TS classi er is a function that is learned from a set of labelled time series (the training data), that takes an unlabelled time series as input and outputs a label.</p><p>Our method is based on the bag-of-pa erns (BOP) model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Algorithms following the BOP model build a classi cation function by (1) extracting subsequences from a TS, (2) discretizing each real valued subsequence into a discrete-valued word (a sequence of symbols over a xed alphabet), (3) building a histogram (feature vector) from word counts, and (4) nally using a classi cation model from the machine learning repertoire on these feature vectors. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates these steps from a raw time series to a BOP model using overlapping windows. Overlapping subsequences of xed length are extracted from a time series (second from top), each subsequences is discretized to a word (second from bo om), and nally a histogram is built over the word counts. Di erent discretization functions have been used in literature, including SAX <ref type="bibr" target="#b12">[13]</ref> and SFA <ref type="bibr" target="#b20">[21]</ref>. SAX is based on the discretization of mean values and SFA is based on the discretization of coe cients of the Fourier transform.</p><p>In the BOP model, two TS are similar, if the subsequences have similar frequencies in both TS. Feature selection and weighting can be used to damper of emphasize important subsequences, like in the WEASEL model <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Research in univariate TSC has a long tradition and dozens of approaches have been proposed, refer to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> for summary. e techniques used for TSC can broadly be categorized into two classes: (a) similarity-based (distance-based) methods and (b) feature-based methods.</p><p>Similarity-based methods make use of a similarity measure like Dynamic Time Warping (DTW) <ref type="bibr" target="#b17">[18]</ref> to compare two TS. 1-Nearest Neighbour DTW is commonly used as a baseline in TSC comparisons <ref type="bibr" target="#b1">[2]</ref>. In contrast, feature-based TSC rely on comparing features, typically generated from substructures of a TS. e most successful approaches are shapelets or bag-of-pa erns (BOP). Shapelets are de ned as TS subsequences that are maximally representative of a class <ref type="bibr" target="#b28">[29]</ref>. e standard BOP model <ref type="bibr" target="#b13">[14]</ref> breaks up a TS into windows, represent these as discrete features, and nally build a histogram of feature counts as basis for classi cation.</p><p>In previous research we have studied the BOP model for univariate TSC. e BOSS (Bag-of-SFA-Symbols) <ref type="bibr" target="#b19">[20]</ref> classi er is based on the (unsupervised) Symbolic Fourier Approximation (SFA) <ref type="bibr" target="#b20">[21]</ref> to generate discrete features and uses a similarity measure on the histogram of feature counts. e WEASEL classi er <ref type="bibr" target="#b22">[23]</ref> applies a supervised symbolic representation to transform subsequences to words, uses statistical feature selection, and subsequently feeds the words into a logistic regression classi er. WEASEL is among the most accurate and fastest univariate TSC <ref type="bibr" target="#b22">[23]</ref>. WEASEL was optimized to extract discriminative words to ease classi cation of univariate TS. We observed that this led to an overall low accuracy for MTSC due to the increased number of possible features along all dimensions (see <ref type="bibr">Section 5)</ref>. WEASEL+MUSE was designed on the WEASEL pipeline, but adding sensor identi ers to each word, generating unsupervised discrete features to minimize over ing, as opposed to WEASEL that uses a supervised transformation. WEASEL+MUSE further adds derivatives (di erences between all neighbouring points) to the feature space to increase accuracy.</p><p>For multivariate time series classi cation (MTSC), the most basic approach is to apply rigid dimensionality reduction (i.e., PCA) or simply concatenate all dimensions of the MTS to obtain a univariate TS and use proven univariate TSC. Some domain agnostic MTSC have been proposed.</p><p>Symbolic Representation for Multivariate Time series (SMTS) <ref type="bibr" target="#b2">[3]</ref> uses codebook learning and the bag-of-words (BOW) model for classi cation. First, a random forest is trained on the raw MTS to partition the MTS into leaf nodes. Each leaf node is then labelled by a word of a codebook. ere is no additional feature extraction, apart from calculating derivatives for the numerical dimensions ( rst order di erences). For classi cation a second random forest is trained on the BOW representation of all MTS.</p><p>Ultra Fast Shapelets (UFS) <ref type="bibr" target="#b26">[27]</ref> applies the shapelet discovery method to MTS classi cation. e major limiting factor for shapelet discovery is the time to nd discriminative subsequences, which becomes even more demanding when dealing with MTS. UFS solves this by extracting random shapelets. On this transformed data, a linear SVM or a Random Forest is trained. Unfortunately, the code is not available to allow for reproducibility Generalized Random Shapelet Forests (gRSF) <ref type="bibr" target="#b11">[12]</ref> also generates a set of shapelet-based decision trees over randomly extracted shapelets. In their experimental evaluation, gRSF was the best MTSC when compared to SMTS, LPS and UFS on 14 MTS datasets.</p><p>us, we use gRFS as a representative for random shapelets. Learned Pa ern Similarity (LPS) <ref type="bibr" target="#b3">[4]</ref> extracts segments from an MTS. It then trains regression trees to identify structural dependencies between segments. e regression trees trained in this manner represent a non-linear AR model. LPS next builds a BOW representation based on the labels of the leaf nodes similar to SMTS. Finally a similarity measure is de ned on the BOW representations of the MTS. LPS showed be er performance than DTW in a benchmark using 15 MTS datasets. Autoregressive (AR) Kernel <ref type="bibr" target="#b4">[5]</ref> proposes an AR kernel-based distance measure for MTSC.</p><p>Autoregressive forests for multivariate time series modelling (mv-ARF) <ref type="bibr" target="#b24">[25]</ref> proposes a tree ensemble trained on autoregressive models, each one with a di erent lag, of the MTS. is model is used to capture linear and non-linear relationships between features in the dimensions of an MTS. e authors compared mv-ARF to AR Kernel, LPS and DTW on 19 MTS datasets. mv-ARF and AR kernel showed the best results. mv-ARF performs well on motion recognition data. AR kernel outperformed the other methods for sensor readings.</p><p>At the time of writing this paper, Multivariate LSTM-FCN <ref type="bibr" target="#b10">[11]</ref> was proposed that introduces a deep learning architecture based on a long short-term memory (LSTM), a fully convolutional network (FCN) and a squeeze and excitation block. eir method is compared to state-of-the-art and shows the overall best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEASEL+MUSE</head><p>We present our novel method for domain agnostic multivariate time series classi cation (MTSC) called WEASEL+MUSE (WEASEL+Multivariate Unsupervised Symbols and dErivatives). WEASEL+MUSE addresses the major challenges of MTSC in a speci c manner (using gesture recognition as an example):</p><p>( We engineered WEASEL+MUSE to address these challenges. Our method conceptually builds on our previous work on the bagof-pa erns (BOP) model and univariate TSC <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, yet uses a di erent approach in many of the individual steps to deal with the aforementioned challenges. We will use the terms feature and word interchangeably throughout the text. In essence, WEASEL+MUSE makes use of a histogram of feature counts. In this feature vector it captures information about local and global changes in the MTS along di erent dimensions. It then learns weights to boost or damper characteristic features. e interplay of features is represented by high weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We rst give an overview of our basic idea and an example how we deal with the challenges described above. In WEASEL+MUSE a feature is represented by a word that encodes the identi ers (sensor id, window size, and discretized Fourier coe cients) and counts its occurrences. <ref type="figure" target="#fig_4">Figure 3</ref> shows an example for the WEASEL+MUSE model of a xed window length 15 on motion capture data. e data has 3 dimensions (x,y,z coordinates). A feature ( 3 15 ad , 2) (see <ref type="figure" target="#fig_4">Figure 3</ref> (b)) represents a unigram 'ad' for the z-dimension with window length 15 and frequency 2, or the feature ( 2 15 bd ad , 2) represents a bigram 'bd ad' for the y-dimension with length 15 and frequency 2.</p><p>Pipeline: WEASEL+MUSE is composed of the building blocks depicted in <ref type="figure" target="#fig_5">Figure 4</ref>: the symbolic representation SFA <ref type="bibr" target="#b20">[21]</ref>, BOP models for each dimension, feature selection and the WEASEL+MUSE model. WEASEL+MUSE conceptionally builds upon the univariate BOP model applied to each dimension. Multivariate words are obtained from the univariate words of each BOP model by concatenating each word with an identi er (representing the sensor and the window size). is maintains the association between the dimension and the feature space.</p><p>More precisely, an MTS is rst split into its dimensions. Each dimension can now be considered as a univariate TS and transformed using the classical BOP approach. To this end, z-normalized windows of varying lengths are extracted. Next, each window is approximated using the truncated Fourier transform, keeping only lower frequency components of each window. Fourier values (real and imaginary part separately) are then discretized into words based on equi-depth or equi-frequency binning using a symbolic transformation (details will be given in Subsection 4.2). ereby, words (unigrams) and pairs of words (bigrams) with varying window lengths are computed. ese words are concatenated with their identi ers, i.e., the sensor id (dimension) and the window length.</p><p>us, WEASEL+MUSE keeps a disjoint word space for each dimension and two words from di erent dimensions can never coincide. To deal with irrelevant features and dimensions, a Chi-squared test is applied to all multivariate words (Subsection 4.4). As a result, a highly discriminative feature vector is obtained and a fast linear time logistic regression classi er can be trained (Subsection 4.4). It further captures the interplay of features in di erent dimensions by learning high weights for important features in each dimension (Subsection 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Extraction: Symbolic Fourier Approximation</head><p>Instead of training a multivariate symbolic transformation, we train and apply the univariate symbolic transformation SFA to each dimension of the MTS separately. is allows for (a) phase invariance between di erent dimensions, as a separate BOP model is built for each dimension, but (b) the information that two features occurred at exactly the same time instant in two di erent dimensions is lost. Semantically, spli ing an MTS into its dimensions results in two MTS T 1 and T 2 to be similar, if both share similar substructures within the i-th dimension at arbitrary time stamps. SFA transforms a real-valued TS window to a word using an alphabet of size c as in <ref type="bibr" target="#b20">[21]</ref>:</p><p>(1) Approximation: Each normalized window of length w is subjected to dimensionality reduction by the use of the truncated Fourier transform, keeping only the rst l w coe cients for further analysis. is step acts as a low pass (noise) lter, as higher order Fourier coe cients typically represent rapid changes like drop-outs or noise. <ref type="bibr" target="#b1">(2)</ref> antization: Each Fourier coe cient is then discretized to a symbol of an alphabet of xed size c, which in turn achieves further robustness against noise. SFA is a data-adaptive symbolic transformation, as opposed to SAX <ref type="bibr" target="#b12">[13]</ref>    the data distribution.</p><p>antization boundaries are derived from a (sampled) train dataset using either (a) equi-depth or (b) equifrequency binning, such that (a) the Fourier frequency range is divided into equal-sized bins or (b) the boundaries are chosen to hold an equal number of Fourier values. SFA is trained for each dimension separately, resulting in m SFA transformations. Each SFA transformation is then used to transform only its dimension of the MTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Univariate Bag-of-Patterns: Unigrams, bigrams, derivatives, window lengths</head><p>In the BOP model, two TS are distinguished by the frequencies of certain subsequences rather than their presence or absence. A TS is represented by word counts, obtained from the windows of the time series. BOP-based methods have a number of parameters, and of particular importance is the window length, which heavily in uences its performance. For dealing with MTS, we have to nd the best window lengths for each dimension, as one cannot assume that there is a single optimal value for all dimensions. WEASEL+MUSE addresses this issue by building a large feature space using multiple window lengths, the MTS dimensions, unigrams, bigrams, and derivatives. is very large feature space is aggressively reduced in a second separate step 4.4. e feature set of WEASEL+MUSE, given an MTS T = (t 1 , . . . , t m ) is composed of (see also Section 4.4):</p><p>(1) Derivatives: Derivatives are added to the MTS. ese are the di erences between all neighbouring points in one dimension (see <ref type="bibr">Section 2)</ref>. is captures information about how much a signal changes in time. It has been shown that this additional information can improve the accuracy <ref type="bibr" target="#b2">[3]</ref>. We show the utility of derivatives in Section 5.6. (2) Local and Global Substructures: For each possible window lengths w ∈ [4..len(t i )], windows are extracted from the dimensions and the derivatives, and each window is transformed to a word using the SFA transformation. is helps to capture both local and global pa erns in an MTS. (3) Unigrams and Bigrams: Once we have extracted all words (unigrams), we enrich this feature space with cooccurrences of words (bigrams). It has been shown in <ref type="bibr" target="#b22">[23]</ref> that the usage of bigrams reduces the order-invariance of the BOP model. We could include m-grams, but the feature space grow polynomial in the m-gram number, such that it is infeasible to use anything larger than bigrams (resulting in O(n 2 ) features). (4) Identi ers: Each word is concatenated with it's sensor id and window size (see <ref type="figure" target="#fig_4">Figure 3</ref>). It is rather meaningless to compare features from di erent sensors: if a temperature sensor measures 10 and a humidity sensor measures 10, these capture totally di erent concepts. To distinguish between sensors, the features are appended with sensor ids. e.g., (temp: 10) and (humid: 10). However, both measurements can be important for classi cation. us, we add them to the same feature vector and use feature selection and feature weights to identify the important ones. Pseudocode: Algorithm 1 illustrates WEASEL+MUSE: sliding windows of length w are extracted in each dimension (line 7). We empirically set the window lengths to all values in <ref type="bibr">[4, . . . , n]</ref>. Smaller values are possible, but the feature space can become untraceable, and small window lengths are basically meaningless for TS of length &gt; 10 3 . e SFA transformation is applied to each realvalued sliding window (line <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15)</ref>. Each word is concatenated with the window length and dimension identi er, and its occurrence is counted (line 12,17). Lines 15-17 illustrate the use of bigrams: the preceding sliding window is concatenated with the current window. Note, that all words (each dimension, unigrams, bigrams, each window-length) are joined within a single bag-of-pa erns. Finally irrelevant words are removed from this bag-of-pa erns using the Chi-squared test (line 20). e target SFA length l is learned through cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Feature Selection and Weighing:</head><p>Chi-squared Test and Logistic Regression WEASEL+MUSE applies the Chi-squared (χ 2 ) test to identify the most relevant features, only features passing a certain threshold are kept to reduce this feature space prior to training the classi er. We set the threshold such that it is high enough for the logistic regression classi er to train a model in reasonable time (and when set too low, training takes longer). If a feature is irrelevant but not removed due to the χ 2 -test, it will get assigned a low weight by the logistic regression classi er. It would be possible to use di erent feature selection methods. As our main aim is to reduce the runtime for training, we did not look into other feature selection techniques. For a set of N m-dimensional MTS of length n, the size of the BOP feature space is O(min(Nn 2 , c l ) × m) for word length l, c symbols and m dimensions. e number of MTS N and length n a ects the actual word frequencies. But in the worst case each TS window can only produce one distinct word, and there are Nn 2 windows in each dimension. WEASEL+MUSE further uses bigrams, derivatives, and O(n) window lengths. WEASEL+MUSE keeps a disjoint word space for each dimension and window lengths, thus two words from di erent dimensions can never collide (no false positives).</p><p>us, the theoretical dimensionality of this feature space rises to O(min[N n 2 , c 2l · n] × m). Essentially, the feature space can grow quadratically with the number of observations of an MTS, if every observation generates a distinct word. However, in practice we never observed that many features due to the periodicity of TS or super uous data/dimensions. Statistical feature selection reduces the total number of features to just a few hundred features.</p><p>We use sparse vectors to store the words for each MTS, as each feature vector only contains a few features a er feature selection. We implemented our MTS classi er using liblinear <ref type="bibr" target="#b7">[8]</ref> as it scales linearly with the dimensionality of the feature space <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Feature Interplay</head><p>e WEASEL+MUSE model is essentially a histogram of discrete features extracted from all dimensions. e logistic regression classi er trains for each class a weight vector, to assign high weights to those features that are relevant within each dimension. ereby, it captures the feature interplay, as dimensions are not treated separately but the weight vector is trained over all dimensions. Still, this approach allows for phase-invariance as classes (events) are represented by the frequency of occurrence of discrete features rather than the exact time instance of an event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Experimental Setup</head><p>• Datasets: We evaluated our WEASEL+MUSE classi er using 20 publicly available MTS dataset listed in <ref type="table" target="#tab_2">Table 1</ref>. Furthermore, we compared its performance on a real-life dataset taken from the motion capture domain; results are reported in Section 5.7. Each MTS dataset provides a train and test split set which we use unchanged to make our results comparable to prior publications. • Competitors: We compare WEASEL+MUSE to the 7 domain agnostic state-of-the-art MTSC methods we are aware of ARKernel <ref type="bibr" target="#b4">[5]</ref>, LPS <ref type="bibr" target="#b3">[4]</ref>, mv-ARF <ref type="bibr" target="#b24">[25]</ref>, SMTS <ref type="bibr" target="#b2">[3]</ref>, gRSF <ref type="bibr" target="#b11">[12]</ref>, MLSTM-FCN <ref type="bibr" target="#b10">[11]</ref>, and the common baseline Dynamic Time Warping independent (DTWi), implemented as the sum of DTW distances in each dimension with a full warping window. We use the reported test accuracies on the MTS datasets given by the authors in their publications, thereby avoiding any bias in training se ings parameters. All reported numbers in our experiments correspond to the accuracy on the test split. We were not able to reproduce the published results for MLSTM-FCN using their code. e authors told us that this is due to random seeding and #classes m n N Train N Test  <ref type="table" target="#tab_2">Table 1</ref>: 20 multivariate time series datasets collected from <ref type="bibr" target="#b14">[15]</ref>.</p><p>their results are based on a single run. Instead, we report the median over 5 runs using their published code <ref type="bibr" target="#b10">[11]</ref>. For SMTS and gRSF, we additionally ran their code on the missing 5 and 7 datasets. e code for UFS is not available, thus we did not include it into the experiments. e webpage 1 lists state-of-the-art univariate TSC. However, we cannot use univariate TSC to classify multivariate datasets.</p><p>• Server: e experiments were carried out on a server running LINUX with 2xIntel Xeon E5-2630v3 and 64GB RAM, using JAVA JDK x64 1.8. • Training WEASEL+MUSE: For WEASEL+MUSE we performed 10-fold cross-validation on the train datasets to nd the most appropriate parameters for the SFA word lengths l ∈ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> and SFA quantization method equidepth or equi-frequency binning. All other parameters are constant: chi = 2, as we observed that varying these values has only a negligible e ect on the accuracy. We used liblinear with default parameters (bias = 1, p = 0.1, c = 5 and solver L2R LR DUAL). To ensure reproducible results, we provide the WEASEL+MUSE source code and the raw measurement sheets <ref type="bibr" target="#b25">[26]</ref>.  <ref type="figure">Figure 6</ref>: Average ranks on the 20 MTS datasets. WEASEL+MUSE and MLSTM-FCN are the most accurate. <ref type="figure">Figure 6</ref> shows a critical di erence diagram (as introduced in [6]) over the average ranks of the di erent MTSC methods. Classi ers with the lowest (best) ranks are to the right. e group of classi ers that are not signi cantly di erent in their rankings are connected by a bar. e critical di erence (CD) length at the top represents statistically signi cant di erences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy</head><p>MLSTM-FCN and WEASEL+MUSE show the lowest overall ranks and are in the group of best classi ers.</p><p>ese are also signicantly be er than the baseline DTWi. When compared to the plain WEASEL classi er, we can see that the MUSE extension to WEASEL also leads to signi cantly be er ranks (6.05 vs 2.45). is is a result of using feature identi ers and using derivatives.</p><p>Overall, WEASEL+MUSE has 12 wins (or ties) on the MTS datasets ( <ref type="table" target="#tab_5">Table 2)</ref>, which is the highest of all classi ers. With a mean of 93.5% it shows a similar average accuracy as MLSTN-FCN with mean accuracy 92.1%.</p><p>In the next section we look into the di erences between MLSTM-FCN and WEASEL+MUSE and identify the domains for which each classi er is best suited for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Domain-dependent strength or limitation</head><p>We studied the individual accuracy of each method on the 20 di erent MTS datasets, and grouped datasets by domain (Handwriting, Motion Sensors, Sensor Readings, Speech) to test if our method has a domain-dependent strength or limitation. <ref type="figure" target="#fig_8">Figure 7</ref> shows the accuracies of WEASEL+MUSE (orange line), MLSTM-FCN (black line) vs. the other six MTS classi ers (green area).</p><p>Overall, the performance of WEASEL+MUSE is very competitive for all datasets. e black line is mostly very close to the upper outline of the orange area, indicating that WEASEL+MUSE's performance is close to that of its best competitor in many cases. In total WEASEL+MUSE has 12 out of 20 possible wins (or ties). WEASEL+MUSE has the highest percentage of wins in the groups of motion sensors, followed by speech and handwriting.</p><p>WEASEL+MUSE and MLSTM-FCN perform similar on many dataset domain. WEASEL+MUSE performs best for sensor reading datasets and MLSTM-FCN performs best for motion and speech datasets. Sensor readings are the datasets with the least number  is might indicate that WEASEL+MUSE performs well, even for small-sized datasets, whereas MLSTM-FCN seems to require larger training corpora to be most accurate. Furthermore, WEASEL+MUSE is based on the BOP model that compares signal based on the frequency of occurrence of subsequences rather than their absence or presence. us, signals with some repetition pro t from using WEASEL+MUSE, such as ECG-signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">E ects of Gaussian Noise on Classi cation Accuracy</head><p>WEASEL+MUSE applies the truncated Fourier Transform and discretization to generate features. is acts as a low-pass lter. To illustrate the relevance of noise to the classi cation task, we performed another experiment on the two multivariate synthetic datasets Shapes and DigitShapes.</p><p>First, all dimensions of each dataset were z-normalised to have a standard deviation (SD) of 1. We then added an increasing Gaussian noise with a SD of 0 to 1.0 to each dimension, equal to noise levels of 0% to 100%. <ref type="figure">Figure 8</ref> shows the two classi ers DTWi and WEASEL+MUSE. For DTWi the classi cation accuracy drops by up to 30 percentage points for increasing levels of noise. At the same time, WEASEL+MUSE was robust to Gaussian noise and its accuracy remains stable up to a noise level of 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Relative Prediction Times</head><p>In addition to achieving state-of-the-art accuracy, WEASEL+MUSE is also competitive in terms of prediction times. In this experiment, we compare WEASEL+MUSE to DTWi. We could not perform a meaningful comparison to the other competitors, as we either do not have the source codes or the implementation is given in a di erent language (R, Matlab).</p><p>In general, 1-NN DTW has a computational complexity of O(Nn 2 ) for TS of length n. For the implementation of DTWi we make use of the state-of-the-art cascading lower bounds from <ref type="bibr" target="#b17">[18]</ref>. In this experiment, we measure CPU time to address parallel and single threaded codes. e DTWi prediction times is reported relative to that of WEASEL+MUSE, i.e., a number lower than 1 means that DTW is faster than WEASEL+MUSE.  the highest number of dimensions m = 62. us, WEASEL+MUSE is not only signi cantly more accurate then DTWi but also orders of magnitude faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">In uence of Design Decisions on Accuracy</head><p>We next look into the impact of several design decisions of the WEASEL+MUSE classi er. <ref type="figure" target="#fig_9">Figure 9</ref> shows the average ranks of the WEASEL+MUSE classi er on the 20 MTS datasets where each of the following extension is disabled or enabled:</p><p>(1) Multivariate vs Univariate A key design decision of WEASEL+MUSE is to keep sensor ids for each word. e opposite is to treat all dimensions equally, i.e., concatenate CD  all dimensionality information and treat the data like a univariate TS classi cation problem.</p><p>(2) Derivatives vs raw TS: Following <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b2">[3]</ref>, we have added derivatives for each dimension to add trend information. e univariate without derivatives approach is in concept similar to WEASEL (without MUSE). ere is a big gap between the multivariate and univariate models of WEASEL+MUSE. e univariate approaches are the least accurate, as the association of features to sensors is lost. e use of derivatives results in a slightly be er score. Both extensions (multivariate and derivatives) combined improve ranks the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Use Case: Motion Capture Data (Kinect)</head><p>is real world dataset was part of a challenge for gesture recognition <ref type="bibr" target="#b0">[1]</ref> and represents isolated gestures of di erent users captured by a Kinect camera system. e task was to predict the gestures performed by the users. e dataset consists of 180 labelled train and 180 test MTS. e labels on the test set were not revealed. A total of 8 sensors were used to record x,y,z coordinates with a total of 51 time instances, i.e., an MTS with 24 streams of 51 values each. e sensors are placed at the le /right hand, le /right elbow, le /right wrist and le /right thumb (see <ref type="figure" target="#fig_1">Figure 1</ref> for an example gesture).</p><p>WEASEL+MUSE (alias MWSL) scored 171 correct predictions, which is equal to a test accuracy of 95%. e winning approach scored 173 (96.1%), based on an ensemble of random shapelets and domain speci c feature extraction, and the SMTS <ref type="bibr" target="#b2">[3]</ref> classi er scored 172 (95.6%). is challenge underlined that WEASEL+MUSE is applicable out-of-the-box to real-world use cases and competitive with domain-speci c tailored approaches. Motion captured data is characterized by noisy data, that contains many super uous information among dimensions. By design WEASEL+MUSE is able to deal with this kind of data e ectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have presented WEASEL+MUSE, a novel multivariate time series classi cation method following the bag-of-pa ern approach and achieving highly competitive classi cation accuracies. e novelty of WEASEL+MUSE is its feature space engineering using statistical feature selection, derivatives, variable window lengths, bi-grams, and a symbolic representation for generating discriminative words. WEASEL+MUSE provides tolerance to noise (by use of the truncated Fourier transform), phase invariance, and super uous data/dimensions. ereby, WEASEL+MUSE assigns high weights to characteristic, local and global substructures along dimensions of a multivariate time series. In our evaluation on altogether 21 datasets, WEASEL+MUSE is consistently among the most accurate classi ers and outperforms state-of-the-art similarity measures or shapelet-based approaches. It performs well even for small-sized datasets, where deep learning based approaches typically tend to perform poorly. When looking into application domains, it is best for sensor readings, followed by speech, motion and handwriting recognition tasks.</p><p>Future work could direct into di erent feature selection methods, benchmarking approaches based on train and prediction times, or use ensembling to build more powerful classi ers, which has been successfully used for univariate time series classi cation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Conference'17, Washington, DC, USA © 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Motion data recorded from 8 sensors recording x/y/z coordinates (indicated by di erent line styles) at the le /right hand, le /right elbow, le /right wrist and le -/right thumb (indicated by di erent colours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Transformation of a TS into the Bag-of-Patterns (BOP) model using overlapping windows (second to top), discretization of windows to words (second from bottom), and word counts (bottom) (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>exempli es this process for a univariate time series, resulting in the word ABDDABBB. As a result, each real-valued window in the i-th dimension is transformed into a word of length l with an alphabet of size c. For a given window length, there are a maximum of O(n) windows in each of the m dimensions, resulting in a total of O(n × m) words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>WEASEL+MUSE model of a motion capture. (a) motion of a le hand in x/y/z coordinates. (b) the WEASEL+MUSE model for each of these coordinates. A feature in the WEASEL+MUSE model encodes the dimension, window length and actual word, e.g., 1 15 aa for 'le Hand', window length 15 and word 'aa'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>WEASEL+MUSE Pipeline: Feature extraction, univariate Bag-of-Patterns (BOP) models and WEASEL+MUSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Approximation (SFA): A time series (le ) is approximated using the truncated Fourier transform (centre) and discretized to the word AB-DDABBB (right) with the four-letter alphabet ('a' to 'd'). e inverse transform is depicted by an orange area (right), representing the tolerance for all signals that will be mapped to the same word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1</head><label>1</label><figDesc>Build one BOP model using SFA, multiple window lengths, bigrams and the Chi-squared test for feature selection. l is the number of Fourier values to keep and wLen are the window lengths used for sliding window extraction.1 f u n c t i o n WEASEL MUSE ( mts , l , wLen ) 2 bag = empty B a g O f P a t t e r n 3 / / e x t r a c t from e a c h d i m e n s i o n 4 f o r each dimId in mts : 5 / / u s e m u l t i p l e window l e n g h t s 6 f o r each window− l e n g t h w in wLen : 7 f o r each ( prevWindow , window ) in SLIDING WINDOWS ( mts [ dimId ] , wLen ) : 8 9 / / BOP c o m p u t e d from u n i g r a m s 10 word = SFA ( window , l ) 11 unigram = c o n c a t ( dim , w , word ) 12 bag [ unigram ] . i n c r e a s e C o u n t s ( ) 13 14 / / BOP c o m p u t e d from b i g r a m s 15 prevWord =SFA ( prevWindow , l ) 16 bigram = c o n c a t ( dim , w , prevWord , word ) 17 bag [ bigram ] . i n c r e a s e C o u n t s ( ) 18 19 / / f e a t u r e s e l e c t i o n u s i n g C h i S q u a r e d 20 r et u r n CHI SQUARED FILTER ( bag )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Classi cation accuracies on the 20 MTS datasets for WEASEL+MUSE (orange), MLSTM-FCN (black) vs six state-ofthe-art MTSC. e green area represents the classi ers' accuracies. of samples N or features n in the range of a few dozens. On the other hand, speech and motion datasets contain the most samples or features in the range of hundreds to thousands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Impact of the design decisions of the WEASEL+MUSE classi er on accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Bag-of-Patterns model</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">0 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5</cell><cell></cell><cell></cell><cell cols="2">200</cell><cell></cell><cell></cell><cell></cell><cell cols="2">400</cell><cell cols="2">600 Sample</cell><cell>800</cell><cell>1000</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(1) Windowing</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell cols="3">......</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>......</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0</cell><cell></cell><cell></cell><cell cols="2">200</cell><cell></cell><cell></cell><cell></cell><cell cols="2">400</cell><cell></cell><cell>600</cell><cell>800</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell>bcc</cell><cell>ccc</cell><cell cols="2">bcb</cell><cell cols="2">bcb</cell><cell cols="6">bbb (2) Discretization bab cac ddd bdb aab</cell><cell>bac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell>bcc</cell><cell>ccc</cell><cell></cell><cell>bcb</cell><cell cols="2">bcb</cell><cell>bbb</cell><cell></cell><cell cols="2">bab</cell><cell cols="2">cac</cell><cell>ddc</cell><cell>bdc</cell><cell>bab</cell><cell>bac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell>bcc</cell><cell cols="2">bcc</cell><cell>bcb</cell><cell></cell><cell>ccc</cell><cell cols="2">abb</cell><cell cols="3">cac</cell><cell>cab</cell><cell>cdc</cell><cell>bdc</cell><cell>bab</cell><cell>bac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell>bcc</cell><cell cols="2">bcb</cell><cell cols="2">bcb</cell><cell>ccc</cell><cell cols="3">abb</cell><cell cols="2">cac</cell><cell>cab</cell><cell>cdb</cell><cell>bdc</cell><cell>bab</cell><cell>bac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell cols="2">bcc</cell><cell>bcb</cell><cell cols="2">bcb</cell><cell cols="2">ccc</cell><cell cols="2">abb</cell><cell></cell><cell>cac</cell><cell>cab</cell><cell>bda</cell><cell>bdc</cell><cell>bab</cell><cell>cac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell cols="2">bcc</cell><cell cols="2">bcb</cell><cell>bcb</cell><cell cols="2">ccc</cell><cell></cell><cell>abb</cell><cell></cell><cell>cac</cell><cell>cac</cell><cell>bda</cell><cell>bdc</cell><cell>bab</cell><cell>cac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bcc</cell><cell cols="2">bcb</cell><cell cols="2">bcb</cell><cell>ccc</cell><cell></cell><cell>abb</cell><cell></cell><cell cols="2">cac</cell><cell>dbc</cell><cell>bda</cell><cell>adb</cell><cell>bab</cell><cell>cac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bcc</cell><cell></cell><cell>bcb</cell><cell cols="2">bcb</cell><cell>ccc</cell><cell></cell><cell cols="2">abb</cell><cell cols="2">cac</cell><cell>dbd</cell><cell>bda</cell><cell>ada</cell><cell>bac</cell><cell>cac</cell><cell>ccc</cell><cell>bdb</cell></row><row><cell></cell><cell cols="2">0</cell><cell cols="2">...</cell><cell cols="2">200 ...</cell><cell>...</cell><cell cols="2">...</cell><cell cols="3">400 ...</cell><cell>...</cell><cell>600 ...</cell><cell>...</cell><cell>...</cell><cell>800 ...</cell><cell>...</cell><cell>1000 ... ...</cell></row><row><cell>Counts</cell><cell>0 20 40 60 80 100 120 140</cell><cell cols="12">aaa aab aba abb abc aca acb acc ada adb adc baa bab bac bba bbb bbc bcb bcc bda bdb bdc bdd cab cac cba cbb cbc ccb ccc cda cdb cdc cdd dab dac dad dbb dbc dbd dcb dcc dcd ddc ddd (3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 )</head><label>1</label><figDesc>Interplay of dimensions: MTS are not only characterized by individual features at a single time instance, but also by the interplay of features in di erent dimensions. For example, to predict a hand gesture, a complex orchestration of interactions between hand, nger and elbow may have to be considered. (2) Phase invariance: Relevant events in an MTS do not necessarily reappear at the same time instances in each dimension. us, characteristic features may appear anywhere in an MTS (or not at all). For example, a hand gesture should allow for considerable di erences in time schedule. (3) Invariance to irrelevant dimensions: Only small periods in time and in some streams may contain relevant information for classi cation. What makes things even harder is the fact that whole sensor streams may be irrelevant for classi cation. For instance, a movement of a leg is irrelevant to capture hand gestures and vice versa.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>which always uses the same set of bins irrelevant of</figDesc><table><row><cell></cell><cell>(a) Raw Time Series</cell><cell></cell><cell></cell><cell>2</cell><cell>(b) WEASEL+MUSE words per dimension</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell>Counts</cell><cell></cell><cell></cell></row><row><cell>0 2</cell><cell>10 1. Hand tip left, X coordinate 20 30 3. Hand tip left, Z coordinate 2. Hand tip left, Y coordinate</cell><cell>40</cell><cell>50</cell><cell>0</cell><cell>1-15-aa 1-15-aa ba 1-15-ab 1-15-ab aa 1-15-ad 1-15-ad aa 1-15-ad ab 1-15-ba 1-15-ba ad 1-15-ba bb 1-15-ba bc 1-15-ba bd 1-15-ba ca 1-15-bb 1-15-bb bc 1-15-bc 1-15-bc ad 1-15-bc bb 1-15-bd 1-15-bd ad 1-15-ca 1-15-ca ba 1-15-ca cc 1-15-cb 1-15-cb da 1-15-cc 1-15-cc dd 1-15-cd 1-15-cd cb 1-15-da 1-15-da ca 1-15-da db 1-15-db 1-15-db ca 1-15-db da 1-15-dd 1-15-dd db</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Counts</cell><cell>2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2-15-aa 2-15-aa ba 2-15-ab 2-15-ab aa 2-15-ad 2-15-ad aa 2-15-ad ab 2-15-ba 2-15-ba ad 2-15-ba bb 2-15-ba bc 2-15-ba bd 2-15-ba ca 2-15-bb 2-15-bb bc 2-15-bc 2-15-bc ad 2-15-bc bb 2-15-bd 2-15-bd ad 2-15-ca 2-15-ca ba 2-15-ca cc 2-15-cb 2-15-cb da 2-15-cc 2-15-cc dd 2-15-cd 2-15-cd cb 2-15-da 2-15-da ca 2-15-da db 2-15-db 2-15-db ca 2-15-db da 2-15-dd 2-15-dd db</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Accuracies for each dataset. e best approaches are highlighted using a bold font.</figDesc><table><row><cell cols="3">Dataset</cell><cell></cell><cell></cell><cell>SMTS</cell><cell></cell><cell>LPS</cell><cell></cell><cell>mvARF</cell><cell>DTWi</cell><cell>ARKernel</cell><cell>gRSF</cell><cell>MLSTMFCN</cell><cell>MUSE</cell></row><row><cell cols="3">ArabicDigits</cell><cell></cell><cell></cell><cell>96.4%</cell><cell></cell><cell>97.1%</cell><cell></cell><cell>95.2%</cell><cell>90.8%</cell><cell>98.8%</cell><cell>97.5%</cell><cell>99.0%</cell><cell>99.2%</cell></row><row><cell cols="3">AUSLAN</cell><cell></cell><cell></cell><cell>94.7%</cell><cell></cell><cell>75.4%</cell><cell></cell><cell>93.4%</cell><cell>72.7%</cell><cell>91.8%</cell><cell>95.5%</cell><cell>95.0%</cell><cell>97%</cell></row><row><cell cols="4">CharTrajectories</cell><cell></cell><cell>99.2%</cell><cell></cell><cell>96.5%</cell><cell></cell><cell>92.8%</cell><cell>94.8%</cell><cell>90%</cell><cell>99.4%</cell><cell>99.0%</cell><cell>97.3%</cell></row><row><cell cols="3">CMUsubject16</cell><cell></cell><cell></cell><cell>99.7%</cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>93%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell></cell><cell>ECG</cell><cell></cell><cell></cell><cell></cell><cell>81.8%</cell><cell></cell><cell>82%</cell><cell></cell><cell>78.5%</cell><cell>79%</cell><cell>82%</cell><cell>88%</cell><cell>87%</cell><cell>88%</cell></row><row><cell cols="4">JapaneseVowels</cell><cell></cell><cell>96.9%</cell><cell></cell><cell>95.1%</cell><cell></cell><cell>95.9%</cell><cell>96.2%</cell><cell>98.4%</cell><cell>80%</cell><cell>100%</cell><cell>97.6%</cell></row><row><cell cols="3">KickvsPunch</cell><cell></cell><cell></cell><cell>82%</cell><cell></cell><cell>90%</cell><cell></cell><cell>97.6%</cell><cell>60%</cell><cell>92.7%</cell><cell>100%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell cols="2">Libras</cell><cell></cell><cell></cell><cell>90.9%</cell><cell></cell><cell>90.3%</cell><cell></cell><cell>94.5%</cell><cell>88.8%</cell><cell>95.2%</cell><cell>91.1%</cell><cell>97%</cell><cell>89.4%</cell></row><row><cell cols="3">NetFlow</cell><cell></cell><cell></cell><cell>97.7%</cell><cell></cell><cell>96.8%</cell><cell></cell><cell>NaN</cell><cell>97.6%</cell><cell>NaN</cell><cell>91.4%</cell><cell>95%</cell><cell>96.1%</cell></row><row><cell></cell><cell cols="2">UWave</cell><cell></cell><cell></cell><cell>94.1%</cell><cell></cell><cell>98%</cell><cell></cell><cell>95.2%</cell><cell>91.6%</cell><cell>90.4%</cell><cell>92.9%</cell><cell>97%</cell><cell>91.6%</cell></row><row><cell></cell><cell cols="2">Wafer</cell><cell></cell><cell></cell><cell>96.5%</cell><cell></cell><cell>96.2%</cell><cell></cell><cell>93.1%</cell><cell>97.4%</cell><cell>96.8%</cell><cell>99.2%</cell><cell>99%</cell><cell>99.7%</cell></row><row><cell cols="3">WalkvsRun</cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell></cell><cell>LP1</cell><cell></cell><cell></cell><cell></cell><cell>85.6%</cell><cell></cell><cell>86.2%</cell><cell></cell><cell>82.4%</cell><cell>76%</cell><cell>86%</cell><cell>84%</cell><cell>80%</cell><cell>94%</cell></row><row><cell></cell><cell>LP2</cell><cell></cell><cell></cell><cell></cell><cell>76%</cell><cell></cell><cell>70.4%</cell><cell></cell><cell>62.6%</cell><cell>70%</cell><cell>63.4%</cell><cell>66.7%</cell><cell>80%</cell><cell>73.3%</cell></row><row><cell></cell><cell>LP3</cell><cell></cell><cell></cell><cell></cell><cell>76%</cell><cell></cell><cell>72%</cell><cell></cell><cell>77%</cell><cell>56.7%</cell><cell>56.7%</cell><cell>63.3%</cell><cell>73%</cell><cell>90%</cell></row><row><cell></cell><cell>LP4</cell><cell></cell><cell></cell><cell></cell><cell>89.5%</cell><cell></cell><cell>91%</cell><cell></cell><cell>90.6%</cell><cell>86.7%</cell><cell>96%</cell><cell>86.7%</cell><cell>89%</cell><cell>96%</cell></row><row><cell></cell><cell>LP5</cell><cell></cell><cell></cell><cell></cell><cell>65%</cell><cell></cell><cell>69%</cell><cell></cell><cell>68%</cell><cell>54%</cell><cell>47%</cell><cell>45%</cell><cell>65%</cell><cell>69%</cell></row><row><cell cols="3">PenDigits</cell><cell></cell><cell></cell><cell>91.7%</cell><cell></cell><cell>90.8%</cell><cell></cell><cell>92.3%</cell><cell>92.7%</cell><cell>95.2%</cell><cell>93.2%</cell><cell>97%</cell><cell>91.2%</cell></row><row><cell></cell><cell cols="2">Shapes</cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell cols="3">DigitShapes</cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>93.8%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell cols="3">Wins/Ties</cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>6</cell><cell></cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>8</cell><cell>13</cell></row><row><cell></cell><cell cols="2">Mean</cell><cell></cell><cell></cell><cell>90.7%</cell><cell></cell><cell>89.8%</cell><cell></cell><cell>90%</cell><cell>84.6%</cell><cell>88.4%</cell><cell>88.7%</cell><cell>92.1%</cell><cell>93.5%</cell></row><row><cell cols="3">Avg. Rank</cell><cell></cell><cell></cell><cell>4.05</cell><cell></cell><cell>4.05</cell><cell></cell><cell>4.7</cell><cell>6.6</cell><cell>4.35</cell><cell>3.85</cell><cell>2.75</cell><cell>2.45</cell></row><row><cell></cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>9</cell><cell>8</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell>DTWi</cell><cell>6.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.45</cell><cell>WEASEL+MUSE</cell></row><row><cell>WEASEL</cell><cell>6.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.75</cell><cell>MLSTM-FCN</cell></row><row><cell>mv-ARF</cell><cell>4.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.85</cell><cell>gRSF</cell></row><row><cell>ARKernel</cell><cell>4.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.05</cell><cell>SMTS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.05</cell><cell>LPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1-NN DTW is a neatest neighbour classi er, so its prediction times directly depend on the size of the train dataset. For WEASEL+MUSE the length of the time series n and the number of dimensions m are most important For all but three datasets WEASEL+MUSE is faster than DTWi. It is up to 400 times faster for the Robot Failure LP1 dataset and 200 times faster for ArabicDigits. On average it is 43 times faster than DTWi. ere are three datasets for which DTW is faster: Walkvs-Run, KickvsPunch and CMU-MOCAP. ese are the datasets with</figDesc><table><row><cell></cell><cell>100</cell><cell cols="5">Influence of Noise: DigitShapes</cell><cell>100</cell><cell>Influence of Noise: Shapes</cell></row><row><cell>Accuracy in %</cell><cell>85 90 95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DTWi MUSE</cell><cell>Accuracy in %</cell><cell>70 80 90</cell><cell>DTW MUSE</cell></row><row><cell></cell><cell cols="2">0.0</cell><cell cols="3">0.2 Stddev of Gaussian Noise added 0.4 0.6 0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.2 Stddev of Gaussian Noise added 0.4 0.6 0.8</cell><cell>1.0</cell></row><row><cell cols="7">Figure 8: E ects of Gaussian noise on classi cation accuracy. With increasing levels of noise added, the accuracy of DTW</cell></row><row><cell cols="6">drops and remains stable for WEASEL+MUSE.</cell></row><row><cell cols="3">Average</cell><cell>43.3</cell><cell>1</cell><cell>1130119</cell><cell>133656</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Relative and absolute prediction times (lower is always better) of WEASEL+MUSE compared to DTWi.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">h p://www.timeseriesclassi cation.com</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Time</head><p>Absolute </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="//aaltd16.irisa.fr/challenge/." />
		<title level="m">AALTD Time Series Classi cation Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Great Time Series Classi cation Bake O : An Experimental Evaluation of Recently Proposed Algorithms. Extended Version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a symbolic representation for multivariate time series classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mustafa Gokce Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Time series representation and similarity based on local autopa erns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mustafa Gokce Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Autoregressive kernels for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.0673</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classi ers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Time-series data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Esling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Agon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of the value for unit commitment of improved load forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suradet</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreenivas</forename><surname>Jitprapaikulsarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vira</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chankong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><forename type="middle">J</forename><surname>Loparo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maratukulam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1342" to="1348" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">e DEBS 2014 Grand Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Ziekow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Conference on Distributed Event-based Systems</title>
		<meeting>the 2014 ACM International Conference on Distributed Event-based Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="266" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fazle</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houshang</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Harford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04503</idno>
		<title level="m">Multivariate LSTM-FCNs for Time Series Classi cation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalized random shapelet forests. Data mining and knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Isak Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Papapetrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boström</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1053" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiencing SAX: a novel symbolic representation of time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eamonn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="107" to="144" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotation-invariant similarity in time series using bag-of-pa erns representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Khade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="287" to="315" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multivariate Time Series Classi cation Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mustafa Gokce Baydogan</surname></persName>
		</author>
		<ptr target="//www.mustafabaydogan.com" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">e DEBS 2013 grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Mutschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Ziekow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Jerzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM International Conference on Distributed Event-based Systems</title>
		<meeting>the 2013 ACM International Conference on Distributed Event-based Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="289" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature selection, L 1 vs. L 2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM International Conference on Machine Learning</title>
		<meeting>the 2004 ACM International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching and mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilson</forename><surname>Anawin Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 2012 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable time series classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">e BOSS is concerned with time series classi cation in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1505" to="1530" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SFA: a symbolic fourier approximation and index for similarity search in high dimensional datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Högqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Extending Database Technology</title>
		<meeting>the 2012 International Conference on Extending Database Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="516" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmarking Univariate Time Series Classi ers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and Accurate Time Series Classi cation with WEASEL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="637" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">e Value of Wind Power Forecasting</title>
		<ptr target="//www.nrel.gov/docs/fy11osti/50814.pdf." />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autoregressive forests for multivariate time series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa Gokce</forename><surname>Kerem Sinan Tuncel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pa ern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">WEASEL+MUSE Classi er Source Code and Raw Results</title>
		<ptr target="//www2.informatik.hu-berlin.de/∼schaefpa/muse" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ultra-fast shapelets for time series classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Ieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.05018</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Begum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
		<ptr target="//www.cs.ucr.edu/∼eamonn/timeseriesdata." />
		<title level="m">UCR Time Series Classi cation Archive</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 2009 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
