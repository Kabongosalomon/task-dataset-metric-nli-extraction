<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Alonso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Riazuelo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
						</author>
						<title level="a" type="main">3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Scene Understanding</term>
					<term>Deep Learning for Visual Perception</term>
					<term>LiDAR Point clouds</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LIDAR semantic segmentation is an essential task that provides 3D semantic information about the environment to robots. Fast and efficient semantic segmentation methods are needed to match the strong computational and temporal restrictions of many real-world robotic applications. This work presents 3D-MiniNet, a novel approach for LIDAR semantic segmentation that combines 3D and 2D learning layers. It first learns a 2D representation from the raw points through a novel projection which extracts local and global information from the 3D data. This representation is fed to an efficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D semantic segmentation. These 2D semantic labels are re-projected back to the 3D space and enhanced through a post-processing module. The main novelty in our strategy relies on the projection learning module. Our detailed ablation study shows how each component contributes to the final performance of 3D-MiniNet. We validate our approach on well known public benchmarks (SemanticKITTI and KITTI), where 3D-MiniNet gets state-ofthe-art results while being faster and more parameter-efficient than previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-LIDAR semantic segmentation is an essential task that provides 3D semantic information about the environment to robots. Fast and efficient semantic segmentation methods are needed to match the strong computational and temporal restrictions of many real-world robotic applications. This work presents 3D-MiniNet, a novel approach for LIDAR semantic segmentation that combines 3D and 2D learning layers. It first learns a 2D representation from the raw points through a novel projection which extracts local and global information from the 3D data. This representation is fed to an efficient 2D Fully Convolutional Neural Network (FCNN) that produces a 2D semantic segmentation. These 2D semantic labels are re-projected back to the 3D space and enhanced through a post-processing module. The main novelty in our strategy relies on the projection learning module. Our detailed ablation study shows how each component contributes to the final performance of 3D-MiniNet. We validate our approach on well known public benchmarks (SemanticKITTI and KITTI), where 3D-MiniNet gets state-ofthe-art results while being faster and more parameter-efficient than previous methods.</p><p>Index Terms-Semantic Scene Understanding, Deep Learning for Visual Perception, LiDAR Point clouds</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTONOMOUS robotic systems use sensors to perceive the world around them. RGB cameras and LIDAR are very common due to the essential data they provide. One of the key building blocks of autonomous robots is semantic segmentation. Semantic segmentation assigns a class label to each LIDAR point or camera pixel. This detailed semantic information is essential for decision making in real-world dynamic scenarios. LIDAR semantic segmentation provides very useful information to autonomous robots when performing tasks such as Simultaneous Localization And Mapping (SLAM) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, autonomous driving <ref type="bibr" target="#b2">[3]</ref> or inventory tasks <ref type="bibr" target="#b3">[4]</ref>, especially for identifying dynamic objects. In these scenarios, it is critical to have models that provide accurate semantic information in a fast and efficient manner, which is particularly challenging working with 3D LIDAR data. On one hand, the commonly called point-based approaches <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> tackle this This paper was recommended for publication by Tamim Asfour upon evaluation of the Associate Editor and Reviewers' comments. This project was partially funded by projects FEDER/ Ministerio de Ciencia, Innovación y Universidades/ Agencia Estatal de Investigación/RTC-2017-6421-7, PGC2018-098817-A-I00 and PID2019-105390RB-I00, Aragón regional government (DGA T45 17R/FSE) and the Office of Naval Research Global project ONRG-NICOP-N62909-19-1-2027. <ref type="figure">Fig. 1</ref>. 3D LIDAR semantic segmentation accuracy vs speed on Se-manticKITTI test set <ref type="bibr" target="#b13">[14]</ref>. Green circles depict point-based methods and red squares are projection-based methods. Area of these shapes is proportional to the method number of parameters. The proposed 3D-MiniNet outperforms previous methods with less parameters and faster execution. problem directly executing 3D point-based operations, which is computationally expensive to operate at high frame rates. On the other hand, approaches that project the 3D information into a 2D image (projection-based approaches) are more efficient <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> but do not exploit the raw 3D information. Recent results on fast <ref type="bibr" target="#b2">[3]</ref> and parameter-efficient <ref type="bibr" target="#b11">[12]</ref> semantic segmentation models are facilitating the adoption of semantic segmentation in real-world robotic applications <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>This work presents a novel fast and parameter-efficient approach for 3D LIDAR semantic segmentation that consists of three modules (as detailed in Sec. III). The main contribution relies on our 3D-MiniNet module. 3D-MiniNet runs the following two steps: (1) It learns a 2D representation from the 3D point cloud (following previous works on 3D object detection <ref type="bibr">[?]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>); (2) It computes the segmentation through a fast 2D fully convolutional neural network.</p><p>Our best configuration achieves state-of-the-art results in well known public benchmarks (SemanticKITTI <ref type="bibr" target="#b13">[14]</ref> and KITTI dataset <ref type="bibr" target="#b17">[18]</ref>) while being faster and more parameter efficient that prior work. <ref type="figure">Figure 1</ref> shows how 3D-MiniNet achieves better precision-speed trade-off than previous methods. The main novelties with respect to existing approaches, that facilitate these improvements, are:</p><p>• An extension of MiniNet-v2 for 3D LIDAR semantic segmentation: 3D-MiniNet. • Our novel projection module. • A validation of 3D-MiniNet on the SemanticKITTI benchmark <ref type="bibr" target="#b13">[14]</ref> and KITTI dataset <ref type="bibr" target="#b17">[18]</ref>. The proposed projection module learns a rich 2D representation through different operations. It consists of four submodules: a context feature extractor, a local feature ex-arXiv:2002.10893v5 [cs.CV] 27 Apr 2021 tractor, a spatial feature extractor and the feature fusion. We provide a detailed ablation study on this module showing how each proposed components contributes to improve the final performance of 3D-MiniNet. Besides, we implemented a fast version of the point neighbor search based on a sliding-window on the spherical projection <ref type="bibr" target="#b18">[19]</ref> in order to compute it at an acceptable frame-rate. All the code and trained models are available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Semantic Segmentation</head><p>Current 2D semantic segmentation state-of-the-art methods are deep learning solutions <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Semantic segmentation architectures are evolved from convolutional neural networks (CNNs) architectures for classification tasks, adding a decoder on top of the CNN. Fully Convolutional Neural Networks for Semantic Segmentation (FCNN) <ref type="bibr" target="#b22">[23]</ref> carved the path for modern semantic segmentation architectures. The authors of this work propose to upsample the learned features of classification CNNs using bilinear interpolation up to the input resolution and compute the cross-entropy loss per pixel. Another of the early approaches, SegNet <ref type="bibr" target="#b23">[24]</ref>, proposes a symmetric encoderdecoder structure using the unpooling operation as upsampling layer. More recent works improve these earlier segmentation architectures by adding novel operations or modules proposed initially within CNNs architectures for classification tasks. FC-DenseNet <ref type="bibr" target="#b21">[22]</ref> follows DenseNet work <ref type="bibr" target="#b24">[25]</ref> using dense modules. PSPNet <ref type="bibr" target="#b25">[26]</ref> uses ResNet <ref type="bibr" target="#b26">[27]</ref> as its encoder and introduces the Pyramid Pooling Module incorporated at the end of the CNN allowing to learn effective global contextual priors. Deeplab-v3+ <ref type="bibr" target="#b19">[20]</ref> is one of the top-performing architectures for segmentation. Its encoder is based on Xception <ref type="bibr" target="#b27">[28]</ref>, which makes use of depthwise separable convolutions <ref type="bibr" target="#b28">[29]</ref> and atrous (dilated) convolutions <ref type="bibr" target="#b29">[30]</ref>.</p><p>With respect to efficiency, ENet <ref type="bibr" target="#b30">[31]</ref> set up certain basis which following works, such as ERFNet <ref type="bibr" target="#b31">[32]</ref>, ICNet <ref type="bibr" target="#b32">[33]</ref>, have built upon. The main idea is to work at low resolutions, i.e., quick downsampling, and to focus the computation on the encoder having a very light decoder. MiniNetV2 <ref type="bibr" target="#b33">[34]</ref> uses a multi-dilation depthwise separable convolution, which efficiently learns both local and global spatial relationships. In this work, we take MiniNetV2 as our backbone and adapt it to capture information from raw LIDAR points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Semantic Segmentation</head><p>There are three main groups of strategies to approach this problem: point-based methods, 3D representations and projection-based methods.</p><p>1) Point-based Methods: Point-based methods work directly on raw point clouds. The order-less structure of the point clouds prevents standard CNNs to work on this data. The pioneer approach and base of the following point-based works is PointNet <ref type="bibr" target="#b5">[6]</ref>. PointNet proposes to learn per-point features through shared MLP (multi-layer perceptron) followed by symmetrical pooling functions to be able to work on unordered 1 https://sites.google.com/a/unizar.es/semanticseg/ data. Lots of works have been later proposed based on PointNet. Following with the point-wise MLP idea, PoinNet++ <ref type="bibr" target="#b4">[5]</ref> groups points in an hierarchical manner and learns from larger local regions. The authors also propose a multi-scale grouping for coping with the non-uniformity nature of the data. In contrast, other approaches propose different types of operations following the convolution idea. Hua et al. <ref type="bibr" target="#b34">[35]</ref> propose to bin neighboring points into kernel cells for being able to perform point-wise convolutions. Other works resort to graph networks to capture the underlying geometric structure of the point cloud. Loic et al. <ref type="bibr" target="#b35">[36]</ref> use a directed graph to capture the structure and context information. For this, the authors represent the point cloud as a set of interconnected superpoints.</p><p>2) 3D representations: There are different kinds of representations of the raw point cloud data which have been used for 3D semantic segmentation. SegCloud <ref type="bibr" target="#b36">[37]</ref> makes use of a volumetric or voxel representation, which is a very common way for encoding and discretizing the 3D space. This approach feeds the 3D voxels into a 3D-FCNN <ref type="bibr" target="#b22">[23]</ref>. Then, the authors introduce a deterministic trilinear interpolation to map the coarse voxel predictions back to the original point cloud and apply a CRF as a final step. The main drawback of this voxel representation is that 3D-FCNN has very slow execution times for real-time applications. Su et al. <ref type="bibr" target="#b37">[38]</ref> proposed SPLATNet, making use of another type of representation: Permutohedral Lattice representation. This approach interpolates the 3D point cloud to a permutohedral sparse lattice and then bilateral convolutional layers are applied to convolve on occupied parts of the representation. LatticeNet <ref type="bibr" target="#b38">[39]</ref> was later proposed improving SPLATNet proposing its DeformSlice module for re-projecting the lattice feature back to the point cloud.</p><p>3) Projection-based Methods: This type of approaches rely on projections of the 3D data into a 2D space. For example, TangentConv <ref type="bibr" target="#b6">[7]</ref> proposes to project the neighboring points into a common tangent plane where they perform convolutions. Another type of projection-based method is the spherical representation. This strategy consists of projecting the 3D points into a spherical projection and has been widely used for LIDAR semantic segmentation. This representation is a 2D projection that allows the application of 2D images operations, which are very fast and work very well on recognition tasks. SqueezeSeg <ref type="bibr" target="#b8">[9]</ref> and its posterior improvement SqueezeSegV2 <ref type="bibr" target="#b7">[8]</ref>, based on SqueezeNet architecture <ref type="bibr" target="#b39">[40]</ref>, show that very efficient semantic segmentation can be done through this projection. The more recent work from Milioto et al. <ref type="bibr" target="#b2">[3]</ref> combines the DarkNet architecture <ref type="bibr" target="#b40">[41]</ref> with a GPU based post-processing method for real-time semantic segmentation.</p><p>Projection-based approaches tend to be faster than other representations, but they lose the potential of learning 3D features. LuNet <ref type="bibr" target="#b41">[42]</ref> is a recent work which proposes to learn local features using point-based operations before projecting into the 2D space. Our novel projection module tackles with this issue by including a context feature extractor based on point-based operations. Besides, we build a faster and more parameter-efficient architecture and a faster implementation of LuNet's neighbor search method. <ref type="figure">Fig. 2</ref>. Proposed approach overview. The M points from the input point cloud (with C 1 features) are split into P groups of N points with our fast 3D point neighbor search. Each point has a C 1 feature vector, which is extended to C 2 in this process with data relative to each group. The proposed 3D-MiniNet takes the point groups and predicts one semantic label per point. A post-processing method <ref type="bibr" target="#b2">[3]</ref> is used to refine the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. 3D-MININET: LIDAR POINT CLOUD SEGMENTATION</head><p>Our novel approach for LIDAR semantic segmentation is summarized in <ref type="figure">Fig. 2</ref>. It consists of three modules: (A) fast 3D point neighbor search, (B) 3D-MiniNet, which takes P groups of N points and outputs the segmented point cloud and, (C) the KNN-based post-processing which refines the final segmentation.</p><p>There are two main issues that typically prevent pointbased models to run at an acceptable frame-rate compared to projection-based methods: 3D point neighbor search is a required, but slow, operation and performing 3D operations is slower than using 2D convolutions. In order to alleviate these two issues, our approach includes a fast point neighbor search proxy (subsection III-A), and a module to minimize expensive point-based operations, which takes raw 3D points and outputs a 2D representation to be processed with a 2D CNN (subsection III-B1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fast 3D Point Neighbor Search</head><p>We need to find the 3D neighbors because we want to learn features that encode the relationship of each point with their neighbors in order to learn information about the shape of the point-cloud. In order to perform the 3D neighbor search more efficiently, we first project the point cloud into a spherical projection of shape W × H, mapping every 3D point (x, y, z) into a 2D coordinate (u, v), i.e., R 3 − → R 2 :</p><formula xml:id="formula_0">u v = 1 2 1 − arctan(y, x)π −1 W 1 − arcsin zr −1 + f up f −1 H ,<label>(1)</label></formula><p>where f = f up +f down is the vertical field-of-view of the sensor and r is the depth of each point. We perform the projection of Eq. 1 following <ref type="bibr" target="#b2">[3]</ref>, where each pixel encodes one 3D point with five features:</p><formula xml:id="formula_1">C 1 = {x, y, z, depth, remission}.</formula><p>We perform the point neighbor search in the spherical projection space using a sliding-window approach. Similarly to a convolutional layer, we get groups of pixels, i.e., projected points, by sliding a k × k window across the image. The generated groups of points have no intersection, i.e., each point belongs only to one group. This step generates P point groups of N points each (N = k 2 ), where all points from the spherical projection are used (P × N = W × H).</p><p>Before feeding the actual segmentation module, 3D-MiniNet, with these point groups, the features of each point are augmented. For each group we compute the relative (r) feature values for each point. They are computed with respect to the group mean for each C 1 feature (similar to previous works which compute features relative to a center point <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>). Besides, similar to <ref type="bibr" target="#b42">[43]</ref>, we compute the 3D euclidean distance of each point to the mean point. Therefore, each point has now eleven features:</p><formula xml:id="formula_2">C 2 = {x, x r , y, y r , z, z r , depth, depth r , remission, remission r , d Euc }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D-MiniNet</head><p>3D-MiniNet consists of two modules, as represented in <ref type="figure" target="#fig_0">Fig.  3</ref>: the proposed projection module, which takes the raw point cloud and computes a 2D representation, and our efficient backbone network based on MiniNetV2 <ref type="bibr" target="#b33">[34]</ref> to compute the semantic segmentation.</p><p>1) Projection Learning Module: The goal of this module is to transform raw 3D points to a 2D representation that can be used for efficient segmentation. The input of this module if the output of the point neighbor search described in the previous subsection. It is a set of P groups, where each group contains N points with C 2 features each, gathered through the slidingwindow search on the spherical projection as explained in the previous subsection.</p><p>The following three kinds of features are extracted from the input data (see left part of <ref type="figure" target="#fig_0">Fig. 3</ref> for a visual description of this proposed module) and fused in the final module step:</p><p>Local Feature Extractor: The first feature is a PointNetlike local feature extraction (see projection learning module (a) of <ref type="figure" target="#fig_0">Fig. 3</ref>). It runs four linear layers shared across the groups followed by a BatchNorm <ref type="bibr" target="#b43">[44]</ref> and LeakyRelu <ref type="bibr" target="#b44">[45]</ref>. We follow PointPillars <ref type="bibr" target="#b14">[15]</ref> implementation of these shared linear layers using 1 × 1 convolutions across the tensor resulting in very efficient computation when handling lots of point groups.</p><p>Context Feature Extractor: The second feature extraction (projection learning module (b) of <ref type="figure" target="#fig_0">Fig. 3</ref>) learns context information from the points.This is a very important module because although context information can be learned through the posterior CNN, point-based operations learn different features than convolutions. Therefore, this module helps learning a richer representation with information than might not be learned through the CNN.</p><p>The input of this context feature extractor is the output of the second linear layer of the local feature extractor (giving the last linear layer as input would drop significantly the frame-rate due to the high number of features). This tensor is maxpooled (in order to complete the PointNet-like operation which work on unordered points) and then, our fast neighbor search is run to get point groups. In this case, three different groupings (using our point neighbor search) are performed with a 3 × 3 sliding window with different dilation rates of 1, 2, 3 respectively. Dilation rates, as in convolutional kernels <ref type="bibr" target="#b29">[30]</ref>, keep the number of grouped points low while increasing the receptive field allowing a faster context learning. We use zero-padding and a stride of 1 for keeping the same size. After every grouping we perform a linear, BatchNorm and LeakyRelu. The outputs of these two feature extractor modules are concatenated and applied a maxpool operation over the N dimension. This maxpool operation keeps the feature with higher response along the neighbor dimension, being orderinvariant with respect to the neighbor dimension. The maxpool operation also makes the learning robust to pixels with no point information (spherical projection coordinates with no point projected).</p><p>Spatial Feature Extractor: The last feature extraction operation is a convolutional layer of kernel 1 × N (projection learning module (c) of <ref type="figure" target="#fig_0">Fig. 3</ref>). Convolutions can extract features of each point with respect to the neighbors when there is an underlying spatial structure which is the case, as the point groups are extracted from a 2D spherical projection. In the experiment section, we take this feature extractor as our baseline without the two others which is equivalent of performing only standard convolutions on the spherical projection.</p><p>Feature Fusion: Lastly, a feature fusion with selfattention module is applied. It learns to reduce the feature space into an specified number of features, learning which features are more important. It consists of three stages: (1) concatenation of the feature extraction outputs reshaping the resulting tensor to (W/4 × H/4 × C 7 ), (2) a self-attention operation which multiplies the reshaped tensor by the output of a pooling, 1 × 1 convolution and sigmoid function which has the same concatenated tensor as its input and, (3) a 1 × 1 convolutional layer followed by a BatchNorm and LeakyRelu which acts as a bottleneck limiting the output to C 6 features.</p><p>All implementation details, such as the number of features of each layer, are specified in Sect. IV. The experiments in Sect. V show how each part of this learning module contributes to improve 3D-MiniNet's performance.</p><p>2) 2D Segmentation Module (MiniNet Backbone): Once the previous module has computed a W/4 × H/4 × C 6 tensor, the 2D semantic segmentation is obtained running an efficient CNN (see MiniNet backbone in <ref type="figure" target="#fig_0">Fig. 3</ref> for a visual description). Our module uses a FCNN instead of performing more MLP operations because convolutional layers have lower inference time when working on high dimensional spaces. Our FCNN is based on MiniNetV2 architecture <ref type="bibr" target="#b33">[34]</ref>. Our encoder performs L 1 depthwise separable convolutions and L 2 multi-dilation depthwise separable convolutions. For the decoder, we use bilinear interpolations as upsampling layers. It performs L 3 depthwise separable convolutions at W/4 × H/4 resolution and L 4 at W/2 × H/2 resolution. Finally, a convolution is performed at W × H resolution to get the 2D semantic segmentation prediction.</p><p>Similarly to MiniNetV2, we also include a second convolutional branch to extract fine-grained information, i.e., highresolution low-level features. The input of this second branch is the spherical projection. The number of layers and features at each layer is specified in Sect. IV-B.</p><p>As a final step, the predicted 2D semantic segmentation labels are re-projected back into the 3D space (R 2 − → R 3 ). For the points projected into the spherical representation, this reprojection is a straightforward step, as it just implies assigning the semantic label predicted in the spherical projection. Nevertheless, the points that had not been projected into the spherical projection (one 2D coordinate can have more than one 3D point) have no semantic label. For these points, the semantic label of its corresponding 2D coordinate is assigned. As this issue may lead to miss-predictions, a post-processing method is performed to refine the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Post-Processing</head><p>In order to cope with the miss-predictions of non-projected 3D points, we follow Milioto et al. <ref type="bibr" target="#b2">[3]</ref> post-processing method. All 3D points get a new semantic label based on K Nearest Neighbors (KNN). The criteria for selecting the K nearest points is not based on the relative euclidean distances but on relative depth values. Besides, the search is narrowed down based on 2D spherical coordinate distances. Milioto et al. implementation is GPU-based and is able to run in 7ms keeping the frame-rate high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>This section details the setup used in our experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>SemanticKITTI Benchmark: The SemanticKITTI dataset <ref type="bibr" target="#b13">[14]</ref> is a recent large-scale dataset that provides dense pointwise annotations for the entire KITTI Odometry Benchmark <ref type="bibr" target="#b17">[18]</ref>. The dataset consists of over 43000 scans from which over 21000 are available for training (sequences 00 to 10) and the rest (sequences 11 to 21) are used as test set. The dataset distinguishes 22 different semantic classes from which 19 classes are evaluated on the test set via the official online platform of the benchmark. As this is the current most relevant and largest dataset of single-scan 3D LIDAR semantic segmentation, we perform our ablation study and our more thorough evaluation on this dataset.</p><p>KITTI Benchmark: SqueezeSeg <ref type="bibr" target="#b8">[9]</ref> work provided semantic segmentation labels exported from the 3D object detection challenge of the KITTI dataset <ref type="bibr" target="#b17">[18]</ref>. It is a mediumsize dataset split into 8057 training scans and 2791 validation scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Settings a) 3D Point Neighbor Search Parameters:</head><p>We set the resolution of the spherical projection to 2048 × 64 for the SemanticKITTI dataset and 512 × 64 for the KITTI (same resolution than previous works to be able to make fair comparisons). We set a 4 × 4 window size with a stride of 4 and no zero-padding for our fast point neighbor search leading to 8192 groups of 3D points for the SemanticKITTI data and 2048 groups for the KITTI data. Our projection module is fed with these groups and generates a learned representation of resolution 512 × 16 for the SemanticKITTI configuration and 128 × 16 for the KITTI.</p><p>b) Network Parameters: The full architecture and all its parameters are described in <ref type="figure" target="#fig_0">Fig. 3</ref>. We considered three different configurations for evaluating the proposed approach: 3D-MiniNet, 3D-MiniNet-small, 3D-MiniNet-tiny. The number of features (C 3 , C 4 , C 5 , C 6 ) for the projection module of the different 3D-MiniNet configurations are <ref type="bibr" target="#b23">(24,</ref><ref type="bibr">48,</ref><ref type="bibr">96,</ref><ref type="bibr">192)</ref> features for 3D-MiniNet, <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128)</ref> for 3D-MiniNetsmall and <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">48,</ref><ref type="bibr">96)</ref> for 3D-MiniNet-tiny. The number of layers (L 1 , L 2 , L 3 , L 4 ) of the FCNN backbone network are (50, 30, 4, 2) features for 3D-MiniNet, (24, 20, 2, 1) for 3D-MiniNet-saml and (14, 10, 2, 1) for 3D-MiniNet-tiny. N c is the number of semantic classes of the dataset. c) Post-processing Parameters: For the K Nearest Neigbors post-process method <ref type="bibr" target="#b2">[3]</ref>, we set as 7 × 7 the windows size of the neighbor search on the 2D segmentation and we set K to 7. d) Training protocol: We train the different 3D-MiniNet configurations for 500 epochs with batch size of 3, 6 and 8 for 3D-MiniNet, 3D-MiniNet-small, and 3D-MiniNet-tiny respectively (different due to memory constraints). We use Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 4 · 10 −3 and a decay of 0.99 every epoch. For the optimization, we use the cross-entropy loss function, see eq. 2.</p><formula xml:id="formula_3">L = − 1 M M m=1 C c=1 ( f t f c ) i y c,m ln(ŷ c,m ),<label>(2)</label></formula><p>where M is the number of labeled points and C is the number of classes. Y c,m is a binary indicator (0 or 1) of point m belonging to a certain class c andŷ c,m is the CNN predicted probability of point m belonging to a certain class c. This probability is calculated by applying the soft-max function to the networks' output. To account for class imbalance, we use the median frequency class balancing, as applied in SegNet <ref type="bibr" target="#b23">[24]</ref>. To smooth the resulting class weights, we propose to apply a power operation, w c = ( ft fc ) i , with f c being the frequency of class c and f t the median of all frequencies.</p><p>We set i to 0.25. e) Data augmentation: During the training, we randomly rotate and shift the whole 3D point cloud. We randomly invert the sign for X and Z values for all the point cloud. We also drop some points. The rotation angle is a Gaussian distribution with mean 0 and standard deviation (std) of 40º. The shifts we perform are Gaussian distributions with mean 0 and std of 0.35, 0.35 and 0.01 (meters) for the X, Y, Z axis (being Z the height). The percentage of dropped points is a uniform distribution between 0 and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study of the Projection Module</head><p>The projection module is the main novelty from our approach. This subsection shows how each part helps to improve the learned representation. For this experiment, we use 3D-MiniNet-small configuration. <ref type="table" target="#tab_1">Table I</ref> shows the ablation study of our proposed module, measuring the mIoU, speed and learning parameters needed with each configuration. The first row and baseline is working on the spherical projection using a convolution as the projection method, i.e., just a downsampling in that case.</p><p>As the projection used is neither rotation nor shift invariant, performing this data augmentation helps to our network generalization as first row shows. Second row shows the performance using only 1 × N convolutions in the learning layers with the 5-channel input (C 1 ) used in RangeNet <ref type="bibr" target="#b2">[3]</ref> which we establish as our baseline, i.e, our spatial feature extractor. The third row shows the performance if we replace the 1 × N convolution for point-based operations, i.e, our local feature extractor. These results point that MLP operations work better for 3D points but take more execution time.</p><p>The fourth row combines both the convolution and local MLP operation. Combining convolutions and MLP operations increases performance due to the different type of features learned by each type of operation as explained in Sect. III-B1. The attention module also increases the performance with almost no extra computational effort. It reduces the feature space into a specified number of features, learning which features are more important. The sixth row shows the results adding our context feature extractor. Context is also learned later through the FCNN via convolutions but here, the context feature extractor learns different context through with MLP operations. Context information is often very useful in semantic tasks, e.g., for distinguishing between a bicyclist, a cyclist and a motorcyclist. This context information gives a boost higher than the other feature extractors showing its relevance. Finally, increasing the number of features of each point with features relative to the point group (C 2 ) also leads to better performance without decreasing the frame-rate and without adding any learning parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks results</head><p>This subsection presents quantitative and qualitative results of 3D-MiniNet and comparisons with other relevant works.</p><p>a) Quantitative Analysis: <ref type="table" target="#tab_1">Table II</ref> compares our method with several point-based approaches (rows 1-4), 3D representation methods (row 5) and projection-based approaches (rows 6-11) measuring the mIoU, the processing speed (FPS) and the number of parameters required by each method. As we can see, point-based methods for semantic segmentation of LIDAR scans tend to be slower than projection ones without providing better performance. As LIDAR sensors such as Velodyne usually work at 5-20 FPS, only RandLA-Net and projection-based approaches are currently able to process in real time the full amount of data made available by the sensor. Looking at the different configurations of 3D-MiniNet, it gets state-of-the-art using fewer parameters and being faster (3D-MiniNet-small-KNN) beating both RandLANet (point-based method), SPLATNet (3D representation) and RangeNet53-KNN (projection-based). Besides, 3D-MiniNet-KNN configuration is able to get even better performance although it needs more parameters than RandLANet. If efficiency can be traded off for performance, smaller versions of Mininet also obtain better performance metrics at higher frame-rates. 3D-MiniNet-tiny is able to run at 98 fps and, with only a 9% drop in mIoU (46.9% compared to the 29% of SqueezeSeg version that runs at 90 fps).</p><p>The post-processing method applied <ref type="bibr" target="#b2">[3]</ref> shows its effectiveness improving the results the same way it improved RangeNet. This step is crucial to correctly process points that were not included in the spherical projection, as discussed in more detail in Sect. III. The scans of the KITTI dataset <ref type="bibr" target="#b17">[18]</ref> have a lower resolution (64x512) as we can see in the evaluation reported in <ref type="table" target="#tab_1">Table III</ref>. 3D-MiniNet also gets state-of-the-art performance on LIDAR semantic segmentation on this dataset. Our approach gets considerably better performance than SqueezeSeg versions (+10-20 mIoU). 3D-MiniNet also gets better performance than LuNet and DBLiDARNet which were the previous best methods on this dataset.</p><p>Note that in this case, we did not evaluate the KNN postprocessing since this dataset only provides 2D labels.</p><p>The experiments show that projection-based methods are more suitable for the LIDAR semantic segmentation with a good speed-performance trade-off. Besides, better results are obtained when including point-based operations to extract both context and local information from the 3D raw points into the 2D projection. b) Qualitative Analysis: <ref type="figure" target="#fig_1">Fig. 4</ref> shows a few examples of 3D-MiniNet inference on test data. The supplementary video includes inference results on a full sequence 2 . As test groundtruth is not provided for the test set (evaluation is performed externally on the online platform), we can only show visual results with no label comparison.</p><p>Note the high quality results on our method in relevant classes such as cars, as well as in challenging classes such as traffic signs. In the supplementary video we can also appreciate some of the 3D-MiniNet failure cases. As it could be expected, the biggest difficulties happen distinguishing between classes with similar geometric shapes and structures like building and fences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, we propose 3D-MiniNet, a fast and efficient approach for 3D LIDAR semantic segmentation. 3D-MiniNet projects the 3D point cloud into a 2-Dimensional space and then learns the semantic segmentation using a fully convolutional neural network. Differently from common projectionbased approaches that perform a predefined projection, 3D-MiniNet learns this projection from the raw 3D points, learning both local and context information from point-based operations, showing very promising and effective results. Our ablation study shows how each part of the proposed approach contributes to the learning of the representation. We validate our approach on the SemanticKITTI and KITTI public benchmarks. 3D-MiniNet gets state-of-the-art results while being faster and more efficient than previous methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>3D-MiniNet overview. It takes P groups of N points each and computes semantic segmentation of the M points of the point cloud where P ×N = M . It consists of two main modules: our proposed learning module (on the left) which learns a 2D tensor which is fed to the second module, an efficient FCNN backbone (on the right) which computes the 2D semantic segmentation. Each 3D point of the point cloud is given a semantic label based on the 2D segmentation. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>3D-MiniNet LIDAR semantic segmentation predictions on the SemanticKITTI benchmark (test sequence 11). LIDAR point cloud are on top where color represents depth. Predictions are on bottom where color represents semantic classes: cars in blue, road in purple, vegetation in green, fence in orange, building in yellow and traffic sign in red. For the full video sequence, go to https://www.youtube.com/watch?v=5ozNkgFQmSM. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation Iñigo Alonso 1 , Luis Riazuelo 1 , Luis Montesano 1,2 , and Ana C. Murillo 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY OF THE DIFFERENT PARTS OF THE PROJECTION MODULE EVALUATED ON THE TEST SET OF SEMANTIKITTI.</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell>Local</cell><cell></cell><cell cols="2">Context Relative</cell><cell></cell><cell></cell><cell>Params</cell></row><row><cell>Method</cell><cell>Aug.</cell><cell>Conv</cell><cell>MLP</cell><cell>Attention</cell><cell>MLP</cell><cell>features</cell><cell cols="2">mIoU FPS</cell><cell>(M)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.4</cell><cell>73</cell><cell>0.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47.6</cell><cell>73</cell><cell>0.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.7</cell><cell>69</cell><cell>0.93</cell></row><row><cell>3D-MiniNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.5</cell><cell>66</cell><cell>0.96</cell></row><row><cell>Small</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.9</cell><cell>65</cell><cell>1.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.2</cell><cell>61</cell><cell>1.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.8</cell><cell>61</cell><cell>1.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON SINGLE-SCAN TEST SET IN SEMANTICKITTI<ref type="bibr" target="#b13">[14]</ref>. POINT-BASED METHODS: ROWS 1-4. 3D REPRESENTATIONS: ROW 5. PROJECTION-BASED METHODS: ROWS 6-11.</figDesc><table><row><cell>Methods</cell><cell>Size</cell><cell>mIoU</cell><cell>Frame-rate (FPS)</cell><cell>Params(M)</cell><cell>road IoU</cell><cell>sidewalk IoU</cell><cell>parking IoU</cell><cell>other-ground IoU</cell><cell>building IoU</cell><cell>car IoU</cell><cell>truck IoU</cell><cell>bicycle IoU</cell><cell>motorcycle IoU</cell><cell>other-vehicle IoU</cell><cell>vegetation IoU</cell><cell>trunk IoU</cell><cell>terrain IoU</cell><cell>person IoU</cell><cell>bicyclist IoU</cell><cell>motorcyclist IoU</cell><cell>fence IoU</cell><cell>pole IoU</cell><cell>traffic-sign IoU</cell></row><row><cell>PointNet [6]</cell><cell></cell><cell>14.6</cell><cell>2</cell><cell>3</cell><cell>61.6</cell><cell cols="2">35.7 15.8</cell><cell>1.4</cell><cell cols="2">41.4 46.3</cell><cell>0.1</cell><cell>1.3</cell><cell>0.3</cell><cell>0.8</cell><cell>31.0</cell><cell>4.6</cell><cell>17.6</cell><cell>0.2</cell><cell>0.2</cell><cell>0.0</cell><cell>12.9</cell><cell>2.4</cell><cell>3.7</cell></row><row><cell>SPG [36]</cell><cell></cell><cell>17.4</cell><cell>0.2</cell><cell>0.25</cell><cell cols="2">45.0 28.5</cell><cell>0.6</cell><cell>0.6</cell><cell cols="2">64.3 49.3</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.8</cell><cell cols="3">48.9 27.2 24.6</cell><cell>0.3</cell><cell>2.7</cell><cell>0.1</cell><cell cols="2">20.8 15.9</cell><cell>0.8</cell></row><row><cell>PointNet++ [5]</cell><cell>50K pts</cell><cell cols="2">20.1 0.1</cell><cell>6</cell><cell cols="3">72.0 41.8 18.7</cell><cell>5.6</cell><cell cols="2">62.3 53.7</cell><cell>0.9</cell><cell>1.9</cell><cell>0.2</cell><cell>0.2</cell><cell cols="3">46.5 13.8 30.0</cell><cell>0.9</cell><cell>1.0</cell><cell>0.0</cell><cell>16.9</cell><cell>6.0</cell><cell>8.9</cell></row><row><cell>RandLA-Net [?]</cell><cell></cell><cell>53.9</cell><cell>22</cell><cell cols="6">1.24 90.7 73.7 60.3 20.4 86.9</cell><cell cols="2">94.2 40.1</cell><cell cols="2">26.0 25.8</cell><cell>38.9</cell><cell>81.4</cell><cell cols="4">61.3 66.8 49.2 48.2</cell><cell>7.2</cell><cell cols="2">56.3 49.2</cell><cell>47.7</cell></row><row><cell>SPLATNet [38]</cell><cell>50K pts</cell><cell>18.4</cell><cell>1</cell><cell>0.8</cell><cell>64.6</cell><cell>39.1</cell><cell>0.4</cell><cell>0.0</cell><cell cols="2">58.3 58.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>71.1</cell><cell>9.9</cell><cell>19.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>23.1</cell><cell>5.6</cell><cell>0.0</cell></row><row><cell>SqueezeSeg [9]</cell><cell></cell><cell>29.5</cell><cell>90</cell><cell>1</cell><cell>85.4</cell><cell cols="2">54.3 26.9</cell><cell>4.5</cell><cell cols="2">57.4 68.8</cell><cell>3.3</cell><cell>16.0</cell><cell>4.1</cell><cell>3.6</cell><cell cols="5">60.0 24.3 53.7 12.9 13.1</cell><cell>0.9</cell><cell>29.0</cell><cell>17.5</cell><cell>24.5</cell></row><row><cell>DBLiDARNet [11]</cell><cell></cell><cell>37.6</cell><cell>-</cell><cell>2.8</cell><cell>85.8</cell><cell>59.3</cell><cell>8.7</cell><cell>1.0</cell><cell>78.6</cell><cell>81.5</cell><cell>6.6</cell><cell cols="2">29.4 19.6</cell><cell>6.5</cell><cell cols="5">77.1 46.0 58.1 23.7 20.1</cell><cell>2.4</cell><cell>39.6</cell><cell>32.6</cell><cell>39.1</cell></row><row><cell>SqueezeSegV2 [8]</cell><cell></cell><cell>39.7</cell><cell>83</cell><cell>1</cell><cell>88.6</cell><cell>67.6</cell><cell cols="13">45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1</cell><cell>3.9</cell><cell cols="2">41.1 20.2</cell><cell>36.3</cell></row><row><cell>TangentConv [7]</cell><cell>64x2048 px</cell><cell cols="2">40.9 0.3</cell><cell>0.4</cell><cell>83.9</cell><cell>63.9</cell><cell cols="5">33.4 15.4 83.4 90.8 15.2</cell><cell>2.7</cell><cell cols="7">16.5 12.1 79.5 49.3 58.1 23.0 28.4</cell><cell>8.1</cell><cell>49.0</cell><cell cols="2">35.8 28.5</cell></row><row><cell>RangeNet21 [3]</cell><cell></cell><cell>47.4</cell><cell>25</cell><cell>25</cell><cell cols="8">91.4 74.0 57.0 26.4 81.9 85.4 18.6 26.2</cell><cell cols="5">26.5 15.6 77.6 48.4 63.6</cell><cell cols="2">31.8 33.6</cell><cell>4.0</cell><cell>52.3</cell><cell cols="2">36.0 50.0</cell></row><row><cell>RangeNet53 [3]</cell><cell></cell><cell>49.9</cell><cell>13</cell><cell>50</cell><cell cols="8">91.7 74.0 65.1 28.2 82.9 85.3 25.8 22.7</cell><cell cols="5">33.6 22.2 77.3 50.0 64.6</cell><cell cols="2">36.8 31.4</cell><cell>4.7</cell><cell>54.8</cell><cell cols="2">39.1 52.3</cell></row><row><cell>RangeNet53-KNN [3]</cell><cell></cell><cell>52.2</cell><cell>12</cell><cell>50</cell><cell cols="15">91.8 75.2 65.0 27.8 87.4 91.4 25.7 25.7 34.4 23.0 80.5 55.1 64.6 38.3 38.8</cell><cell>4.8</cell><cell>58.6</cell><cell cols="2">47.9 55.9</cell></row><row><cell>3D-MiniNet-tiny (Ours)</cell><cell></cell><cell>46.9</cell><cell>98</cell><cell cols="16">0.44 90.7 70.7 59.4 20.0 83.4 82.0 19.0 29.3 25.4 20.8 77.9 50.6 60.8 35.1 32.3</cell><cell>3.2</cell><cell>51.0</cell><cell>32.7</cell><cell>46.7</cell></row><row><cell>3D-MiniNet-small (Ours)</cell><cell></cell><cell>51.8</cell><cell>61</cell><cell>1.13</cell><cell>91.5</cell><cell cols="16">72.3 61.7 25.1 83.9 83.4 25.4 35.6 25.4 25.1 80.3 53.9 64.3 43.4 42.3 20.7 53.0</cell><cell>36.4</cell><cell>50.3</cell></row><row><cell>3D-MiniNet (Ours)</cell><cell>64x2048 px</cell><cell>53.0</cell><cell>36</cell><cell>3.97</cell><cell>91.6</cell><cell cols="16">74.0 64.1 25.9 85.8 85.2 28.3 37.9 39.3 28.8 80.3 54.5 65.9 43.8 40.3 14.4 57.0</cell><cell>37.9</cell><cell>51.5</cell></row><row><cell>3D-MiniNet-tiny-KNN (Ours)</cell><cell></cell><cell>49.0</cell><cell>55</cell><cell>0.44</cell><cell>90.7</cell><cell cols="14">71.0 59.5 19.7 86.4 86.6 19.2 31.6 27.8 21.3 80.0 55.4 61.4 38.1 35.0</cell><cell>3.0</cell><cell>53.7</cell><cell>40.5</cell><cell>51.0</cell></row><row><cell>3D-MiniNet-small-KNN (Ours)</cell><cell></cell><cell>54.4</cell><cell>40</cell><cell>1.13</cell><cell>91.5</cell><cell cols="14">72.7 61.8 24.6 87.1 88.1 25.6 39.3 38.0 25.6 82.5 59.7 65.0 47.2 46.2</cell><cell>22.4</cell><cell cols="2">56.1 45.8</cell><cell>54.9</cell></row><row><cell>3D-MiniNet-KNN (Ours)</cell><cell></cell><cell>55.8</cell><cell>28</cell><cell>3.97</cell><cell cols="4">91.6 74.5 64.2 25.4</cell><cell>89.4</cell><cell>90.5</cell><cell>28.5</cell><cell cols="2">42.3 42.1</cell><cell>29.4</cell><cell>82.8</cell><cell cols="6">60.8 66.7 47.8 44.1 14.5 60.8</cell><cell>48.0</cell><cell>56.6</cell></row><row><cell cols="4">Scans per second have been measured using a Nvidia gtx 2080ti</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Not reported by the authors.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON KITTI [18] VALIDATION SET.</figDesc><table><row><cell>Methods</cell><cell>Size</cell><cell>mIoU</cell><cell>Frame-rate (fps)</cell><cell>Params(M)</cell><cell>car IoU</cell><cell>pedestrian IoU</cell><cell>cyclist IoU</cell></row><row><cell>SqueezeSeg [9]</cell><cell></cell><cell cols="2">37.2 227</cell><cell>1</cell><cell cols="3">64.6 21.8 25.1</cell></row><row><cell>PointSeg [10]</cell><cell></cell><cell cols="2">39.7 160</cell><cell>-</cell><cell cols="3">67.4 19.2 32.7</cell></row><row><cell>SqueezeSegv2 [8]</cell><cell>64x512 px</cell><cell cols="2">44.9 143</cell><cell>1</cell><cell>73.2</cell><cell cols="2">27.8 33.6</cell></row><row><cell>LuNet [19]</cell><cell></cell><cell cols="6">55.4 67* 23.4 72.7 46.9 46.5</cell></row><row><cell>DBLiDARNet [11]</cell><cell></cell><cell>56.0</cell><cell>-</cell><cell>2.8</cell><cell cols="3">75.1 47.4 45.4</cell></row><row><cell>3D-MiniNet-tiny (Ours)</cell><cell></cell><cell>45.5</cell><cell cols="5">245 0.44 69.6 37.5 29.5</cell></row><row><cell cols="8">3D-MiniNet-small (Ours) 64x512 px 50.6 161 1.13 74.4 40.7 36.7</cell></row><row><cell>3D-MiniNet (Ours)</cell><cell></cell><cell>58.0</cell><cell>92</cell><cell cols="4">3.97 75.5 49.6 48.9</cell></row><row><cell cols="5">Scans per second have been measured using a Nvidia gtx 2080ti</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">* Offline neighboring point search is not taken into account.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Not reported by the authors.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.youtube.com/watch?v=5ozNkgFQmSM</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A semantic segmentation based lidar slam system towards dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robotics and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lidar mapping optimization based on lightweight semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sloam: Semantic lidar odometry and mapping for forest inventory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Nardari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A F</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="619" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pointseg: Real-time semantic segmentation based on 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06288</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeptemporalseg: Temporally consistent semantic segmentation of 3d lidar scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrate point-cloud segmentation with 3d lidar scan-matching for mobile robot localization and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">237</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06528</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LU-Net: An efficient network for 3d lidar point cloud semantic segmentation based on end-to-end-learned 3d features and u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biasutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brëdif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bugeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on CVPR</title>
		<meeting>IEEE Conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on learning representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ICNet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>European Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mininet: An efficient semantic segmentation convnet for real-time robotic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05905</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lu-net: An efficient network for 3d lidar point cloud semantic segmentation based on end-to-end-learned 3d features and u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biasutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brédif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bugeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rotation invariant convolutions for 3d point clouds deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
