<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VidTr: Video Transformer Without Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VidTr: Video Transformer Without Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Video Transformer (VidTr) with separableattention for video classification. Comparing with commonly used 3D networks, VidTr is able to aggregate spatiotemporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present VidTr which reduces the memory cost by 3.3× while keeping the same performance. To further compact the model, we propose the standard deviation based topK pooling attention, which reduces the computation by dropping non-informative features. VidTr achieves state-of-theart performance on five commonly used dataset with lower computational requirement, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that VidTr is especially good at predicting actions that require long-term temporal reasoning. The code and pre-trained weights will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We introduce Video Transformer (VidTr) with separableattention, one of the first transformer-based video action classification architecture that performs global spatiotemporal feature aggregation. Convolution-based architectures have dominated the video classification literature in recent years <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref>, and although successful, the convolution-based approaches have two drawbacks: 1. they have limited receptive field on each layer and 2. information is slowly aggregated through stacked convolution layers, which is inefficient and might be ineffective <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">51]</ref>. Attention is a potential candidate to overcome these limitations as it has a large receptive field which can be leveraged for spatio-temporal modeling. Previous works use attention to modeling long-range spatio-temporal features in videos but still rely on convoluational backbones <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">51]</ref>. * Equally contributed <ref type="figure" target="#fig_5">Figure 1</ref>: The proposed VidTr first locates the spatial important regions on each temporal instances via spatial attention (third row) and further select informative temporal instance via temporal attention (bottom row). I3D usually only focus on features in a local region even they are not informative to decision making (top row).</p><p>Inspired by recent successful applications of transformers on NLP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48]</ref> and computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, we propose a transformer-based video network that directly applies attentions on raw video pixels for video classification, aiming at higher efficiency and better performance ( <ref type="figure" target="#fig_5">Figure 1)</ref>.</p><p>We first introduce a vanilla video transformer that directly learns spatio-temporal features from raw-pixel inputs via vision transformer <ref type="bibr" target="#b12">[13]</ref>, showing that it is possible to perform pixel-level spatio-temporal modeling. However, as discussed in <ref type="bibr" target="#b52">[53]</ref>, the transformer has O(n 2 ) complexity with respect to the sequence length. The vanilla video transformer is memory consuming, as training on a 16-frame clip (224 × 224) with only batch size of 1 requires more than 16GB GPU memory, which makes it infeasible on most commercial devices. Inspired by the R(2+1)D convolution <ref type="bibr" target="#b45">[46]</ref>, we further introduce our separable-attention, which performs spatial and temporal attention separately. This reduces the memory consumption by 3.3× with no drop in accuracy. We can further reduce the memory and computa-arXiv:2104.11746v1 [cs.CV] 23 Apr 2021 tional requirements of our system by exploiting the fact that a large portion of many videos have redundant information as they contain many near duplicate frames. This notion has been explored in the context of convolutional networks to reduce computation previously <ref type="bibr" target="#b29">[30]</ref>. We build on this intuition and propose a standard deviation based topK pooling operation (topK std pooling), which reduces the sequence length and encourages the transformer network to focus on representative frames.</p><p>We evaluated our VidTr on 6 most commonly used datasets, including Kinetics 400/700, Charades, Somethingsomething V2, UCF-101 and HMDB-51. Our model achieved state-of-the-art (SOTA) or comparable performance on five datasets with lower computational requirements and latency compared to previous SOTA approaches. Our error analysis and ablation experiments show that the VidTr works significantly better than I3D on activities that requires longer temporal reasoning (e.g. making a cake vs. eating a cake), which aligns well with our intuition. This also inspires us to ensemble the VidTr with the I3D convolutional network as features from global and local modeling methods should be complementary. We show that simply combining the VidTr with a light weight I3D50 model (8 frames input) via ensemble can lead to roughly a 2% performance improvement on Kinetics 400 (see Appendix C for details). We further illustrate how and why the VidTr works by visualizing the separable-attention using attention rollout <ref type="bibr" target="#b0">[1]</ref>, and show that the spatial-attention is able to focus on informative patches while temporal attention is able to reduce the duplicated/non-informative temporal instances. Our contributions are: 1. Video transformer: We propose to efficiently and effectively aggregate spatio-temporal information with stacked attentions as opposed to convolution based approaches. We introduce vanilla video transformer as proof of concept with SOTA comparable performance on video classification. 2. VidTr: We introduce VidTr and its permutations, including the VidTr with SOTA performance and the compact-VidTr with significantly reduced computational costs using the proposed standard deviation based pooling method, that fit in different application scenarios. 3. Results and model weights: We provide detailed results and analysis on 6 commonly used datasets which can be used as reference for future research. Our pre-trained model can be used for many down-streaming tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action Classification</head><p>The early research on video based action recognition relies on 2D convolutions <ref type="bibr" target="#b25">[26]</ref>. The LSTM <ref type="bibr" target="#b22">[23]</ref> was later proposed to model the image feature based on ConvNet features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60]</ref>. However, the combination of Con-vNet and LSTM did not lead to significantly better perfor-mance. Instead of relying on RNNs, the segment based method TSN <ref type="bibr" target="#b49">[50]</ref> and its permutations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b60">61]</ref> were proposed with good performance.</p><p>Although 2D network was proved successful, the spatiotemporal modeling was still separated. Using 3D convolution for spatio-temporal modeling was initially proposed in <ref type="bibr" target="#b23">[24]</ref> and further extended to the C3D network <ref type="bibr" target="#b43">[44]</ref>. However, training 3D convnet from scratch was hard, initializing the 3D convnet weights by inflate from 2D networks was initially proposed in I3D <ref type="bibr" target="#b5">[6]</ref> and soon proved applicable with different type of 2D network <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55]</ref>. The I3D was used as backbone for many following work including two-stream network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51]</ref>, the networks with focus on temporal modeling <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56]</ref>, and the 3D networks with refined 3D convolution kernels <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>The 3D networks are proved effective but often not efficient, the 3D networks with better performance often requires larger kernels or deeper structures. The recent research demonstrates that depth convolution significantly reduce the computation <ref type="bibr" target="#b44">[45]</ref>, but depth convolution also increase the network inference latency. TSM <ref type="bibr" target="#b34">[35]</ref> and TAM <ref type="bibr" target="#b14">[15]</ref> proposed a more efficient backbone for temporal modeling, however, such design couldn't achieve SOTA performance on Kinetics dataset. The neural architecture search was proposed for action recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref> recently with competitive performance, however, the high latency and limited generalizability remain to be improved.</p><p>The previous methods heavily rely on convolution to aggregate features spatio-temporally, which is not efficient. A few previous work tried to perform global spatio-temporal modeling <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">51]</ref> but still limited by the convolution backbone. The proposed VidTr is fundamentally different from previous works based on convolutions, the VidTr doesn't require heavily stacked convolutions <ref type="bibr" target="#b55">[56]</ref> for feature aggregation but efficiently learn feature globally via attention from first layer. Besides, the VidTr don't rely on sliding convolutions and depth convolutions, which runs at less FLOPs and lower latency compared with 3D convolutions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision Transformer</head><p>The transformers <ref type="bibr" target="#b47">[48]</ref> was previously proposed for NLP tasks <ref type="bibr" target="#b11">[12]</ref> and recently adopted for computer vision tasks. The transformers were roughly used in three different ways in previous works: 1.To bridge the gap between different modalities, e.g. video captioning <ref type="bibr" target="#b61">[62]</ref>, video retrieval <ref type="bibr" target="#b17">[18]</ref> and dialog system <ref type="bibr" target="#b33">[34]</ref>. 2. To aggregate convolutional features for down-streaming tasks, e.g. object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, pose estimation <ref type="bibr" target="#b57">[58]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14]</ref> and action recognition <ref type="bibr" target="#b18">[19]</ref>. 3. To perform feature learning on raw pixels, e.g. most recently image classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Action recognition with self-attention on convolution features <ref type="bibr" target="#b18">[19]</ref> is proved successful, however, convolution also generates local feature and gives redundant computa- tions. Different from <ref type="bibr" target="#b18">[19]</ref> and inspired by very recent work on applying transformer on raw pixels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, we pioneer the work on aggregating spatio-temporal feature from raw videos without relying on convolution features. Different from very recent work <ref type="bibr" target="#b37">[38]</ref> that extract spatial feature with vision transformer on every video frames and then aggregate feature with attention, our proposed method jointly learns spatio-temporal feature with lower computational cost and higher performance. Our work also differs from very recent work <ref type="bibr" target="#b2">[3]</ref>, we present a split attention with better performance without requiring larger video resolution nor extra long clip length. Our intention is not to challenge the convolution based approach but rather explore an alternative to existing methods which can be complementary to convolution based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Transformer</head><p>We introduce the Video Transformer starting with the vanilla video transformer (section 3.1) which illustrates our idea of video action recognition without convolutions. We then present VidTr by first introducing separable-attention (section 3.2), and then the attention pooling to drop nonrepresentative information temporally (section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vanilla Video Transformer</head><p>Following previous efforts in NLP <ref type="bibr" target="#b11">[12]</ref> and image classification <ref type="bibr" target="#b12">[13]</ref>, we adopted the transformer <ref type="bibr" target="#b47">[48]</ref> encoder structure for action recognition that operates on raw pixels. Given a video clip V ∈ R C×T ×W ×H , where T denotes the clip length, W and H denote the video frame width and height, and C denotes the number of channel, we first convert V to a sequence of s × s spatial patches, and apply a linear embedding to each patch, namely S ∈ R T H s W s ×C , where C is the channel dimension after the linear embedding. We add a 1D learnable positional embedding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> to S and following previous work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, append a class token as well, whose purpose is to aggregate features from the whole sequence for classification. This results in S ∈ R ( T W H s 2 +1)×C , where S 0 ∈ R 1×C is the attached class token. S is feed into our transformer encoder structure detailed next.</p><p>As <ref type="figure" target="#fig_0">Figure 2</ref> middle shows, we expand the previous successful ViT transformer architecture for 3D feature learning. Specifically, we stack 12 encoder layers, with each encoder layer consisting of an 8-head self-attention layer and two dense layers with 768 and 3072 hidden units. Different from transformers for 2D images, each attention layer learns a spatio-temporal affinity map Attn ∈</p><formula xml:id="formula_0">R ( T W H s 2 +1)×( T W H s 2 +1) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VidTr</head><p>In <ref type="table" target="#tab_1">Table 2</ref> we show that this simple formulation is capable of learning 3D motion features on a sequence of local patches. However, as explained in <ref type="bibr" target="#b1">[2]</ref>, the affinity attention matrix Attn ∈ R ( T W H s 2 +1)×( T W H s 2 +1) needs to be stored in memory for back propagating, and thus the memory consumption is quadratically related to the sequence length. We can see that the vanilla video transformer increases memory usage for the affinity map from O(W 2 H 2 ) to O(T 2 W 2 H 2 ), leading to T 2 × memory usage for training, which makes it impractical on most available GPU devices. We now address this inefficiency with a separable attention architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Separable-Attention</head><p>To address such memory constraint, we introduce a multihead separable-attention (MSA) by decoupling the 3D selfattention to a spatial attention MSA s and a temporal attention MSA t ( <ref type="figure" target="#fig_0">Figure 2)</ref>:</p><formula xml:id="formula_1">MSA(S) = MSA s (MSA t (S))<label>(1)</label></formula><p>Different from the vanilla video transformer that applies 1D sequential modeling on S, we decouple S to a 2D se-quenceŜ ∈ R (T +1)×( W H s 2 +1)×C with positional embedding and two types of class tokens that append additional tokens along the spatial and temporal dimensions. Here, the spatial class tokens gather information from spatial patches in a single frame using spatial attention, and the temporal class tokens gather information from patches across frames (at same location) using temporal attention. Then the intersection of the spatial and temporal class tokensŜ (0,0,:) is used for the final classification. To decouple 1D selfattention functions on 2D sequential featuresŜ, we first operate on each spatial location (i) independently, applying temporal attention as:</p><formula xml:id="formula_2">S (:,i,:) t = MSA t (k = q = v =Ŝ (:,i,:) ) (2) = pool(Attn t ) · v t (3) = pool(Softmax(q t · k T t )) · v t (4) whereŜ t ∈ R (τ +1)×( W H s 2 +1)×C is the output of MSA t ,</formula><p>pool denotes the down-sampling method to reduce the redundant information (from T to τ , τ = T when no downsampling is performed) that will be described later, q t , k t , and v t denote key, query, and value features after applying independent linear functions (LN) onŜ:</p><formula xml:id="formula_3">qt = LNq(Ŝ (:,i,:) ); kt = LN k (Ŝ (:,i,:) ); vt = LNv(Ŝ (:,i,:) )<label>(5)</label></formula><p>Moreover, Attn t ∈ R (τ +1)×(T +1) represent a temporal attention obtained from matrix multiplication between q t and k t . Following MSA s , we apply a similar 1D sequential selfattention MSA s on spatial dimension: We did not apply a down-sampling method on the spatial attention because there was significant performance drop in our preliminary experiments.</p><formula xml:id="formula_4">S (i,:,:) st = MSA s (k = q = v =Ŝ (i,:,:) t ) (6) = Attn s · v s (7) = Softmax(q s · k T s ) · v s (8) whereŜ st ∈ R (τ +1)×( W H s 2 +1)×C is the output of MSA s , q s ,</formula><p>Our spatio-temporal split attention decreased the memory usage of the transformer layer by reducing the affinity matrix from O(T 2 W 2 H 2 ) to O(τ 2 + W 2 H 2 ). This allows us to explore longer temporal sequence lengths that were infeasible on modern hardware with the vanilla transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Temporal Down-sampling method</head><p>The temporal dimension in video clips usually contains redundant information <ref type="bibr" target="#b28">[29]</ref>. We introduce compact VidTr (C-VidTr) by applying temporal down-sampling within our transformer architecture. We study different temporal down-sampling methods (pool in Eq. 3) including temporal average pooling and 1D convolutions with stride 2, which reduce the temporal dimension by half (details in <ref type="table" target="#tab_6">Table 4d</ref>).</p><p>A limitation of these pooling the methods is that they uniformly aggregate information across time but often in video clips the informative frames are not uniformly distributed. We adopted the idea of non-uniform temporal feature aggregation from previous work <ref type="bibr" target="#b28">[29]</ref>. Different from  <ref type="table" target="#tab_10">Table 1</ref>: Detailed configuration of different VidTr permutations. clip len denotes the sampled clip length and sr stands for the sample rate. We uniformly sample clip len frames out of clip len × sr consecutive frames. The configurations are empirically selected, details in Ablations.</p><p>previous work <ref type="bibr" target="#b28">[29]</ref> that directly down-sample the query using average pooling, we noticed that when a temporal instance is informative, the temporal attention highly activated on a small number of temporal instances, while if a temporal instance is non-informative, the attention is more likely to equally distributed over the length of the clip. Building on this intuition, we propose a topK based pooling (topK std pooling) that orders instances by the standard deviation of each row in the attention matrix. This pooling selects the rows with topK highest standard deviation in the affinity matrix:</p><formula xml:id="formula_5">pool topK std (Attn (1:,:) t ) = Attn topK σ(Attn (1:,:) t ) ,: t<label>(9)</label></formula><p>where σ ∈ R T is row-wise standard deviation of Attn</p><formula xml:id="formula_6">(1:,:) t</formula><p>. Note that the topK std pooling was applied to the affinity matrix excludes the token (Attn (0,:,:) t ) as we always preserve token for information aggregation (more details in Appendix A). Our experiments show that topK std pooling gives better performance than average pooling or convolution. The topK std pooling can be intuitively understood as selecting the frames with strong localized attention and removing frames with uniform attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Model Instantiating: Based on the input clip length and sample rate, we introduce 3 base VidTr models (VidTr-S,VidTr-M and VidTr-L). By applying the different pooling strategies we introduce three compact VidTr permutations (C-VidTr-S,C-VidTr-M and C-VidTr-L). To normalize the feature space, we apply layer normalization before and after the residual connection of each transformer layer and adopt the GELU activation as suggested in <ref type="bibr" target="#b12">[13]</ref>. Detailed configurations can be found in <ref type="table" target="#tab_10">Table 1</ref>. We empirically determined the configuration for different clip length to produce a set of models from low FLOPs and low latency to high accuracy (details in Ablations).</p><p>During training we initialize our model weights from ViT-B <ref type="bibr" target="#b12">[13]</ref>. To avoid over fitting, we adopted the commonly used augmentation strategies including random crop, random horizontal flip. We trained the model using 64 Tesla V100 GPUs, with batch size of 6 per-GPU (for VidTr-S) and weight decay of 1e-5. We adopted SGD as the optimizer but found the Adam optimizer also gives us the same performance. We trained our network for 50 epochs in total with initial learning rate of 0.01, and reduced it by 10 times after epochs 25 and 40. It takes about 12 hours for VidTr-S model to converge, the training process also scales well with fewer GPUs (e.g. 8 GPUs for 4 days). During inference we adopted the commonly used 30-crop evaluation for VidTr and compact VidTr, with 10 uniformly sampled temporal segments and 3 uniformly sampled spatial crop on each temporal segment <ref type="bibr" target="#b50">[51]</ref>. It is worth mentioning that we can further boost the inference speed of compact VidTr by adopting a single pass inference mechanise, this is because the attention mechanism captures global information more effectively than 3D convolution. We do this by training a model with frames sampled in TSN <ref type="bibr" target="#b49">[50]</ref> style, and uniformly sampling N frames in inference (details in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on six of the most widely used datasets. Kinetics 400 <ref type="bibr" target="#b6">[7]</ref> and Kinetics 700 <ref type="bibr" target="#b4">[5]</ref>. Kinetics 400/700 consists of approximately 240K/650K training videos and 20K/35K validation videos trimmed to 10 seconds from 400/700 human action categories. We report top-1 and top-5 classification accuracy on the validation sets. Something-Something V2 <ref type="bibr" target="#b20">[21]</ref> dataset consists of 174 actions and contains 168.9K training videos and 24.7K evaluation videos. We report top-1 accuracy following previous works <ref type="bibr" target="#b34">[35]</ref> evaluation setup. Charades <ref type="bibr" target="#b40">[41]</ref> has 9.8k training videos and 1.8k validation videos spanning about 30 seconds on average. Charades contains 157 multi-label classes with longer activities, performance is measured in mean Average Precision (mAP). UCF-101 <ref type="bibr" target="#b41">[42]</ref> and HMDB-51 <ref type="bibr" target="#b26">[27]</ref> are two smaller datasets. UCF-101 contains 13320 videos with an average length of 180 frames per video and 101 action categories. The HMDB-51 has 6,766 videos and 51 action categories. We report the top-1 classification on the validation videos based on split 1 for both dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Kinetics 400 Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison To SOTA</head><p>We report results on the validation set of Kinetics 400 in <ref type="table" target="#tab_1">Table 2</ref>, including the top-1 and top-5 accuracy, GFLOPs (Giga Floating-Point Operations) and latency (ms) required to compute results on one view.</p><p>As shown in  significantly outperform previous SOTA at roughly same computational budget, e.g. at around 200 GFLOPs, the VidTr-M outperform I3D50 by 3.6%, NL50 by 2.1%,and TPN50 by 0.9%. Given the similar performance, the VidTr is also significantly computationally efficient comparing with previous SOTA, e.g. at around 78% top-1 accuracy, the VidTr-S has 6× less FLOPs than NL-101, 2× less FLOPs than TPN and 12% less FLOPs than Slowfast-101. We also see that our VidTr outperform I3D based networks at higher sample rate (e.g. s = 8, TPN achieved 76.1% top-1 accuracy), this denotes, the global attention learns temporal information more effectively than 3D convolutions. X3D-XXL from architecture search is the only network outperforms us, using architecture search technique for attention based architecture design will be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Compact VidTr</head><p>We evaluate the effectiveness of our compact VidTr with the proposed temporal down-sampling method ( <ref type="table" target="#tab_10">Table 1</ref>). The results <ref type="table" target="#tab_4">(Table 3)</ref> show that the proposed down-sampling strategy reduced about 56% of the computation required by VidTr with only 2% performance drop in accuracy. The compact VidTr complete the VidTr family from small models (only 39GFLOPs) to high performance models (up to 79.1% accuracy). Comparing with previous SOTA compact models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37]</ref>, our compact VidTr achieves better or sim-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Error Analysis</head><p>We compare the errors made by VidTr-S and the I3D50 network to better understand the local networks (I3D) and global networks (VidTr) behavior. We provide the top-5 activities that our VidTr-S gain most significant improvement over the I3D50 (details in Appendix D). We find that our VidTr-S outperformed the I3D on the activities that requires long-term video contexts to be recognized. For example, our VidTr-S outperformed the I3D50 on "making a cake" by 26% in accuracy. The I3D50 overfits to "cakes" and often recognize making a cake as eating a cake. We also analyze the top-5 activities where I3D does better than our VidTr-S. Our VidTr-S performs poorly on the activities that need to capture fast and local motions. For example, our VidTr-S performs 21% worse in accuracy on "shaking head" (detailed results in Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Ensemble Analysis</head><p>Inspired by the findings in our error analysis that the VidTr and I3D seem to have different strengths. We ensemble our VidTr with a light weight I3D50 network by averaging the output values between the two networks. Based on the results in <ref type="table" target="#tab_1">Table 2</ref>, the VidTr ensemble with I3D50 achieves a roughly 2% performance improvement on Kinetics 400 with limited additional FLOPs (37G). The performance gain by ensembling the VidTr with I3D is more significant than the improvement by ensembling other networks with I3D (see Appendix C for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Ablations</head><p>We perform all ablation experiments with our VidTr-S model on Kinetics 400. We used 8 × 224 × 224 input with * we measure latency of X3D using the authors' code https://github.com/facebookresearch/SlowFast/ blob/master/projects/x3d/README.md, which only has models for X3D-M and X3D-L and not the XL and XXL variants   <ref type="figure" target="#fig_0">16 2 )</ref>, where the video is represented as a sequence of spatial patches. Our results <ref type="table" target="#tab_6">(Table 4a</ref>) show that the model using cubic patches with longer temporal size has fewer FLOPs but results to significant performance drop (73.1 vs. 75.5). The model using square patch significantly outperform all the cubic patch based models, because the linear embedding is not enough to represent the shot-term temporal association in the cubic. We further compared the performance of using different patch sizes (1 × 16 2 vs. 1 × 32 2 ), using 32 2 patches lead to 4× decreasing of the sequence length, which decreases memory consumption of the affinity matrices by 16×, however, using 16 2 patches significantly outperform the model using 32 2 patches (77.7 vs. 71.2). We did not evaluate the model using smaller patching sizes (e.g., 8 × 8) because of the high memory consumption. Attention module design: We compare different attention module designs, including spatial modeling only, jointly spatio-temporal modeling module (vanilla-Tr), and our proposed separable-attention (VidTr). We first evaluate an spatio-only transformer. We average the class token for each input frame for our final output. Our results <ref type="table" target="#tab_6">(Table  4b)</ref> show that the spatio-only transformer requires the least memory but also has worst performance among different attention modules. This shows that temporal modeling is critical for attention based architecture as well. The joint spatio-temporal transformer significantly outperforms the spatio-only transformer but is also heavily memory consuming (T 2 times for the affinity matrices). Our VidTr using separable-attention requires 3.3× less memory at no performance drop. We further test if the order of spatial attention and temporal attention matters, the results <ref type="table" target="#tab_6">(Table  4b)</ref> show that the order of temporal attention does not affect the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence down-sampling comparison:</head><p>We compare different down-sampling strategy including temporal average pooling, 1D temporal convolution and the proposed STD-based topK pooling method. The results <ref type="table" target="#tab_6">(Table  4d)</ref> show that our proposed STD-based down-sampling method outperformed the temporal average pooling and the convolution-based down-sampling strategies that uniformly aggregate information over time.</p><p>Backbone generalization: We evaluate our VidTr initialized with different models, including T2T <ref type="bibr" target="#b58">[59]</ref>, ViT-B, and ViT-L. The results on <ref type="table" target="#tab_6">Table 4c</ref> show that our VidTr achieves reasonable performance across all backbones. The VidTr using T2T as the backbone has the lowest FLOPs but also the lowest accuracy. The Vit-L-based VidTr achieve similar performance with the Vit-B-based VidTr even with 3× FLOPs. As showed in previous work <ref type="bibr" target="#b12">[13]</ref>, transformerbased network are more likely to over-fit and Kinetics-400 is relatively small for Vit-L-based VidTr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Where to down-sample:</head><p>Finally we study where to perform temporal down-sampling. We perform temporal down-sampling at different layers <ref type="table" target="#tab_6">(Table 4e</ref>). Our results <ref type="table" target="#tab_6">(Table 4e)</ref> show that starting to perform down-sampling after the first encoder layer has the best trade-off between the performance and FLOPs. Starting to perform downsampling at very beginning leads to the fewest FLOPs but   <ref type="table" target="#tab_10">Table 6</ref>: Results on Charades dataset and somethingsomething-V2 dataset. The evaluation metrics are mean average precision (mAP) in percentage for charades, top-1 accuracy for something-something-V2 (TSN styled dataloader is used), UCF and HMDB.</p><p>has a significant performance drop (72.9 vs. 74.9). Performing down-sampling later only has slight performance improvement but requires higher FLOPs. We then analyze how many layers should we skip between two down-sample layers. Based on the results in <ref type="table" target="#tab_6">Table 4f</ref>, skip one layer between the two down-samples has the best trade-off. Performing down-samples on consecutive layers (0 skip layers) has lowest FLOPs but the performance decreases (73.9 vs. 74.9). Skipping more layers did not show significant performance improvement but have higher FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Run-time Analysis</head><p>We further analyzed the trade-off between latency, FLOPs and accuracy. We note that the VidTr achieved the best balance between these factors <ref type="figure" target="#fig_2">(Figure 3)</ref>. The VidTr-S achieve similar performance but significantly less FLOPs compare with I3D101-NL (5× less FLOPs), Slowfast101 8 × 8 (12% less FLOPs), TPN101 (2× less FLOPs), and CorrNet50 (20× less FLOPs). Note that the X3D has very low FLOPs but high latency due to the use of depth convolution. Our experiments show that the X3D-L has about 3.6× higher latency comparing with VidTr-S <ref type="figure" target="#fig_2">(Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">More Results</head><p>Kinetics-700 Results: Our experiments show a consistent performance trend on Kinetics 700 ( <ref type="table" target="#tab_8">Table 5</ref>). The VidTr-S significantly outperformed the baseline I3D model (+9%), the VidTr-M achieved the performance comparable to NUTA-50, Slowfast101 8×8 and the VidTr-L is comparable to previous SOTA slowfast101-nonlocal and NUTA101. There is a small performance gap between our model and Slowfast-NL <ref type="bibr" target="#b16">[17]</ref>, because Slowfast is pre-trained on both Kinetics 400 and 600 while we only pre-trained on Kinetics 400. Previous finding on VidTr and I3D as being complementary is consistent on Kinetics 700, ensemble VidTr-L with I3D leads to +0.6% performance boost.</p><p>Charades Results: We compare our VidTr with previous SOTA models on Charades. Our VidTr-L outperformed previous SOTA methods LFB and NUTA101, and achieved the performance comparable to Slowfast101-NL ( <ref type="table" target="#tab_10">Table 6</ref>).</p><p>The results on Charades demonstrates that our VidTr generalizes well to multi-label activity datasets. Our VidTr performs worse than the current SOTA networks (X3D-XL) on Charades likely due to overfitting. As discussed in previous work <ref type="bibr" target="#b12">[13]</ref>, the transformer-based networks overfit easier than convolution-based models, and Charades is relatively small. We observed a similar finding with our ensemble, ensembling our VidTr with a I3D network (40.3 mAP) achieved SOTA performance (additional ensemble results including ensembling with CSN-152 to achieve 51.2% mAP are in Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Something-something V2 Results:</head><p>We noticed that the VidTr doesn't work well on the something-something dataset ( <ref type="table" target="#tab_10">Table 6</ref>), probably because purely transformer based approaches do not model local motion as well as convolutions. This aligns with our observation in our error analysis. Further improving the local motion modeling ability is a good area for future work. UCF and HMDB Results: Finally we train our VidTr on two small dataset UCF-101 and HMDB-51 to test if VidTr generalizes to smaller datasets. The VidTr achieved SOTA comparable performance with 6 epochs of training (96.6% on UCF and 74.4% on HMDB), showing that the model generalize well on small dataset <ref type="table" target="#tab_10">(Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Visualization and Understanding VidTr</head><p>We first visualized the VidTr's separable-attention with attention roll-out method <ref type="bibr" target="#b0">[1]</ref>  <ref type="figure" target="#fig_4">(Figure 4a</ref>, implementation details and more samples in Appendix E). We find that the spatial attention is able to focus on informative regions and temporal attention is able to skip the duplicated/nonrepresentative information temporally. We then visualized the attention at 4th, 8th and 12th layer of VidTr <ref type="figure" target="#fig_4">(Figure 4b)</ref>, we found the spatial attention is getting to concentrate bet-  ter when it goes to the deeper layer. The attention did not capture meaningful temporal instances at early stages because the temporal feature relies on the spatial information to determine informative temporal instances. Finally we compared the I3D activation map and rollout attention from VidTr <ref type="figure" target="#fig_4">(Figure 4c</ref>). The I3D mis-classified the catching fish as sailing, as the I3D attention focused on the people sitting behind and water. The VidTr is able to make the correct prediction and the attention showed that the VidTr is able to focus on the action related regions across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present video transformer with separable-attention, an novel stacked attention based architecture for video action recognition. Our experimental results show that the proposed VidTr achieves state-of-the-art or comparable performance on five public action recognition datasets. The experiments and error analysis show that the VidTr is especially good at modeling the actions that requires long-term reasoning. Further combining the advantage of VidTr and convolution for better local-global action modeling <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b53">54]</ref> and adopt self-supervised training <ref type="bibr" target="#b7">[8]</ref> on large-scaled data will be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Compact-VidTr implementation details</head><p>We provide some details for topK std pooling. We calculate the row-wise standard deviation as:</p><formula xml:id="formula_7">σ (i) = 1 T T i=1 (Attn (i,:) t − µ) 2<label>(10)</label></formula><formula xml:id="formula_8">µ (i) = 1 T T i=1 Attn (i,:) t<label>(11)</label></formula><p>where σ ∈ R T and µ ∈ R T are row-wise standard deviation, and mean of Attn . Note that the topK std pooling was applied to the affinity map excluded the token Attn (1:,:) t as we will always preserve token for information aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fast VidTr</head><p>As a common practice, 3D ConvNets are usually tested on 30 crops per video clip (3 spatial and 10 temporal) that show performance boost while greatly increase the computation cost. The VidTr has been proved that learn longterm global spatio-temporal features better in a video clip, thus we propose to sample the data in TSN style (segment video into N chunks and randomly pick one frame from each chunk). During testing, we uniformly sample N frames from the video regardless the length of the video, and perform single-pass inference (center crop). Such design significantly reduce the inference computation and latency caused by the dense sampling with a very small performance drop (about 2%, see <ref type="table" target="#tab_10">Table A</ref>.1). Note that the R2D and I3D based methods do not work well with sparsely sampled frames, mainly because the convolution kernel has limited receptive field and can only aggregate features slowly. If adjacent frames are too far away from each other, the temporal convolution will not be able to establish the temporal relations well. We compare our fast VidTr model with pre-  vious SOTA light-weight models including TSM, TEINet and models from architecture search such as X3D on Kinetics 400 dataset and report the FLOPs, the latency and top1 accuracy with 10 center crops <ref type="table" target="#tab_10">(Table A.</ref>1) The results show that our proposed one-pass inference significantly outperforms the competitors with less FLOPs, lower latency and higher accuracy. The Fast VidTr (16 frames) is able to outperform TSM (+0.6% accuracy, 70% less FLOPs, 68% less latency); TEINet (-0.2% accuracy, 94% less FLOPs, 95% less latency), also note that the reported TEINet score is based on 30 crop evaluation; and X3D-M (+0.1% accuracy, 24% more FLOPs, 96% less latency). The results proves that the VidTr is able to aggregate long-term spatiotemporal features more effectively comparing the 3D Con-vNets.</p><p>It is worth mentioning that: 1. Even without considering the 10-crop evaluation required for ConvNets to achieve reported scores, the VidTr is still able to inference roughly at same speed comparing with TEINet and significantly faster than X3D. 2. X3D has low FLOPs but high latency mainly due to the heavily use of depth convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Ensemble Results</head><p>We provide additional ensemble results on Kinetics 400 <ref type="table" target="#tab_10">(Table A.</ref>2) and charades ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Error Analysis Details</head><p>We show the top 5 classes that gains performance boost from VidTr and top 5 classes that got reduced performance from VidTr. The results <ref type="table" target="#tab_10">(Table A.</ref> <ref type="bibr" target="#b3">4)</ref> show that the I3D generally performance well on local and fast action while the VidTr works well on actions require long-term temporal information. For example, the VidTr achieved 21.2 % accuracy improvement over I3D on "catching fish" that requires long-term information from the status when the fish is in water to the final status after the fish is caught ( <ref type="figure" target="#fig_5">Figure  A.1a</ref>). The VidTr performs worse than I3D on the activities that rely on slight motions (e.g., playing guitar, and shaking head, <ref type="figure">Figure A.</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization Details</head><p>We visualized the VidTr's separable-attention with attention roll-out method <ref type="bibr" target="#b0">[1]</ref>. We multiplied all the affinity matrices between every two encoder layers and get mask t ∈ R (W H+1)×(T +1)×(T +1) for the temporal roll-out attention and mask s ∈ R (T +1)×(W H+1)×(W H+1) for the spatial roll-out attention. We selected the rows of class token from the roll-out attention for visualization as: </p><p>We multiplied mask t and mask s to represent the spatialtemporal attention for visualize as:</p><formula xml:id="formula_10">mask st = Re(mask t ) × mask s<label>(14)</label></formula><p>where mask st is the spatio-temporal attention for visualize, and Re denotes a reshape function. We threshold mask s and mask st by only highlighting the top 30% of values of them, and attached them onto the original frames for visualizing the spatio-only and spatio-temporal attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. More Visualizations</head><p>We first show more results of the VidTr's separableattention with attention roll-out method <ref type="bibr" target="#b0">[1]</ref>  <ref type="figure" target="#fig_0">(Figure A.2)</ref>. We find that the spatial attention is able to focus on informative regions and temporal attention is able to skip the duplicated/non-representative information temporally.</p><p>We then show more results of the attention at 4th, 8th and 12th layer of VidTr ( <ref type="figure" target="#fig_2">Figure A.3)</ref>, we found the spatial attention is getting to concentrate better when it goes to the deeper layer. The attention did not capture meaningful temporal instances at early stages because the temporal feature relies on the spatial information to determine informative temporal instances.</p><p>Finally we compared the I3D activation map and rollout attention from VidTr <ref type="figure" target="#fig_4">(Figure A.4)</ref>. The I3D mis-classified the catching fish as sailing, as the I3D attention focused on the people sitting behind and water. The VidTr is able to make the correct prediction and the attention showed that the VidTr is able to focus on the action related regions across time.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Spatio-temporal separable-attention video transformer (VidTr). The model takes pixels patches as input and learn the spatial temporal feature via proposed separableattention. The green shaded block denotes the down-sample module which can be inserted into VidTr for higher efficiency. τ denotes the temporal dimension after downsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>k s , and v s denotes key, query, and value features after applying independent linear functions onŜ t . Attn s ∈ R ( W H s 2 +1)×( W H s 2 +1) represent a spatial-wise affinity map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The comparison between different models on accuracy, FLOPs and latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) The spatial and temporal attention in Vidtr. The attention is able to focus on the informative frames and regions.(b) The rollout attentions from different layers of VidTr. (c) Comparison of I3D activations and VidTr attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of spatial and temporal attention of VidTr and comparison with I3D activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 :</head><label>1</label><figDesc>,:) t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) video examples that VidTr performs better than I3D.(b) video examples that VidTr performs worse than I3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 1 :</head><label>1</label><figDesc>Visualizations of video samples that VidTr works better and I3D works better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>mask s = mask (1:,0,1:) s ∈ R T ×W H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A. 2 :</head><label>2</label><figDesc>The spatial and temporal attention in Vidtr. The attention is able to focus on the informative frames and regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A. 3 :</head><label>3</label><figDesc>The rollout attentions from different layers of VidTr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A. 4 :</head><label>4</label><figDesc>Comparison of I3D activations and VidTr attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Input</cell><cell>GFLOPs</cell><cell>Lat.</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>I3D50 [57]</cell><cell>32 × 2</cell><cell>167</cell><cell>74.4</cell><cell>75.0</cell><cell>92.2</cell></row><row><cell>I3D101 [57]</cell><cell>32 × 2</cell><cell>342</cell><cell>118.3</cell><cell>77.4</cell><cell>92.7</cell></row><row><cell>NL50 [51]</cell><cell>32 × 2</cell><cell>282</cell><cell>53.3</cell><cell>76.5</cell><cell>92.6</cell></row><row><cell>NL101 [51]</cell><cell>32 × 2</cell><cell>544</cell><cell>134.1</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>TEA50 [32]</cell><cell>16 × 2</cell><cell>70</cell><cell>-</cell><cell>76.1</cell><cell>92.5</cell></row><row><cell>TEINet [37]</cell><cell>16 × 2</cell><cell>66</cell><cell>49.5</cell><cell>76.2</cell><cell>92.5</cell></row><row><cell>CIDC [30]</cell><cell>32 × 2</cell><cell>101</cell><cell>82.3</cell><cell>75.5</cell><cell>92.1</cell></row><row><cell>SF50 8 × 8 [17]</cell><cell>(32+8)×2</cell><cell>66</cell><cell>49.3</cell><cell>77.0</cell><cell>92.6</cell></row><row><cell>SF101 8 × 8 [17]</cell><cell>(32+8)×2</cell><cell>106</cell><cell>71.9</cell><cell>77.5</cell><cell>92.3</cell></row><row><cell>SF101 16 × 8 [17]</cell><cell>(64+16)×2</cell><cell>213</cell><cell>124.3</cell><cell>78.9</cell><cell>93.5</cell></row><row><cell>TPN50 [57]</cell><cell>32 × 2</cell><cell>199</cell><cell>89.3</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>TPN101 [57]</cell><cell>32 × 2</cell><cell>374</cell><cell>133.4</cell><cell>78.9</cell><cell>93.9</cell></row><row><cell>CorrNet50 [49]</cell><cell>32 × 2</cell><cell>115</cell><cell>-</cell><cell>77.2</cell><cell>N/A</cell></row><row><cell>CorrNet101 [49]</cell><cell>32 × 2</cell><cell>187</cell><cell>-</cell><cell>78.5</cell><cell>N/A</cell></row><row><cell>X3D-XXL [16]</cell><cell>16 × 5</cell><cell>196</cell><cell>-</cell><cell>80.4</cell><cell>94.6</cell></row><row><cell>Vanilla-Tr</cell><cell>8 × 8</cell><cell>89</cell><cell>59.1</cell><cell>77.5</cell><cell>93.2</cell></row><row><cell>VidTr-S</cell><cell>8 × 8</cell><cell>89</cell><cell>65.3</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>VidTr-M</cell><cell>16 × 4</cell><cell>179</cell><cell>114.9</cell><cell>78.6</cell><cell>93.5</cell></row><row><cell>VidTr-L</cell><cell>32 × 2</cell><cell>351</cell><cell>138.8</cell><cell>79.1</cell><cell>93.9</cell></row><row><cell>En-VidTr-S</cell><cell>8 × 8</cell><cell>130</cell><cell>102.3</cell><cell>79.4</cell><cell>94.0</cell></row><row><cell>En-VidTr-M</cell><cell>16 × 4</cell><cell>220</cell><cell>151.9</cell><cell>79.7</cell><cell>94.2</cell></row><row><cell>En-VidTr-L</cell><cell>32 × 2</cell><cell>392</cell><cell>175.8</cell><cell>80.5</cell><cell>94.6</cell></row></table><note>, the VidTr achieved the SOTA per- formance comparing with previous I3D based SOTA archi- tectures at lower FLOPs and latency. The VidTr is able to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on Kinetics-400 dataset. We report top- 1 accuracy(%) on the validation set. The 'Input' column indicates what frames of the 64 frame clip are actually sent to the network. n×s input indicates we feed n frames to the network sampled every s frames. Lat. stands for the latency on single crop.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of VidTr to other fast networks. We present the number of views used for evaluation and FLOPs required for each view. The latency denotes the total time required to get the reported top-1 score.</figDesc><table><row><cell>ilar performance with lower FLOPs and latency, including:</cell></row><row><cell>TEA (+0.6% with 16% less FLOPs) and TEINet (+0.5%</cell></row><row><cell>with 11% less FLOPs).</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Ablation studies on Kinetics 400 dataset. We use an VidTr-S backbone with 8 frames input for (a,b) and C- VidTr-S for (c,d). The evaluation is performed on 30 views with 8 frame input unless specified. FP. stands for FLOPs.a frame sample rate of 8, and 30-view evaluation. Patching strategies: We first compare the cubic patch (4 × 16 2 ), where the video is represented as a sequence of spatio-temporal patches, with the square patch (1 ×</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">Results on Kinetics-700 dataset. We report top-</cell></row><row><cell cols="6">1 and top-5 accuracy (%) on validation set. K4&amp;K6 stand</cell></row><row><cell cols="6">for Kinetics 400 and Kinetics 600. SF-NL 101 stands for</cell></row><row><cell cols="5">Slowfast network with non-local block [17].</cell><cell></cell></row><row><cell>Model</cell><cell>Input</cell><cell>Chad</cell><cell>SSv2</cell><cell>UCF</cell><cell>HMDB</cell></row><row><cell>I3D [6]</cell><cell>64 × 1</cell><cell>32.9</cell><cell>50.0</cell><cell>95.1</cell><cell>74.3</cell></row><row><cell>TSM [35]</cell><cell>8(TSN)</cell><cell>-</cell><cell>59.3</cell><cell>94.5</cell><cell>70.7</cell></row><row><cell>I3D101 [56]</cell><cell>32 × 4</cell><cell>40.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STRG [52]</cell><cell>32 × 4</cell><cell>39.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LFB [53]</cell><cell>32 × 4</cell><cell>42.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NL-101 [51]</cell><cell>32 × 4</cell><cell>37.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TEINet[37]</cell><cell>16 (TSN)</cell><cell>-</cell><cell>62.1</cell><cell>96.7</cell><cell>73.3</cell></row><row><cell>SF101 [17]</cell><cell>(64 + 8)×2</cell><cell>-</cell><cell>60.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SF101-NL [17]</cell><cell>(64 + 8)×2</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>X3D-XL [16]</cell><cell>16 × 5</cell><cell>47.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VidTr-M</cell><cell>16 × 4</cell><cell>-</cell><cell>60.1</cell><cell>96.6</cell><cell>74.4</cell></row><row><cell>VidTr-L</cell><cell>32 × 4</cell><cell>43.5</cell><cell>60.2</cell><cell>96.7</cell><cell>74.4</cell></row><row><cell>En-VidTr-L</cell><cell>32 × 4</cell><cell>47.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>.1: Comparison of VidTr to other fast networks. All</cell></row><row><cell>results from previous methods except for TEINet (30-crops)</cell></row><row><cell>are based on 10 temporal crop and center spatial crop. The</cell></row><row><cell>VidTr was achieved by uniformly sample 8/16/32 frames</cell></row><row><cell>temporally and center-crop spatially.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A</head><label>A</label><figDesc>.3), showing that the VidTr and 3D convolution based models can be complementary to each other, ensemble VidTr and 3D convolution based network significantly outperform the ensemble of any two 3D convolution based models. Our results show that the result level ensemble of I3D-101 and SOTA 3D model TPN-101 lead to about 1% accuracy boost and result level ensemble of VidTr-S with TPN-101 lead to about 3% performance boost. The similar conclusion can be draw from Charades on multi-label activities, where the ensemble of I3D-101 and CSN-152 only gives 2.8%mAP boost, while ensemble of VidTr-L with CSN-152 lead to SOTA (4.8%mAP boost over CSN-152) performance on Charades datasets.Table A.3: Results on Charades dataset. The evaluation metrics are mean average precision (mAP) in percentage.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell cols="3">Res. Ensemble Chad</cell><cell></cell></row><row><cell>I3D-Inception [6]</cell><cell>64 × 1</cell><cell>256</cell><cell>-</cell><cell>32.9</cell><cell></cell></row><row><cell cols="2">SlowFast-101-NL  *  32 × 4</cell><cell>256</cell><cell>-</cell><cell>44.7</cell><cell></cell></row><row><cell>CSN-152  *</cell><cell>32 × 4</cell><cell>256</cell><cell>-</cell><cell>46.4</cell><cell></cell></row><row><cell>En-I3D-101</cell><cell>32 × 4</cell><cell>256</cell><cell>I3D-50</cell><cell>42.1</cell><cell></cell></row><row><cell>En-I3D-101</cell><cell>32 × 4</cell><cell>256</cell><cell>SF-101</cell><cell>47.9</cell><cell></cell></row><row><cell>En-I3D-101</cell><cell>32 × 4</cell><cell>256</cell><cell>CSN-152</cell><cell>49.2</cell><cell></cell></row><row><cell>En-VidTr-L</cell><cell>32 × 4</cell><cell>224</cell><cell>I3D-101</cell><cell>47.3</cell><cell></cell></row><row><cell>En-VidTr-L</cell><cell>32 × 4</cell><cell>224</cell><cell>SF-101</cell><cell>48.9</cell><cell></cell></row><row><cell>En-VidTr-L</cell><cell>32 × 4</cell><cell>224</cell><cell>CSN-152</cell><cell>51.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell>input</cell><cell>Ensemble</cell><cell>input</cell><cell>Top1</cell><cell>Top5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>I3D50 [56]</cell><cell>16 × 4</cell><cell>-</cell><cell>-</cell><cell>75.0</cell><cell>92.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>I3D101 [56]</cell><cell>16 × 4</cell><cell>-</cell><cell>-</cell><cell>77.4</cell><cell>92.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TPN101 [56]</cell><cell>16 × 4</cell><cell>-</cell><cell>-</cell><cell>78.2</cell><cell>93.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>I3D50 [56]</cell><cell>16 × 4</cell><cell>I3D101</cell><cell>16 × 4</cell><cell>77.7</cell><cell>93.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TPN101[56]</cell><cell>16 × 4</cell><cell>I3D50</cell><cell>16 × 4</cell><cell>78.5</cell><cell>93.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TPN101 [56]</cell><cell>16 × 4</cell><cell>I3D101</cell><cell>16 × 4</cell><cell>79.3</cell><cell>93.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VidTr-S</cell><cell>8 × 8</cell><cell>I3D50</cell><cell>16 × 4</cell><cell>79.4</cell><cell>94.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VidTr-S</cell><cell>8 × 8</cell><cell>I3D101</cell><cell>16 × 4</cell><cell>80.3</cell><cell>94.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VidTr-S</cell><cell>8 × 8</cell><cell>TPN101</cell><cell>16 × 4</cell><cell>80.5</cell><cell>94.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Table A.2: More ensemble results on Kinetics-400 dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">We report top 1 and top5 accuracy (%) on validation set.</cell></row></table><note>* denotes the result that we re-produced.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc></figDesc><table /><note>4: Quantitative analysis on Kinetics-400 dataset. The performance gain is defined as the disparity of the top- 1 accuracy between VidTr network and that of I3D.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150,2020.3</idno>
		<title level="m">The long-document transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalla</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08833</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00869</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">STM: SpatioTemporal and Motion Encoding for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multigranular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Nuta: Non-uniform temporal aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08041</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Directional temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="275" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TEA: Temporal Excitation and Aggregation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vijay Mahadevan, and Nuno Vasconcelos. VLAD3: Encoding Dynamics of Deep Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bridging text and video: A universal multimodal transformer for video-audio scene-aware dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00163</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TEINet: Towards an Efficient Architecture for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06961</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tiny video networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal interlacing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11966" to="11973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition in video sequences using deep bi-directional lstm with cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamil</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1155" to="1166" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal Pyramid Network for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Transpose: Towards explainable human pose estimation by transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14214</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal Relational Reasoning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
