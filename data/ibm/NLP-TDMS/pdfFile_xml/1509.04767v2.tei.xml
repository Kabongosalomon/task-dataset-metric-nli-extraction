<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Learning via Semantic Similarity Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
							<email>zzhang14@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Learning via Semantic Similarity Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While there has been significant progress in large-scale classification in recent years <ref type="bibr" target="#b30">[31]</ref>, lack of sufficient training data for every class and the increasing difficulty in finding annotations for a large fraction of data might impact further improvements.</p><p>Zero-shot learning is being increasingly recognized as a way to deal with these difficulties. One version of zero shot learning is based on so-called source and target domains. Source domain is described by a single vector corresponding to each class based on side information such as attributes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, language words/phrases <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, or even learned classifiers <ref type="bibr" target="#b41">[42]</ref>, which we assume can be collected easily. The target domain is described by a joint distribution of images/videos and labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>. During training time, we are given source domain attributes and target domain data corresponding to only a subset of classes, which we call seen classes. During test time, source domain attributes for unseen (i.e. no training data provided)  classes are revealed. The goal during test time is to predict for each target domain instance which of the seen/unseen classes it is associated with. Key Idea: Our proposed method is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. We view target data instances as arising from seen instances and attempt to express source/target data as a mixture of seen class proportions. Our algorithm is based on the postulate that if the mixture proportion from target domain is similar to that from source domain, they must arise from the same class. This leads us to learning source and target domain embedding functions using seen class data that map arbitrary source and target domain data into mixture proportions of seen classes. We propose parameterized-optimization problems for learning semantic similarity embedding (SSE) functions from training data and jointly optimize predefined parameters using cross validation on held-out seen class data. Our method necessitates fundamentally new design choices requiring us to learn class-dependent feature transforms because components of our embedding must account for contribution of each seen class. Our source domain embedding is based on subspace clustering literature <ref type="bibr" target="#b36">[37]</ref> that are known to be resilient to noise. Our target domain embedding is based on a margin-based framework using the intersection function or the rectified linear unit (ReLU) <ref type="bibr" target="#b21">[22]</ref>, which attempts to align seen class source domain data with their corresponding seen class target domain data instances. Finally, we employ a cross validation technique based on holding out seen class data and matching held-out seen classes to optimize parameters used in the optimization problems for source and target domain. In this way we jointly optimize parameters to best align mixture proportions for held-out seen classes and provide a basis for generalizing to unseen classes. Results on several benchmark datasets for zero-shot learning demonstrate that our method significantly improves the current state-of-the-art results. Related Work: Most existing zero-shot learning methods rely on predicting side information for further classification. <ref type="bibr" target="#b23">[24]</ref> proposed a semantic (i.e. attribute) output code classifier which utilizes a knowledge base of semantic properties. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref> proposed several probabilistic attribute prediction methods. <ref type="bibr" target="#b41">[42]</ref> proposed designing discriminative categorylevel attributes. <ref type="bibr" target="#b17">[18]</ref> proposed an optimization formulation to learn source domain attribute classifiers and attribute vectors jointly. <ref type="bibr" target="#b19">[20]</ref> proposed learning the classifiers for unseen classes by linearly combining the classifiers for seen classes. <ref type="bibr" target="#b0">[1]</ref> proposed a label embedding method to embed each class into an attribute vector space. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> directly learned the mapping functions between the feature vectors in source and target domains with deep learning. Such methods may suffer from noisy (e.g. missing or incorrectly annotated) side information or data bias, leading to unreliable prediction.</p><p>Some recent work has been proposed to overcome some issues above. <ref type="bibr" target="#b27">[28]</ref> proposed a propagated semantic transfer method by exploiting unlabeled instances. <ref type="bibr" target="#b9">[10]</ref> discussed the projection domain shift problem and proposed a transductive multi-view embedding method. <ref type="bibr" target="#b13">[14]</ref> investigated the attribute unreliability issue and proposed a random forest approach. <ref type="bibr" target="#b29">[30]</ref> proposed a simple method by introducing a better regularizer.</p><p>An important conceptual difference that distinguishes our method from other existing works such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, is that these methods can be interpreted as learning relationships between source attributes and target feature components (in the encoded space), while our method is based on leveraging similar class relationships (semantic affinities) in source and target domains, requiring class dependent feature transform. This leads to complex scoring functions, which cannot be simplified to linear or bilinear forms as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Semantic similarity embedding (SSE) is widely used to model the relationships among classes, which is quite insensitive to instance level noise. <ref type="bibr" target="#b39">[40]</ref> proposed learning mapping functions to embed input vectors and classes into a low dimensional common space based on class taxonomies. <ref type="bibr" target="#b2">[3]</ref> proposed a label embedding tree method for large multiclass tasks, which also embeds class labels in a low dimensional space. <ref type="bibr" target="#b11">[12]</ref> proposed an analogy-preserving semantic  embedding method for multi-class classification. Later <ref type="bibr" target="#b12">[13]</ref> proposed a unified semantic embedding method to incorporate different semantic information into learning. Recently <ref type="bibr" target="#b22">[23]</ref> proposed a semantic embedding method for zero-shot learning to embed an unseen class as a convex combination of seen classes with heuristic weights. <ref type="bibr" target="#b10">[11]</ref> proposed a semantic ranking representation based on semantic similarity to aggregate semantic information from multiple heterogeneous sources. Our embedding is to represent each class as a mixture of seen classes in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Zero-Shot Learning and Prediction</head><p>Our notation is summarized in <ref type="table" target="#tab_2">Table 1</ref> for future reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>Our method is based on expressing source/target data as a mixture of seen class proportions (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Using seen class data we learn source and target domain embedding functions, ψ, π respectively. Our aim is to construct functions that take an arbitrary source vectors c and target vectors x as inputs and embed them into ∆ |S| (histograms). Observe that components, π y (x), ψ y (c) of π(x), ψ(c), corresponding to seen class y ∈ S, denote the proportion of class y in the instance x, c. During test-time source domain vectors c u ∈ C for all the unseen classes are revealed. We are then presented with an arbitrary target instance x. We predict an unseen label for x by maximizing the semantic similarity between the histograms. Letting z u = ψ(c u ), then our zero-shot recognition rule is defined as follows:</p><formula xml:id="formula_0">u * = arg max u∈U f (x, u) = arg max u∈U π(x), z u ,<label>(1)</label></formula><p>where ·, · denotes the inner product of two vectors. We propose parameterized-optimization problems to learn embedding functions from seen class data. We then optimize these parameters globally using held-out seen class data. We summarize our learning scheme below. (A) Source Domain Embedding Function (ψ): Our embedding function is realized by means of a parameterized optimization problem, which is related to sparse coding. (B) Target Domain Embedding Function (π): We model π y (x) as w, φ y (x) . This consists of a constant weight vector w and a class dependent feature transformation φ y (x). We propose a margin-based optimization problem to jointly learn both the weight vector and the feature transformation. Note that our parameterization may yield negative values and may not be normalized, which can be incorporated as additional constraints but we ignore this issue in our optimization objectives. (C) Cross Validation: Our embedding functions are parameter dependent. We choose these parameters by employing a cross validation technique based on holding out seen class data. First, we learn embedding functions (see (A) and (B)) on the remaining (not held-out) seen class data with different values of the predefined parameters. We then jointly optimize parameters of source/target embedding functions to minimize the prediction error on held-out seen classes. In the end we re-train the embedding functions over the entire seen class data. Salient Aspects of Proposed Method: (a) Decomposition: Our method seeks to decompose source and target domain instances into mixture proportions of seen classes. In contrast much of the existing work can be interpreted as learning cross-domain similarity between source domain attributes and target feature components. (b) Class Dependent Feature Transformation π y (x): The decomposition perspective necessitates fundamentally new design choices. For instance, π y (x), the component corresponding to class y must be dependent on y, which implies that we must choose a class dependent feature transform φ y (x) because w is a constant vector and agnostic to class. (c) Joint Optimization and Generalization to Unseen Classes: Our method jointly optimizes parameters of the embedding functions to best align source and target domain histograms for held-out seen classes, thus providing a basis for generalizing to unseen classes. Even for fixed parameters, embedding functions ψ, π are nonlinear maps and since the parameters are jointly optimized our learned scoring function f (x, y) couples seen source and target domain together in a rather complex way. So we cannot reduce f (·, ·) to a linear or bilinear setting as in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intuitive Justification of Proposed Method</head><p>Recall that our method is based on viewing unseen source and target instances as a histogram of seen classes proportions. <ref type="figure" target="#fig_0">Fig. 1</ref> suggests that a target instance can be viewed as arising from a mixture of seen classes with mixture components dependent on the location of the instance. More precisely, letting P and P y be the unseen and seen classconditional target feature distributions respectively, we can a priori approximate P as a mixture of the P y 's, i.e. P = y∈Sπ y P y + P error (see <ref type="bibr" target="#b4">[5]</ref> for various approaches in this context), whereπ y denotes the mixture weight for class y. Analogously, we can also decompose source domain data as a mixture of source domain seen classes. This leads us to associate mixture proportion vector z u with unseen class u, and represent attribute vector c u as c u ≈ y∈S z u,y c y , with z u = (z u,y ) y∈S ∈ ∆ |S| . Key Postulate: The target domain instance, x, must have on average a similar mixture pattern as the source domain pattern if they both correspond to the same unseen label, u ∈ U, namely, on average π(x) is equal to z u .</p><p>This postulate is essentially Eq. 1. This postulate also motivates our margin-based approach for learning w. Note that since we only have a single source domain vector for each class, a natural constraint is to require that the empirical mean of the mixture corresponding to each example per class in target domain aligns well with the source domain mixture. This is empirically consistent with our postulate. Letting y, y be seen class labels with y = y andπ y denote the average mixture for class y in target domain, our requirement is to guarantee that</p><formula xml:id="formula_1">π y , z y ≥ π y , z y (2) ⇔ s∈S w, 1 N s N i=1 I {yi=s} φ s (x i ) Emp. Mean Embedding z y,s − z y ,s ≥ 0,</formula><p>where I {·} denotes a binary indicator function returning 1 if the condition holds, otherwise 0. Note that the empirical mean embedding corresponds to a kernel empirical mean embedding <ref type="bibr" target="#b32">[33]</ref> if φ s is a valid (characteristic) RKHS kernel, but we do not pursue this point further in this paper. Nevertheless this alignment constraint is generally insufficient, because it does not capture the shape of the underlying sample distribution. We augment misclassification constraints for each seen sample in SVMs to account for shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Source Domain Embedding</head><p>Recall from <ref type="figure" target="#fig_0">Fig. 1 and (B)</ref> in Sec. 2.1 that our embedding aims to map source domain attribute vectors c to histograms of seen class proportions, i.e. ψ : R ds → ∆ |S| . We propose a parameterized optimization problem inspired by sparse coding as follows, given a source domain vector c:</p><formula xml:id="formula_2">ψ(c) = arg min α∈∆ |S|    γ 2 α 2 + 1 2 c − y∈S c y α y 2    ,<label>(3)</label></formula><p>where γ ≥ 0 is a predefined regularization parameter, · denotes the 2 norm of a vector, and α = (α y ) y∈S describes contributions of different seen classes. Note that even though c may not be on the simplex, the embeddings ψ(c) are always. Note that the embedding ψ is in general a nonlinear function. Indeed on account of simplex constraint small values in α vector are zeroed out (i.e. "water-filling").</p><p>To solve Eq. 3, we use quadratic programming. For large-scale cases, we adopt efficient proximal gradient descent methods. Note that there are many alternate ways of embedding such as similarity rescaling, subspace clustering <ref type="bibr" target="#b26">[27]</ref>, sparse learning <ref type="bibr" target="#b6">[7]</ref>, and low rank representation <ref type="bibr" target="#b16">[17]</ref>, as long as the embedding is on the simplex. We tried these different methods with the simplex constraint to learn the embeddings, and our current solution in Eq. 3 works best. We believe that it is probably because the goal in these other methods is subspace clustering, while our goal is to find a noise resilient embedding which has good generalization to unseen class classification.</p><p>We optimize the parameter, γ, globally by cross validation. Once the γ parameter is identified, all of the seen classes are used in our embedding function. Note that when γ = 0 or small, ψ(c y ) will be a coordinate vector, which essentially amounts to coding for multi-class classification but is not useful for unseen class generalization. Conceptually, because we learn tuning parameters to predict well on held-out seen classes, γ is in general not close to zero. We demonstrate class affinity matrices before and after embedding for both seen and unseen classes in <ref type="figure" target="#fig_1">Fig. 2</ref>. Here γ = 10 is obtained by cross validation. We see that in both training and testing source domain embeddings preserve the affinities among classes in the attribute space.</p><p>During test-time when unseen class attribute vectors c u are revealed, we obtain z u as the embeddings using Eq. 3 with the learned γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Target Domain Embedding</head><p>In this paper we define our target domain class dependent mapping function φ y based on (1) intersection function (INT) <ref type="bibr" target="#b18">[19]</ref>, or (2) rectified linear unit (ReLU) <ref type="bibr" target="#b21">[22]</ref>. That is,</p><formula xml:id="formula_3">INT: φ y (x) = min(x, v y ),<label>(4)</label></formula><formula xml:id="formula_4">ReLU: φ y (x) = max(0, x − v y ),<label>(5)</label></formula><p>where min and max are the entry-wise operators. Note that intersection function captures the data patterns in x below the thresholds in each v y , while ReLU captures the data patterns above the thresholds. In this sense, the features generated from these two functions are complementary. This is the reason that we choose the two functions to demonstrate the robustness of our method. Based on Eq. 1 and 2 in Section 2.1, we define the following structured scoring function f (x, y) as follows:</p><formula xml:id="formula_5">f (x, y) = s∈S w, φ s (x) z y,s .<label>(6)</label></formula><p>In test-time for target instance x, we can compute f (x, u) for an arbitrary unseen label u because the source attribute vector is revealed for u. Note that f is highly non-convex, and it cannot reduce to bilinear functions used in existing works such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Max-Margin Formulation</head><p>Based on Eq. 6, we propose the following parameterized learning formulation for zero-shot learning as follows, which learns the embedding function π, and thus f :</p><formula xml:id="formula_6">min V,w,ξ, 1 2 w 2 + λ 1 2 v∈V v 2 + λ 2 y,s ys + λ 3 i,y ξ iy<label>(7)</label></formula><p>s.t. ∀i ∈ {1, · · · , N }, ∀y ∈ S, ∀s ∈ S,</p><formula xml:id="formula_7">N i=1 I {yi=y} N y f (x i , y) − f (x i , s) ≥ ∆(y, s) − ys , (8) f (x i , y i ) − f (x i , y) ≥ ∆(y i , y) − ξ iy ,<label>(9)</label></formula><formula xml:id="formula_8">ys ≥ 0, ξ iy ≥ 0, ∀v ∈ V, v ≥ 0,</formula><p>where ∆(·, ·) denotes a structural loss between the groundtruth class and the predicted class, λ 1 ≥ 0, λ 2 ≥ 0, and λ 3 ≥ 0 are the predefined regularization parameters, ξ = {ξ iy } and = { ys } are slack variables, and 0 is a vector of 0's. In this paper, we define ∆(y i , y) = 1 − c T yi c y and ∆(y, s) = 1 − c T y c s , respectively. Note that in learning we only access and utilize the data from seen classes.</p><p>In fact, Eq. 8 measures the alignment loss for each seen class distribution, and Eq. 9 measures the classification loss for each target domain training instance, respectively, which correspond to the discussion in Sec. 2.2. On one hand, if we only care about the alignment condition, it is likely that there may be many misclassified training data samples (i.e. loose shape) as illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. On the other hand, conventional classification methods only consider separating data instances with tight shape, but are unable to align distributions due to lack of such constraint in training (see <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). By introducing these two constraints into Eq. 7, we are able to learn the target domain embedding function as well as the scoring function to produce the clusters which are well aligned and separated, as illustrated in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. Similarly, we learn the predefined parameters λ 1 , λ 2 , λ 3 through a cross validation step that optimizes the prediction for held-out seen classes. Then once the parameters are determined we re-learn the classifier on all of the seen data. <ref type="figure" target="#fig_1">Fig. 2</ref> depicts class affinity matrices before and after target domain semantic embedding on real data. Our method manages to align source/target domain data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Alternating Optimization Scheme</head><p>To solve Eq. 7, we propose the following alternating optimization algorithm, as seen in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning Embedding Functions</head><p>Input</p><formula xml:id="formula_9">: {x i , y i }, {cy} y∈S , {zy} y∈S , λ 1 , λ 2 , λ 3 , learning rate η ≥ 0 Initialize ν (0) with feature means of seen classes in target domain; for t = 0 to τ do (w, , ξ) ← linearSVM solver({x i , y i }, ν (t) , λ 2 , λ 3 ); ν (t+1) ← max{0, ν (t) − η∇h(ν (t) )};</formula><p>Check monotonic decreasing condition on the objective in Eq. 7; end Output : w, ν (i) Learning w by fixing V: In this step, we can collect all the constraints in Eq. 8 and Eq. 9 by plugging in {(x i , y i )}, V, {c y } y∈S , and then solve a linear SVM to learn w, , ξ, respectively. (ii) Learning V by fixing w using Concave-Convex procedure (CCCP) <ref type="bibr" target="#b42">[43]</ref>: Note that the constraints in Eq. 8 and Eq. 9 consist of difference-of-convex (DoC) functions. To see this, we can rewrite f (x i , y) − f (x i , y i ) as a summation of convex and concave functions as follows:</p><formula xml:id="formula_10">f (x i , y) − f (x i , y i ) = m,s w m (z y,n − z yi,n )φ s,m (x i ),<label>(10)</label></formula><p>where w m and φ s,m (·) denote the mth entries in vectors w and φ s (·), respectively. Let ν ∈ R dt|S| be a vector concatenation of all v's, g 1 (ν) ∆ = g 1 (x i , y, ν) and g 2 (ν) ∆ = g 2 (x i , y, ν) denote the summations of all the convex and all the concave terms in Eq. 10, respectively. Then we have f (x i , y) − f (x i , y i ) = g 1 (ν) − (−g 2 (ν)), i.e. DoC functions. Using CCCP we can relax the constraint in Eq. 9 as</p><formula xml:id="formula_11">ξ iy ≥ ∆(y i , y) + g 1 (ν) + g 2 (ν (t) ) + ∇g 2 (ν (t) ) T (ν − ν (t) ),</formula><p>where ν (t) denotes the solution for ν in iteration t, and ∇ denotes the subgradient operator. Similarly we can perform CCCP to relax the constraint in Eq. 8. Letting h(ν) denote the minimization problem in Eq. 7, 8, and 9, using CCCP we can further write down the subgradient ∇h(ν (t) ) in iteration t + 1 as follows:</p><formula xml:id="formula_12">∇h(ν (t) ) = λ 1 ν (t) + λ 2 y,s,i I { ys&gt;0,yi=y} ∇g 1 (ν (t) ) + ∇g 2 (ν (t) ) + λ 3 yi,y I {ξiy&gt;0} ∇g 1 (ν (t) ) + ∇g 2 (ν (t) ) .<label>(11)</label></formula><p>Then we use subgradient descent to update ν, equivalently learning V. With simple algebra, we can show that the mth entry for class n in ∇g 1 (ν (t) ) + ∇g 2 (ν (t) ) is equivalent to the mth entry in ∂f (xi,y) ∂vs ν (t) − ∂f (xi,yi) ∂vs ν (t) . In order to guarantee the monotonic decrease of the objective in Eq. 7, we add an extra checking step in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Cross Validation on Seen Class Data</head><p>The scoring function in Eq. 6 is obtained by solving Eq. 3 and 7, which in turn depend on parameters θ = (γ, λ 1 , λ 2 , λ 3 ). We propose learning these parameters by means of cross validation using held-out seen class data. Specifically, define S ⊂ S and the held-out set S h = S\S . We learn a collection of embedding functions for source and target domains using Eq. 3 and 7 over a range of parameters θ suitably discretized in 4D space. For each parameter choice θ we obtain a scoring function, which depends on training subset as well as the parameter choice. We then compute the prediction error, namely, the number of times that a held-out target domain sample is misclassified for this parameter choice. We repeat this procedure for different randomly selected subsets S and choose parameters with the minimum average prediction error. Once these parameters are obtained we then plug it back into Eq. 3 and 7, and re-learn the scoring function using all the seen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We test our method on five benchmark image datasets for zero-shot recognition, i.e. CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>, aPascal &amp; aYahoo (aP&amp;Y) <ref type="bibr" target="#b7">[8]</ref>, Animals with Attributes (AwA) <ref type="bibr" target="#b14">[15]</ref>, Caltech-UCSD Birds-200-2011 (CUB-200-2011) <ref type="bibr" target="#b37">[38]</ref>, and SUN Attribute <ref type="bibr" target="#b25">[26]</ref>. For all the datasets, we utilize MatCon-vNet <ref type="bibr" target="#b35">[36]</ref> with the "imagenet-vgg-verydeep-19" pretrained model <ref type="bibr" target="#b31">[32]</ref> to extract a 4096-dim CNN feature vector (i.e. the top layer hidden unit activations of the network) for each image (or bounding box). Verydeep features work well since they lead to good class separation, which is required for our class dependent transform (see <ref type="figure">Fig. 5</ref>). Similar CNN features were used in previous work <ref type="bibr" target="#b1">[2]</ref> for zero-shot learning. We denote the two variants of our general method as SSE-INT and SSE-ReLU, respectively. Note that in terms of experimental settings, the main difference between our method and the competitors is the features. We report the top-1 recognition accuracy averaged over 3 trials.</p><p>We set γ, λ 2 , λ 3 ∈ {0, 10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 } in Eq. 3 and 7 for cross validation. In each iteration, we randomly choose two seen classes for validation, and fix ν in Alg. 1 to its initialization for speeding up computation. For λ 1 , we simply set it to a small number 10 −4 because it is much less important than the others for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CIFAR-10</head><p>This dataset consists of 60000 color images with resolution of 32 × 32 pixels (50000 for training and 10000 for testing) from 10 classes. <ref type="bibr" target="#b33">[34]</ref> enriched it with 25 binary attributes and 50-dim semantic word vectors with real numbers for each class. We follow the settings in <ref type="bibr" target="#b33">[34]</ref>. Precisely, we take cat-dog, plane-auto, auto-deer, deer-ship, and cattruck as test categories for zero-shot recognition, respectively, and use the rest 8 classes as seen class data. Our training and testing is performed on the split of training and test data provided in the dataset, respectively.</p><p>We first summarize the accuracy of <ref type="bibr" target="#b33">[34]</ref> and our method in <ref type="table">Table 2</ref>. Clearly our method outperforms <ref type="bibr" target="#b33">[34]</ref> significantly, and SSE-INT and SSE-ReLU perform similarly. We observe that for cat-dog our method performs similarly as <ref type="bibr" target="#b33">[34]</ref>, while for others our method can easily achieve very high accuracy. We show the class affinity matrix in <ref type="figure" target="#fig_3">Fig.  4(a)</ref> using the binary attribute vectors, and it turns out that cat and dog have a very high similarity. Similarly the word vectors between cat and dog provide more discrimination than attribute vectors but still much less than others.</p><p>To better understand our SSE learning method, we visualize the target domain CNN features as well as the learned SSE features using t-SNE <ref type="bibr" target="#b34">[35]</ref> in <ref type="figure" target="#fig_3">Fig. 4(b-d)</ref>. Due to different seen classes, the learned functions and embeddings for <ref type="figure" target="#fig_3">Fig. 4(c)</ref> and <ref type="figure" target="#fig_3">Fig. 4(d)</ref> are different. In <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, CNN features seem to form clusters for different classes with some overlaps, and there is a small gap between "an- imals" and "artifacts". In contrast, our SSE features are guided by source domain attribute vectors, and indeed preserve the affinities between classes in the attribute space. In other words, our learning algorithm manages to align the target domain distributions with their corresponding source domain embeddings in SSE space, as well as discriminating each target domain instance from wrong classes. As we see, the gaps between animals and artifacts are much clearer in <ref type="figure" target="#fig_3">Fig. 4</ref>(c) and <ref type="figure" target="#fig_3">Fig. 4(d)</ref> than that in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>. For cat and dog, however, there is still a large overlap in SSE space, leading to poor recognition. The overall sample distributions in <ref type="figure" target="#fig_3">Fig. 4(c)</ref> and <ref type="figure" target="#fig_3">Fig. 4(d)</ref> are similar, because they both preserve the same class affinities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other Benchmark Comparison</head><p>For the detail of each dataset, please refer to its original paper. For aP&amp;Y, CUB-200-2011, and SUN Attribute datasets, we take the means of attribute vectors from the same classes to generate source domain data. For AwA dataset, we utilize the real-number attribute vectors since they are more discriminative.</p><p>We utilize the same training/testing splits for zero-shot recognition on aP&amp;Y and AwA as others. For CUB-200-2011, we follow <ref type="bibr" target="#b0">[1]</ref> to use the same 150 bird spices as seen classes for training and the left 50 spices as unseen classes for testing. For SUN Attribute, we follow <ref type="bibr" target="#b13">[14]</ref> to use the <ref type="table">Table 2</ref>. Zero-shot recognition accuracy comparison (%, mean±standard deviation) on CIFAR-10. The compared numbers are best estimated from <ref type="figure" target="#fig_2">Fig. 3</ref> in <ref type="bibr" target="#b33">[34]</ref>. Notice that all the methods here utilize deep features to represent images in target domain.  same 10 classes as unseen classes for testing (see their supplementary file) and take the rest as seen classes for training. We summarize our comparison in <ref type="table" target="#tab_3">Table 3</ref>, where the blank spaces indicate that the proposed methods were not tested on the datasets in their original papers. Still there is no big performance difference between our SSE-INT and SSE-ReLU. On 4 out of the 5 datasets, our method works best except for CUB-200-2011. On one hand, <ref type="bibr" target="#b1">[2]</ref> specifically targets at fine-grained zero-shot recognition such as this dataset, while ours aims for general zero-shot learning. On the other hand, we suspect that the source domain projection function may not work well in fine-grained recognition, and we will investigate more on it in our future work.</p><p>To understand our method better with different features, we test 7 features on AwA dataset 1 . We show the SSE distribution comparison using decaf CNN features and vggverydeep-19 CNN features in <ref type="figure">Fig. 5</ref>. There is a large difference between the two distributions: (a) while with decaf features clusters are slightly separated they are still cluttered with overlaps among different classes. (b) vgg-verydeep-19 features, in contrast, form crisp clusters for different classes, which is useful for zero-shot recognition. Also we plot the cosine similarity matrices created using different features in <ref type="figure">Fig. 6</ref>. As we see, the matrix from vgg-verydeep-19 features (i.e. the last) is the most similar to that from the source domain attribute vectors (i.e. the first). This demonstrates that our learning method with vgg-verydeep-19 features can align the target domain distribution with the source domain attribute vectors. We can attribute this to the fact that we need a class dependent feature transform φ y (x) that has good separation on seen classes. Our implementation 2 is based on unoptimized MATLAB code. However, it can return the prediction results on any of these 5 datasets within 30 minutes using a multi-thread CPU (Xeon E5-2696 v2), starting from loading CNN features. For instance, on CIFAR-10 we manage to finish running the code less than 5 minutes.  <ref type="figure">Figure 6</ref>. Cosine similarity matrices created using different features on AwA testing data. The numbers in the brackets are the mean accuray (%) achieved using the corresponding features. Our learning method performs the best with vgg-verydeep-19 features. We can attribute this to the fact that we need a class dependent feature transform φy(x) that has good separation on seen classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Towards Large-Scale Zero-Shot Recognition</head><p>We test the generalization ability of our method on the SUN Attribute dataset for large-scale zero-shot recognition. We design two experimental settings: (1) Like in benchmark comparison, we randomly select M classes as seen classes for training, and then among the rest 717 − M classes, we also randomly select 10, 20, · · · , 717 − M classes as unseen classes for testing; <ref type="bibr" target="#b1">(2)</ref> We randomly select 10, 20, · · · , 700 classes as seen classes for training, and categorize each data sample from the rest unseen classes into one of the 717 classes. <ref type="figure" target="#fig_6">Fig. 7</ref> shows our results, where (a) and (b) correspond to the settings (1) and (2), respectively.</p><p>In <ref type="figure" target="#fig_6">Fig. 7(a)</ref>, we can see that with very few seen classes, we can achieve reasonably good performance when unseen classes are a few. However, with the increase of the number of unseen classes, the curve drops rapidly and then changes slowly when the number is large. From 200 to 700 unseen classes, our performance is reduced from 8.62% to 2.85%. With the increase of the number of seen classes, our performance is improving, especially when the number of unseen classes is small. With 10 unseen classes, our performance increases from 61.00% to 87.17% using 17 and 317 seen classes, respectively. But such improvement is marginal when there are already a sufficient number of seen classes, for instance from 217 to 317 seen classes.</p><p>In <ref type="figure" target="#fig_6">Fig. 7(b)</ref>, generally speaking, with more seen classes our performance will be better, because there will be better chance to preserve the semantic affinities among classes in source domain. With only 10 seen classes, our method can achieve 1.59% mean accuracy, which is much better than the random chance 0.14%. Notice that even though we use all the 717 classes as seen classes, we cannot guarantee that the testing results are similar to those of traditional classification methods, because the source domain attribute vectors will guide our method for learning. If they are less discriminative, e.g. the attribute vectors for cat and dog in CIFAR-10, the recognition performance may be worse.</p><p>To summarize, our method performs well and stably on SUN Attribute with a small set of seen classes and a relatively large set of unseen classes. Therefore, we believe that our method is suitable for large-scale zero-shot recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We proposed learning a semantic similarity embedding (SSE) method for zero-shot recognition. We label the semantic meanings using seen classes, and project all the source domain attribute vectors onto the simplex in SSE space, so that each class can be represented as a probabilistic mixture of seen classes. Then we learn similarity functions to embed target domain data into the same semantic space as source domain, so that not only the empirical mean embeddings of the seen class data distributions are aligned with their corresponding source domain embeddings, but also the data instance itself can be classified correctly. We propose learning two variants using intersection function and rectified linear unit (ReLU). Our method on five benchmark datasets including the large-scale SUN Attribute dataset significantly outperforms other state-of-art methods. As future work, we would like to explore other applications for our method such as person re-identification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and zero-shot activity retrieval <ref type="bibr" target="#b5">[6]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Proposed method with source/target domain data displayed on the leftmost/rightmost figures respectively. Light blue corresponds to unseen classes and other colors depict seen class data. Light-blue data is unavailable during training. During test-time unseen source domain data is revealed along with an arbitrary unseen instance from target domain (light-blue) is presented and we are to identify its unseen class label. Each unseen class source domain data is expressed as a histograms of seen class proportions. Seen class proportions are estimated for the target instance and compared with each of the source domain histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Seen classes in training (b) Unseen classes in testing Cosine similarity matrices among (a) seen and (b) unseen classes on aPascal &amp; aYahoo<ref type="bibr" target="#b7">[8]</ref> dataset. Brighter color depicts larger values. The type of data used to compute the matrix is shown above the corresponding matrix. Observe that in training/testing our source/target domain embedding preserves the inter-class relationships originally defined by the source domain attribute vectors. This also indicates that our target domain embeddings manage to align well the target domain distributions with the source domain attribute vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of three different constraints for learning the target domain semantic embedding function. Different shapes denote differnt classes, fill-in shapes denote the source domain embeddings, and green crosses denote the empirical means of target domain data embeddings. Our method takes into account the zero-shot learning based on both distribution alignment and instance classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) Cosine similarity matrix (b) CNN features (c) SSE embeddings (auto-deer) (d) SSE embeddings (cat-dog) (a) Class affinities for the 10 classes using source domain binary attribute vectors. (b-d) t-SNE visualization of different features with 25 attributes, where 100 samples per class in the test set are selected randomly and the same color denotes the same class. (b) shows the 4096-dim original target domain CNN features. (c) and (d) show the 8-dim learned SSE features by SSE-INT and tested on auto-deer and cat-dog, respectively. The embeddings produced by SSE-ReLU have similar patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>19 Figure 5</head><label>195</label><figDesc>. t-SNE visualization comparison of SSE distributions using the two CNN features on AwA testing data. Our method works well if there is good separation for classes and verydeep features are particularly useful.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>cq-hist (31.5) (c) lss-hist (30.3) (d) rgsift-hist (33.6) (e) sift-hist (29.8) (f) surf-hist (36.5) (g) decaf (52.0) (h) verydeep-19 (71.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>(a) Recognition on unseen classes (b) Recognition on all classes Large-scale zero-shot recognition on SUN Attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>|S| dimensional space {cy}Source domain attribute vector cy ∈ R ds for class y with 2 normalization, i.e. cy = 1.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>S (U )</cell><cell>Set of seen (unseen) classes</cell></row><row><cell>|S|</cell><cell>Number of seen classes</cell></row><row><cell>s (or y) &amp; u</cell><cell>Indexes for seen and unseen classes</cell></row><row><cell cols="2">∆ |S| Simplex in R {(x i , y i )} Training data: x i ∈ R d t -target feature, y i -class</cell></row><row><cell>N (Ny)</cell><cell>Number of training samples (for class y ∈ S)</cell></row><row><cell>ψ, π</cell><cell>Source/Target domain feature embedding functions</cell></row><row><cell>φy</cell><cell>Target domain class dependent feature transformation</cell></row><row><cell>(·)m,n</cell><cell>The nth entry in vector (·)m</cell></row><row><cell>zy = ψ(cy)</cell><cell>Learned source domain embedded histogram zy ∈</cell></row><row><cell></cell><cell>∆ |S| for class y.</cell></row><row><cell>V = {vy}</cell><cell>Learned target domain reference vector vy ∈ R d t</cell></row><row><cell></cell><cell>for class y, one vector per seen class</cell></row><row><cell>w</cell><cell>Learned target domain weight vector</cell></row><row><cell>f (x, y)</cell><cell>Learned structured scoring function relating the tar-</cell></row><row><cell></cell><cell>get domain sample x and class label y.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Some notation used in our method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Zero-shot recognition accuracy comparison (%) on aP&amp;Y, AwA, CUB-200-2011, and SUN Attribute, respectively, in the form of mean±standard deviation. Here except our results, the rest numbers are cited from their original papers. Note that some experimental settings may differ from ours. The results listed here are the ones with 4096-dim CNN features and the continuous attribute vectors provided in the datasets for fair comparison.</figDesc><table><row><cell>Method</cell><cell></cell><cell>cat-dog</cell><cell></cell><cell>plane-auto</cell><cell>auto-deer</cell><cell>deer-ship</cell><cell>cat-truck</cell><cell>Average</cell></row><row><cell cols="2">Socher et al. [34] (50 words)</cell><cell>50</cell><cell></cell><cell>65</cell><cell>76</cell><cell>83</cell><cell>90</cell><cell>72.8</cell></row><row><cell cols="2">SSE-INT (50 words)</cell><cell cols="2">59.00±0.57</cell><cell cols="4">91.62±0.19 97.95±0.13 95.73±0.08 97.20±0.05</cell><cell>88.30</cell></row><row><cell cols="2">SSE-ReLU (50 words)</cell><cell cols="2">58.78±1.60</cell><cell cols="3">91.33±0.53 97.33±0.28 95.37±0.29</cell><cell>97.32±0.12</cell><cell>88.03</cell></row><row><cell cols="2">SSE-INT (25-dim binary vectors)</cell><cell cols="2">48.47±0.08</cell><cell cols="3">93.93±0.59 99.07±0.18 96.03±0.03</cell><cell>96.92±0.14</cell><cell>86.88</cell></row><row><cell cols="4">SSE-ReLU (25-dim binary vectors) 48.52±0.13</cell><cell cols="4">93.68±0.73 98.48±0.15 95.32±0.25 96.43±0.06</cell><cell>86.49</cell></row><row><cell>Feature</cell><cell>Method</cell><cell></cell><cell cols="5">aPascal &amp; aYahoo Animals with Attributes CUB-200-2011 SUN Attribute</cell></row><row><cell></cell><cell>Farhadi et al. [8]</cell><cell></cell><cell>32.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Mahajan et al. [18]</cell><cell></cell><cell>37.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Wang and Ji [39]</cell><cell></cell><cell>45.05</cell><cell></cell><cell>42.78</cell><cell></cell></row><row><cell></cell><cell>Rohrbach et al. [28]</cell><cell></cell><cell></cell><cell></cell><cell>42.7</cell><cell></cell></row><row><cell></cell><cell>Yu et al. [42]</cell><cell></cell><cell></cell><cell></cell><cell>48.30</cell><cell></cell></row><row><cell>Non-CNN</cell><cell>Akata et al. [1]</cell><cell></cell><cell></cell><cell></cell><cell>43.5</cell><cell cols="2">18.0</cell></row><row><cell></cell><cell>Fu et al. [10]</cell><cell></cell><cell></cell><cell></cell><cell>47.1</cell><cell></cell></row><row><cell></cell><cell>Mensink et al. [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">14.4</cell></row><row><cell></cell><cell>Lampert et al. [16]</cell><cell></cell><cell>19.1</cell><cell></cell><cell>40.5</cell><cell></cell><cell>52.50</cell></row><row><cell></cell><cell cols="2">Jayaraman and Grauman [14]</cell><cell cols="2">26.02±0.05</cell><cell>43.01±0.07</cell><cell></cell><cell>56.18±0.27</cell></row><row><cell></cell><cell cols="4">Romera-Paredes and Torr [30] 27.27±1.62</cell><cell>49.30±0.21</cell><cell></cell><cell>65.75±0.51</cell></row><row><cell>AlexNet</cell><cell>Akata et al. [2] a</cell><cell></cell><cell></cell><cell></cell><cell>61.9</cell><cell cols="2">40.3</cell></row><row><cell></cell><cell>Lampert et al. [16]</cell><cell></cell><cell>38.16</cell><cell></cell><cell>57.23</cell><cell></cell><cell>72.00</cell></row><row><cell>vgg-verydeep-19</cell><cell cols="4">Romera-Paredes and Torr [30] 24.22±2.89 SSE-INT 44.15±0.34</cell><cell>75.32±2.28 71.52±0.79</cell><cell cols="2">30.19±0.59</cell><cell>82.10±0.32 82.17±0.76</cell></row><row><cell></cell><cell>SSE-ReLU</cell><cell></cell><cell cols="2">46.23±0.53</cell><cell>76.33±0.83</cell><cell cols="2">30.41±0.20</cell><cell>82.50±1.32</cell></row></table><note>a</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We downloaded these features from http://attributes.kyb. tuebingen.mpg.de/ 2 Our code is available at https://zimingzhang.wordpress. com/source-code/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their very useful comments. This material is based upon work supported in part by the U.S. Department of Homeland Security, Science and Technology Directorate, Office of University Programs, under Grant Award 2013-ST-061-ED0001, by ONR Grant 50202168 and US AF contract FA8650-14-C-1728. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the social policies, either expressed or implied, of the U.S. DHS, ONR or AF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decontamination of mutually contaminated models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient activity retrieval through semantic graph queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Castanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic zero-shot classification with semantic rankings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<idno>abs/1502.08039</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analogy-preserving semantic embedding for visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="639" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified semantic embedding: Relating taxonomies and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images. Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint learning framework for attribute models and object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1227" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Costa: Co-occurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning by Convex Combination of Semantic Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zeroshot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactively building a discriminative vocabulary of nameable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1681" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The sun attribute database: Beyond categories for deeper scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="59" to="81" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Constructing l2-graph for subspace learning and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1209.0841</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A hilbert space embedding for distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for MATLAB. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4564</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A tutorial on subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A unified probabilistic approach modeling relationships between attributes and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large margin taxonomy embedding for document categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1737" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zeroshot event detection using multi-modal fusion of weakly supervised concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bondugula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2665" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A novel visual word cooccurrence model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Visual Surveillance and Re-Identification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group membership prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PRISM: Person re-identification via structured matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4444</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
