<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computational and Statistical Learning and Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Genova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computational and Statistical Learning and Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Genova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DIBRIS</orgName>
								<orgName type="institution">Universita Degli Studi Di Genova</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computational and Statistical Learning and Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HOLE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator, HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. Experimentally, we show that holographic embeddings are able to outperform state-ofthe-art methods for link prediction on knowledge graphs and relational learning benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Relations are a key concept in artificial intelligence and cognitive science. Many of the structures that humans impose on the world, such as logical reasoning, analogies, or taxonomies, are based on entities, concepts and their relationships. Hence, learning from and with relational knowledge representations has long been considered an important task in artificial intelligence (see e.g., <ref type="bibr" target="#b10">Getoor and Taskar (2007)</ref>; <ref type="bibr" target="#b16">Muggleton (1991)</ref>; <ref type="bibr" target="#b9">Gentner (1983)</ref>; <ref type="bibr" target="#b12">Kemp et al. (2006)</ref>; <ref type="bibr" target="#b28">Xu et al. (2006)</ref>; <ref type="bibr" target="#b21">Richardson and Domingos (2006)</ref>). In this work we are concerned with learning from knowledge graphs (KGs), i.e., knowledge bases which model facts as instances of binary relations (e.g., bornIn(BarackObama, Hawaii)). This form of knowledge representation can be interpreted as a multigraph, where entities correspond to nodes, facts correspond to typed edges, and the type of an edge indicates the kind of the relation. Modern knowledge graphs such as YAGO <ref type="bibr" target="#b26">(Suchanek, Kasneci, and Weikum, 2007)</ref>, DBpedia <ref type="bibr" target="#b0">(Auer et al., 2007)</ref>, and Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref> contain billions of facts about millions of entities and have found important applications in question answering, structured search, and digital assistants. Recently, vector space embeddings of knowledge graphs have received considerable attention, as they can be used to create statistical models of entire KGs, i.e., to predict the probability of any possible relation instance (edge) in the graph. Such models can be used to derive new knowledge from known facts (link prediction), to disambiguate entities (entity resolution), to extract taxonomies, and for probabilistic question answering (see e.g., <ref type="bibr" target="#b18">(Nickel, Tresp, and Kriegel, 2011;</ref><ref type="bibr" target="#b3">Bordes et al., 2013;</ref><ref type="bibr" target="#b13">Krompaß, Nickel, and Tresp, 2014)</ref>). Furthermore, embeddings of KGs have been used to support machine reading and to assess the trustworthiness of web sites <ref type="bibr" target="#b5">(Dong et al., 2014</ref><ref type="bibr" target="#b6">(Dong et al., , 2015</ref>. However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive. In this work, we approach learning from KGs within the framework of compositional vector space models. We introduce holographic embeddings (HOLE) which use the circular correlation of entity embeddings (vector representations) to create compositional representations of binary relational data. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. As we will show experimentally, HOLE is able to outperform state-of-the-art embedding models on various benchmark datasets for learning from KGs. Compositional vector space models have also been considered in cognitive science and natural language processing, e.g., to model symbolic structures, to represent the semantic meaning of phrases, and as models for associative memory (see e.g., <ref type="bibr" target="#b23">Smolensky (1990)</ref>; <ref type="bibr" target="#b19">Plate (1995)</ref>; <ref type="bibr" target="#b15">Mitchell and Lapata (2008)</ref>; <ref type="bibr" target="#b24">Socher et al. (2012)</ref>). In this work, we do not only draw inspiration from these models, but we will also highlight the connections of HOLE to holographic models of associative memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compositional Representations</head><p>In this section we introduce compositional vector space models for KGs, the general learning setting, and related work.</p><p>Let E denote the set of all entities and P the set of all relation types (predicates) in a domain. A binary relation is a subset R p ⊆ E × E of all pairs of entities (i.e., those pairs which are in a relation of type p). Higher-arity relations are defined analogously. The characteristic function φ p : E × E → {±1} of a relation R p indicates for each possible pair of entities whether they are part of R p . We will denote (possible) relation instances as R p (s, o), where s, o ∈ E denote the first and second argument of the asymmetric relation R p . We will refer to s, o as subject and object and to R p (s, o) as triples.</p><p>Compositional vector space models provide an elegant way to learn the characteristic functions of the relations in a knowledge graph, as they allow to cast the learning task as a problem of supervised representation learning. Here, we discuss models of the form</p><formula xml:id="formula_0">Pr(φ p (s, o) = 1|Θ) = σ(η spo ) = σ(r p (e s • e o )) (1)</formula><p>where r p ∈ R dr , e i ∈ R de are vector representations of relations and entities; σ(x) = 1/(1 + exp(−x)) denotes the logistic function; Θ = {e i } ne i=1 ∪ {r k } nr k=1 denotes the set of all embeddings; • : R de × R de → R dp denotes the compositional operator which creates a composite vector representation for the pair (s, o) from the embeddings e s , e o . We will discuss possible compositional operators below.</p><p>Let x i ∈ P × E × E denote a triple, and y i ∈ {±1} denote its label. Given a dataset D = {(x i , y i )} m i=1 of true and false relation instances, we then want to learn representations of entities and relations Θ that best explain D according to eq. (1). This can, for instance, be done by minimizing the (regularized) logistic loss</p><formula xml:id="formula_1">min Θ m i=1 log(1 + exp(−y i η i )) + λ Θ 2 2 .<label>(2)</label></formula><p>For relational data, minimizing the logistic loss has the additional advantage that it can help to find low dimensional embeddings for complex relational patterns <ref type="bibr" target="#b4">(Bouchard, Singh, and Trouillon, 2015)</ref>. However, in most cases, KGs store only true triples and non-existing triples can be either missing of false (open-world assumption). In this case, negative examples can be generated by heuristics such as the local closed world assumption <ref type="bibr" target="#b5">(Dong et al., 2014)</ref>. Alternatively, we can use a pairwise ranking loss such as</p><formula xml:id="formula_2">min Θ i∈D+ j∈D− max(0, γ + σ(η j ) − σ(η i ))<label>(3)</label></formula><p>to rank the probability of existing triples higher than the probability of non-existing ones. Here, D + , D − denote the set of existing and non-existing triples and γ &gt; 0 specifies the width of the margin <ref type="bibr" target="#b2">(Bordes et al., 2011)</ref>. An important property of compositional models is that the meaning and representation of entities does not vary with regard to their position in the compositional representation (i.e., the i-th entity has the same representation e i as subject and object). Since the representations of all entities and relations are learned jointly in eqs. (2) and (3), this property allows to propagate information between triples, to capture global dependencies in the data, and to enable the desired relational learning effect. For a review of machine learning on knowledge graphs see also <ref type="bibr" target="#b17">Nickel et al. (2015)</ref>.</p><p>Existing models for knowledge graphs are based on the following compositional operators:</p><p>Tensor Product Given entity embeddings a, b ∈ R d , tensor product models represent pairs of entities via a • b = a ⊗ b ∈ R d 2 , i.e, via all pairwise multiplicative interactions between the features of a and b:</p><formula xml:id="formula_3">[a ⊗ b] ij = a i b j .<label>(4)</label></formula><p>Intuitively, a feature in the tuple representation a ⊗ b is "on" (has a high absolute magnitude), if and only if the corresponding features of both entities are "on" (See also <ref type="figure">fig. 1a</ref>). This allows eq. (1) to capture relational patterns such as liberal persons are typically members of liberal parties since a single feature in a ⊗ b can encode that the subject is a liberal person and that the object is a liberal party. Compositional models using the tensor product such as RESCAL <ref type="bibr" target="#b18">(Nickel, Tresp, and Kriegel, 2011)</ref> and the Neural Tensor Network <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>. have shown state-of-the-art performance for learning from KGs. Furthermore, <ref type="bibr" target="#b11">Guu, Miller, and Liang (2015)</ref> proposed a RESCAL-based model to learn from paths in KGs. <ref type="bibr" target="#b23">Smolensky (1990)</ref> introduced the tensor product as a way to create compositional vector representations. While the tensor product allows to capture rich interactions, its main problem as a compositional operator lies in the fact that it requires a large number of parameters. Since a ⊗ b explicitly models all pairwise interactions, r p in eq. (1) must be of size d 2 . This can be problematic both in terms of overfitting and computational demands. For instance, <ref type="bibr" target="#b17">Nickel, Jiang, and Tresp (2014)</ref> showed that linear tensor factorization can require large d to model certain relations. Since r p (e s ⊗ e o ) = e s R p e o , <ref type="bibr" target="#b29">Yang et al. (2015)</ref> proposed to use diagonal R p 's to reduce the number of parameters. However, this approach can only model symmetric relations and is not suitable to model general knowledge graphs as e s R p e o = e o R p e s for diagonal R p .</p><p>Concatenation, Projection, and Non-Linearity Another way to compute composite representations is via concatenation, projection and subsequent application of a non-linear function. Let ⊕ : R d1 × R d2 → R d1+d2 denote concatenation and ψ : R → R be a non-linear function such as tanh. The composite tuple representation is then given by</p><formula xml:id="formula_4">a • b = ψ(W (a ⊕ b)) ∈ R h , such that [ψ(W (a ⊕ b))] i = ψ j w a ij a j + j w b ij b j<label>(5)</label></formula><p>where the projection matrix W ∈ R h×2d is learned in combination with the entity and relation embeddings. Intuitively, a feature in the tuple representation W (a ⊕ b) is "on" if at least one of the corresponding features is "on". An advantage of this compositional operator is that the mapping from entity embeddings to representations of pairs is learned adaptively via the matrix W . However, the resulting composite representations are also less rich, as they do not consider direct interactions of features. As <ref type="bibr" target="#b25">Socher et al. (2013)</ref> noted, the non-linearity ψ provides only weak interactions while leading to a harder optimization problem. A variant of this compositional operator which also includes a relation embedding in the composite representation has been used in the ER-MLP model of the Knowledge Vault <ref type="bibr" target="#b5">(Dong et al., 2014)</ref>.</p><p>Non-compositional Methods Another class of models does not (explicitly) form compositional representations, but predicts the existence of triples from the similarity of the vector space embeddings. In particular, TRANSE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref> models the score of a fact as the distance between relation-specific translations of entity embeddings: A major appeal of TRANSE is that it requires very few parameters and moreover is easy to train. However, this simplicity comes also at the cost of modeling power. <ref type="bibr" target="#b27">Wang et al. (2014)</ref> and <ref type="bibr" target="#b14">Lin et al. (2015)</ref> proposed TRANSH and TRANSR respectively, to improve the performance of TRANSE on 1-to-N, N-to-1, and N-to-N relations. Unfortunately, these models lose the simplicity and efficiency of TRANSE.</p><formula xml:id="formula_5">score(R p (s, o)) = − dist(e s + r p , e o ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Holographic Embeddings</head><p>In this section, we propose a novel compositional model for KGs and relational data. To combine the expressive power of the tensor product with the efficiency and simplicity of TRANSE, we use the circular correlation of vectors to represent pairs of entities, i.e., we use the compositional operator:</p><formula xml:id="formula_6">a • b = a b,<label>(7)</label></formula><p>where :</p><formula xml:id="formula_7">R d × R d → R d denotes circular correlation: 1 [a b] k = d−1 i=0 a i b (k+i) mod d .<label>(8)</label></formula><p>Hence, we model the probability of a triple as</p><formula xml:id="formula_8">Pr(φ p (s, o) = 1|Θ) = σ(r p (e s e o )).<label>(9)</label></formula><p>Due to its connections to holographic models of associative memory (which we will discuss in the next section) we refer to eq. (9) as holographic embeddings (HOLE) of KGs. As a compositional operator, circular correlation can be interpreted as a compression of the tensor product. While the tensor product assigns a separate component c ij = a i b j for each pairwise interaction of entity features, in correlation each component corresponds to a sum over a fixed partition of pairwise interactions (see also <ref type="figure">fig. 2</ref>). Intuitively, a feature in the tuple representation is "on" if at least one partition of subject-object-interactions is on. This form of compression can be very effective since it allows to share weights in r p for semantically similar interactions. For example, to model relational patterns in the partyOf relation, it might be sufficient to know whether subject and object are a liberal person and liberal party OR if they are a conservative person and conservative party. These interactions can then be grouped 1 For notational convenience, we use zero-indexed vectors. in the same partition. Additionally, it is typically the case that only a subset of all possible interactions of latent features are relevant to model relational patterns. Irrelevant interactions can then be grouped in the same partitions and collectively be assigned a small weight in r p . Please note that the partitioning is not learned but fixed in advance through the correlation operator. This is possible because the entity representations are learned and the latent features can thus be "assigned" to the best partition during learning. Compared to the tensor product, circular correlation has the important advantage that it does not increase the dimensionality of the composite representation (see also <ref type="figure">fig. 1b</ref>). The memory complexity of the tuple representation is therefore linear in the dimensionality d of the entity representations. Moreover, the runtime complexity is quasilinear (loglinear) in d, as circular correlation can be computed via</p><formula xml:id="formula_9">a b = F −1 F(a) F(b)</formula><p>where F(·) and F −1 (·) denote the fast Fourier transform (FFT) and its inverse, x denotes the complex conjugate of</p><formula xml:id="formula_10">a 2 a 1 a 0 b 0 b 1 b 2 c 2 c 1 c 0 c = a b c0 = a0b0 + a1b1 + a2b2 c1 = a0b2 + a1b0 + a2b1 c2 = a0b1 + a1b2 + a2b0</formula><p>Figure 2: Circular correlation as compression of the tensor product. Arrows indicate summation patterns, nodes indicate elements in the tensor product. Adapted from <ref type="bibr" target="#b19">Plate (1995)</ref>.</p><p>x ∈ C d , and denotes the Hadamard (entrywise) product. The computational complexity of the FFT <ref type="figure">is O(d log d)</ref>. <ref type="table" target="#tab_1">Table 1a</ref> summarizes the improvements of circular correlation over the tensor product. <ref type="table" target="#tab_1">Table 1b</ref> lists the memory complexity of HOLE in comparison to other embedding models. Circular convolution * : R d × R d → R d is an operation that is closely related to circular correlation and defined as</p><formula xml:id="formula_11">[a * b] k = d−1 i=0 a i b (k−i) mod d .<label>(10)</label></formula><p>In comparison to convolution, correlation has two main advantages when used as a compositional operator: Non Commutative Correlation, unlike convolution, is not commutative, i.e., a b = b a. Non-commutativity is necessary to model asymmetric relations (directed graphs) with compositional representations. To compute the representations for entities and relations, we minimize either eq. (2) or (3) via stochastic gradient descent (SGD). Let θ ∈ {e i } ne i=1 ∪ {r k } nr k=1 denote the embedding of a single entity or relation and let f spo = σ(r p (e s e o )). The gradients of eq. (9) are then given by</p><formula xml:id="formula_12">∂f spo ∂θ = ∂f spo ∂η spo ∂η spo ∂θ ,</formula><p>where ∂η spo ∂r p = e s e o , ∂η spo ∂e s = r p e o , ∂η spo ∂e o = r p * e s .</p><p>The partial gradients in eq. (11) follow directly from </p><p>and standard vector calculus. Equation <ref type="formula" target="#formula_1">(12)</ref> can be derived as follows: First we rewrite correlation in terms of convolution:</p><formula xml:id="formula_15">a b = a * b</formula><p>where a denotes the involution of a, meaning that a is the mirror image of a such that a i = a −i mod d <ref type="bibr">(Schönemann, 1987, eq. 2.4)</ref>. Equation <ref type="formula" target="#formula_1">(12)</ref> follows then from the following identities in convolution algebra <ref type="bibr" target="#b19">(Plate, 1995)</ref>:</p><formula xml:id="formula_16">c ( a * b) = a ( c * b); c ( a * b) = b (a * c).</formula><p>Similar to correlation, the circular convolution in eq. (11) can be computed efficiently via a * b = F −1 (F(a) F(b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Associative Memory</head><p>In this section we outline the connections of eq. (9) and eq. (11) to holographic models of associative memory. Such models employ a sequence of convolutions and correlations as used in holography to store and retrieve information (e.g., see <ref type="bibr" target="#b8">Gabor (1969)</ref>; <ref type="bibr" target="#b20">Poggio (1973)</ref>). In particular, holographic reduced representations <ref type="bibr" target="#b19">(Plate, 1995)</ref>  Hence, m acts in this scheme as a memory that stores associations between vectors which are stored and retrieved using circular convolution and correlation.</p><p>Consider now the following model of associative memory for relational data: Let S o = {(s, p) | φ p (s, o) = 1} be the set of all subject-predicate indices for which the relation R p (s, o) is true. Next, we store these existing relations via convolution and superposition in the representation e o :</p><formula xml:id="formula_17">e o = (s,p)∈So r p * e s<label>(14)</label></formula><p>In this scheme, the compositional representation e s e o of eq. (7) would be analogous to retrieving the stored predicates p that exist between s and o. Similarly, computing σ(r p (e s e o )) as in eq. (9) is analogous to computing the probability that r p is included in the retrieved relations, i.e., that we have seen the triple R p (s, o). The norm constraints of eq. (13) can either be enforced directly (by projection the embeddings onto the unit circle) or through the regularization of the embeddings (which is equivalent to e i ≤ C e , r k ≤ C r , where C e , C r depend on the regularization parameter).</p><p>An important difference of HOLE to associative memory is that it does not only memorize, but it generalizes in a well defined way: In associative memory we are given the embeddings and store the associations directly, typically via Hebbian learning (e.g., see eq. <ref type="formula" target="#formula_3">(14)</ref>). In HOLE, we do not simply store the associations, but instead learn the embeddings that best explain the observed data. By iterating over D with SGD, we update the embeddings of the objects via</p><formula xml:id="formula_18">e t+1 o ← e t o − µ ∂L ∂f ∂f ∂η (r t p * e t s ),<label>(15)</label></formula><p>where µ denotes the learning rate. Please note that eq. (15) is analogous to the association of predicate and subject in holographic associative memory. Hence, we can interpret eq. (15) as adapting the "memory" e o , such that the retrieval of the observed facts is improved. The same analogy holds for the updates of e s and r p , however with the roles of correlation and convolution in storage and retrieval reversed. Moreover, in minimizing eq. (2) via eq. (15), we are estimating a probability distribution over possible states of the knowledge graph which allows us to predict the probability of any possible triple in the graph <ref type="bibr" target="#b17">Nickel et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Knowledge Graphs</head><p>To evaluate its performance for link prediction on knowledge graphs, we compared HOLE to state-of-the-art models on two commonly used benchmark datasets for this task: WN18 WordNet is a KG that groups words into synonyms and provides lexical relationships between words. The WN18 dataset consists of a subset of WordNet, containing 40,943 entities, 18 relation types, and 151,442 triples. FB15k Freebase is a large knowledge graph that stores general facts about the world (e.g., harvested from Wikipedia, MusicBrainz, etc.). The FB15k dataset consists of a subset of Freebase, containing 14,951 entities, 1345 relation types, and 592,213 triples. For both datasets we used the fixed training-, validation-, and test-splits provided by <ref type="bibr" target="#b3">Bordes et al. (2013)</ref>. As baseline methods, we used RESCAL, TRANSE, TRANSR, and ER-MLP. To facilitate a fair comparison we reimplemented all models and used the identical loss and optimization method for training, i.e., SGD with AdaGrad <ref type="bibr" target="#b7">(Duchi, Hazan, and Singer, 2011)</ref> and the ranking loss of eq. (3). This improved the results of TRANSE and RESCAL significantly on both datasets compared to results reported by <ref type="bibr" target="#b3">Bordes et al. (2013)</ref>. <ref type="bibr">2</ref> Following <ref type="bibr" target="#b3">Bordes et al. (2013)</ref>, we generated negative relation instances for training by corrupting positive triples and used the following evaluation protocol: For each true triple R p <ref type="bibr">(s, o)</ref> in the test set, we replace the subject s with each entity s ∈ E, compute the score for R p (s , o), and rank all these instances by their scores in decreasing order. Since there can exist multiple true triples in the test set for a given predicate-object pair, we remove all instances from the ranking where R p (s , o) = 1 and s = s , i.e., we consider only the ranking of the test instance among all wrong instances (which corresponds to the "Filtered" setting in <ref type="bibr" target="#b3">Bordes et al. (2013)</ref>). We then repeat this procedure by replacing the object o. To measure the quality of the ranking, we use the mean reciprocal rank (MRR) which is commonly used in information retrieval and in contrast to mean rank is less sensitive to outliers. In addition to MRR, we report the ratio in which R p (s, o) occurs within the first n results (denoted by "Hits at n"). We optimized the hyperparameters of all models via extensive grid search and selected the model with the best filtered MRR score on the validation set. The results of these experiments are shown in table 2a. It can be seen that HOLE is able to outperform the considered baseline methods significantly and consistently on both datasets. For instance, TRANSE and TRANSR rank the test instance only in 11.5% and 33.5% of the cases as the most likely triple in WN18 (Hits at 1). In contrast, HOLE ranks the test instance in 93.0% of the cases as the most likely instance. While less pronounced, similar results can be observed on FB15k. In table 1b, we report the dimensionality d and the resulting number of parameters of the selected models. It can be seen that HOLE is far more efficient in the number of parameters compared to the tensor product model RESCAL. Although the dimensionality d of the HOLE embedding is larger than RESCAL's (what is to be expected due to the compressive effect of correlation), the overall number of parameters is significantly reduced as its memory complexity depends only linearly on d. Also, HOLE is typically very fast to compute. On standard hardware (Intel Core(TM) i7U 2.1GHz) and for d = 150 (as used in the experiments) the runtime to compute the probability of a single triple is around 40µs. To compute all embeddings, a single epoch on WN18 takes around 11s (earlier epochs are slower since more examples violate the margin). Typically, we need 200-500 epochs (depending on the dataset) to arrive at the best estimates for the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Learning</head><p>We have shown that HOLE can predict triples successfully in knowledge graphs. In additional experiments, we wanted to test the relational learning capabilities of the compositional representation. For this purpose, we used the countries dataset of <ref type="bibr" target="#b4">Bouchard, Singh, and Trouillon (2015)</ref>, which consists of 244 countries, 22 subregions (e.g., Southern Africa, Western Europe) and 5 regions (e.g., Africa, Americas). Each country is located in exactly one region and subregion, each subregion is located in exactly one region, and each country can have a number of other countries as neighbors. From the raw data we created a relational representation with two predicates: locatedIn(e 1 , e 2 ) and neighborOf(e 1 , e 2 ). The task in the experiment was to predict locatedIn(c, r) instances, where c ranges over all countries and r over all regions in the data. The evaluation protocol was the following: First, we split all countries randomly in train (80%), validation (10%), and test (10%) set, such that for each country in the test set there is at least one neighbor in the training set. Next, we removed triples from the test and validation set in three different settings: S1) In the basic setting we only set locatedIn(c, r) to missing for countries in the test/valid. set. In this setting, the correct where s refers to the country's subregion. S2) In addition to the triples of S1, we set locatedIn <ref type="bibr">(c, s)</ref> to missing for all countries c in the test/valid. set and all subregions s in the data. In this setting, the correct triples can be predicted from:</p><formula xml:id="formula_19">neighborOf(c 1 , c 2 ) ∧ locatedIn(c 2 , r) ⇒ locatedIn(c 1 , r)</formula><p>This is a harder task than S1, since a country can have multiple neighbors and these can be in different regions. S3) In addition to the triples of S1 and S2 we set locatedIn(n, r) to missing for all neighbors n of all countries in the test/valid. set and all regions r in the data. In this setting, the correct triples can be predicted from:</p><formula xml:id="formula_20">neighborOf(c 1 , c 2 ) ∧ locatedIn(c 2 , s) ∧ locatedIn(s, r) ⇒ locatedIn(c 1 , r)</formula><p>This is the most difficult task, as it not only involves the neighborOf relation, but also a path of length 3. See <ref type="figure" target="#fig_3">fig. 3</ref> for an illustration of the data structure and the test settings. We measured the prediction quality via the area under the precision-recall curve (AUC-PR). The results of the experiments are shown in table 2b. It can be seen that HOLE is very successful in these learning tasks. For S1, the missing triples are predicted nearly perfectly. Moreover, even for the most difficult task S3, HOLE achieves very good results, especially since not every country's region can be predicted from its neighbors (e.g., islands have no neighbors). The poorer results of RESCAL and ER-MLP can likely be explained with overfitting (although the models are regularized), since the difference to HOLE is reduced when the hyperparameters are optimized on the test set instead of the validation set. We observed similar results as in this experiment on commonly used benchmark datasets for statistical relational learning. Due to space constraints, we report these experiments in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this work we proposed HOLE, a compositional vector space model for knowledge graphs that is based on the circular correlation of vectors. An attractive property of circular correlation in this context is that it creates fixed-width representations, meaning that the compositional representation has the same dimensionality as the representation of its constituents. In HOLE, we exploited this property to create a compositional model that can capture rich interactions in relational data but simultaneously remains efficient to compute, easy to train, and very scalable. Experimentally we showed that HOLE provides state-of-the-art performance on a variety of benchmark datasets and that it can model complex relational patterns while being very economical in the number of its parameters. Moreover, we highlighted connections of HOLE to holographic models of associative memory and discussed how it can be interpreted in this context. This creates not only a link between relational learning and associative memory, but also allows for principled ways to query the model, for instance in question answering. In future work we plan to further exploit the fixed-width representations of holographic embeddings in complex scenarios, since they are especially suitable to model higher-arity relations (e.g., taughtAt(John, AI, MIT)) and facts about facts (e.g., believes(John, loves(Tom, Mary))).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Similiarity Component In the correlation a b, a single component [a b] 0 = i a i b i corresponds to the dot product a, b . The existence of such a component can be helpful to model relations in which the similarity of entities is important. No such component exists in the convolution a * b (see also fig. 1 in the supplementary material).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>r</head><label></label><figDesc>p (e s e o ) = e s (r p e o ) = e o (r p * e s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>store the association of a with b via their circular convolution m = a * b, and retrieve the item associated with a from m via b ≈ a m = b * (a a) If a a ≈ δ (the identity element of convolution), it holds that b ≈ b and we can retrieve a noisy version of b. For denoising, we can pass the retrieved vector through a clean-up memory, which returns the stored item with the highest similarity to the item retrieved from m. For instance, if a = b i = 1, we can perform the clean-up via b = arg max bi b i (a m) (13) Multiple elements are stored in m via superposition: m = i a i * b i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Removed edges in countries experiment: S1) dotted S2) dotted and dashed S3) dotted, dashed and loosely dotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>RESCAL and HOLE as neural networks. RESCAL represents pairs of entities via d 2 components (middle layer). In contrast, HOLE requires only d components.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Pr(φ p (s, o))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pr(φ p (s, o))</cell><cell></cell></row><row><cell>r p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>r p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>⊗</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e s0</cell><cell>e s1</cell><cell>e s2</cell><cell>e o0</cell><cell>e o1</cell><cell>e o2</cell><cell>e s0</cell><cell>e s1</cell><cell>e s2</cell><cell>e o0</cell><cell>e o1</cell><cell>e o2</cell></row><row><cell></cell><cell>subject</cell><cell></cell><cell></cell><cell>object</cell><cell></cell><cell></cell><cell>subject</cell><cell></cell><cell></cell><cell>object</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(a) RESCAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) HOLE</cell><cell></cell><cell></cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>(a) Memory complexity and runtime complexity for compositional representations with e i ∈ R d . (b) Memory complexity of embedding models.</figDesc><table><row><cell></cell><cell cols="4">(a) Compositional Representations</cell></row><row><cell>Operator</cell><cell></cell><cell>•</cell><cell>Memory</cell><cell>Runtime</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rp</cell><cell>r p (es • eo)</cell></row><row><cell cols="2">Tensor Product</cell><cell>⊗</cell><cell>O(d 2 )</cell><cell>O(d 2 )</cell></row><row><cell cols="2">Circular Correlation</cell><cell></cell><cell>O(d)</cell><cell>O(d log d)</cell></row><row><cell></cell><cell cols="3">(b) Embedding Models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>on FB15k</cell></row><row><cell>Method</cell><cell cols="3">Memory Complexity</cell><cell>d</cell><cell>Params</cell></row><row><cell>TRANSE</cell><cell cols="2">O(ned + nrd)</cell><cell></cell><cell>100</cell><cell>1.6M</cell></row><row><cell>TRANSR</cell><cell cols="3">O(ned + nrd + nrd 2 )</cell><cell>100</cell><cell>15.1M</cell></row><row><cell cols="4">ER-MLP O(ned + nrd + dpd)</cell><cell>200/200</cell><cell>3.3M</cell></row><row><cell>RESCAL</cell><cell cols="2">O(ned + nrd 2 )</cell><cell></cell><cell>150</cell><cell>32.5M</cell></row><row><cell>HOLE</cell><cell cols="2">O(ned + nrd)</cell><cell></cell><cell>200</cell><cell>3.3M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for link prediction on WordNet (WN18), Freebase (FB15k) and Countries data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Countries</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell></cell><cell>Hits at</cell><cell></cell><cell>MRR</cell><cell></cell><cell>Hits at</cell><cell></cell><cell></cell><cell></cell><cell>AUC-PR</cell><cell></cell></row><row><cell>Method</cell><cell>Filter Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>Filter Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell></row><row><cell>TRANSE</cell><cell cols="8">0.495 0.351 11.3 88.8 94.3 0.463 0.222 29.7 57.8 74.9</cell><cell>Random</cell><cell cols="3">0.323 0.323 0.323</cell></row><row><cell>TRANSR</cell><cell cols="8">0.605 0.427 33.5 87.6 94.0 0.346 0.198 21.8 40.4 58.2</cell><cell cols="4">Frequency 0.323 0.323 0.308</cell></row><row><cell cols="9">ER-MLP 0.712 0.528 62.6 77.5 86.3 0.288 0.155 17.3 31.7 50.1</cell><cell>ER-MLP</cell><cell cols="3">0.960 0.734 0.652</cell></row><row><cell>RESCAL</cell><cell cols="8">0.890 0.603 84.2 90.4 92.8 0.354 0.189 23.5 40.9 58.7</cell><cell>RESCAL</cell><cell cols="3">0.997 0.745 0.650</cell></row><row><cell>HOLE</cell><cell cols="8">0.938 0.616 93.0 94.5 94.9 0.524 0.232 40.2 61.3 73.9</cell><cell>HOLE</cell><cell cols="3">0.997 0.772 0.697</cell></row><row><cell cols="6">relations can be predicted from patterns of the form:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">locatedIn(c, s) ∧ locatedIn(s, r) ⇒ locatedIn(c, r)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">TRANSE in its original implementation used SGD without AdaGrad. RESCAL used the least-squares loss and ALS updates.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments &amp; Reproducibility</head><p>This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. The code for models and experiments used in this paper is available at https://github.com/ mnick/holographic-embeddings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4825</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Structured Embeddings of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Approximate Reasoning Capabilities of Low-Rank Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledgebased Trust: Estimating the Trustworthiness of Web Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lugaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="938" to="949" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gabor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Associative Holographic Memories. IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="159" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-Mapping: A Theoretical Framework for Analogy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="170" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traversing Knowledge Graphs in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Systems of Concepts with an Infinite Relational Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Querying Factorized Probabilistic Triple Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web-ISWC 2014</title>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="114" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vector-based Models of Semantic Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New generation computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the Rank in Relational Factorization Models by Including Observable Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
	<note>To appear in Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holographic Reduced Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On Holographic Models of Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetik</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="237" to="238" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Markov Logic Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Some algebraic relations between involutions, convolutions, and correlations, with applications to holographic memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="367" to="374" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="216" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning with Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yago: A Core of Semantic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Infinite Hidden Relational Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Uncertainity in Artificial Intelligence</title>
		<meeting>the 22nd International Conference on Uncertainity in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
