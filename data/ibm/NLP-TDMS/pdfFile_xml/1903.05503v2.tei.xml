<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hardness-Aware Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaodong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hardness-Aware Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a hardness-aware deep metric learning (HDML) framework. Most previous deep metric learning methods employ the hard negative mining strategy to alleviate the lack of informative samples for training. However, this mining strategy only utilizes a subset of training data, which may not be enough to characterize the global geometry of the embedding space comprehensively. To address this problem, we perform linear interpolation on embeddings to adaptively manipulate their hard levels and generate corresponding label-preserving synthetics for recycled training, so that information buried in all samples can be fully exploited and the metric is always challenged with proper difficulty. Our method achieves very competitive performance on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep metric learning methods aim to learn effective metrics to measure the similarities between data points accurately and robustly. They take advantage of deep neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11]</ref> to construct a mapping from the data space to the embedding space so that the Euclidean distance in the embedding space can reflect the actual semantic distance between data points, i.e., a relatively large distance between inter-class samples and a relatively small distance between intra-class samples. Recently a variety of deep metric learning methods have been proposed and have demonstrated strong effectiveness in various tasks, such as image retrieval <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>, person re-identification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b1">2]</ref>, and geo-localization <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="bibr">Figure 1</ref>. Illustration of our proposed hardness-aware feature synthesis. A curve in the feature space represents a manifold near which samples belong to one specific class concentrate. Points with the same color in the feature space and embedding space represent the same sample and points of the same shape denote that they belong to the same class. The proposed hardness-aware augmentation first modifies a sample y − toŷ − . Then a labeland-hardness-preserving generator projects it to y − which is the closest point toŷ − on the manifold. The hardness of synthetic negative y − can be controlled adaptively and does not change the original label so that the synthetic hardness-aware tuple can be favorably exploited for effective training. (Best viewed in color.)</p><p>The overall training of a deep metric learning model can be considered as using a loss weighted by the selected samples, which makes the sampling strategy a critical component. A primary issue concerning the sampling strategy is the lack of informative samples for training. A large fraction of samples may satisfy the constraints imposed by the loss function and provide no supervision information for the training model. This motivates many deep metric learning methods to develop efficient hard negative mining strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b9">10]</ref> for sampling. These strategies typically under-sample the training set for hard informative samples which produce gradients with large magnitude. However, the hard negative mining strategy only selects among a subset of samples, which may not be enough to characterize the global geometry of the embedding space accurately. In other words, some data points are sampled repeatedly while others may never have the possibility to be sampled, resulting in an embedding space over-fitting near the oversampled data points and at the same time under-fitting near the under-sampled data points.</p><p>In this paper, we propose a hardness-aware deep metric learning (HDML) framework as a solution. We sample all data points in the training set uniformly while making the best of the information contained in each point. Instead of only using the original samples for training, we propose to synthesize hardness-aware samples as complements to the original ones. In addition, we control the hard levels of the synthetic samples according to the training status of the model, so that the better-trained model is challenged with harder synthetics. We employ an adaptive linear interpolation method to effectively manipulate the hard levels of the embeddings. Having obtained the augmented embeddings, we utilize a simultaneously trained generator to map them back to the feature space while preserving the label and augmented hardness. These synthetics contain more information than original ones and can be used as complements for recycled training, as shown in <ref type="figure">Figure 1</ref>. We provide an ablation study to demonstrate the effectiveness of each module of HDML. Extensive experiments on the widely-used CUB-200-2011 <ref type="bibr" target="#b35">[36]</ref>, Cars196 <ref type="bibr" target="#b15">[16]</ref>, and Stanford Online Products <ref type="bibr" target="#b29">[30]</ref> datasets illustrate that our proposed HDML framework can improve the performance of existing deep metric learning models in both image clustering and retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Metric Learning: Conventional metric learning methods usually employ the Mahalanobis distance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41]</ref> or kernel-based metric <ref type="bibr" target="#b5">[6]</ref> to characterize the linear and nonlinear intrinsic correlations among data points. Contrastive loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> and triplet loss <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref> are two conventional measures which are widely used in most existing metric learning methods. The contrastive loss is designed to separate samples of different classes with a fixed margin and pull closer samples of the same category as near as possible. The triplet loss is more flexible since it only requires a certain ranking within triplets. Furthermore, there are also some works to explore the structure of quadruplets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>The losses used in recently proposed deep metric learning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref> take into consideration of higher order relationships or global information and therefore achieve better performance. For example, Song et al. <ref type="bibr" target="#b29">[30]</ref> proposed a lifted structured loss function to consider all the positive and negative pairs within a batch. Wang et al. <ref type="bibr" target="#b38">[39]</ref> improved the conventional triplet loss by exploiting a third-order geometry relationship. These meticulously designed losses showed great power in various tasks, yet a more advanced sampling framework <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> can still boost their performance. For example, Wu et al. <ref type="bibr" target="#b41">[42]</ref> presented a distance-weighted sampling method to select samples based on their relative distances. Another trend is to incorporate ensemble technique in deep metric learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43]</ref>, which integrates several diverse embeddings to constitute a more informative representation.</p><p>Hard Negative Mining: Hard negative mining has been employed in many machine learning tasks to enhance the training efficiency and boost performance, like supervised learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45]</ref>, exemplar based learning <ref type="bibr" target="#b20">[21]</ref> and unsupervised learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1]</ref>. This strategy aims at progressively selecting false positive samples that will benefit training the most. It is widely used in deep metric learning methods because of the vast number of tuples that can be formed for training. For example, Schroff et al. <ref type="bibr" target="#b24">[25]</ref> proposed to sample semi-hard triplets within a batch, which avoids using too confusing triplets that may result from noisy data. Harwood et al. <ref type="bibr" target="#b9">[10]</ref> presented a smart mining procedure utilizing approximate nearest neighbor search methods to adaptively select more challenging samples for training. The advantage of <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b9">[10]</ref> lies in the selection of samples with suitably hard level with the model. However, they can not control the hard level accurately and do not exploit the information contained in the easy samples.</p><p>Recently proposed methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47]</ref> begin to consider generating potential hard samples to fully train the model. However, there are several drawbacks of the current methods. Firstly, the hard levels of the generated samples cannot be controlled. Secondly, they all require an adversarial manner to train the generator, rendering the model hard to be learned end-to-end and the training process very unstable. Differently, the proposed HDML framework can generate synthetic hardness-aware label-preserving samples with adequate information and adaptive hard levels, further boosting the performance of current deep metric learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we first formulate the problem of deep metric learning and then present the basic idea of the proposed HDML framework. At last, we elaborate on the approach of deep metric learning under this framework.   <ref type="figure">Figure 2</ref>. Illustration of the proposed hardness-aware augmentation. Points with the same shape are from the same class. We performs linear interpolation on the negative pair in the embedding space to obtain a harder tuple, where the hard level is controlled by the training status of the model. As the training proceeds, harder and harder tuples are generated to train the metric more efficiently.</p><formula xml:id="formula_0">V 5 + j 0 R g A K u 3 C V 8 u P W R p i p J m g S j U d O 9 F u R q X m T O A w 3 0 o V J p T 1 a A e b B i M a o n K z 6 d F D c m o c n w S x N C / S Z O r + n s h o q N Q g 9 E x n S H V X L d Y m 5 n + 1 Z q q D S z f j U Z J q j N h s U Z A K o m M y S Y D 4 X C L T Y m C A M s n N r Y R 1 q a R M m 5 z y J g R n 8 c v L U D s v O Y b v n W L 5 B m b K w T G c w B k 4 c A F l u I U K V I H B I z z D G F 6 t v v V i v V n v s 9 Y V</formula><p>(Best viewed in color.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Let X X X denote the data space where we sample a set of data points</p><formula xml:id="formula_1">X = [x 1 , x 2 , · · · , x N ]. Each point x i has a label l i ∈ {1, · · · , C} which constitutes the label set L = [l 1 , l 2 , · · · , l N ]. Let f : X X X f − → Y Y Y</formula><p>be a mapping from the data space to a feature space, where the extracted feature y i has semantic characteristics of its corresponding data point x i . The objective of metric learning is to learn a distance metric in the feature space so that it can reflect the actual semantic distance. The distance metric can be defined as:</p><formula xml:id="formula_2">D(x i , x j ) = m(θ m ; y i , y j ) = m(θ m ; f (x i ), f (x j )), (1)</formula><p>where m is a consistently positive symmetric function and θ m is the corresponding parameters.</p><p>Deep learning methods usually extract features using a deep neural network. A standard procedure is to first project the features into an embedding space (or metric space) Z Z Z with a mapping g :</p><formula xml:id="formula_3">Y Y Y g − → Z Z Z,</formula><p>where the distance metric is then a simple Euclidean distance. Since the projection can be incorporated into the deep network, we can directly learn a mapping h = g • f : X X X h − → Z Z Z from the data space to the embedding space, so that the whole model can be trained end-to-end without explicit feature extraction. In this case, the distance metric is defined as:</p><formula xml:id="formula_4">D(x i , x j ) = d(z i , z j ) = d(θ h ; h(x i ), h(x j )),<label>(2)</label></formula><p>where d indicates the Euclidean distance</p><formula xml:id="formula_5">d(z i , z j ) = ||z i − z j || 2 , z = g(y) = h(x)</formula><p>is the learned embedding, θ f , θ g and θ h are the parameters of mappings f , g and h respectively, and θ h = {θ f , θ g }. Metric learning models are usually trained based on tuples {T i } composed of several samples with certain simi-larity relations. The network parameters are learned by minimizing a specific loss function:</p><formula xml:id="formula_6">θ * h = arg min θ h J(θ h ; {T i }).<label>(3)</label></formula><p>For example, the triplet loss <ref type="bibr" target="#b24">[25]</ref> samples triplets consisting of three examples, the anchor x, the positive x + with the same label with the anchor, and the negative x − with a different label. The triplet loss forces the distance between the anchor and the negative to be larger than the distance between the anchor and the positive by a fixed margin.</p><p>Furthermore, the N-pair Loss <ref type="bibr" target="#b27">[28]</ref> samples tuples with N positive pairs of distinctive classes, and attempts to push away N − 1 negatives altogether.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hardness-Aware Augmentation</head><p>There may exist a great many tuples that can be used during training, yet the vast majority of them actually lack direct information and produce gradients that are approximately zero. To only select among the informative ones we limit ourselves to a small set of tuples. However, this small set may not be able to accurately characterize the global geometry of the embedding space, leading to a biased model.</p><p>To address the above limitations, we propose an adaptive hardness-aware augmentation method, as shown in <ref type="figure">Figure  2</ref>. We modify and construct the hardness-aware tuples in the embedding space, where manipulation of the distances among samples will directly alter the hard level of the tuple. A reduction in the distance between negative pairs will create a rise of the hard level and vice versa.</p><p>Given a set we can usually form more negative pairs than positive pairs, so for simplicity, we only manipulate the distances of negative pairs. For other samples in the tuple, we perform no transformation, i.e.,ẑ = z. Still, our model can be easily extended to deal with positive pairs. Having obtained the embeddings of a negative pair (an anchor z and a negative z − ), we construct an augmented harder negative sampleẑ − by linear interpolation:</p><formula xml:id="formula_7">z − = z + λ 0 (z − − z), λ 0 ∈ [0, 1].<label>(4)</label></formula><p>However, an example too close to the anchor is very likely to share the label, thus no longer constitutes a negative pair. Therefore, it is more reasonable to set λ 0 ∈ ( d + d(z,z − ) , 1], where d + is a reference distance that we use to determine the scale of manipulation (e.g., the distance between a positive pair or a fixed value), and d(z, z − ) = ||z − − z|| 2 . To achieve this, we introduce a variable λ ∈ (0, 1] and set</p><formula xml:id="formula_8">λ 0 = λ + (1 − λ) d + d(z,z − ) , if d(z, z − ) &gt; d + 1 , if d(z, z − ) ≤ d + .<label>(5)</label></formula><p>On condition that d(z, z − ) &gt; d + , the augmented negative   <ref type="figure">Figure 3</ref>. The overall network architecture of the our HDML framework. The red dashed arrow points from the part that the loss is computed on, and to the module that the loss directly supervises. The metric model is a CNN network followed by a fully connected layer. The augmentor is a linear manipulation of the input and the generator is composed of two fully connected layers with increasing dimensions. Part of the metric and the following generator form a similar structure to the well-known autoencoder. (Best viewed in color.) sample can be presented as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L a F r 0 Y 8 I a F m d U t Y A d x M o 6 7 E R q + I = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N s A i u S i K C L g t u X F a w D 2 l C m U w n 7 d D J J M z c F G r o n 7 h x o Y h b / 8 S d f + O k z U J b D w w c z r m X e + a E q e A a X f f b W l v f 2 N z a r u x U d / f 2 D w 7 t o + O 2 T j J F W Y s m I l H d k G g m u G Q t 5 C h Y N 1 W M x K F g n X B 8 W / i d C V O a J / I B p y k L Y j K U P O K U o J H 6 t u 2 P C O Z + T H A U R v n T b N a 3 a 2 7 d n c N Z J V 5 J a l C i 2 b e / / E F C s 5 h J p I J o 3 f P c F I O c K O R U s F n V z z R L C R 2 T I e s Z K k n M d J D P k 8 + c c 6 M M n C h R 5 k l 0 5 u r v j Z z E W k / j 0 E w W E f W y V 4 j / e b 0 M o 5 s g 5 z L N k E m 6 O B R l w s H E K W p w B l w x i m J q C K G K m 6 w O H R F F K J q y q q Y E b / n L q 6 R 9 W f c M v 7 + q N R 7 L O i p w C m d w A R 5 c Q w P u o A k t o D C B Z 3 i F N y u 3 X q x 3 6 2 M x u m a V O y f w B 9 b n D 1 / + l D M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L a F r 0 Y 8 I a F m d U t Y A d x M o 6 7 E R q + I = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N s A i u S i K C L g t u X F a w D 2 l C m U w n 7 d D J J M z c F G r o n 7 h x o Y h b / 8 S d f + O k z U J b D w w c z r m X e + a E q e A a X f f b W l v f 2 N z a r u x U d / f 2 D w 7 t o + O 2 T j J F W Y s m I l H d k G g m u G Q t 5 C h Y N 1 W M x K F g n X B 8 W / i d C V O a J / I B p y k L Y j K U P O K U o J H 6 t u 2 P C O Z + T H A U R v n T b N a 3 a 2 7 d n c N Z J V 5 J a l C i 2 b e / / E F C s 5 h J p I J o 3 f P c F I O c K O R U s F n V z z R L C R 2 T I e s Z K k n M d J D P k 8 + c c 6 M M n C h R 5 k l 0 5 u r v j Z z E W k / j 0 E w W E f W y V 4 j / e b 0 M o 5 s g 5 z L N k E m 6 O B R l w s H E K W p w B l w x i m J q C K G K m 6 w O H R F F K J q y q q Y E b / n L q 6 R 9 W f c M v 7 + q N R 7 L O i p w C m d w A R 5 c Q w P u o A k t o D C B Z 3 i F N y u 3 X q x 3 6 2 M x u m a V O y f w B 9 b n D 1 / + l D M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L a F r 0 Y 8 I a F m d U t Y A d x M o 6 7 E R q + I = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N s A i u S i K C L g t u X F a w D 2 l C m U w n 7 d D J J M z c F G r o n 7 h x o Y h b / 8 S d f + O k z U J b D w w c z r m X e + a E q e A a X f f b W l v f 2 N z a r u x U d / f 2 D w 7 t o + O 2 T j J F W Y s m I l H d k G g m u G Q t 5 C h Y N 1 W M x K F g n X B 8 W / i d C V O a J / I B p y k L Y j K U P O K U o J H 6 t u 2 P C O Z + T H A U R v n T b N</head><formula xml:id="formula_9">z − = z + [λd(z, z − ) + (1 − λ)d + ] z − − z d(z, z − ) .<label>(6)</label></formula><p>Since the overall hardness of original tuples gradually decreases during training, it's reasonable to increase progressively the hardness of synthetic tuples for compensation. The hardness of a triplet increases when λ gets smaller, so we can intuitively set λ to e − α Javg , where J avg is the average metric loss over the last epoch, and α is the pulling factor used to balance the scale of J avg . We exploit the average metric loss to control the hard level since it is a good indicator of the training process. The augmented negative is closer to the anchor if a smaller average loss, leading to harder tuples as training proceeds. The proposed hardnessaware negative augmentation can be represented as:</p><formula xml:id="formula_10">z − =      z + [e − α Javg d(z, z − ) + (1 − e − α Javg )d + ] z − −z d(z,z − ) if d(z, z − ) &gt; d + z − if d(z, z − ) ≤ d + .<label>(7)</label></formula><p>The necessity of adaptive hardness-aware synthesis lies in two aspects. Firstly, in the early stages of training, the embedding space does not have an accurate semantic structure, so currently hard samples may not truly be informative or meaningful, and hard synthetics in this situation may be even inconsistent. Also, hard samples usually result in significant changes of the network parameters. Thus the use of meaningless ones can easily damage the embedding space structure, leading to a model that is trained in the wrong direction from the beginning. On the other hand, as the training proceeds, the model is more tolerant of hard samples, so harder and harder synthetics should be generated to keep the learning efficiency at a high level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hardness-and-Label-Preserving Synthesis</head><p>Having obtained the hardness-aware tuple in the embedding space, our objective is to map it back to the feature space so they can be exploited for training. However, this mapping is not trivial, since a negative sample constructed following <ref type="bibr" target="#b6">(7)</ref> may not necessarily benefit the training process: there is no guarantee thatẑ − shares the same label with z − . To address this, we formulate this problem from a manifold perspective, and propose a hardness-and-labelpreserving feature synthesis method.</p><p>As shown in <ref type="figure">Figure 1</ref>, the two curves in the feature space represent two manifolds near which the original data points belong to class l and l − concentrate respectively. Points with the same color in the feature and embedding space represent the same example. So below we do not distinguish operations acting on features and embeddings. y n is a real data point of class l n , and we first augment it toŷ − following <ref type="bibr" target="#b6">(7)</ref>.ŷ − is more likely to be outside and further from the manifold compared with original data points since it is close to y that belongs to another category. Intuitively, the goal is to learn a generator that mapsŷ − , a data point away from the manifold (less likely belonging to class l − ), to a data point that lies near the manifold (more likely belonging to class l − ). Moreover, to best preserve the hardness, this mapped point should be close toŷ − as much as possible. These two conditions restrict the target point to y − , which is the closest point toŷ − on the manifold. We achieve this by learning a generator i :</p><formula xml:id="formula_11">Z Z Z i − → Y Y Y,</formula><p>which maps the augmented embeddings of a tuple back to the feature space for recycled training. Since a generator usually cannot perfectly map all the embeddings back to the feature space, the synthetic features must lie in the same space to provide meaningful information. Therefore, we map not only the synthetic negative sample but also the other unaltered samples in one tuple:</p><formula xml:id="formula_12">T( y) = i(θ i ; T(ẑ)),<label>(8)</label></formula><p>where T( y) and T(ẑ) are tuples in the feature and embedding space respectively, and θ i is the parameters of the generative mapping i. We exploit an auto-encoder architecture to implement the mapping g and mapping i. The encoder g takes as input a feature vector y which is extracted by CNN from the image, and first maps it to an embedding z. In the embedding space, we modify z toẑ using the hardness-aware augmentation described in the last subsection. The generator i then maps the original embedding z and the augmented embeddingẑ to y and y respectively.</p><p>In order to exploit the synthetic features y for effective training, they should preserve the labels of the original samples as well as the augmented hardness. We formulate the objective of the generator as follows:</p><formula xml:id="formula_13">J gen = J recon + λJ sof t = c(Y, Y ) + λJ sof t ( Y, L) = y∈Y y ∈Y ||y − y || 2 + λ y∈ Y l∈L j sof t ( y, l), (9)</formula><p>where λ is a balance factor, y = i(θ i ; z) is the unaltered synthetic feature, y is the hardness-aware synthetic feature of origin y with label l, Y , Y and Y are the corresponding feature distributions, c(Y, Y ) is the reconstruction cost between the two distributions, and J sof t is the softmax loss function. Note that J gen is only used to train the decoder/generator and has no influence on the metric.</p><p>The overall objective function is composed of two parts: the reconstruction loss and the softmax loss. The synthetic negative should be as close to the augmented negative as possible so that it can constitute a tuple with hardness we require. Thus we utilize the reconstruction loss J recon = ||y − y || 2 2 to restrict the encoder &amp; decoder to map each point close to itself. The softmax loss J sof t ensures that the augmented synthetics do not change the original label. Directly penalizing the distance between y and y can also achieve this, but is too strict to preserve the hardness. Alternatively, we simultaneously learn a fully connected layer with the softmax loss on y, where the gradients only update the parameters in this layer. We employ the learned softmax layer to compute the softmax loss j sof t ( y, l) between the synthetic hardness-aware negative y and the original label l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hardness-Aware Deep Metric Learning</head><p>We present the framework of the proposed method, which is mainly composed of three parts, a metric network to obtain the embeddings, a hardness-aware augmentor to perform augmentation of the hard level and a hardness-andlabel-preserving generator network to generate the corresponding synthetics, as shown in <ref type="figure">Figure 3</ref>.</p><p>Having obtained the embeddings of a tuple, we first perform linear interpolation to modify the hard level, weighted by a factor indicating the current training status of the model. Then we utilize a simultaneously trained generator to generate synthetics for the augmented hardness-aware tuple, meanwhile ensuring the synthetics are realistic and maintain their original labels. Compared to conventional deep metric learning methods, we additionally utilize the hardness-aware synthetics to train the metric:</p><formula xml:id="formula_14">θ * h = arg min θ h J(θ h ; {T i } ∪ { T i }),<label>(10)</label></formula><p>where T i is the synthetic hardness-aware tuple. The proposed framework can be applied to a variety of deep metric learning methods to boost their performance. For a specific loss J in metric learning, the objective function to train the metric is: <ref type="bibr" target="#b10">(11)</ref> where β is a pre-defined parameter, J m = J(T) is the loss J over original samples, J syn = J( T) is the loss J over synthetic samples, and T denotes the synthetic tuple in the feature space. We use e − β Jgen as the balance factor to assign smaller weights to synthetic features when J gen is high, since the generator is not fully trained and the synthetic features may not have realistic meanings.</p><formula xml:id="formula_15">J metric = e − β Jgen J m + (1 − e − β Jgen )J syn = e − β Jgen J(T) + (1 − e − β Jgen )J( T),</formula><p>J m aims to learn the embedding space so that inter-class distances are large and intra-class distances are small. J syn utilizes synthetic hardness-aware samples to train the metric more effectively. As the training proceeds, harder tuples are synthesized to keep the high efficiency of learning.</p><p>We demonstrate our framework on two losses with different tuple formations: triplet loss <ref type="bibr" target="#b24">[25]</ref> and N-pair loss <ref type="bibr" target="#b27">[28]</ref>.</p><p>For the triplet loss <ref type="bibr" target="#b24">[25]</ref>, we use the distance of the positive pair as the reference distance and generate the negative with our hardness-aware synthesis:</p><formula xml:id="formula_16">J( T(x, x + , x − )) = [D(x, x + ) − D(x, x − ) + m] + ,<label>(12)</label></formula><p>where [·] + = max(·, 0) and m is the margin.</p><p>For the N-pair loss <ref type="bibr" target="#b27">[28]</ref>, we also use the distance of the positive pair as the reference distance, but generate all the N − 1 negatives for each anchor in an (N+1)-tuple:</p><formula xml:id="formula_17">J( T({x, x + , x + } i ))<label>(13)</label></formula><formula xml:id="formula_18">= 1 N N i=1 log (1 + j =i exp (D(x i , x + i ) − D(x i , x + j ))).</formula><p>The metric and the generator network are trained simultaneously, without any interruptions for auxiliary sampling processes as most hard negative mining methods do. The augmentor and generator are only used in the training stage, which introduces no additional workload to the resulting embedding computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conducted various experiments to evaluate the proposed HDML in both image clustering and retrieval tasks. We performed an ablation study to analyze the effectiveness of each module. For the clustering task, we employed NMI and F 1 as performance metrics. The normalized mutual information (NMI) is defined by the ratio of the mutual information of clusters and ground truth labels and the arithmetic mean of their entropy. F 1 is the harmonic mean of precision and recall. See <ref type="bibr" target="#b29">[30]</ref> for more details. For the retrieval task, we employed Recall@Ks as performance metrics. They are determined by the existence of at least one correct retrieved sample in the K nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluated our method under a zero-shot setting, where the training set and test set contain image classes with no intersection. We followed <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5]</ref> to perform the training/test set split.</p><p>• The CUB-200-2011 dataset <ref type="bibr" target="#b35">[36]</ref> consists of 11,788 images of 200 bird species. We split the first 100 species (5,864 images) for training and the rest 100 species (5,924 images) for testing.</p><p>• The Cars196 dataset <ref type="bibr" target="#b15">[16]</ref> consists of 16,185 images of 196 car makes and models. We split the first 98 models (8,054 images) for training and the rest 100 models (8,131 images) for testing.</p><p>• The Stanford Online Products dataset <ref type="bibr" target="#b29">[30]</ref> consists of 120,053 images of 22,634 online products from eBay.com. We split the first 11,318 products (59,551 images) for training and the rest 11,316 products (60,502 images) for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>We used the Tensorflow package throughout the experiments. For a fair comparison with previous works on deep metric learning, we used GoogLeNet <ref type="bibr" target="#b30">[31]</ref> architecture as the CNN feature extractor (i.e., f ) and added a fully connected layer as the embedding projector (i.e., g). We implemented the generator (i.e., i) with two fully connected layers of increasing output dimensions 512 and 1,024. We fixed the embedding size to 512 for all the three datasets. For training, we initialized the CNN with weights pretrained on ImageNet ILSVRC dataset <ref type="bibr" target="#b23">[24]</ref> and all other  fully connected layers with random weights. We first resized the images to 256 by 256, then performed random cropping at 227 by 227 and horizontal random mirror for data augmentation. We tuned all the hyperparameters via 5-fold cross-validation on the training set. We set the learning rate for CNNs to 10 −4 and multiplied it by 10 for other fully connected layers. We set the batch size to 120 for the triplet loss and 128 for the N-pair loss. We fixed the balance factors β and λ to 10 4 and 0.5, and set α to 7 for the triplet loss and 90 for the N-pair loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Analysis</head><p>Ablation Study: We present the ablation study of the proposed method. We conducted all the following experiments on the Cars196 dataset with the N-pair loss, but we observe similar results with the triplet loss. <ref type="figure">Figures 4 and 5</ref> show the learning curves of different model settings in the clustering and retrieval task, including the baseline model, the proposed framework with the N-pair loss, the HDML framework without the softmax loss and the HDML framework without the reconstruction loss. We  observe that the absence of the softmax loss results in dramatic performance reduction. This is because the synthetic samples might not preserve the label information, leading to inconsistent tuples. It is surprising that the proposed method without the reconstruction loss still achieves better results than the baseline. We speculate it is because the softmax layer itself learns to distinguish realistic synthetics from false ones in this situation. <ref type="figure" target="#fig_6">Figures 6 and 7</ref> show the effect of different pulling factors. A larger α means we generate harder tuples each time, and α = 0 means we do not apply hard synthesis at all. We see that as α grows, the performance increases at first and achieves the best result at α = 90, then gradually decreases. This justifies the synthesis of tuples with suitable and adaptive hardness. A too light hard synthesis may not fully exploit the underlying information, while a too strong hard synthesis may lead to inconsistent tuples and destroy the structure of the embedding space.</p><p>Quantitative Results: We compared our model with several baseline methods, including the conventional contrastive loss <ref type="bibr" target="#b8">[9]</ref> and triplet loss <ref type="bibr" target="#b40">[41]</ref>, more recent DDML <ref type="bibr" target="#b40">[41]</ref> and triplet loss with semi-hard negative mining <ref type="bibr" target="#b24">[25]</ref>, the state-of-the-art lifted structure <ref type="bibr" target="#b29">[30]</ref>, N-pair loss <ref type="bibr" target="#b27">[28]</ref> and angular loss <ref type="bibr" target="#b38">[39]</ref>, and the hard negative gen- eration method DAML <ref type="bibr" target="#b4">[5]</ref>. We employed the proposed framework to the triplet loss and N-pair loss as illustrated before. We evaluated all the methods mentioned above using the same pre-trained CNN model for fair comparison. <ref type="table">Tables 1, 2, and 3</ref> show the quantitative results on the CUB-200-2011, Cars196, and Stanford Online Products datasets respectively. Red numbers indicate the best results and bold numbers mean our method achieves better results than the associated method without HDML. We observe our proposed framework can achieve very competitive performance on all the three datasets in both tasks. Compared with the original triplet loss and N-pair loss, our framework can further boost their performance for a fairly large margin. This demonstrates the effectiveness of the proposed hardness-aware synthesis strategy. The performance improvement on the Stanford Online Products dataset is relatively small compared with the other two datasets. We think this difference comes from the size of the training set. Our proposed framework generates synthetic samples with suitable and adaptive hardness, which can exploit more information from a limited training set than conventional sampling strategies. This advantage becomes more significant on small-sized datasets like CUB-200-2011 and Cars196.</p><p>Qualitative Results: <ref type="figure" target="#fig_8">Figure 8</ref> shows the Barnes-Hut t-SNE visualization <ref type="bibr" target="#b32">[33]</ref> of the learned embedding using the proposed HDML (N-pair) method. We magnify several areas for a better view, where the color on the boundary of each image represents the category. The test split of the CUB-200-2011 dataset contains 5,924 images of birds from 100 different species. The visual differences between two species tend to be very subtle, making it difficult for humans to distinguish. We observe that despite the subtle inter-class differences and large intra-class variations, such as illumination, backgrounds, viewpoints, and poses, our method can still be able to group similar species, which intuitively verify the effectiveness of the proposed HDML framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a hardness-aware synthesis framework for deep metric learning. Our proposed HDML framework boosts the performance of original metric learning losses by adaptively generating hardness-aware and label-preserving synthetics as complements to the training data. We have demonstrated the effectiveness of the proposed framework on three widely-used datasets in both clustering and retrieval task. In the future, it is interesting to apply our framework to the more general data augmentation problem, which can be utilized to improve a wide variety of machine learning approaches other than metric learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " o K T V Q s t e s u Y 7 8 s M D r e U y r W m L / W 8 = " &gt; A A A B 9 H i c b Z D J S g N B E I Z r X G P c o h 6 9 N A b B U 5 j x o j c D e v A k E c w C y R B 6 e m q S J j 2 L 3 T 2 R M O Q 5 J O A h I l 5 9 D 6 / e f B s 7 y 0 E T f 2 j 4 + K u K q v 6 9 R H C l b f v b W l l d W 9 / Y z G 3 l t 3 d 2 9 / Y L B 4 c 1 F a e S Y Z X F I p Y N j y o U P M K q 5 l p g I 5 F I Q 0 9 g 3 e t d T + r 1 P k r F 4 + h B D x J 0 Q 9 q J e M A Z 1 c Z y W 0 / c R 8 2 F j 9 n d s F 0 o 2 i V 7 K r I M z h y K V 5 + j 0 R g A K u 3 C V 8 u P W R p i p J m g S j U d O 9 F u R q X m T O A w 3 0 o V J p T 1 a A e b B i M a o n K z 6 d F D c m o c n w S x N C / S Z O r + n s h o q N Q g 9 E x n S H V X L d Y m 5 n + 1 Z q q D S z f j U Z J q j N h s U Z A K o m M y S Y D 4 X C L T Y m C A M s n N r Y R 1 q a R M m 5 z y J g R n 8 c v L U D s v O Y b v n W L 5 B m b K w T G c w B k 4 c A F l u I U K V I H B I z z D G F 6 t v v V i v V n v s 9 Y V a z 5 z B H 9 k f f w A B w W V M g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L j K j c Y h D Z u u K 8 g F s w K u T U 7 K p o I = " &gt; A A A B 9 H i c b Z A 9 S w N B E I b n 4 l e M X 1 F L m 8 U g W I U 7 G y 0 D W l h J B P M B y R H 2 9 i b J k r 2 9 c 3 c v E o 7 8 D h s L R W z 9 M X b + G z f J F Z r 4 w s L D O z P M 7 B s k g m v j u t 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j V 1 n C q G D R a L W L U D q l F w i Q 3 D j c B 2 o p B G g c B W M L q e 1 V t j V J r H 8 s F M E v Q j O p C 8 z x k 1 1 v K 7 T z x E w 0 W I 2 d 2 0 V 6 6 4 V X c u s g p e D h X I V e + V v 7 p h z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 H T U j f V m F A 2 o g P s W J Q 0 Q u 1 n 8 6 O n 5 M w 6 I e n H y j 5 p y N z 9 P Z H R S O t J F N j O i J q h X q 7 N z P 9 q n d T 0 r / y M y y Q 1 K N l i U T 8 V x M R k l g A J u U J m x M Q C Z Y r b W w k b U k W Z s T m V b A j e 8 p d X o X l R 9 S z f e 5 X a T R 5 H E U 7 g F M 7 B g 0 u o w S 3 U o Q E M H u E Z X u H N G T s v z r v z s W g t O P n M M f y R 8 / k D L L e S W A = = &lt; / l a t e x i t &gt; e N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o K T V Q s t e s u Y 7 8 s M D r e U y r W m L / W 8 = " &gt; A A A B 9 H i c b Z D J S g N B E I Z r X G P c o h 6 9 N A b B U 5 j x o j c D e v A k E c w C y R B 6 e m q S J j 2 L 3 T 2 R M O Q 5 J O A h I l 5 9 D 6 / e f B s 7 y 0 E T f 2 j 4 + K u K q v 6 9 R H C l b f v b W l l d W 9 / Y z G 3 l t 3 d 2 9 / Y L B 4 c 1 F a e S Y Z X F I p Y N j y o U P M K q 5 l p g I 5 F I Q 0 9 g 3 e t d T + r 1 P k r F 4 + h B D x J 0 Q 9 q J e M A Z 1 c Z y W 0 / c R 8 2 F j 9 n d s F 0 o 2 i V 7 K r I M z h y K V 5 + j 0 R g A K u 3 C V 8 u P W R p i p J m g S j U d O 9 F u R q X m T O A w 3 0 o V J p T 1 a A e b B i M a o n K z 6 d F D c m o c n w S x N C / S Z O r + n s h o q N Q g 9 E x n S H V X L d Y m 5 n + 1 Z q q D S z f j U Z J q j N h s U Z A K o m M y S Y D 4 X C L T Y m C A M s n N r Y R 1 q a R M m 5 z y J g R n 8 c v L U D s v O Y b v n W L 5 B m b K w T G c w B k 4 c A F l u I U K V I H B I z z D G F 6 t v v V i v V n v s 9 Y V a z 5 z B H 9 k f f w A B w W V M g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L j K j c Y h D Z u u K 8 g F s w K u T U 7 K p o I = " &gt; A A A B 9 H i c b Z A 9 S w N B E I b n 4 l e M X 1 F L m 8 U g W I U 7 G y 0 D W l h J B P M B y R H 2 9 i b J k r 2 9 c 3 c v E o 7 8 D h s L R W z 9 M X b + G z f J F Z r 4 w s L D O z P M 7 B s k g m v j u t 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j V 1 n C q G D R a L W L U D q l F w i Q 3 D j c B 2 o p B G g c B W M L q e 1 V t j V J r H 8 s F M E v Q j O p C 8 z x k 1 1 v K 7 T z x E w 0 W I 2 d 2 0 V 6 6 4 V X c u s g p e D h X I V e + V v 7 p h z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 H T U j f V m F A 2 o g P s W J Q 0 Q u 1 n 8 6 O n 5 M w 6 I e n H y j 5 p y N z 9 P Z H R S O t J F N j O i J q h X q 7 N z P 9 q n d T 0 r / y M y y Q 1 K N l i U T 8 V x M R k l g A J u U J m x M Q C Z Y r b W w k b U k W Z s T m V b A j e 8 p d X o X l R 9 S z f e 5 X a T R 5 H E U 7 g F M 7 B g 0 u o w S 3 U o Q E M H u E Z X u H N G T s v z r v z s W g t O P n M M f y R 8 / k D L L e S W A = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " o K T V Q s t e s u Y 7 8 s M D r e U y r W m L / W 8 = " &gt; A A A B 9 H i c b Z D J S g N B E I Z r X G P c o h 6 9 N A b B U 5 j x o j c D e v A k E c w C y R B 6 e m q S J j 2 L 3 T 2 R M O Q 5 J O A h I l 5 9 D 6 / e f B s 7 y 0 E T f 2 j 4 + K u K q v 6 9 R H C l b f v b W l l d W 9 / Y z G 3 l t 3 d 2 9 / Y L B 4 c 1 F a e S Y Z X F I p Y N j y o U P M K q 5 l p g I 5 F I Q 0 9 g 3 e t d T + r 1 P k r F 4 + h B D x J 0 Q 9 q J e M A Z 1 c Z y W 0 / c R 8 2 F j 9 n d s F 0 o 2 i V 7 K r I M z h y K V 5 + j 0 R g A K u 3 C V 8 u P W R p i p J m g S j U d O 9 F u R q X m T O A w 3 0 o V J p T 1 a A e b B i M a o n K z 6 d F D c m o c n w S x N C / S Z O r + n s h o q N Q g 9 E x n S H V X L d Y m 5 n + 1 Z q q D S z f j U Z J q j N h s U Z A K o m M y S Y D 4 X C L T Y m C A M s n N r Y R 1 q a R M m 5 z y J g R n 8 c v L U D s v O Y b v n W L 5 B m b K w T G c w B k 4 c A F l u I U K V I H B I z z D G F 6 t v v V i v V n v s 9 Y V a z 5 z B H 9 k f f w A B w W V M g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L j K j c Y h D Z u u K 8 g F s w K u T U 7 K p o I = " &gt; A A A B 9 H i c b Z A 9 S w N B E I b n 4 l e M X 1 F L m 8 U g W I U 7 G y 0 D W l h J B P M B y R H 2 9 i b J k r 2 9 c 3 c v E o 7 8 D h s L R W z 9 M X b + G z f J F Z r 4 w s L D O z P M 7 B s k g m v j u t 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j V 1 n C q G D R a L W L U D q l F w i Q 3 D j c B 2 o p B G g c B W M L q e 1 V t j V J r H 8 s F M E v Q j O p C 8 z x k 1 1 v K 7 T z x E w 0 W I 2 d 2 0 V 6 6 4 V X c u s g p e D h X I V e + V v 7 p h z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 H T U j f V m F A 2 o g P s W J Q 0 Q u 1 n 8 6 O n 5 M w 6 I e n H y j 5 p y N z 9 P Z H R S O t J F N j O i J q h X q 7 N z P 9 q n d T 0 r / y M y y Q 1 K N l i U T 8 V x M R k l g A J u U J m x M Q C Z Y r b W w k b U k W Z s T m V b A j e 8 p d X o X l R 9 S z f e 5 X a T R 5 H E U 7 g F M 7 B g 0 u o w S 3 U o Q E M H u E Z X u H N G T s v z r v z s W g t O P n M M f y R 8 / k D L L e S W A = = &lt; / l a t e x i t &gt; e N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o K T V Q s t e s u Y 7 8 s M D r e U y r W m L / W 8 = " &gt; A A A B 9 H i c b Z D J S g N B E I Z r X G P c o h 6 9 N A b B U 5 j x o j c D e v A k E c w C y R B 6 e m q S J j 2 L 3 T 2 R M O Q 5 J O A h I l 5 9 D 6 / e f B s 7 y 0 E T f 2 j 4 + K u K q v 6 9 R H C l b f v b W l l d W 9 / Y z G 3 l t 3 d 2 9 / Y L B 4 c 1 F a e S Y Z X F I p Y N j y o U P M K q 5 l p g I 5 F I Q 0 9 g 3 e t d T + r 1 P k r F 4 + h B D x J 0 Q 9 q J e M A Z 1 c Z y W 0 / c R 8 2 F j 9 n d s F 0 o 2 i V 7 K r I M z h y K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a z 5 z B H 9 k f f w A B w W V M g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L j K j c Y h D Z u u K 8 g F s w K u T U 7 K p o I = " &gt; A A A B 9 H i c b Z A 9 S w N B E I b n 4 l e M X 1 F L m 8 U g W I U 7 G y 0 D W l h J B P M B y R H 2 9 i b J k r 2 9 c 3 c v E o 7 8 D h s L R W z 9 M X b + G z f J F Z r 4 w s L D O z P M 7 B s k g m v j u t 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H j V 1 n C q G D R a L W L U D q l F w i Q 3 D j c B 2 o p B G g c B W M L q e 1 V t j V J r H 8 s F M E v Q j O p C 8 z x k 1 1 v K 7 T z x E w 0 W I 2 d 2 0 V 6 6 4 V X c u s g p e D h X I V e + V v 7 p h z N I I p W G C a t 3 x 3 M T 4 G V W G M 4 H T U j f V m F A 2 o g P s W J Q 0 Q u 1 n 8 6 O n 5 M w 6 I e n H y j 5 p y N z 9 P Z H R S O t J F N j O i J q h X q 7 N z P 9 q n d T 0 r / y M y y Q 1 K N l i U T 8 V x M R k l g A J u U J m x M Q C Z Y r b W w k b U k W Z s T m V b A j e 8 p d X o X l R 9 S z f e 5 X a T R 5 H E U 7 g F M 7 B g 0 u o w S 3 U o Q E M H u E Z X u H N G T s v z r v z s W g t O P n M M f y R 8 / k D L L e S W A = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D P N i z g A + o p O B j 8 0 l w I S v V i 4 T i C g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 9 A 2 l M l 0 0 g 6 d T M L M j R B C / 8 K N C 0 X c + j f u / B s n b R f a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 8 L v P H F t R K z u M U u 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w z 6 a D a s 2 t u z O Q V e I t S A 0 W a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 n G o U T P J p p Z 8 a n l A 2 o S P e s 1 T R i B s / n y W e k j O r D E k Y a / s U k p n 6 e y O n k T F Z F N j J I q F Z 9 g r x P 6 + X Y n j t 5 0 I l K X L F 5 h + F q S Q Y k + J 8 M h S a M 5 S Z J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f c s v 7 u s N R 4 W d Z T h B E 7 h H D y 4 g g b c Q h N a w E D B M 7 z C m 2 O c F + f d + Z i P l p z F z j H 8 g f P 5 A w W 6 k T Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D P N i z g A + o p O B j 8 0 l w I S v V i 4 T i C g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 9 A 2 l M l 0 0 g 6 d T M L M j R B C / 8 K N C 0 X c + j f u / B s n b R f a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 8 L v P H F t R K z u M U u 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w z 6 a D a s 2 t u z O Q V e I t S A 0 W a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 n G o U T P J p p Z 8 a n l A 2 o S P e s 1 T R i B s / n y W e k j O r D E k Y a / s U k p n 6 e y O n k T F Z F N j J I q F Z 9 g r x P 6 + X Y n j t 5 0 I l K X L F 5 h + F q S Q Y k + J 8 M h S a M 5 S Z J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f c s v 7 u s N R 4 W d Z T h B E 7 h H D y 4 g g b c Q h N a w E D B M 7 z C m 2 O c F + f d + Z i P l p z F z j H 8 g f P 5 A w W 6 k T Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D P N i z g A + o p O B j 8 0 l w I S v V i 4 T i C g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 9 A 2 l M l 0 0 g 6 d T M L M j R B C / 8 K N C 0 X c + j f u / B s n b R f a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 8 L v P H F t R K z u M U u 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w z 6 a D a s 2 t u z O Q V e I t S A 0 W a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 n G o U T P J p p Z 8 a n l A 2 o S P e s 1 T R i B s / n y W e k j O r D E k Y a / s U k p n 6 e y O n k T F Z F N j J I q F Z 9 g r x P 6 + X Y n j t 5 0 I l K X L F 5 h + F q S Q Y k + J 8 M h S a M 5 S Z J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f c s v 7 u s N R 4 W d Z T h B E 7 h H D y 4 g g b c Q h N a w E D B M 7 z C m 2 O c F + f d + Z i P l p z F z j H 8 g f P 5 A w W 6 k T Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D P N i z g A + o p O B j 8 0 l w I S v V i 4 T i C g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g x m U F + 9 A 2 l M l 0 0 g 6 d T M L M j R B C / 8 K N C 0 X c + j f u / B s n b R f a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T m 8 L v P H F t R K z u M U u 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w z 6 a D a s 2 t u z O Q V e I t S A 0 W a A 6 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 n G o U T P J p p Z 8 a n l A 2 o S P e s 1 T R i B s / n y W e k j O r D E k Y a / s U k p n 6 e y O n k T F Z F N j J I q F Z 9 g r x P 6 + X Y n j t 5 0 I l K X L F 5 h + F q S Q Y k + J 8 M h S a M 5 S Z J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f c s v 7 u s N R 4 W d Z T h B E 7 h H D y 4 g g b c Q h N a w E D B M 7 z C m 2 O c F + f d + Z i P l p z F z j H 8 g f P 5 A w W 6 k T Q = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d H H n 0 f I H A t 7 b f 3 1 P S 8 3 3 + v 9 C c i E = " &gt; A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q E X R b c u K x g H 9 q G M p n e t E M n k z A z E W r o X 7 h x o Y h b / 8 a d f + O k z U J b D w w c z r m X O f c E i e D a u O 6 3 s 7 K 6 t r 6 x W d o q b + / s 7 u 1 X D g 5 b O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f 5 3 7 7 E Z X m s b w z k w T 9 i A 4 l D z m j x k o P v Y i a U R B m T 9 N + p e r W 3 B n I M v E K U o U C j X 7 l q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 7 q U a E 8 r G d I h d S y W N U P v Z L P G U n F p l Q M J Y 2 S c N m a m / N z I a a T 2 J A j u Z J 9 S L X i 7 + 5 3 V T E 1 7 5 G Z d J a l C y + U d h K o i J S X 4 + G X C F z I i J J Z Q p b r M S N q K K M m N L K t s S v M W T l 0 n r v O Z Z f n t R r d 8 X d Z T g G E 7 g D D y 4 h D r c Q A O a w E D C M 7 z C m 6 O d F + f d + Z i P r j j F z h H 8 g f P 5 A w c / k T U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d H H n 0 f I H A t 7 b f 3 1 P S 8 3 3 + v 9 C c i E = " &gt; A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q E X R b c u K x g H 9 q G M p n e t E M n k z A z E W r o X 7 h x o Y h b / 8 a d f + O k z U J b D w w c z r m X O f c E i e D a u O 6 3 s 7 K 6 t r 6 x W d o q b + / s 7 u 1 X D g 5 b O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f 5 3 7 7 E Z X m s b w z k w T 9 i A 4 l D z m j x k o P v Y i a U R B m T 9 N + p e r W 3 B n I M v E K U o U C j X 7 l q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 7 q U a E 8 r G d I h d S y W N U P v Z L P G U n F p l Q M J Y 2 S c N m a m / N z I a a T 2 J A j u Z J 9 S L X i 7 + 5 3 V T E 1 7 5 G Z d J a l C y + U d h K o i J S X 4 + G X C F z I i J J Z Q p b r M S N q K K M m N L K t s S v M W T l 0 n r v O Z Z f n t R r d 8 X d Z T g G E 7 g D D y 4 h D r c Q A O a w E D C M 7 z C m 6 O d F + f d + Z i P r j j F z h H 8 g f P 5 A w c / k T U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d H H n 0 f I H A t 7 b f 3 1 P S 8 3 3 + v 9 C c i E = " &gt; A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q E X R b c u K x g H 9 q G M p n e t E M n k z A z E W r o X 7 h x o Y h b / 8 a d f + O k z U J b D w w c z r m X O f c E i e D a u O 6 3 s 7 K 6 t r 6 x W d o q b + / s 7 u 1 X D g 5 b O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f 5 3 7 7 E Z X m s b w z k w T 9 i A 4 l D z m j x k o P v Y i a U R B m T 9 N + p e r W 3 B n I M v E K U o U C j X 7 l q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 7 q U a E 8 r G d I h d S y W N U P v Z L P G U n F p l Q M J Y 2 S c N m a m / N z I a a T 2 J A j u Z J 9 S L X i 7 + 5 3 V T E 1 7 5 G Z d J a l C y + U d h K o i J S X 4 + G X C F z I i J J Z Q p b r M S N q K K M m N L K t s S v M W T l 0 n r v O Z Z f n t R r d 8 X d Z T g G E 7 g D D y 4 h D r c Q A O a w E D C M 7 z C m 6 O d F + f d + Z i P r j j F z h H 8 g f P 5 A w c / k T U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d H H n 0 f I H A t 7 b f 3 1 P S 8 3 3 + v 9 C c i E = " &gt; A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U Q E X R b c u K x g H 9 q G M p n e t E M n k z A z E W r o X 7 h x o Y h b / 8 a d f + O k z U J b D w w c z r m X O f c E i e D a u O 6 3 s 7 K 6 t r 6 x W d o q b + / s 7 u 1 X D g 5 b O k 4 V w y a L R a w 6 A d U o u M S m 4 U Z g J 1 F I o 0 B g O x h f 5 3 7 7 E Z X m s b w z k w T 9 i A 4 l D z m j x k o P v Y i a U R B m T 9 N + p e r W 3 B n I M v E K U o U C j X 7 l q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 7 q U a E 8 r G d I h d S y W N U P v Z L P G U n F p l Q M J Y 2 S c N m a m / N z I a a T 2 J A j u Z J 9 S L X i 7 + 5 3 V T E 1 7 5 G Z d J a l C y + U d h K o i J S X 4 + G X C F z I i J J Z Q p b r M S N q K K M m N L K t s S v M W T l 0 n r v O Z Z f n t R r d 8 X d Z T g G E 7 g D D y 4 h D r c Q A O a w E D C M 7 z C m 6 O d F + f d + Z i P r j j F z h H 8 g f P 5 A w c / k T U = &lt; / l a t e x i t &gt;ẑ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 3 a 2 7 d n c N Z J V 5 J a l C i 2 b e / / E F C s 5 h J p I J o 3 f P c F I O c K O R U s F n V z z R L C R 2 T I e s Z K k n M d J D P k 8 + c c 6 M M n C h R 5 k l 0 5 u r v j Z z E W k / j 0 E w W E f W y V 4 j / e b 0 M o 5 s g 5 z L N k E m 6 O B R l w s H E K W p w B l w x i m J q C K G K m 6 w O H R F F K J q y q q Y E b / n L q 6 R 9 W f c M v 7 + q N R 7 L O i p w C m d w A R 5 c Q w P u o A k t o D C B Z 3 i F N y u 3 X q x 3 6 2 M x u m a V O y f w B 9 b n D 1 / + l D M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L a F r 0 Y 8 I a F m d U t Y A d x M o 6 7 E R q + I = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N s A i u S i K C L g t u X F a w D 2 l C m U w n 7 d D J J M z c F G r o n 7 h x o Y h b / 8 S d f + O k z U J b D w w c z r m X e + a E q e A a X f f b W l v f 2 N z a r u x U d / f 2 D w 7 t o + O 2 T j J F W Y s m I l H d k G g m u G Q t 5 C h Y N 1 W M x K F g n X B 8 W / i d C V O a J / I B p y k L Y j K U P O K U o J H 6 t u 2 P C O Z + T H A U R v n T b N a 3 a 2 7 d n c N Z J V 5 J a l C i 2 b e / / E F C s 5 h J p I J o 3 f P c F I O c K O R U s F n V z z R L C R 2 T I e s Z K k n M d J D P k 8 + c c 6 M M n C h R 5 k l 0 5 u r v j Z z E W k / j 0 E w W E f W y V 4 j / e b 0 M o 5 s g 5 z L N k E m 6 O B R l w s H E K W p w B l w x i m J q C K G K m 6 w O H R F F K J q y q q Y E b / n L q 6 R 9 W f c M v 7 + q N R 7 L O i p w C m d w A R 5 c Q w P u o A k t o D C B Z 3 i F N y u 3 X q x 3 6 2 M x u m a V O y f w B 9 b n D 1 / + l D M = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>pair) w/o Jsoft HDML(N-pair) w/o Jrecon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Comparisons of different settings in the clustering task. Comparisons of different settings in the retrieval task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparisons of converged results using different pulling factors in the clustering and retrieval task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Comparisons of using different pulling factors in the retrieval task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Barnes-Hut t-SNE visualization [33] of the proposed HDML (N-pair) method on the test split of CUB-200-2011, where we magnify several areas for a better view. The color of the boundary of each image represent the category. (Best viewed when zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 3 .</head><label>13</label><figDesc>Experimental results (%) on the CUB-200-2011 dataset in comparison with other methods. Experimental results (%) on the Stanford Online Products dataset in comparison with other methods.</figDesc><table><row><cell>Method</cell><cell>NMI</cell><cell>F1</cell><cell cols="4">R@1 R@2 R@4 R@8</cell></row><row><cell>Contrastive</cell><cell cols="2">47.2 12.5</cell><cell>27.2</cell><cell>36.3</cell><cell>49.8</cell><cell>62.1</cell></row><row><cell>DDML</cell><cell cols="2">47.3 13.1</cell><cell>31.2</cell><cell>41.6</cell><cell>54.7</cell><cell>67.1</cell></row><row><cell>Lifted</cell><cell cols="2">56.4 22.6</cell><cell>46.9</cell><cell>59.8</cell><cell>71.2</cell><cell>81.5</cell></row><row><cell>Angular</cell><cell cols="2">61.0 30.2</cell><cell>53.6</cell><cell>65.0</cell><cell>75.3</cell><cell>83.7</cell></row><row><cell>Triplet</cell><cell cols="2">49.8 15.0</cell><cell>35.9</cell><cell>47.7</cell><cell>59.1</cell><cell>70.0</cell></row><row><cell>Triplet hard</cell><cell cols="2">53.4 17.9</cell><cell>40.6</cell><cell>52.3</cell><cell>64.2</cell><cell>75.0</cell></row><row><cell cols="3">DAML (Triplet) 51.3 17.6</cell><cell>37.6</cell><cell>49.3</cell><cell>61.3</cell><cell>74.4</cell></row><row><cell cols="3">HDML (Triplet) 55.1 21.9</cell><cell>43.6</cell><cell>55.8</cell><cell>67.7</cell><cell>78.3</cell></row><row><cell>N-pair</cell><cell cols="2">60.2 28.2</cell><cell>51.9</cell><cell>64.3</cell><cell>74.9</cell><cell>83.2</cell></row><row><cell>DAML (N-pair)</cell><cell cols="2">61.3 29.5</cell><cell>52.7</cell><cell>65.4</cell><cell>75.5</cell><cell>84.3</cell></row><row><cell>HDML (N-pair)</cell><cell cols="2">62.6 31.6</cell><cell>53.7</cell><cell>65.7</cell><cell>76.7</cell><cell>85.7</cell></row><row><cell cols="7">Table 2. Experimental results (%) on the Cars196 dataset in com-</cell></row><row><cell cols="2">parison with other methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>NMI</cell><cell>F1</cell><cell cols="4">R@1 R@2 R@4 R@8</cell></row><row><cell>Contrastive</cell><cell cols="2">42.3 10.5</cell><cell>27.6</cell><cell>38.3</cell><cell>51.0</cell><cell>63.9</cell></row><row><cell>DDML</cell><cell cols="2">41.7 10.9</cell><cell>32.7</cell><cell>43.9</cell><cell>56.5</cell><cell>68.8</cell></row><row><cell>Lifted</cell><cell cols="2">57.8 25.1</cell><cell>59.9</cell><cell>70.4</cell><cell>79.6</cell><cell>87.0</cell></row><row><cell>Angular</cell><cell cols="2">62.4 31.8</cell><cell>71.3</cell><cell>80.7</cell><cell>87.0</cell><cell>91.8</cell></row><row><cell>Triplet</cell><cell cols="2">52.9 17.9</cell><cell>45.1</cell><cell>57.4</cell><cell>69.7</cell><cell>79.2</cell></row><row><cell>Triplet hard</cell><cell cols="2">55.7 22.4</cell><cell>53.2</cell><cell>65.4</cell><cell>74.3</cell><cell>83.6</cell></row><row><cell cols="3">DAML (Triplet) 56.5 22.9</cell><cell>60.6</cell><cell>72.5</cell><cell>82.5</cell><cell>89.9</cell></row><row><cell cols="3">HDML (Triplet) 59.4 27.2</cell><cell>61.0</cell><cell>72.6</cell><cell>80.7</cell><cell>88.5</cell></row><row><cell>N-pair</cell><cell cols="2">62.7 31.8</cell><cell>68.9</cell><cell>78.9</cell><cell>85.8</cell><cell>90.9</cell></row><row><cell>DAML (N-pair)</cell><cell cols="2">66.0 36.4</cell><cell>75.1</cell><cell>83.8</cell><cell>89.7</cell><cell>93.5</cell></row><row><cell>HDML (N-pair)</cell><cell cols="2">69.7 41.6</cell><cell>79.1</cell><cell>87.1</cell><cell>92.1</cell><cell>95.5</cell></row><row><cell>Method</cell><cell>NMI</cell><cell>F1</cell><cell cols="4">R@1 R@10 R@100</cell></row><row><cell>Contrastive</cell><cell cols="2">82.4 10.1</cell><cell>37.5</cell><cell>53.9</cell><cell>71.0</cell></row><row><cell>DDML</cell><cell cols="2">83.4 10.7</cell><cell>42.1</cell><cell>57.8</cell><cell>73.7</cell></row><row><cell>Lifted</cell><cell cols="2">87.2 25.3</cell><cell>62.6</cell><cell>80.9</cell><cell>91.2</cell></row><row><cell>Angular</cell><cell cols="2">87.8 26.5</cell><cell>67.9</cell><cell>83.2</cell><cell>92.2</cell></row><row><cell>Triplet</cell><cell cols="2">86.3 20.2</cell><cell>53.9</cell><cell>72.1</cell><cell>85.7</cell></row><row><cell>Triplet hard</cell><cell cols="2">86.7 22.1</cell><cell>57.8</cell><cell>75.3</cell><cell>88.1</cell></row><row><cell cols="3">DAML (Triplet) 87.1 22.3</cell><cell>58.1</cell><cell>75.0</cell><cell>88.0</cell></row><row><cell cols="3">HDML (Triplet) 87.2 22.5</cell><cell>58.5</cell><cell>75.5</cell><cell>88.3</cell></row><row><cell>N-pair</cell><cell cols="2">87.9 27.1</cell><cell>66.4</cell><cell>82.9</cell><cell>92.1</cell></row><row><cell>DAML (N-pair)</cell><cell cols="2">89.4 32.4</cell><cell>68.4</cell><cell>83.5</cell><cell>92.3</cell></row><row><cell>HDML (N-pair)</cell><cell cols="2">89.3 32.2</cell><cell>68.7</cell><cell>83.2</cell><cell>92.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61672306, Grant U1813218, Grant 61822603, Grant U1713214, and Grant 61572271.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep unsupervised similarity learning using partially ordered sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1923" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1320" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2780" to="2789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning globallyconsistent local distance functions for shape-based image retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learned contextual feature reweighting for image geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3251" to="3260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="760" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bierboosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5189" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting im2gps in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Belongie. The Caltech-UCSD Birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2859" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Correcting the triplet selection bias for triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hardaware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An adversarial approach to hard triplet generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="501" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient online local metric adaptation via negative samples for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2420" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
