<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FinBERT: Financial Sentiment Analysis with Pre-trained Language Models FinBERT: Financial Sentiment Analysis with Pre-trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-27">27 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogu</forename><forename type="middle">Tan</forename><surname>Araci</surname></persName>
							<email>dogu.araci@student.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FinBERT: Financial Sentiment Analysis with Pre-trained Language Models FinBERT: Financial Sentiment Analysis with Pre-trained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-27">27 Aug 2019</date>
						</imprint>
					</monogr>
					<note>Internal Supervisor External Supervisor Title, Name Dr Pengjie Ren Dr Zulkuf Genc Affiliation UvA, ILPS Naspers Group</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. Generalpurpose models are not effective enough because of specialized language used in financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domainspecific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in financial domain. Our results show improvement in every measured metric on current state-of-theart results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Prices in an open market reflects all of the available information regarding assets exchanged in an economy <ref type="bibr" target="#b15">[16]</ref>. When new information becomes available, all actors in the economy update their positions and prices adjust accordingly, which makes beating the markets consistently impossible. However, the definition of "new information" might change as new information retrieval technologies become available and early-adoption of such technologies might provide an advantage in the short-term.</p><p>Analysis of financial texts, be it news, analyst reports or official company announcements is a possible source of new information. With unprecedented amount of such text being created every day, manually analyzing these and deriving actionable insights from them is too big of a task for any single entity. Hence, automated sentiment or polarity analysis of texts produced by financial actors using natural language processing (NLP) methods has gained popularity during the last decade <ref type="bibr" target="#b3">[4]</ref>.</p><p>The principal research interest for this thesis is the polarity analysis, which is classifying text as positive, negative or neutral, in a specific domain. It requires to address two challenges: 1) The most sophisticated classification methods that make use of neural nets require vast amounts of labeled data and labeling financial text snippets requires costly expertise. 2) The sentiment analysis models trained on general corpora are not suited to the task, because financial texts have a specialized language with unique vocabulary and have a tendency to use vague expressions instead of easilyidentified negative/positive words.</p><p>Using carefully crafted financial sentiment lexicons such as <ref type="bibr" target="#b10">Loughran and McDonald (2011)</ref>  <ref type="bibr" target="#b10">[11]</ref> may seem a solution because they incorporate existing financial knowledge into textual analysis. However, they are based on "word counting" methods, which come short in analyzing deeper semantic meaning of a given text. NLP transfer learning methods look like a promising solution to both of the challenges mentioned above, and are the focus of this thesis. The core idea behind these models is that by training language models on very large corpora and then initializing down-stream models with the weights learned from the language modeling task, a much better performance can be achieved. The initialized layers can range from the single word embedding layer <ref type="bibr" target="#b22">[23]</ref> to the whole model <ref type="bibr" target="#b4">[5]</ref>. This approach should, in theory, be an answer to the scarcity of labeled data problem. Language models don't require any labels, since the task is predicting the next word. They can learn how to represent the semantic information. That leaves the fine-tuning on labeled data only the task of learning how to use this semantic information to predict the labels.</p><p>One particular component of the transfer learning methods is the ability to further pre-train the language models on domain specific unlabeled corpus. Thus, the model can learn the semantic relations in the text of the target domain, which is likely to have a different distribution than a general corpus. This approach is especially promising for a niche domain like finance, since the language and vocabulary used is dramatically different than a general one.</p><p>The goal of this thesis is to test these hypothesized advantages of using and fine-tuning pre-trained language models for financial domain. For that, sentiment of a sentence from a financial news article towards the financial actor depicted in the sentence will be tried to be predicted, using the Financial PhraseBank created by <ref type="bibr" target="#b16">Malo et al. (2014)</ref>  <ref type="bibr" target="#b16">[17]</ref> and FiQA Task 1 sentiment scoring dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>The main contributions of this thesis are the following:</p><p>• We introduce FinBERT, which is a language model based on BERT for financial NLP tasks. We evaluate FinBERT on two financial sentiment analysis datasets. • We achieve the state-of-the-art on FiQA sentiment scoring and Financial PhraseBank. • We implement two other pre-trained language models, ULM-Fit and ELMo for financial sentiment analysis and compare these with FinBERT. • We conduct experiments to investigate several aspects of the model, including: effects of further pre-training on financial corpus, training strategies to prevent catastrophic forgetting and fine-tuning only a small subset of model layers for decreasing training time without a significant drop in performance.</p><p>The rest of the thesis is structured as follows: First, relevant literature in both financial polarity analysis and pre-trained language models are discussed (Section 2). Then, the evaluated models are described (Section 3). This is followed by the description of the experimental setup being used (Section 4). In Section 5, we present the experimental results on the financial sentiment datasets. Then we further analyze FinBERT from different perspectives in Section 6. Finally, we conclude with Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED LITERATURE</head><p>This section describes previous research conducted on sentiment analysis in finance (2.1) and text classification using pre-trained language models (2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment analysis in finance</head><p>Sentiment analysis is the task of extracting sentiments or opinions of people from written language <ref type="bibr" target="#b9">[10]</ref>. We can divide the recent efforts into two groups: 1) Machine learning methods with features extracted from text with "word counting" <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, 2) Deep learning methods, where text is represented by a sequence of embeddings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. The former suffers from inability to represent the semantic information that results from a particular sequence of words, while the latter is often deemed as too "data-hungry" as it learns a much higher number of parameters <ref type="bibr" target="#b17">[18]</ref>.</p><p>Financial sentiment analysis differs from general sentiment analysis not only in domain, but also the purpose. The purpose behind financial sentiment analysis is usually guessing how the markets will react with the information presented in the text <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b11">Loughran and McDonald (2016)</ref> presents a thorough survey of recent works on financial text analysis utilizing machine learning with "bag-ofwords" approach or lexicon-based methods <ref type="bibr" target="#b11">[12]</ref>. For example, in <ref type="bibr" target="#b10">Loughran and McDonald (2011)</ref>, they create a dictionary of financial terms with assigned values such as "positive" or "uncertain" and measure the tone of a documents by counting words with a specific dictionary value <ref type="bibr" target="#b10">[11]</ref>. Another example is Pagolu et al. (2016), where n-grams from tweets with financial information are fed into supervised machine learning algorithms to detect the sentiment regarding the financial entity mentioned.</p><p>On of the first papers that used deep learning methods for textual financial polarity analysis was <ref type="bibr" target="#b6">Kraus and Feuerriegel (2017)</ref>  <ref type="bibr" target="#b6">[7]</ref>. They apply an LSTM neural network to ad-hoc company announcements to predict stock-market movements and show that method to be more accurate than traditional machine learning approaches. They find pre-training their model on a larger corpus to improve the result, however their pre-training is done on a labeled dataset, which is a more limiting approach then ours, as we pre-train a language model as an unsupervised task.</p><p>There are several other works that employ various types of neural architectures for financial sentiment analysis. <ref type="bibr" target="#b25">Sohangir et al. (2018)</ref>  <ref type="bibr" target="#b25">[26]</ref> apply several generic neural network architectures to a StockTwits dataset, finding CNN as the best performing neural network architecture. Lutz et al. 2018 <ref type="bibr" target="#b12">[13]</ref> take the approach of using doc2vec to generate sentence embeddings in a particular company ad-hoc announcement and utilize multi-instance learning to predict stock market outcomes.   <ref type="bibr" target="#b13">[14]</ref> use a combination of text simplification and LSTM network to classify a set of sentences from financial news according to their sentiment and achieve stateof-the-art results for the Financial PhraseBank, which is used in thesis as well.</p><p>Due to lack of large labeled financial datasets, it is difficult to utilize neural networks to their full potential for sentiment analysis.</p><p>Even when their first (word embedding) layers are initialized with pre-trained values, the rest of the model still needs to learn complex relations with relatively small amount of labeled data. A more promising solution could be initializing almost the entire model with pre-trained values and fine-tuning those values with respect to the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text classification using pre-trained language models</head><p>Language modeling is the task of predicting the next word in a given piece of text. One of the most important recent developments in natural language processing is the realization that a model trained for language modeling can be successfully fine-tuned for most down-stream NLP tasks with small modifications. These models are usually trained on very large corpora, and then with addition of suitable task-specific layers fine-tuned on the target dataset <ref type="bibr" target="#b5">[6]</ref>.</p><p>Text classification, which is the focus of this thesis, is one of the obvious use-cases for this approach. ELMo (Embeddings from Language Models) <ref type="bibr" target="#b22">[23]</ref> was one of the first successful applications of this approach. With ELMo, a deep bidirectional language model is pre-trained on a large corpus. For each word, hidden states of this model is used to compute a contextualized representation. Using the pre-trained weights of ELMo, contextualized word embeddings can be calculated for any piece of text. Initializing embeddings for down-stream tasks with those were shown to improve performance on most tasks compared to static word embeddings such as word2vec or GloVe. For text classification tasks like SST-5, it achieved state-of-the-art performance when used together with a bi-attentive classification network <ref type="bibr" target="#b19">[20]</ref>.</p><p>Although ELMo makes use of pre-trained language models for contextualizing representations, still the information extracted using a language model is present only in the first layer of any model using it. ULMFit (Universal Language Model Fine-tuning) <ref type="bibr" target="#b4">[5]</ref> was the first paper to achieve true transfer learning for NLP, as using novel techniques such as discriminative fine-tuning, slanted triangular learning rates and gradual unfreezing. They were able to efficiently fine-tune a whole pre-trained language model for text classification. They also introduced further pre-training of the language model on a domain-specific corpus, assuming target task data comes from a different distribution than the general corpus the initial model was trained on.</p><p>ULMFit's main idea of efficiently fine-tuning a pre-trained a language model for down-stream tasks was brought to another level with Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b2">[3]</ref>, which is also the main focus of this paper. BERT has two important differences from what came before: 1) It defines the task of language modeling as predicting randomly masked tokens in a sequence rather than the next token, in addition to a task of classifying two sentences as following each other or not. 2) It is a very big network trained on an unprecedentedly large corpus. These two factors enabled in to achieve state-of-the-art results in multiple NLP tasks such as, natural language inference or question answering.</p><p>The specifics of fine-tuning BERT for text classification has not been researched thoroughly. One such recent work is Sun et al.</p><p>(2019) <ref type="bibr" target="#b26">[27]</ref>. They conduct a series of experiments regarding different configurations of BERT for text classification. Some of their results will be referenced throughout the rest of the thesis, for the configuration of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we will present our BERT implementation for financial domain named as FinBERT, after giving a brief background on relevant neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>3.1.1 LSTM. Long short-term memory (LSTM) is a type of recurrent neural network that allows long-term dependencies in a sequence to persist in the network by using "forget" and "update" gates. It is one of the primary architectures for modeling any sequential data generation process, from stock prices to natural language. Since a text is a sequence of tokens, the first choice for any LSTM natural language processing model is determining how to initially represent a single token. Using pre-trained weights for initial token representation is the common practice. One such pre-training algorithm is GLoVe (Global Vectors for Word Representation) <ref type="bibr" target="#b21">[22]</ref>. GLoVr is a model for calculating word representations with the unsupervised task of training a log-bilinear regression model on a word-word co-occurance matrix from a large corpus. It is an effective model for representing words in a vector space, however it doesn't contextualize these representations with respect to the sequence they are actually used in 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">ELMo.</head><p>ELMo embeddings <ref type="bibr" target="#b22">[23]</ref> are contextualized word representations in the sense that the surrounding words influence the representation of the word. In the center of ELMo, there is a bidirectional language model with multiple LSTM layers. The goal of a language model is to learn the probability distribution over sequences of tokens in a given vocabulary. ELMo models the probability of a token given the previous (and separately following) tokens in the sequence. Then the model also learns how to weight different representations from different LSTM layers in order to calculate one contextualized vector per token. Once the contextualized representations are extracted, these can be used to initialize any down-stream NLP task 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">ULMFit.</head><p>ULMFit is a transfer learning model for down-stream NLP tasks, that make use of language model pre-training <ref type="bibr" target="#b4">[5]</ref>. Unlike ELMo, with ULMFit, the whole language model is fine-tuned together with the task-specific layers. The underlying language model used in ULMFit is AWD-LSTM, which uses sophisticated dropout tuning strategies to better regularize its LSTM model <ref type="bibr" target="#b20">[21]</ref>. For classification using ULMFit two linear layers are added to the pre-trained AWD-LSTM, first of which takes the pooled last hidden states as input.</p><p>ULMFit comes with novel training strategies for further pretraining the language model on domain-specific corpus and finetuning on the down-stream task. We implement these strategies with FinBERT as explained in section 3.2. <ref type="bibr" target="#b0">1</ref> The pre-trained weights for GLoVE can be found here: https://nlp.stanford.edu/projects/glove/ 2 The pre-trained ELMo models can be found here: https://allennlp.org/elmo 3.1.4 Transformer. The Transformer is an attention-based architecture for modeling sequential information, that is an alternative to recurrent neural networks <ref type="bibr" target="#b28">[29]</ref>. It was proposed as a sequence-tosequence model, therefore including encoder and decoder mechanisms. Here, we will focus only on the encoder part (though decoder is quite similar). The encoder consists of multiple identical Transformer layers. Each layer has a multi-headed self-attention layer and a fully connected feed-forward network. For one self-attention layer, three mappings from embeddings (key, query and value) are learned. Using each token's key and all tokens' query vectors, a similarity score is calculated with dot product. These scores are used to weight the value vectors to arrive at the new representation of the token. With the multi-headed self-attention, these layers are concatenated together, so that the sequence can be evaluated from varying "perspectives". Then the resulted vectors go through fully connected networks with shared parameters.</p><p>As it was argued by Vaswani 2017 <ref type="bibr" target="#b28">[29]</ref>, Transformer architecture has several advantages over the RNN-based approaches. Because of RNNs' sequential nature, they are much harder to parallelize on GPUs and too many steps between far away elements in a sequence make it hard for information to persist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">BERT.</head><p>BERT <ref type="bibr" target="#b2">[3]</ref> is in essence a language model that consists of a set of Transformer encoders stacked on top of each other. However it defines the language modeling task differently from ELMo and AWD-LSTM. Instead of predicting the next word given previous ones, BERT "masks" a randomly selected 15% of all tokens. With a softmax layer over vocabulary on top of the last encoder layer the masked tokens are predicted. A second task BERT is trained on is "next sentence prediction". Given two sentences, the model predicts whether or not these two actually follow each other.</p><p>The input sequence is represented with token and position embeddings. Two tokens denoted by [CLS] and [SEP] are added to the beginning and end of the sequence respectively. For all classification tasks, including the next sentence prediction, [CLS] token is used.</p><p>BERT has two versions: BERT-base, with 12 encoder layers, hidden size of 768, 12 multi-head attention heads and 110M parameters in total and BERT-large, with 24 encoder layers, hidden size of 1024, 16 multi-head attention heads and 340M parameters. Both of these models have been trained on BookCorpus <ref type="bibr" target="#b32">[33]</ref> and English Wikipedia, which have in total more than 3,500M words 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BERT for financial domain: FinBERT</head><p>In this subsection we will describe our implementation of BERT: 1) how further pre-training on domain corpus is done, 2-3) how we implemented BERT for classification and regression tasks, 4) training strategies we used during fine-tuning to prevent catastrophic forgetting.</p><p>Regardless, we implement further pre-training in order to observe if such adaptation is going to be beneficial for financial domain.</p><p>For further pre-training, we experiment with two approaches. The first is pre-training the model on a relatively large corpus from the target domain. For that, we further pre-train a BERT language model on a financial corpus (details of the corpus can be found on section 4.2.1). The second approach is pre-training the model only on the sentences from the training classification dataset. Although the second corpus is much smaller, using data from the direct target might provide better target domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>FinBERT for text classification. Sentiment classification is conducted by adding a dense layer after the last hidden state of the [CLS] token. This is the recommended practice for using BERT for any classification task <ref type="bibr" target="#b2">[3]</ref>. Then, the classifier network is trained on the labeled sentiment dataset. An overview of all the steps involved in the procedure is presented on figure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">FinBERT for regression.</head><p>While the focus of this paper is classification, we also implement regression with almost the same architecture on a different dataset with continuous targets. The only difference is that the loss function being used is mean squared error instead of the cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.4</head><p>Training strategies to prevent catastrophic forgetting. As it was pointed out by Howard and Ruder (2018) <ref type="bibr" target="#b4">[5]</ref>, catastrophic forgetting is a significant danger with this fine-tuning approach. Because the fine-tuning procedure can quickly cause model to "forget" the information from language modeling task as it tries to adapt to the new task. In order to deal with this phenomenon, we apply three techniques as it was proposed by <ref type="bibr" target="#b4">Howard and Ruder (2018)</ref>: slanted triangular learning rates, discriminative fine-tuning and gradual unfreezing.</p><p>Slanted triangular learning rate applies a learning rate schedule in the shape of a slanted triangular, that is, learning rate first linearly increases up to some point and after that point linearly decreases.</p><p>Discriminative fine-tuning is using lower learning rates for lower layers on the network. Assume our learning rate at layer l is α. Then for discrimination rate of θ we calculate the learning rate for layer l − 1 as α l −1 = θα l . The assumption behind this method is that the lower layers represent the deep-level language information, while the upper ones include information for actual classification task. Therefore we fine-tune them differently.</p><p>With gradual freezing, we start training with all layers but the classifier layer as frozen. During training we gradually unfreeze all of the layers starting from the highest one, so that the lower level features become the least fine-tuned ones. Hence, during the initial stages of training it is prevented for model to "forget" low-level language information that it learned from pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP 4.1 Research Questions</head><p>We aim to answer the following research questions:</p><p>(RQ1) What is the performance of FinBERT in short sentence classification compared with the other transfer learning methods like ELMo and ULMFit? In order to further pre-train BERT, we use a financial corpus we call TRC2-financial. It is a subset of Reuters' TRC2 4 , which consists of 1.8M news articles that were published by Reuters between 2008 and 2010. We filter for some financial keywords in order to make corpus more relevant and in limits with the compute power available. The resulting corpus, TRC2-financial, includes 46,143 documents with more than 29M words and nearly 400K sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Financial</head><p>PhraseBank. The main sentiment analysis dataset used in this paper is Financial PhraseBank 5 from Malo et al. 2014 <ref type="bibr" target="#b16">[17]</ref>. Financial Phrasebank consists of 4845 english sentences selected randomly from financial news found on LexisNexis database. These sentences then were annotated by 16 people with background in finance and business. The annotators were asked to give labels according to how they think the information in the sentence might affect the mentioned company stock price. The dataset also includes information regarding the agreement levels on sentences among annotators. The distribution of agreement levels and sentiment labels can be seen on   <ref type="bibr" target="#b14">[15]</ref> is a dataset that was created for WWW '18 conference financial opinion mining and question answering challenge <ref type="bibr" target="#b5">6</ref> . We use the data for Task 1, which includes 1,174 financial news headlines and tweets with their corresponding sentiment score. Unlike Financial Phrasebank, the targets for this datasets are continuous ranging between [−1, 1] with 1 being the most positive. Each example also has information regarding which financial entity is targeted in the sentence. We do 10-fold cross validation for evaluation of the model for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Methods</head><p>For contrastive experiments, we consider baselines with three different methods: LSTM classifier with GLoVe embeddings, LSTM classifier with ELMo embeddings and ULMFit classifier. It should be noted that these baseline methods are not experimented with as thoroughly as we did with BERT. Therefore the results should not be interpreted as definitive conclusions of one method being better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">LSTM classifiers.</head><p>We implement two classifiers using bidirectional LSTM models. In both of them, a hidden size of 128 is used, with the last hidden state size being 256 due to bidirectionality. A fully connected feed-forward layer maps the last hidden state to a vector of three, representing likelihood of three labels. The difference between two models is that one uses GLoVe embeddings, while the other uses ELMo embeddings. A dropout probability of 0.3 and a learning rate of 3e-5 is used in both models. We train them until there is no improvement in validation loss for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">ULMFit.</head><p>As it was explained in section 3.1.3, classification with ULMFit consists of three steps. The first step of pre-training a language model is already done and the pre-trained weights are released by <ref type="bibr" target="#b4">Howard and Ruder (2018)</ref>. We first further pre-train AWD-LSTM language model on TRC2-financial corpus for 3 epochs. After that, we fine-tune the model for classification on Financial <ref type="bibr" target="#b5">6</ref> Data can be found here: https://sites.google.com/view/fiqa/home PhraseBank dataset, by adding a fully-connected layer to the output of pre-trained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>For evaluation of classification models, we use three metrics: Accuracy, cross entropy loss and macro F1 average. We weight cross entropy loss with square root of inverse frequency rate. For example if a label constitutes 25% of the all examples, we weight the loss attributed to that label by 2. Macro F1 average calculates F1 scores for each of the classes and then takes the average of them. Since our data, Financial PhraseBank suffers from label imbalance (almost 60% of all sentences are neutral), this gives another good measure of the classification performance. For evaluation of regression model, we report mean squared error and R 2 , as these are both standard and also reported by the state-of-the-art papers for FiQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Details</head><p>For our implementation BERT, we use a dropout probability of p = 0.1, warm-up proportion of 0.2, maximum sequence length of 64 tokens, a learning rate of 2e − 5 and a mini-batch size of 64. We train the model for 6 epochs, evaluate on the validation set and choose the best one. For discriminative fine-tuning we set the discrimination rate as 0.85. We start training with only the classification layer unfrozen, after each third of a training epoch we unfreeze the next layer. An Amazon p2.xlarge EC2 instance with one NVIDIA K80 GPU, 4 vCPUs and 64 GiB of host memory is used to train the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS (RQ1 &amp; RQ2)</head><p>The results of FinBERT, the baseline methods and state-of-the-art on Financial PhraseBank dataset classification task can be seen on table 2. We present the result on both the whole dataset and subset with 100% annotator agreement. Bold face indicates best result in the corresponding metric. LPS <ref type="bibr" target="#b16">[17]</ref>, HSC <ref type="bibr" target="#b7">[8]</ref> and FinSSLX <ref type="bibr" target="#b14">[15]</ref> results are taken from their respective papers. For LPS and HSC, overall accuracy is not reported on the papers. We calculated them using recall scores reported for different classes. For the models implemented by us, we report 10-fold cross validation results.</p><p>For all of the measured metrics, FinBERT performs clearly the best among both the methods we implemented ourselves (LSTM and ULMFit) and the models reported by other papers (LPS <ref type="bibr" target="#b16">[17]</ref>, HSC <ref type="bibr" target="#b7">[8]</ref>, FinSSLX <ref type="bibr" target="#b13">[14]</ref>). LSTM classifier with no language model information performs the worst. In terms of accuracy, it is close to LPS and HSC, (even better than LPS for examples with full agreement), however it produces a low F1-score. That is due to it performing much better in neutral class. LSTM classifier with ELMo embeddings improves upon LSTM with static embeddings in all of the measured metrics. It still suffers from low average F1-score due to poor performance in less represented labels. But it's performance is comparable with LPS and HSC, besting them in accuracy. So contextualized word embeddings produce close performance to machine learning based methods for dataset of this size.</p><p>ULMFit significantly improves on all of the metrics and it doesn't suffer from model performing much better in some classes than the others. It also handily beats the machine learning based models LPS and HSC. This shows the effectiveness of language model pretraining. AWD-LSTM is a very large model and it would be expected to suffer from over-fitting with this small of a dataset. But due to language model pre-training and effective training strategies, it is able to overcome small data problem. ULMFit also outperforms FinSSLX, which has a text simplification step as well as pre-training of word embeddings on a large financial corpus with sentiment labels.</p><p>FinBERT outperforms ULMFit, and consequently all of the other methods in all metrics. In order to measure the performance of the models on different sizes of labeled training datasets, we ran LSTM classifiers, ULMFit and FinBERT on 5 different configurations. The result can be seen on figure 2, where the cross entropy losses on test set for each model are drawn. 100 training examples is too low for all of the models. However, once the training size becomes 250, ULMFit and FinBERT starts to successfully differentiate between labels, with an accuracy as high as 80% for FinBERT. All of the methods consistently get better with more data, but ULMFit and FinBERT does better with 250 examples than LSTM classifiers do with the whole dataset. This shows the effectiveness of language model pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Test loss different training set sizes</head><p>The results for FiQA sentiment dataset, are presented on table 3. Our model outperforms state-of-the-art models for both MSE and R 2 . It should be noted that the test set these two papers <ref type="bibr" target="#b30">[31]</ref>  <ref type="bibr" target="#b23">[24]</ref> use is the official FiQA Task 1 test set. Since we don't have access to that we report the results on 10-Fold cross validation. There is no indication on <ref type="bibr" target="#b14">[15]</ref> that the train and test sets they publish come from different distributions and our model can be interpreted to be at disadvantage since we need to set aside a subset of training set as test set, while state-of-the-art papers can use the complete training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL ANALYSIS 6.1 Effects of further pre-training (RQ3)</head><p>We first measure the effect of further pre-training on the performance of the classifier. We compare three models: 1) No further pre-training (denoted by Vanilla BERT), 2) Further pre-training on classification training set (denoted by FinBERT-task), 3) Further pre-training on domain corpus, TRC2-financial (denoted by FinBERT-domain). Models are evaluated with loss, accuracy and  <ref type="bibr" target="#b23">[24]</ref> report results on the official test set. Since we don't have access to that set our MSE, and R 2 are calculated with 10-Fold cross validation. The classifier that were further pre-trained on financial domain corpus performs best among the three, though the difference is not very high. There might be four reasons behind this result: 1) The corpus might have a different distribution than the task set, 2) BERT classifiers might not improve significantly with further pre-training, 3) Short sentence classification might not benefit significantly from further pre-training, 4) Performance is already so good, that there is not much room for improvement. We think that the last explanation is the likeliest, because for the subset of Financial Phrasebank that all of the annotators agree on the result, accuracy of Vanilla BERT is already 0.96. The performance on the other agreement levels should be lower, as even the humans can't agree fully on them. More experiments with another financial labeled dataset is necessary to conclude that effect of further pre-training on domain corpus is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Catastrophic forgetting (RQ4)</head><p>For measuring the performance of the techniques against catastrophic forgetting, we try four different settings: No adjustment (NA), only with slanted triangular learning rate (STL), slanted triangular learning rate and gradual unfreezing (STL+GU) and the techniques in the previous one, together with discriminative finetuning. We report the performance of these four settings with loss on test function and trajectory of validation loss over training epochs. The results can be seen on <ref type="table" target="#tab_3">table 5 and figure 3</ref>.</p><p>Applying all three of the strategies produce the best performance in terms of test loss and accuracy. Gradual unfreezing and discriminative fine-tuning have the same reasoning behind them: higher level features should be fine-tuned more than the lower level  ones, since information learned from language modeling are mostly present in the lower levels. We see from table 5 that using only discriminative fine-tuning with slanted triangular learning rates performs worse than using the slanted triangular learning rates alone. This shows that gradual unfreezing is the most important technique for our case. One way that catastrophic forgetting can show itself is the sudden increase in validation loss after several epochs. As model is trained, it quickly starts to overfit when no measure is taken accordingly. As it can be seen on the figure 3, that is the case when none of the aforementioned techniques are applied. The model achieves the best performance on validation set after the first epoch and then starts to overfit. While with all three techniques applied, model is much more stable. The other combinations lie between these two cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Choosing the best layer for classification (RQ5)</head><p>BERT has 12 Transformer encoder layers. It is not necessarily a given that the last layer captures the most relevant information regarding classification task during language model training. For this experiment, we investigate which layer out of 12 Transformer encoder layers give the best result for classification. We put the classification layer after the CLS] tokens of respective representations. We also try taking the average of all layers. As shown in table 6the last layer contributes the most to the model performance in terms of all the metrics measured. This might be indicative of two factors: 1) When the higher layers are used the model that is being trained is larger, hence possibly more powerful, 2) The lower layers capture deeper semantic information, hence they struggle to fine-tune that information for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Training only a subset of the layers (RQ6)</head><p>BERT is a very large model. Even on small datasets, fine-tuning the whole model requires significant time and computing power. Therefore if a slightly lower performance can be achieved with fine-tuning only a subset of all parameters, it might be preferable in some contexts. Especially if training set is very large, this change might make BERT more convenient to use. Here we experiment with fine-tuning only the last k many encoder layers.</p><p>The results are presented on table 7. Fine-tuning only the classification layer does not achieve close performance to fine-tuning other layers. However fine-tuning only the last layer handily outperforms the state-of-the-art machine learning methods like HSC. After Layer-9, the performance becomes virtually the same, only to be outperformed by fine-tuning the whole model. This result shows that in order to utilize BERT, an expensive training of the whole model is not mandatory. A fair trade-off can be made for much less training time with a small decrease in model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Where does the model fail?</head><p>With 97% accuracy on the subset of Financial PhraseBank with 100% annotator agreement, we think it might be an interesting exercise to examine cases where the model failed to predict the true label. Therefore in this section we will present several examples where model makes the wrong prediction. Also in Malo et  <ref type="bibr" target="#b16">[17]</ref>, it is indicated that most of the inter-annotator disagreements are between positive and neutral labels (agreement for separating positive-negative, negative-neutral and positive-neutral are 98.7%, 94.2% and 75.2% respectively). Authors attribute that the difficulty of distinguishing "commonly used company glitter and actual positive statements". We will present the confusion matrix in order to observe whether this is the case for FinBERT as well. The situation of coated magazine printing paper will continue to be weak .</p><p>True value: Negative Predicted: Neutral</p><p>The first example is actually the most common type of failure. The model fails to do the math in which figure is higher, and in the absence of words indicative of direction like "increased", might make the prediction of neutral. However, there are many similar cases where it does make the true prediction too. Examples 2 and 3 are different versions of the same type of failure. The model fails to distinguish a neutral statement about a given situation from a statement that indicated polarity about the company. In the third example, information about the company's business would probably help.</p><p>The confusion matrix is presented on <ref type="figure" target="#fig_3">figure 4</ref>. 73% of the failures happen between labels positive and negative, while same number is 5% for negative and positive. That is consistent with both the inter-annotator agreement numbers and common sense. It is easier </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we implemented BERT for the financial domain by further pre-training it on a financial corpus and fine-tuning it for sentiment analysis (FinBERT). This work is the first application of BERT for finance to the best of our knowledge and one of the few that experimented with further pre-training on a domain-specific corpus. On both of the datasets we used, we achieved state-of-theart results by a significant margin. For the classification task, we increased the state-of-the art by 15% in accuracy.</p><p>In addition to BERT, we also implemented other pre-training language models like ELMo and ULMFit for comparison purposes. ULMFit, further pre-trained on a financial corpus, beat the previous state-of-the art for the classification task, only to a smaller degree than BERT. These results show the effectiveness of pre-trained language models for a down-stream task such as sentiment analysis especially with a small labeled dataset. The complete dataset included more than 3000 examples, but FinBERT was able to surpass the previous state-of-the art even with a training set as small as 500 examples. This is an important result, since deep learning techniques for NLP have been traditionally labeled as too "data-hungry", which is apparently no longer the case.</p><p>We conducted extensive experiments with BERT, investigating the effects of further pre-training and several training strategies. We couldn't conclude that further pre-training on a domain-specific corpus was significantly better than not doing so for our case. Our theory is that BERT already performs good enough with our dataset that there is not much room for improvement that further pretraining can provide. We also found that learning rate regimes that fine-tune the higher layers more aggressively than the lower ones perform better and are more effective in preventing catastrophic forgetting. Another conclusion from our experiments was that, comparable performance can be achieved with much less training time by fine-tuning only the last 2 layers of BERT.</p><p>Financial sentiment analysis is not a goal on its own, it is as useful as it can support financial decisions. One way that our work might be extended, could be using FinBERT directly with stock market return data (both in terms of directionality and volatility) on financial news. FinBERT is good enough for extracting explicit sentiments, but modeling implicit information that is not necessarily apparent even to those who are writing the text should be a challenging task. Another possible extension can be using FinBERT for other natural language processing tasks such as named entity recognition or question answering in financial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>I would like to show my gratitude to Pengjie Ren and Zulkuf Genc for their excellent supervision. They provided me with both independence in setting my own course for the research and valuable suggestions when I need them. I would also like to thank Naspers AI team, for entrusting me with this project and always encouraging me to share my work. I am grateful to NIST, for sharing Reuters TRC-2 corpus with me and to Malo et al. for making the excellent Financial PhraseBank publicly available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of pre-training, further pre-training and classification fine-tuning 4.2.3 FiQA Sentiment. FiQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Validation loss trajectories with different training strategies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Example 1 : 2 :</head><label>12</label><figDesc>Pre-tax loss totaled euro 0.3 million , compared to a loss of euro 2.2 million in the first quarter of 2005 . True value: Positive Predicted: Negative Example This implementation is very important to the operator , since it is about to launch its Fixed to Mobile convergence service in Brazil True value: Neutral Predicted: Positive Example 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrixto differentiate between positive and negative. But it might be more challenging to decide whether a statement indicates a positive outlook or merely an objective observation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Distribtution of sentiment labels and agreement levels in Financial PhraseBank</figDesc><table><row><cell cols="5">Agreement level Positive Negative Neutral Count</cell></row><row><cell>100%</cell><cell>%25.2</cell><cell>%13.4</cell><cell>%61.4</cell><cell>2262</cell></row><row><cell>75% -99%</cell><cell>%26.6</cell><cell>%9.8</cell><cell>%63.6</cell><cell>1191</cell></row><row><cell>66% -74%</cell><cell>%36.7</cell><cell>%12.3</cell><cell>%50.9</cell><cell>765</cell></row><row><cell>50% -65%</cell><cell>%31.1</cell><cell>%14.4</cell><cell>%54.5</cell><cell>627</cell></row><row><cell>All</cell><cell>%28.1</cell><cell>%12.4</cell><cell>%59.4</cell><cell>4845</cell></row><row><cell cols="5">(RQ2) How does FinBERT compare to the state-of-the-art in finan-</cell></row><row><cell cols="5">cial sentiment analysis with targets discrete or continuous?</cell></row><row><cell cols="5">(RQ3) How does futher pre-training BERT on financial domain, or</cell></row><row><cell cols="5">target corpus, affect the classification performance?</cell></row><row><cell cols="5">(RQ4) What are the effects of training strategies like slanted trian-</cell></row><row><cell cols="5">gular learning rates, discriminative fine-tuning and gradual</cell></row><row><cell cols="5">unfreezing on classification performance? Do they prevent</cell></row><row><cell cols="2">catastrophic forgetting?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(RQ5) Which encoder layer performs best (or worse) for sentence</cell></row><row><cell>classification?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(RQ6) How much fine-tuning is enough? That is, after pre-training, how many layers should be fine-tuned to achieve comparable performance to fine-tuning the whole model? 4.2 Datasets4.2.1 TRC2-financial.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table 1 .</head><label>1</label><figDesc>We set aside 20% of all sentences as test and 20% of the remaining as validation set. In the end, our train set includes 3101 examples. For some of the experiments, we also make use of 10-fold cross validation.</figDesc><table><row><cell cols="3">[is next sentence] prediction</cell><cell cols="2">Masked LM prediction</cell><cell></cell><cell cols="2">[is next sentence] prediction</cell><cell cols="2">Masked LM prediction</cell><cell></cell><cell>Sentiment prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dense</cell><cell></cell><cell></cell><cell>Dense</cell><cell></cell><cell>Dense</cell><cell></cell><cell></cell><cell>Dense</cell><cell></cell><cell>Dense</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder 12</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>Token k</cell><cell>[SEP]</cell></row><row><cell>Encoder 2</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>Token k</cell><cell>[SEP]</cell></row><row><cell>Encoder 1</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>Token k</cell><cell>[SEP]</cell></row><row><cell>Embeddings</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>Token k</cell><cell>[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BookCorpus +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reuters TRC2-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Financial</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wikipedia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>financial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Phrasebank</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Language model on general corpus</cell><cell></cell><cell></cell><cell cols="3">Language model on financial corpus</cell><cell></cell><cell cols="4">Classification model on financial sentiment dataset</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental Results on the Financial PhraseBank dataset</figDesc><table><row><cell></cell><cell></cell><cell>All data</cell><cell></cell><cell cols="3">Data with 100% agreement</cell></row><row><cell>Model</cell><cell cols="6">Loss Accuracy F1 Score Loss Accuracy F1 Score</cell></row><row><cell>LSTM</cell><cell>0.81</cell><cell>0.71</cell><cell>0.64</cell><cell>0.57</cell><cell>0.81</cell><cell>0.74</cell></row><row><cell cols="2">LSTM with ELMo 0.72</cell><cell>0.75</cell><cell>0.7</cell><cell>0.50</cell><cell>0.84</cell><cell>0.77</cell></row><row><cell>ULMFit</cell><cell>0.41</cell><cell>0.83</cell><cell>0.79</cell><cell>0.20</cell><cell>0.93</cell><cell>0.91</cell></row><row><cell>LPS</cell><cell>-</cell><cell>0.71</cell><cell>0.71</cell><cell>-</cell><cell>0.79</cell><cell>0.80</cell></row><row><cell>HSC</cell><cell>-</cell><cell>0.71</cell><cell>0.76</cell><cell>-</cell><cell>0.83</cell><cell>0.86</cell></row><row><cell>FinSSLX</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.91</cell><cell>0.88</cell></row><row><cell>FinBERT</cell><cell>0.37</cell><cell>0.86</cell><cell>0.84</cell><cell>0.13</cell><cell>0.97</cell><cell>0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental Results on FiQA Sentiment Dataset</figDesc><table><row><cell>Model</cell><cell>MSE</cell><cell>R 2</cell></row><row><cell>Yang et. al. (2018)</cell><cell cols="2">0.08 0.40</cell></row><row><cell>Piao and Breslin (2018)</cell><cell cols="2">0.09 0.41</cell></row><row><cell>FinBERT</cell><cell cols="2">0.07 0.55</cell></row><row><cell cols="3">Bold face indicated best result in corresponding metric.</cell></row><row><cell cols="3">Yang et. al. (2018) [31] and Piao and Breslin (2018)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Performance with different pre-</cell></row><row><cell>training strategies</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Loss Accuracy F1 Score</cell></row><row><cell>Vanilla BERT</cell><cell>0.38</cell><cell>0.85</cell><cell>0.84</cell></row><row><cell>FinBERT-task</cell><cell>0.39</cell><cell>0.86</cell><cell>0.85</cell></row><row><cell cols="2">FinBERT-domain 0.37</cell><cell>0.86</cell><cell>0.84</cell></row></table><note>Bold face indicates best result in the corresponding met- ric. Results are reported on 10-fold cross validation.macro average F1 scores on the test dataset. The results can be seen on table 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Performance with different fine-</cell></row><row><cell cols="2">tuning strategies</cell><cell></cell><cell></cell></row><row><cell>Strategy</cell><cell cols="3">Loss Accuracy F1 Score</cell></row><row><cell>None</cell><cell>0.48</cell><cell>0.83</cell><cell>0.83</cell></row><row><cell>STL</cell><cell>0.40</cell><cell>0.81</cell><cell>0.82</cell></row><row><cell>STL + GU</cell><cell>0.40</cell><cell>0.86</cell><cell>0.86</cell></row><row><cell cols="2">STL + DFT 0.42</cell><cell>0.79</cell><cell>0.79</cell></row><row><cell>All three</cell><cell>0.37</cell><cell>0.86</cell><cell>0.84</cell></row><row><cell cols="4">Bold face indicates best result in the correspond-</cell></row><row><cell cols="4">ing metric. Results are reported on 10-fold cross</cell></row><row><cell cols="4">validation. STL: slanted triangular learning rates,</cell></row><row><cell cols="4">GU: gradual unfreezing, DFT: discriminative fine-</cell></row><row><cell>tuning.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance on different encoder layers used for classification</figDesc><table><row><cell cols="4">Layer for classification Loss Accuracy F1 Score</cell></row><row><cell>Layer-1</cell><cell>0.65</cell><cell>0.76</cell><cell>0.77</cell></row><row><cell>Layer-2</cell><cell>0.54</cell><cell>0.78</cell><cell>0.78</cell></row><row><cell>Layer-3</cell><cell>0.52</cell><cell>0.76</cell><cell>0.77</cell></row><row><cell>Layer-4</cell><cell>0.48</cell><cell>0.80</cell><cell>0.77</cell></row><row><cell>Layer-5</cell><cell>0.52</cell><cell>0.80</cell><cell>0.80</cell></row><row><cell>Layer-6</cell><cell>0.45</cell><cell>0.82</cell><cell>0.82</cell></row><row><cell>Layer-7</cell><cell>0.43</cell><cell>0.82</cell><cell>0.83</cell></row><row><cell>Layer-8</cell><cell>0.44</cell><cell>0.83</cell><cell>0.81</cell></row><row><cell>Layer-9</cell><cell>0.41</cell><cell>0.84</cell><cell>0.82</cell></row><row><cell>Layer-10</cell><cell>0.42</cell><cell>0.83</cell><cell>0.82</cell></row><row><cell>Layer-11</cell><cell>0.38</cell><cell>0.84</cell><cell>0.83</cell></row><row><cell>Layer-12</cell><cell>0.37</cell><cell>0.86</cell><cell>0.84</cell></row><row><cell>All layers -mean</cell><cell>0.41</cell><cell>0.84</cell><cell>0.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance on starting training from different layersFirst layer unfreezed Loss Accuracy Training time</figDesc><table><row><cell>Embeddings layer</cell><cell>0.37</cell><cell>0.86</cell><cell>332s</cell></row><row><cell>Layer-1</cell><cell>0.39</cell><cell>0.83</cell><cell>302s</cell></row><row><cell>Layer-2</cell><cell>0.39</cell><cell>0.83</cell><cell>291s</cell></row><row><cell>Layer-3</cell><cell>0.38</cell><cell>0.83</cell><cell>272s</cell></row><row><cell>Layer-4</cell><cell>0.38</cell><cell>0.82</cell><cell>250s</cell></row><row><cell>Layer-5</cell><cell>0.40</cell><cell>0.83</cell><cell>240s</cell></row><row><cell>Layer-6</cell><cell>0.40</cell><cell>0.81</cell><cell>220s</cell></row><row><cell>Layer-7</cell><cell>0.39</cell><cell>0.82</cell><cell>205s</cell></row><row><cell>Layer-8</cell><cell>0.39</cell><cell>0.84</cell><cell>188s</cell></row><row><cell>Layer-9</cell><cell>0.39</cell><cell>0.84</cell><cell>172s</cell></row><row><cell>Layer-10</cell><cell>0.41</cell><cell>0.84</cell><cell>158s</cell></row><row><cell>Layer-11</cell><cell>0.45</cell><cell>0.82</cell><cell>144s</cell></row><row><cell>Layer-12</cell><cell>0.47</cell><cell>0.81</cell><cell>133s</cell></row><row><cell>Classification layer</cell><cell>1.04</cell><cell>0.52</cell><cell>119s</cell></row><row><cell>al. (2014 )</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">.2.1 Further pre-training.<ref type="bibr" target="#b4">Howard and Ruder (2018)</ref> <ref type="bibr" target="#b4">[5]</ref> shows that futher pre-training a language model on a target domain corpus improves the eventual classification performance. For BERT, there is not decisive research showing that would be the case as well.<ref type="bibr" target="#b2">3</ref> The pre-trained weights are made public by creators of BERT. The code and weights can be found here: https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The corpus can be obtained for research purposes by applying here: https://trec.nist.gov/data/reuters/reuters.html<ref type="bibr" target="#b4">5</ref> The dataset can be found here: https://www.researchgate.net/publication/251231364 _FinancialPhraseBank-v10</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine Learning Approach for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basant</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namita</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-25343-5_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-25343-5_3" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="21" to="45" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing deep learning sentiment analysis with ensemble techniques in social applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Araque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Corcuera-Platas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Fernando</forename><surname>Sánchez-Rada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Iglesias</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2017.02.002</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.02.002" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="236" to="246" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600v2</idno>
		<idno>arXiv:1810.04805</idno>
		<ptr target="https://doi.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Textual analysis and machine leaning: Crack unstructured data in finance and accounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.JFDS.2017.02.001</idno>
		<ptr target="https://doi.org/10.1016/J.JFDS.2017.02.001" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance and Data Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="170" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Universal Language Model Finetuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<ptr target="http://arxiv.org/abs/1801.06146" />
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Practical Text Classification With Large Pre-Trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Yakovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01207</idno>
		<ptr target="http://arxiv.org/abs/1812.01207" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Decision support from financial disclosures with deep neural networks and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Feuerriegel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2017.10.001</idno>
		<idno type="arXiv">arXiv:1710.03954</idno>
		<ptr target="https://doi.org/10.1016/j.dss.2017.10.001" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="38" to="48" />
		</imprint>
	</monogr>
	<note>Decision Support Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentiment analysis of financial news articles using performance indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-017-1134-1</idno>
		<ptr target="https://doi.org/10.1007/s10115-017-1134-1" />
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="373" to="394" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">News impact on stock price return via sentiment analysis. Knowledge-Based Systems 69</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2014.04.022</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2014.04.022" />
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.2200/s00416ed1v01y201204hlt016</idno>
		<ptr target="https://doi.org/10.2200/s00416ed1v01y201204hlt016" />
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Loughran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1540-6261.2010.01625.x</idno>
		<ptr target="https://doi.org/10.1111/j.1540-6261.2010.01625.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="35" to="65" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Textual Analysis in Accounting and Finance: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Loughran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1111/1475-679X.12123</idno>
		<idno>1187-1230</idno>
		<ptr target="https://doi.org/10.1111/1475-679X.12123" />
	</analytic>
	<monogr>
		<title level="j">Journal of Accounting Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sentence-Level Sentiment Analysis of Financial News Using Distributed Text Representations and Multi-Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pröllochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00400</idno>
		<ptr target="http://arxiv.org/abs/1901.00400" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FinSSLx: A Sentiment Analysis Model for the Financial Domain Using Text Simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrï£¡</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSC.2018.00065</idno>
		<ptr target="https://doi.org/10.1109/ICSC.2018.00065" />
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mc-Dermott</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558</idno>
		<ptr target="https://doi.org/10.1145/3184558" />
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference</title>
		<meeting><address><addrLine>Lyon , France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Efficient Market Hypothesis and Its Critics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malkiel</surname></persName>
		</author>
		<idno type="DOI">10.1257/089533003321164958</idno>
		<ptr target="https://doi.org/10.1257/089533003321164958" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="59" to="82" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Good debt or bad debt: Detecting semantic orientations in economic texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Malo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyrki</forename><surname>Wallenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyry</forename><surname>Takala</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23062</idno>
		<idno type="arXiv">arXiv:arXiv:1307.5336v2</idno>
		<ptr target="https://doi.org/10.1002/asi.23062" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="782" to="796" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<idno>arXiv:cs.AI/1801.00631</idno>
		<title level="m">Deep Learning: A Critical Appraisal. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delta TFIDF: An Improved Feature Space for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/icwsm/icwsm2009.html#MartineauF09" />
	</analytic>
	<monogr>
		<title level="m">The AAAI Press</title>
		<editor>ICWSM, Eytan Adar, Matthew Hurst, Tim Finin, Natalie S. Glance, Nicolas Nicolov, and Belle L. Tseng</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00107</idno>
		<ptr target="http://arxiv.org/abs/1708.00107" />
		<title level="m">Learned in Translation: Contextualized Word Vectors. Nips</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Regularizing and Optimizing LSTM Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<ptr target="http://arxiv.org/abs/1708.02182" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1202" />
	</analytic>
	<monogr>
		<title level="j">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Financial Aspect and Sentiment Predictions with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John G Breslin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3191829</idno>
		<ptr target="https://doi.org/10.1145/3184558.3191829" />
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Twitter Sentiment Analysis with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767830</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767830" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;15</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Big Data: Deep Learning for financial sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Sohangir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Pomeranets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-017-0111-6</idno>
		<ptr target="https://doi.org/10.1186/s40537-017-0111-6" />
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How to Fine-Tune BERT for Text Classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05583</idno>
		<ptr target="https://arxiv.org/pdf/1905.05583v1.pdfhttp://arxiv.org/abs/1905.05583" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification of sentiment reviews using n-gram machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abinash</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santanu</forename><surname>Kumar Rath</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.03.028</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.03.028" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="117" to="126" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using appraisal groups for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navendu</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<idno type="DOI">10.1145/1099554.1099714</idno>
		<ptr target="https://doi.org/10.1145/1099554.1099714" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Information and knowledge management -CIKM &apos;05</title>
		<meeting>the 14th ACM international conference on Information and knowledge management -CIKM &apos;05</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Financial Aspect-Based Sentiment Analysis using Deep Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Makutonin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07931</idno>
		<ptr target="https://arxiv.org/pdf/1808.07931v1.pdfhttp://arxiv.org/abs/1808.07931" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1253</idno>
		<ptr target="https://doi.org/10.1002/widm.1253" />
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06724</idno>
		<ptr target="http://arxiv.org/abs/1506.06724" />
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
