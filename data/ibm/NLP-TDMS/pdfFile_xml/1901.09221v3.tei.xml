<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Image Deraining Networks: A Better and Simpler Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Image Deraining Networks: A Better and Simpler Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Along with the deraining performance improvement of deep networks, their structures and learning become more and more complicated and diverse, making it difficult to analyze the contribution of various network modules when developing new deraining networks. To handle this issue, this paper provides a better and simpler baseline deraining network by considering network architecture, input and output, and loss functions. Specifically, by repeatedly unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take advantage of recursive computation. A recurrent layer is further introduced to exploit the dependencies of deep features across stages, forming our progressive recurrent network (PReNet). Furthermore, intra-stage recursive computation of ResNet can be adopted in PRN and PReNet to notably reduce network parameters with unsubstantial degradation in deraining performance. For network input and output, we take both stage-wise result and original rainy image as input to each ResNet and finally output the prediction of residual image. As for loss functions, single MSE or negative SSIM losses are sufficient to train PRN and PReNet. Experiments show that PRN and PReNet perform favorably on both synthetic and real rainy images. Considering its simplicity, efficiency and effectiveness, our models are expected to serve as a suitable baseline in future deraining research. The source codes are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Rain is a common weather condition, and has severe adverse effect on not only human visual perception but also the performance of various high level vision tasks such as image classification, object detection, and video surveillance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">14]</ref>. Single image deraining aims at restoring clean background image from a rainy image, and has drawn con-Rainy image RESCAN <ref type="bibr" target="#b20">[20]</ref> t = 1 t = 2 t = 4 t = 6 <ref type="figure">Figure 1</ref>. Deraining results by RESCAN <ref type="bibr" target="#b20">[20]</ref> and PReNet (T = 6) at stage t = 1, 2, 4, 6, respectively. siderable recent research attention. For example, several traditional optimization based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> have been suggested for modeling and separating rain streaks from background clean image. However, due to the complex composition of rain and background layers, image deraining remains a challenging ill-posed problem. Driven by the unprecedented success of deep learning in low level vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">34]</ref>, recent years have also witnessed the rapid progress of deep convolutional neural network (CNN) in image deraining. In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr">Fu et al.</ref> show that it is difficult to train a CNN to directly predict background (a) PRN and the illustration of PRN with T stages recursion (b) PReNet and the illustration of PReNet with T stages recursion <ref type="figure">Figure 2</ref>. The architectures of progressive networks, where fin is a convolution layer with ReLU, fres is ResBlocks, fout is a convolution layer, frecurrent is a convolutional LSTM and c is a concat layer. fres can be implemented as conventional ResBlocks or recursive ResBlocks as shown in <ref type="figure" target="#fig_0">Fig. 3.</ref> image from rainy image, and utilize a 3-layer CNN to remove rain streaks from high-pass detail layer instead of the input image. Subsequently, other formulations are also introduced, such as residual learning for predicting rain steak layer <ref type="bibr" target="#b20">[20]</ref>, joint detection and removal of rain streaks <ref type="bibr" target="#b30">[30]</ref>, and joint rain density estimation and deraining <ref type="bibr" target="#b32">[32]</ref>.</p><p>On the other hand, many modules are suggested to constitute different deraining networks, including residual blocks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, dilated convolution <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>, dense blocks <ref type="bibr" target="#b32">[32]</ref>, squeeze-and-excitation <ref type="bibr" target="#b20">[20]</ref>, and recurrent layers <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b25">25]</ref>. Multi-stream <ref type="bibr" target="#b32">[32]</ref> and multi-stage <ref type="bibr" target="#b20">[20]</ref> networks are also deployed to capture multi-scale characteristics and to remove heavy rain. Moreover, several models are designed to improve computational efficiency by utilizing lightweight networks in a cascaded scheme <ref type="bibr" target="#b3">[4]</ref> or a Laplacian pyramid framework <ref type="bibr" target="#b6">[7]</ref>, but at the cost of obvious degradation in deraining performance. To sum up, albeit the progress of deraining performance, the structures of deep networks become more and more complicated and diverse. As a result, it is difficult to analyze the contribution of various modules and their combinations, and to develop new models by introducing modules to existing deeper and complex deraining networks.</p><p>In this paper, we aim to present a new baseline network for single image deraining to demonstrate that: (i) by combining only a few modules, a better and simpler baseline network can be constructed and achieve noteworthy performance gains over state-of-the-art deeper and complex deraining networks, (ii) unlike <ref type="bibr" target="#b4">[5]</ref>, the improvement of de-raining networks may ease the difficulty of training CNNs to directly recover clean image from rainy image. Moreover, the simplicity of baseline network makes it easier to develop new deraining models by introducing other network modules or modifying the existing ones.</p><p>To this end, we consider the network architecture, input and output, and loss functions to form a better and simpler baseline networks. In terms of network architecture, we begin with a basic shallow residual network (ResNet) with five residual blocks (ResBlocks). Then, progressive ResNet (PRN) is introduced by recursively unfolding the ResNet into multiple stages without the increase of model parameters (see <ref type="figure">Fig. 2(a)</ref>). Moreover, a recurrent layer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">27]</ref> is introduced to exploit the dependencies of deep features across recursive stages to form the PReNet in <ref type="figure">Fig. 2(b)</ref>. From <ref type="figure">Fig. 1</ref>, a 6-stage PReNet can remove most rain streaks at the first stage, and then remaining rain streaks can be progressively removed, leading to promising deraining quality at the final stage. Furthermore, PRN r and PReNet r are presented by adopting intra-stage recursive unfolding of only one ResBlock, which reduces network parameters only at the cost of unsubstantial performance degradation.</p><p>Using PRN and PReNet, we further investigate the effect of network input/output and loss function. In terms of network input, we take both stage-wise result and original rainy image as input to each ResNet, and empirically find that the introduction of original image does benefit deraining performance. In terms of network output, we adopt the residual learning formulation by predicting rain streak layer, and find that it is also feasible to directly learn a PRN or PReNet model for predicting clean background from rainy image. Finally, instead of hybrid losses with careful hyperparameters tuning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, a single negative SSIM <ref type="bibr" target="#b29">[29]</ref> or MSE loss can readily train PRN and PReNet with favorable deraining performance.</p><p>Comprehensive experiments have been conducted to evaluate our baseline networks PRN and PReNet. On four synthetic datasets, our PReNet and PRN are computationally very efficient, and achieve much better quantitative and qualitative deraining results in comparison with the stateof-the-art methods. In particular, on heavy rainy dataset Rain100H <ref type="bibr" target="#b30">[30]</ref>, the performance gains by our PRN and PReNet are still significant. The visually pleasing deraining results on real rainy images and videos have also validated the generalization ability of the trained PReNet and PRN models.</p><p>The contribution of this work is four-fold:</p><p>• Baseline deraining networks, i.e., PRN and PReNet, are proposed, by which better and simpler networks can work well in removing rain streaks, and provide a suitable basis to future studies on image deraining. • By taking advantage of intra-stage recursive computation, PRN r and PReNet r are also suggested to reduce network parameters while maintaining state-of-the-art deraining performance. • Using PRN and PReNet, the deraining performance can be further improved by taking both stage-wise result and original rainy image as input to each ResNet, and our progressive networks can be readily trained with single negative SSIM or MSE loss. • Extensive experiments show that our baseline networks are computationally very efficient, and perform favorably against state-of-the-arts on both synthetic and real rainy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we present a brief review on two representative types of deraining methods, i.e., traditional optimization-based and deep network-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Optimization-based Deraining Methods</head><p>In general, a rainy image can be formed as the composition of a clean background image layer and a rain layer. On the one hand, linear summation is usually adopted as the composition model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b35">35]</ref>. Then, image deraining can be formulated by incorporating with proper regularizers on both background image and rain layer, and solved by specific optimization algorithms. Among these methods, Gaussian mixture model (GMM) <ref type="bibr" target="#b21">[21]</ref>, sparse representation <ref type="bibr" target="#b35">[35]</ref>, and low rank representation <ref type="bibr" target="#b0">[1]</ref> have been adopted for modeling background image or a rain layers. Based on linear summation model, optimization-based methods have been also extended for video deraining <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr">19</ref>]. On the other hand, screen blend model <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b26">26]</ref> is assumed to be more realistic for the composition of rainy image, based on which Luo et al. <ref type="bibr" target="#b22">[22]</ref> use the discriminative dictionary learning to separate rain streaks by enforcing the two layers share fewest dictionary atoms. However, the real composition generally is more complicated and the regularizers are still insufficient in characterizing background and rain layers, making optimization-based methods remain limited in deraining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Network-based Deraining Methods</head><p>When applied deep network to single image deraining, one natural solution is to learn a direct mapping to predict clean background image x from rainy image y. However, it is suggested that plain fully convolutional networks (FCN) are ineffective in learning the direct mapping <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Instead, Fu et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> apply a low-pass filter to decompose y into a base layer y base and a detail layer y detail . By assuming y base ≈ x base , FCNs are then deployed to predict x detail from y detail . In contrast, Li et al. <ref type="bibr" target="#b20">[20]</ref> adopt the residual learning formulation to predict rain layer y − x from y. More complicated learning formulations, such as joint detection and removal of rain streaks <ref type="bibr" target="#b30">[30]</ref>, and joint rain density estimation and deraining <ref type="bibr" target="#b32">[32]</ref>, are also suggested. And adversarial loss is also introduced to enhance the texture details of deraining result <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b33">33]</ref>. In this work, we show that the improvement of deraining networks actually eases the difficulty of learning, and it is also feasible to train PRN and PReNet to learn either direct or residual mapping.</p><p>For the architecture of deraining network, Fu et al. first adopt a shallow CNN <ref type="bibr" target="#b4">[5]</ref> and then a deeper ResNet <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b30">[30]</ref>, a multi-task CNN architecture is designed for joint detection and removal of rain streaks, in which contextualized dilated convolution and recurrent structure are adopted to handle multi-scale and heavy rain steaks. Subsequently, Zhang et al. <ref type="bibr" target="#b32">[32]</ref> propose a density aware multi-stream densely connected CNN for joint estimating rain density and removing rain streaks. In <ref type="bibr" target="#b25">[25]</ref>, attentive-recurrent network is developed for single image raindrop removal. Most recently, Li et al. <ref type="bibr" target="#b20">[20]</ref> recurrently utilize dilated CNN and squeeze-and-excitation blocks to remove heavy rain streaks. In comparison to these deeper and complex networks, our work incorporates ResNet, recurrent layer and multi-stage recursion to constitute a better, simpler and more efficient deraining network.</p><p>Besides, several lightweight networks, e.g., cascaded scheme <ref type="bibr" target="#b3">[4]</ref> and Laplacian pyrimid framework <ref type="bibr" target="#b6">[7]</ref> are also developed to improve computational efficiency but at the cost of obvious performance degradation. As for PRN and PReNet, we further introduce intra-stage recursive computation to reduce network parameters while maintain-ing state-of-the-art deraining performance, resulting in our PRN r and PReNet r models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Progressive Image Deraining Networks</head><p>In this section, progressive image deraining networks are presented by considering network architecture, input and output, and loss functions. To this end, we first describe the general framework of progressive networks as well as input/output, then implement the network modules, and finally discuss the learning objectives of progressive networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Progressive Networks</head><p>A simple deep network generally cannot succeed in removing rain streaks from rainy images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Instead of designing deeper and complex networks, we suggest to tackle the deraining problem in multiple stages, where a shallow ResNet is deployed at each stage. One natural multi-stage solution is to stack several sub-networks, which inevitably leads to the increase of network parameters and susceptibility to over-fitting. In comparison, we take advantage of inter-stage recursive computation <ref type="bibr">[15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28]</ref> by requiring multiple stages share the same network parameters. Besides, we can incorporate intra-stage recursive unfolding of only 1 ResBlock to significantly reduce network parameters with graceful degradation in deraining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Progressive Residual Network</head><p>We first present a progressive residual network (PRN) as shown in <ref type="figure">Fig. 2(a)</ref>. In particular, we adopt a basic ResNet with three parts: (i) a convolution layer f in receives network inputs, (ii) several residual blocks (ResBlocks) f res extract deep representation, and (iii) a convolution layer f out outputs deraining results. The inference of PRN at stage t can be formulated as</p><formula xml:id="formula_0">x t−0.5 = f in (x t−1 , y), x t = f out (f res (x t−0.5 )),<label>(1)</label></formula><p>where f in , f res and f out are stage-invariant, i.e., network parameters are reused across different stages.</p><p>We note that f in takes the concatenation of the current estimation x t−1 and rainy image y as input. In comparison to only x t−1 in <ref type="bibr" target="#b20">[20]</ref>, the inclusion of y can further improve the deraining performance. The network output can be the prediction of either rain layer or clean background image. Our empirical study show that, although predicting rain layer performs moderately better, it is also possible to learn progressive networks for predicting background image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Progressive Recurrent Network</head><p>We further introduce a recurrent layer into PRN, by which feature dependencies across stages can be propagated to facilitate rain streak removal, resulting in our progressive recurrent network (PReNet). The only difference between PReNet and PRN is the inclusion of recurrent state s t ,</p><formula xml:id="formula_1">x t−0.5 = f in (x t−1 , y), s t = f recurrent (s t−1 , x t−0.5 ), x t = f out (f res (s t )),<label>(2)</label></formula><p>where the recurrent layer f recurrent takes both x t−0.5 and the recurrent state s t−1 as input at stage t − 1. f recurrent can be implemented using either convolutional Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">27]</ref> or convolutional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">[2]</ref>. In PReNet, we choose LSTM due to its empirical superiority in image deraining.</p><p>The architecture of PReNet is shown in <ref type="figure">Fig. 2(b)</ref>. By unfolding PReNet with T recursive stages, the deep representation that facilitates rain streak removal are propagated by recurrent states. The deraining results at intermediate stages in <ref type="figure">Fig. 1</ref> show that the heavy rain streak accumulation can be gradually removed stage-by-stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architectures</head><p>We hereby present the network architectures of PRN and PReNet. All the filters are with size 3×3 and padding 1×1. Generally, f in is a 1-layer convolution with ReLU nonlinearity <ref type="bibr" target="#b23">[23]</ref>, f res includes 5 ResBlocks and f out is also a 1layer convolution. Due to the concatenation of 3-channel RGB y and 3-channel RGB x t−1 , the convolution in f in has 6 and 32 channels for input and output, respectively. f out takes the output of f res (or f recurrent ) with 32 channels as input and outputs 3-channel RGB image for PRN (or PReNet). In f recurrent , all the convolutions in LSTM have 32 input channels and 32 output channels. f res is the key component to extract deep representation for rain streak removal, and we provide two implementations, i.e., conventional ResBlocks shown in <ref type="figure" target="#fig_0">Fig. 3</ref>(a) and recursive Res-Blocks shown in <ref type="figure" target="#fig_0">Fig. 3(b)</ref>. Conventional ResBlocks: As shown in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>, we first implement f res with 5 ResBlocks as its conventional form, i.e., each ResBlock includes 2 convolution layers followed by ReLU <ref type="bibr" target="#b23">[23]</ref>. All the convolution layers receive 32channel features without downsampling or upsamping operations. Conventional ResBlocks are adopted in PRN and PReNet.</p><p>Recursive ResBlocks: Motivated by <ref type="bibr">[15,</ref><ref type="bibr" target="#b28">28]</ref>, we also implement f res by recursively unfolding one ResBlock 5 times, as shown in <ref type="figure" target="#fig_0">Fig. 3(b)</ref>. Since network parameters mainly come from ResBlocks, the intra-stage recursive computation leads to a much smaller model size, resulting in PRN r and PReNet r . We have evaluated the performance of recursive ResBlocks in Sec. 4.2, indicating its elegant tradeoff between model size and deraining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Objective</head><p>Recently, hybrid loss functions, e.g., MSE+SSIM [4], 1 +SSIM <ref type="bibr" target="#b6">[7]</ref> and even adversarial loss <ref type="bibr" target="#b33">[33]</ref>, have been widely adopted for training deraining networks, but incredibly increase the burden of hyper-parameter tuning. In contrast, benefited from the progressive network architecture, we empirically find that a single loss function, e.g., MSE loss or negative SSIM loss <ref type="bibr" target="#b29">[29]</ref>, is sufficient to train PRN and PReNet. For a model with T stages, we have T outputs, i.e., x 1 , x 2 ,..., x T . By only imposing supervision on the final output x T , the MSE loss is</p><formula xml:id="formula_2">L = x T − x gt 2 ,<label>(3)</label></formula><p>and the negative SSIM loss is</p><formula xml:id="formula_3">L = −SSIM x T , x gt ,<label>(4)</label></formula><p>where x gt is the corresponding ground-truth clean image. It is worth noting that, our empirical study shows that negative SSIM loss outperforms MSE loss in terms of both PSNR and SSIM. Moreover, recursive supervision can be imposed on each intermediate result,</p><formula xml:id="formula_4">L = − T t=1 λ t SSIM x t , x gt ,<label>(5)</label></formula><p>where λ t is the tradeoff parameter for stage t. Experimental result in Sec. 4.1.1 shows that recursive supervision cannot achieve any performance gain when t = T , but can be adopted to generate visually satisfying result at early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first conduct ablation studies to verify the main components of our methods, then quantitatively and qualitatively evaluate progressive networks, and finally assess PReNet on real rainy images and video. All the source code, pre-trained models and results can be found at https://github.com/csdwren/PReNet.</p><p>Our progressive networks are implemented using Pytorch <ref type="bibr" target="#b24">[24]</ref>, and are trained on a PC equipped with two NVIDIA GTX 1080Ti GPUs. In our experiments, all the progressive networks share the same training setting. The patch size is 100 × 100, and the batch size is 18. The ADAM <ref type="bibr" target="#b17">[17]</ref> algorithm is adopted to train the models with an initial learning rate 1 × 10 −3 , and ends after 100 epochs. When reaching 30, 50 and 80 epochs, the learning rate is decayed by multiplying 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head><p>All the ablation studies are conducted on a heavy rainy dataset <ref type="bibr" target="#b30">[30]</ref> with 1,800 rainy images for training and 100 rainy images (Rain100H) for testing. However, we find that 546 rainy images from the 1,800 training samples have the same background contents with testing images. Therefore, we exclude these 546 images from training set, and train all our models on the remaining 1,254 training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Loss Functions</head><p>Using PReNet (T = 6) as an example, we discuss the effect of loss functions on deraining performance, including MSE loss, negative SSIM loss, and recursive supervision loss.</p><p>Negative SSIM v.s. MSE. We train two PReNet models by minimizing MSE loss (PReNet-MSE) and negative SSIM loss (PReNet-SSIM), and <ref type="table" target="#tab_0">Table 1</ref> lists their PSNR and SSIM values on Rain100H. Unsurprisingly, PReNet-SSIM outperforms PReNet-MSE in terms of SSIM. We also note that PReNet-SSIM even achieves higher PSNR, partially attributing to that PReNet-MSE may be inclined to get trapped into poor solution. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the deraining result by PReNet-SSIM is also visually more plausible than that by PReNet-MSE. Therefore, negative SSIM loss is adopted as the default in the following experiments.  <ref type="bibr" target="#b4">(5)</ref>. For PReNet-RecSSIM, we set λ t = 0.5 (t = 1, 2, ..., 5) and λ 6 = 1.5, where the tradeoff parameter for the final stage is larger than the others. From <ref type="table" target="#tab_0">Table 1</ref>, PReNet-RecSSIM performs moderately inferior to PReNet-SSIM. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>     ing resource constrained environments by stopping the inference at any stage t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Network Architecture</head><p>In this subsection, we assess the effect of several key modules of progressive networks, including recurrent layer, multi-stage recursion, and intra-stage recursion.</p><p>Recurrent Layer. Using PReNet (T = 6), we test two types of recurrent layers, i.e., LSTM (PReNet-LSTM) and GRU (PReNet-GRU). It can be seen from <ref type="table">Table 3</ref> that LSTM performs slightly better than GRU in terms of quantitative metrics, and thus is adopted as the default implementation of recurrent layer in our experiments. We further compare progressive networks with and without recurrent layer, i.e., PReNet and PRN, in <ref type="table">Table 4</ref>, and obviously the introduction of recurrent layer does benefit the deraining performance in terms of PSNR and SSIM.</p><p>Intra-stage Recursion. From <ref type="table">Table 4</ref>, intra-stage recursion, i.e., recursive ResBlocks, is introduced to significantly reduce the number of parameters of progressive networks, resulting in PRN r and PReNet r . As for deraining performance, it is reasonable to see that PRN and PReNet respectively achieve higher average PSNR and SSIM values than PRN r and PReNet r . But in terms of visual quality, PRN r and PReNet r are comparable with PRN and PReNet, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>Recursive Stage Number T . <ref type="table" target="#tab_1">Table 2</ref> lists the PSNR and SSIM values of four PReNet models with stages T = 2, 3, 4, 5, 6, 7. One can see that PReNet with more stages (from 2 stages to 6 stages) usually leads to higher average PSNR and SSIM values. However, larger T also makes PReNet more difficult to train. When T = 7, PReNet 7 performs a little inferior to PReNet 6 . Thus, we set T = 6 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Effect of Network Input/Output</head><p>Network Input. We also test a variant of PReNet by only taking x t−1 at each stage as input to f in (i.e., PReNet x ), where such strategy has been adopted in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b30">30]</ref>. From <ref type="table">Table 3</ref>, PReNet x is obviously inferior to PReNet in terms of both PSNR and SSIM, indicating the benefit of receiving y at each stage.</p><p>Network Output. We consider two types of network outputs by incorporating residual learning formulation (i.e., PReNet in <ref type="table">Table 3</ref>) or not (i.e., PReNet-LSTM in <ref type="table">Table 3</ref>). From <ref type="table">Table 3</ref>, residual learning can make a further contribution to performance gain. It is worth noting that, benefited from progressive networks, it is feasible to learn PReNet for directly predicting clean background from rainy image, and even PReNet-LSTM can achieve appealing deraining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Synthetic Datasets</head><p>Our progressive networks are evaluated on three synthetic datasets, i.e., Rain100H <ref type="bibr" target="#b30">[30]</ref>, Rain100L <ref type="bibr" target="#b30">[30]</ref> and Rain12 <ref type="bibr" target="#b21">[21]</ref>. Five competing methods are considered, including one traditional optimization-based method (GMM <ref type="bibr" target="#b21">[21]</ref>) and three state-of-the-art deep CNN-based models, i.e., DDN <ref type="bibr" target="#b5">[6]</ref>, JORDER <ref type="bibr" target="#b30">[30]</ref> and RESCAN <ref type="bibr" target="#b20">[20]</ref>, and one lightweight network (RGN <ref type="bibr" target="#b3">[4]</ref>). For heavy rainy images (Rain100H) and light rainy images (Rain100L), the models are respectively trained, and the models for light rain are directly applied on Rain12. Since the source codes of RGN are not available, we adopt the numerical results reported in <ref type="bibr" target="#b3">[4]</ref>. As for JORDER, we directly compute average PSNR and SSIM on deraining results provided by the authors. We re-train RESCAN <ref type="bibr" target="#b20">[20]</ref> for Rain100H with the default settings. Besides, we have tried to train RESCAN for light rainy images, but the results are much inferior to the others. So its results on Rain100L and Rain12 are not reported in our experiments.</p><p>Our PReNet achieves significant PSNR and SSIM gains over all the competing methods. We also note that for Rain100H, RESCAN <ref type="bibr" target="#b20">[20]</ref> is re-trained on the full 1,800 rainy images, the performance gain by our PReNet trained on the strict 1,254 rainy images is still notable. Moreover, even PReNet r can perform better than all the competing methods. From <ref type="figure">Fig. 7</ref>, visible dark noises along rain directions can still be observed from the results by the other methods. In comparison, the results by PRN and PReNet are visually more pleasing.</p><p>We further evaluate progressive networks on another dataset <ref type="bibr" target="#b5">[6]</ref> which includes 12,600 rainy images for training and 1,400 rainy images for testing (Rain1400). From <ref type="table" target="#tab_3">Table 6</ref>, all the four versions of progressive networks outperform DDN in terms of PSNR and SSIM. As shown in <ref type="figure">Fig. 8</ref>, the visual quality improvement by our methods is also significant, while the result by DDN still contains visible rain streaks. <ref type="table" target="#tab_4">Table 7</ref> lists the running time of different methods based on a computer equipped with a NVIDIA GTX 1080Ti GPU. We only give the running time of DDN <ref type="bibr" target="#b5">[6]</ref>, JORDER <ref type="bibr" target="#b30">[30]</ref> and RESCAN <ref type="bibr" target="#b20">[20]</ref>, due to the codes of the other competing methods are not available. We note that the running time of DDN <ref type="bibr" target="#b5">[6]</ref> takes the separation of details layer into account. Unsurprisingly, PRN and PReNet are much more efficient due to its simple network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Real Rainy Images</head><p>Using two real rainy images in <ref type="figure">Fig. 9</ref>, we compare PReNet with two state-of-the-art deep methods, i.e., JORDER <ref type="bibr" target="#b30">[30]</ref> and DDN <ref type="bibr" target="#b5">[6]</ref>. It can be seen that PReNet and JORDER perform better than DDN in removing rain streaks. For the first image, rain streaks remain visible in the result by DDN, while PReNet and JORDER can generate satisfying deraining results. For the second image, there are more or less rain streaks in the results by DDN and JORDER, while the result by PReNet is more clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real Rainy Videos</head><p>Finally, PReNet is adopted to process a rainy video in a frame-by-frame manner, and is compared with state-of-theart video deraining method, i.e., FastDerain <ref type="bibr" target="#b11">[12]</ref>. As shown in <ref type="figure">Fig. 10</ref>, for frame #510, both FastDerain and our PReNet can remove all the rain streaks, indicating the performance of PReNet even without the help of temporal consistency. However, FastDerain fails in switching frames, since it is developed by exploiting the consistency of adjacent frames. As a result, for frame #571, #572 and 640, rain streaks are remained in the results by FastDerain, while our PReNet performs favorably and is not affected by switching frames and accumulation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a better and simpler baseline network is presented for single image deraining. Instead of deeper and complex networks, we find that the simple combination of ResNet and multi-stage recursion, i.e., PRN, can result in favorable performance. Moreover, the deraining <ref type="table">Table 5</ref>. Average PSNR and SSIM comparison on the synthetic datasets Rain100H <ref type="bibr" target="#b30">[30]</ref>, Rain100L <ref type="bibr" target="#b30">[30]</ref> and Rain12 <ref type="bibr" target="#b21">[21]</ref>. Red, blue and cyan colors are used to indicate top 1 st , 2 nd and 3 rd rank, respectively. means these metrics are copied from <ref type="bibr" target="#b3">[4]</ref>. • means the metrics are directly computed based on the deraining images provided by the authors <ref type="bibr" target="#b30">[30]</ref>. donates the method is re-trained with their default settings (i.e., all the 1800 training samples for Rain100H).</p><p>Method GMM <ref type="bibr" target="#b21">[21]</ref>    performance can be further boosted by the inclusion of recurrent layer, and stage-wise result is also taken as input to each ResNet, resulting in our PReNet model. Further-more, the network parameters can be reduced by incorporating inter-and intra-stage recursive computation (PRN r and PReNet r ). And our progressive deraining networks can be readily trained with single negative SSIM or MSE loss. Extensive experiments validate the superiority of our PReNet and PReNet r on synthetic and real rainy images in comparison to state-of-the-art deraining methods. Taking their simplicity, effectiveness and efficiency into account, it is also  <ref type="figure">Figure 9</ref>. Visual quality comparison on two real rainy images.</p><p>appealing to exploit our models as baselines when developing new deraining networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Implementations of fres: (a) conventinal ResBlocks and (b) recursive ResBlocks where one ResBlock is recursively unfolded five times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, the deraining results by PReNet-SSIM and PReNet-RecSSIM are visually indistinguishable. The results indicate that a single loss on the final stage is sufficient to train progressive networks. Furthermore, Fig. 5 shows the intermediate PSNR and SSIM results at each stage for PReNet-SSIM (T = 6) and PReNet-RecSSIM (T = 6). It can be seen that PReNet-RecSSIM can achieve much better intermediate results than PReNet-SSIM, making PReNet-RecSSIM (T = 6) very promising in comput-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visual quality comparison of PReNet models trained by different loss functions, including single MSE loss (PReNet-MSE), single negative SSIM loss (PReNet-SSIM) and recursive negative SSIM supervision (PReNet-RecSSIM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Average PSNR and SSIM of PReNet-SSIM (T = 6) and PReNet-RecSSIM (T = 6) at stage t = 1, 2, 3, 4, 5, 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visual effects of recursive ResBlocks. The deraining results by PRNr and PReNetr are visually indistinguishable with those by PRN and PReNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Visual quality comparison on an image from Rain100H<ref type="bibr" target="#b30">[30]</ref>. Visual quality comparison on an image from Rain1400<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Comparison of PReNet (T = 6) with different loss func-</cell></row><row><cell>tions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell cols="3">PReNet-MSE PReNet-SSIM PReNet-RecSSIM</cell></row><row><cell>PSNR</cell><cell>29.08</cell><cell>29.32</cell><cell>29.12</cell></row><row><cell>SSIM</cell><cell>0.880</cell><cell>0.898</cell><cell>0.895</cell></row><row><cell cols="4">Single v.s. Recursive Supervision. The negative SSIM loss</cell></row><row><cell cols="4">can be imposed only on the final stage (PReNet-SSIM) in</cell></row><row><cell cols="4">Eqn. (4) or recursively on each stage (PReNet-RecSSIM)</cell></row><row><cell>in Eqn.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of PReNet models with different T stages.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model PReNet2 PReNet3 PReNet4 PReNet5 PReNet6 PReNet7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell>27.27</cell><cell>28.01</cell><cell>28.60</cell><cell>28.92</cell><cell>29.32</cell><cell>29.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell>0.882</cell><cell>0.885</cell><cell>0.890</cell><cell>0.895</cell><cell>0.898</cell><cell>0.898</cell></row><row><cell cols="8">Table 3. Comparisons of PReNet variants for ablation studies.</cell></row><row><cell cols="8">PReNetx, PReNet-LSTM, and PReNet-GRU learn direct mapping</cell></row><row><cell cols="8">for predicting background image. In particular, PReNetx only</cell></row><row><cell cols="8">takes current deraining result x t−1 as input, the recurrent lay-</cell></row><row><cell cols="8">ers in PReNet-LSTM and PReNet-GRU are implemented using</cell></row><row><cell cols="8">LSTM and GRU, respectively. PReNet is the final model by adopt-</cell></row><row><cell cols="8">ing residual learning and LSTM recurrent layer, and taking y and</cell></row><row><cell cols="3">x t−1 as input.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Model PReNetx PReNet-LSTM PReNet-GRU PReNet</cell></row><row><cell cols="2">PSNR</cell><cell>28.91</cell><cell></cell><cell cols="2">29.32</cell><cell>29.08</cell><cell>29.46</cell></row><row><cell cols="2">SSIM</cell><cell>0.895</cell><cell></cell><cell cols="2">0.898</cell><cell>0.896</cell><cell>0.899</cell></row><row><cell cols="8">Table 4. Effect of recursive ResBlocks. PRN and PReNet contain</cell></row><row><cell cols="8">5 ResBlocks. PRNr and PReNetr unfold 1 ResBlock 5 times.</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>PRN</cell><cell>PReNet</cell><cell>PRNr</cell><cell>PReNetr</cell></row><row><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell>28.07</cell><cell>29.46</cell><cell>27.43</cell><cell>28.98</cell></row><row><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell>0.884</cell><cell>0.899</cell><cell>0.874</cell><cell>0.892</cell></row><row><cell cols="3">#. Parameters</cell><cell></cell><cell>95,107</cell><cell>168,963</cell><cell>21,123</cell><cell>94,979</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Stage t</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>DDN [6] RGN [4] JORDER [30] • RESCAN [20] PRN PReNet PRNr PReNetr Rain100H 15.05/0.425 21.92/0.764 25.25/0.841 26.54/0.835 28.88/0.866 28.07/0.884 29.46/0.899 27.43/0.874 28.98/0.892 Rain100L 28.66/0.865 32.16/0.936 33.16/0.963 36.61/0.974 --36.99/0.977 37.48/0.979 36.11/0.973 37.10/0.977 Rain12 32.02/0.855 31.78/0.900 29.45/0.938 33.92/0.953 --36.62/0.952 36.66/0.961 36.16/0.961 36.69/0.962</figDesc><table><row><cell>Rainy image</cell><cell>GMM [21]</cell><cell>DDN [6]</cell><cell>RESCAN [20]</cell></row><row><cell>Ground-truth</cell><cell>JORDER [30]</cell><cell>PRN</cell><cell>PReNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Quantitative comparison on Rain1400<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell cols="6">Method DDN [6] PRN PReNet PRNr PReNetr</cell></row><row><cell>PSNR</cell><cell>29.91</cell><cell>31.69</cell><cell>32.60</cell><cell>31.31</cell><cell>32.44</cell></row><row><cell>SSIM</cell><cell>0.910</cell><cell>0.941</cell><cell>0.946</cell><cell>0.937</cell><cell>0.944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Comparison of running time (s) Image Size DDN [6] JORDER [30] RESCAN [20] PRN PReNet</figDesc><table><row><cell>500 × 500</cell><cell>0.407</cell><cell>0.179</cell><cell>0.448</cell><cell>0.088 0.156</cell></row><row><cell cols="2">1024 × 1024 0.754</cell><cell>0.815</cell><cell>1.808</cell><cell>0.296 0.551</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Residualguide feature fusion network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hunag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clearing the skies: A deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2944" to="2956" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06173</idno>
		<title level="m">Lightweight pyramid networks for image deraining</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint convolutional analysis and synthesis sparse representation for single image layer separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel tensor-based video rain streaks removal approach via utilizing discriminatively intrinsic priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The first row is rainy frames, the second row is the results by FastDerain [12] and the third row is the results by PReNet</title>
	</analytic>
	<monogr>
		<title level="m">Figure 10. Visual quality comparison on a real rainy video</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fastderain: A novel video rain streak removal method using directional gradient priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07487</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic singleimage-based rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1742</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video deraining and desnowing using temporal correlation and low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2658" to="2670" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video rain streak removal by multiscale convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6644" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent squeezeand-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop: The Future of Gradient-based Machine Learning Software and Techniques</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04522</idno>
		<title level="m">Simultaneous fidelity and regularization learning for image restoration</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Density-aware single image deraining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint bilayer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
