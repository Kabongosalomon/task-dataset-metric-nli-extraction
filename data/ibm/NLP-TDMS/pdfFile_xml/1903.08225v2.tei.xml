<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-task weakly supervised learning from instructional videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
						</author>
						<title level="a" type="main">Cross-task weakly supervised learning from instructional videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we investigate learning visual models for the steps of ordinary tasks using weak supervision via instructional narrations and an ordered list of steps instead of strong supervision via temporal annotations. At the heart of our approach is the observation that weakly supervised learning may be easier if a model shares components while learning different steps: "pour egg" should be trained jointly with other tasks involving "pour" and "egg". We formalize this in a component model for recognizing steps and a weakly supervised learning framework that can learn this model under temporal constraints from narration and the list of steps. Past data does not permit systematic studying of sharing and so we also gather a new dataset, CrossTask, aimed at assessing cross-task sharing. Our experiments demonstrate that sharing across tasks improves performance, especially when done at the component level and that our component model can parse previously unseen tasks by virtue of its compositionality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Suppose you buy a fancy new coffee machine and you would like to make a latte. How might you do this? After skimming the instructions, you may start watching instructional videos on YouTube to figure out what each step entails: how to press the coffee, steam the milk, and so on. In the process, you would obtain a good visual model of what each step, and thus the entire task, looks like. Moreover, you could use parts of this visual model of making lattes to help understand videos of a new task, e.g., making filter coffee, since various nouns and verbs are shared. The goal of this paper is to build automated systems that can <ref type="bibr">Figure 1</ref>. Our method begins with a collection of tasks, each consisting of an ordered list of steps and a set of instructional videos from YouTube. It automatically discovers both where the steps occur and what they look like. To do this, it uses the order, narration and commonalities in appearance across tasks (e.g., the appearance of pour in both making pancakes and making meringue). similarly learn visual models from instructional videos and in particular, make use of shared information across tasks (e.g., making lattes and making filter coffee).</p><p>The conventional approach for building visual models of how to do things <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> is to first annotate each step of each task in time and then train a supervised classifier for each. Obtaining strong supervision in the form of temporal step annotations is time-consuming, unscalable and, as demonstrated by humans' ability to learn from demonstrations, unnecessary. Ideally, the method should be weakly supervised (i.e., like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>) and jointly learn when steps occur and what they look like. Unfortunately, any weakly supervised approach faces two large challenges. Temporally localizing steps in the input videos for each task is hard as there is a combinatorial set of options for the step locations; and, even if the steps were localized, each visual model learns from limited data and may work poorly.</p><p>We show how to overcome these challenges by sharing across tasks and using weaker and naturally occurring forms of supervision. The related tasks let us learn better visual models by exploiting commonality across steps as illustrated in <ref type="figure">Figure 1</ref>. For example, while learning about pour water in making latte, the model for pour also depends on pour milk in making pancakes and the model for water also depends on put vegetables in water in making bread and butter pickles. We assume an ordered list of steps is given per task and that the videos are instructional (i.e., have a natural language narration describing what is being done). As it is often the case in weakly supervised video learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>, these assumptions constrain the search for when steps occur, helping tackle a combinatorial search space.</p><p>We formalize these intuitions in a framework, described in Section B, that enables compositional sharing across tasks together with temporal constraints for weakly supervised learning. Rather than learning each step as a monolithic weakly-supervised classifier, our formulation learns a component model that represents the model for each step as the combination of models of its components, or the words in each step (e.g., pour in pour water). This empirically improves learning performance and these component models can be recombined in new ways to parse videos for tasks for which it was not trained, simply by virtue of their representation. This component model, however, prevents the direct application of techniques previously used for weakly supervised learning in similar settings (e.g., DIFFRAC <ref type="bibr" target="#b2">[3]</ref> in <ref type="bibr" target="#b1">[2]</ref>); we therefore introduce a new and more general formulation that can handle more arbitrary objectives.</p><p>Existing instructional video datasets do not permit the systematic study of this sharing. We gather a new dataset, CrossTask, which we introduce in Section C. This dataset consists of 4.7K instructional videos for 83 different tasks, covering 376 hours of footage. We use this dataset to compare our proposed approach with a number of alternatives in experiments described in Section D. Our experiments aim to assess the following three questions: how well does the system learn in a standard weakly supervised setup; can it exploit related tasks to improve performance; and how well can it parse previously unseen tasks.</p><p>The paper's contributions include: (1) A component model that shares information between steps for weakly supervised learning from instructional videos; (2) A weakly supervised learning framework that can handle such a model together with constraints incorporating different forms of weak supervision; and (3) A new dataset that is larger and more diverse than past efforts, which we use to empirically validate the first two contributions. We make our dataset and our code publically available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning the visual appearance of steps of a task from instructional videos is a form of action recognition. Most work in this area, e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, uses strong supervision in the form of direct labels, including a lot of work that focuses on similar objectives <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. We build our feature representations on top of advances in this area <ref type="bibr" target="#b7">[8]</ref>, but our proposed method does not depend on having lots of annotated data for our problem.</p><p>We are not the first to try to learn with weak supervision in videos and our work bears resemblances to past efforts. For instance, we make use of ordering constraints to obtain supervision, as was done in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. The aim of our work is perhaps closest to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref> as they also use narrations in the context of instructional videos. Among a number of distinctions with each individual work, one significant novelty of our work is the compositional model used, where instead of learning a monolithic model independently per-step as done in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>, the framework shares components (e.g., nouns and verbs) across steps. This sharing improves performance, as we empirically confirm, and enables the parsing of unseen tasks.</p><p>In order to properly evaluate the importance of sharing, we gather a dataset of instructional videos. These have attracted a great deal of attention recently <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> since the co-occurrence of demonstrative visual actions and natural language enables many interesting tasks ranging from coreference resolution <ref type="bibr" target="#b18">[19]</ref> to learning person-object interaction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Existing data, however, is either not large (e.g., only 5 tasks <ref type="bibr" target="#b1">[2]</ref>), not diverse (e.g., YouCookII <ref type="bibr" target="#b34">[35]</ref> is only cooking), or not densely temporally annotated (e.g., What's Cooking? <ref type="bibr" target="#b23">[24]</ref>). We thus collect a dataset that is: (i) relatively large (83 tasks, 4.7K videos); (ii) simultaneously diverse (Covering car maintenance, cooking, crafting) yet also permitting the evaluation of sharing as it has related tasks; and (iii) annotated for temporal localization, permitting evaluation. The scale, and relatedness, as we demonstrate empirically contribute to increased performance of visual models.</p><p>Our technical approach to the problem builds particularly heavily on the use of discriminative clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>, or the simultaneous constrained grouping of data samples and learning of classifiers for groups. Past work in this area has either had operated with complex constraints and a restricted classifier (e.g., minimizing the L2 loss with linear model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>) or an unrestricted classifier, such as a deep network, but no constraints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. Our weakly supervised setting requires the ability to add constraints in order to converge to a good solution while our compositional model and desired loss function requires the ability to use an unrestricted classifier. We therefore propose an optimization approach that handles both, letting us train with a compositional model while also using temporal constraints.</p><p>Finally, our sharing between tasks is enabled via the composition of the components of each step (e.g., nouns, verbs). This is similar to attributes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which have been used in action recognition in the past <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>. Our components are meaningful (representing, e.g., "lemon") but also automatically built; they are thus different than pre-defined semantic attributes (not automatic) and the non-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Step Classifier  <ref type="figure" target="#fig_4">Figure 2</ref>. Our approach expresses classifiers for each step of each task in terms of a component model (e.g., writing the pour milk as a pour and milk classifier). We thus cast the problem of learning the steps as learning an underlying set of component models. We learn these models by alternating between updating labels for these classifiers and the classifiers themselves while using constraints from narrations. semantic attributes (not intrinsically meaningful) as defined in <ref type="bibr" target="#b11">[12]</ref>. It is also related to methods that compose new classifiers from others, including <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref> among many others. Our framework is orthogonal, and shows how to learn these in a weakly-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Our goal is to build visual models for a set of tasks from instructional videos. Each task is a multi-step process such as making latte consisting of multiple steps, such as pour milk. We aim to learn a visual model for each of these steps. Our approach uses component models that represent each step in terms of its constituent components as opposed to a monolithic entity, as illustrated in <ref type="figure" target="#fig_4">Figure 2</ref>. For instance, rather than building a classifier solely for whisk mixture in the context of make pancakes, we learn a set of classifiers per-component, one for whisk, spread, mixture and so on, and represent whisk mixture as the combination of whisk and mixture and share mixture with spread mixture. This shares data between steps and enables the parsing of previously unseen tasks, which we both verify empirically.</p><p>We make a number of assumptions. Throughout, we assume that we are given an ordered list of steps for each task. This list is our only source of manual supervision and is done once per-task and is far less time consuming than annotating a temporal segmentation of each step in the input videos. At training time, we also assume that our training videos contain audio that explains what actions are being performed. At test time, however, we do not use the audio track: just like a person who watches a video online, once our system is shown how to make a latte with narration, it is expected to follow along without step-by-step narrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modeling Instructional Videos</head><p>We now describe our technical approach for using a list of steps to jointly learn the labels and visual models on a set of narrated instructional videos. This is weakly supervised since we provide only the list of steps, but not their temporal locations in training videos.</p><p>Problem formulation. We denote the set of narrated instructional videos V. Each video v ∈ V contains a sequence of N v segments of visual features X v = (x 1 , . . . , x Nv ) as well as narrations we use later. For every task τ we assume to be given a set of videos V τ together with a set of ordered natural language steps K τ .</p><p>Our goal is then to discover a set of classifiers F that can identify the steps of the tasks. In other words, if τ is a task and k is its step, the classifier f τ k determines whether a visual feature depicts step k of τ or not. To do this, we also learn a labeling Y of the training set for the classifiers, or for every video v depicting task τ , a binary label matrix</p><formula xml:id="formula_0">Y v ∈ {0, 1} Nv×Kτ where Y v tk = 1</formula><p>if time t depicts step k and 0 otherwise. While jointly learning labels and classifiers leads to trivial solutions, we can eliminate these and make meaningful progress by constraining Y and by sharing information across the classifiers of F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Component Classifiers</head><p>One of the main focuses of this paper is in the form of the step classifier f . Specifically, we propose a component model that represents each step (e.g., "pour milk") as a combination of components (e.g., "pour" and "milk"). Before explaining how we formulate this, we place it in context by introducing a variety of alternatives that vary in terms of how they are learned and formulated.</p><p>The simplest approach, a task-specific step model, is to learn a classifier for each step in the training set (i.e., a model for pour egg for the particular task of making pancakes). Here, the model simply learns τ K τ classifiers, one for each of the K τ steps in each task, which is simple but which permits no sharing.</p><p>One way of adding sharing would be to have a shared step model, where a single classifier is learned for each unique step in the dataset. For instance, the pour egg classifier learns from both making meringues and making pancakes. This sharing, however, would be limited to exact duplicates of steps, and so while whisk milk and pour milk both share an object, they would be learned separately.</p><p>Our proposed component model fixes this issue. We au-tomatically generate a vocabulary of components by taking the set of stemmed words in all the steps. These components are typically objects, verbs and prepositions and we combine classifiers for each component to yield our steps. In particular, for a vocabulary of M components, we define a per-task matrix A τ ∈ {0, 1} Kτ ×M where A τ k,m = 1 if the step k involves components m and 0 otherwise. We then learn M classifiers g 1 , . . . , g M such that the prediction of a step f τ k is the average of predictions provided by component classifiers</p><formula xml:id="formula_1">f τ k (x) = m A τ km g m (x)/ m A τ km .<label>(1)</label></formula><p>For instance, the score for pour milk is the average of outputs of g pour and g milk . In other words, when optimizing over the set of functions F , we optimize over the parameters of {g i } so that when combined together in step models via (1), they produce the desired results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective and Constraints</head><p>Having described the setup and classifiers, we now describe the objective function we minimize. Our goal is to simultaneously optimize over step location labels Y and classifiers F over all videos and tasks</p><formula xml:id="formula_2">min Y ∈C,F ∈F τ v∈V(τ ) h(X v , Y v ; F ),<label>(2)</label></formula><p>where C is the set of temporal constraints on Y defined below, and F is a family of considered classifiers. Our objective function per-video is a standard cross-entropy loss</p><formula xml:id="formula_3">h(X v , Y v ; F ) = − t,k Y v tk log   exp (f τ k (x v t )) k exp(f τ k (x v t ))   .</formula><p>(3) Optimizing (2) may lead to trivial solutions (e.g., Y v = 0 and F outputting all zeros). We thus constrain our labeling of Y to avoid this and ensure a sensible solution. In particular, we impose three constraints: At least once. We assume that every video v of a task depicts each step k at least once, or t Y v tk ≥ 1. Temporal ordering. We assume that steps occur in the given order. While not always strictly correct, this dramatically reduces the search space and leads to better classifiers. Temporal text localization. We assume that the steps and corresponding narrations happen close in time, e.g., the narrator of a grill steak video may say "just put the marinated steak on the grill". We automatically compare the text description of each step to automatic YouTube subtitles. For a task with K τ steps and a video with N v frames, we construct a [0, 1] Nv×Kτ matrix of cosine similarities between steps and a sliding-window word vector representations of narrations (more details in supplementary materials). Since narrated videos contain spurious mentions of tasks (e.g., "before putting the steak on the grill, we clean the grill") we do not directly use this matrix, but instead find an assignment of steps to locations that maximizes the total similarity while respecting the ordering constraints. The visual model must then more precisely identify when the action appears. We then impose a simple hard constraint of disallowing labelings Y v where any step is outside of the textbased interval (average length 9s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization and Inference</head><p>We solve problem (2) by alternating between updating assignments Y and the parameters of the classifiers F . Updating Y . When F is fixed, we can minimize (2) w.r.t. Y independently for each video. In particular, fixing F fixes the classifier scores, meaning that minimizing (2) with respect to Y v is a constrained minimization of a linear cost in Y subject to constraints. Our supplemental shows that this can be done by dynamic programming. Updating F . When Y is fixed, our cost function reduces to a standard supervised classification problem. We can thus apply standard techniques for solving these, such as stochastic gradient descent. More details are provided below and in the supplemental material. Initialization. Our objective is non-convex and has local minima, thus a proper initialization is important. We obtain such an initialization by treating all assignments that satisfy the temporal text localization constraints as groundtruth and optimizing for F for 30 epochs, each time drawing a random sample that satisfies the constraints.</p><p>Inference. Once the model has been fit to the data, inference on a new video v of a task τ is simple. After extracting features, we run each classifier f on every temporal segment, resulting in a N v × K τ score matrix. To obtain a hard labeling, we use dynamic programming to find the best-scoring labeling that respects the given order of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>Networks: Due to the limited data size and noisy supervision, we use a linear classifier with dropout for regularization. Preliminary experiments with deeper models did not yield improvements. We use ADAM <ref type="bibr" target="#b20">[21]</ref> with the learning rate of 10 −5 for optimization. Features: We represent each video segment x i using RGB I3D features <ref type="bibr" target="#b7">[8]</ref> (1024D), Resnet-152 features <ref type="bibr" target="#b15">[16]</ref> (2048D) extracted at each frame and averaged over one-second temporal windows, and audio features from <ref type="bibr" target="#b16">[17]</ref> (128D). Components: We obtain the dictionary of components by finding the set of unique stemmed words over all step descriptions. The total number of components is 383. Hyperparameters: Dropout and the learning rate are chosen on a validation data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Tasks Related Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Make Bread &amp; Butter Pickles Can Tomato Sauce</head><p>Boil tomatoes, remove tomato skin, cut tomato, …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Make Latte</head><p>Cut cucumber, cut onion, add salt, pour water, …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Make Caramel Macchiato</head><p>Build Simple Floating Shelves Build a Desk <ref type="figure" target="#fig_5">Figure 3</ref>. Our new dataset, used to study sharing in a weakly supervised learning setting. It contains primary tasks, such as make bread and butter pickles, as well as related tasks, such as can tomato sauce. This lets us study whether learning multiple tasks improves performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CrossTask dataset</head><p>One goal of this paper is to investigate whether sharing improves the performance of weakly supervised learning from instructional videos. To do this, we need a dataset covering a diverse set of interrelated tasks and annotated with temporal segments. Existing data fails to satisfy at least one of these criteria and we therefore collect a new dataset (83 tasks, 4.7K videos) related to cooking, car maintenance, crafting, and home repairs. These tasks and their steps are derived from wikiHow, a website that describes how to solve many tasks, and the videos come from YouTube.</p><p>CrossTask dataset is divided into two sets of tasks to investigate sharing. The first is primary tasks, which are the main focus of our investigation and the backbone of the dataset. These are fully annotated and form the basis for our evaluations. The second is related tasks with videos gathered in a more automatic way to share some, but not all, components with the primary tasks. One goal of our experiments is to assess whether these related tasks improve the learning of primary tasks, and whether one can learn a good model only on related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Video Collection Procedure</head><p>We begin the collection process by defining our tasks. These must satisfy three criteria: they must entail a sequence of physical interactions with objects (unlike e.g., how to get into a relationship); their step order must be deterministic (unlike e.g., how to play chess); and they must appear frequently on YouTube. We asked annotators to review the tasks in five sections of wikihow to get tasks satisfying the first two criteria, yielding ∼ 7K candidate tasks, and manually filter for the third criteria.</p><p>We select 18 primary tasks and 65 related tasks from these 7K candidate tasks. The primary tasks cover a variety of themes (e.g., auto repair to cooking to DIY) and include building floating shelves and making latte. We find 65 related tasks by finding related tasks for each primary task. We generate potential related tasks for a primary task by comparing the wikiHow articles using a TF-IDF on a bag-of-words representation, which finds tasks with similar descriptions. We then filter out near duplicates (e.g., how to jack up a car and how to use a car jack) by comparing top YouTube search results and removing candidates with overlaps, and manually remove a handful of irrelevant tasks.</p><p>We define steps and their order for each task by examining the wikiHow articles, beginning with the summaries of each step. Using the wikiHow summary itself is insufficient, since many articles contain non-visual steps and some steps combine multiple physical actions. We thus manually correct the list yielding a set of tasks with 7.4 steps on average for primary tasks and 8.8 for related tasks.</p><p>We then obtain videos for each task by searching YouTube. Since the related tasks are only to aid the primary tasks, we take the top 30 results from YouTube. For primary tasks, we ask annotators to filter a larger pool of top results while examining the video, steps, and wikiHow illustrations, yielding at least 80 videos per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Annotations and Statistics</head><p>Task localization annotations. Since our focus is the primary tasks, annotators mark the temporal extent of each primary task step independently. We do this for our 18 primary tasks and make annotations publically available <ref type="bibr" target="#b0">1</ref>   <ref type="table" target="#tab_1">Table 1</ref>. Our dataset is simultaneously large while also having precise temporal segment annotations.</p><p>To illustrate the dataset, we report a few summary statistics about the primary task videos. The videos are quite long, with an average length of 4min 57sec, and depict fairly complex tasks, with 7.4 steps on average. Less complex tasks include jack up a car (3 steps); more complex ones include pickle cucumbers or change tire (11 steps each).</p><p>Challenges. In addition to being long and complex, these videos are challenging since they do not precisely show the ordered steps we have defined. For instance, in add oil to car, 85% of frames instead depict background information such as shots of people talking or other things. This is not an outlier: on average 72% of the dataset is background. On the other hand, on average 31% of steps are not depicted due to variances in procedures and omissions (pickle cucumber has 48% of steps missing). Moreover, the steps do not necessarily appear in the correct order: to estimate the order consistency, we compute an upper bound on performance using our given order and found that the best orderrespecting parse of the data still missed 14% of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Our experiments aim to address the following three questions about cross-task sharing in the weakly-supervised setting: (1) Can the proposed method use related data to improve performance? (2) How does the proposed component model compare to sharing alternatives? (3) Can the component model transfer to previously unseen tasks? Throughout, we evaluate on the large dataset introduced in Section C that consists of primary tasks and related tasks. We address (1) in Section 6.1 by comparing our proposed approach with methods that do not share and show that our proposed approach can use related tasks to improve performance on primary asks. Section 6.2 addresses (2) by analyzing the performance of the model and showing that it outperforms step-based alternatives. We answer (3) empirically in Section 6.3 by training only on related tasks, and show that we are able to perform well on primary tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Cross-task Learning</head><p>We begin by evaluating whether our proposed component model approach can use sharing to improve performance on a fixed set of tasks. We fix our evaluation to be the 18 primary tasks and evaluate whether the model can use the 65 related tasks to improve performance. Metrics and setup. We evaluate results on 18 primary tasks over the videos that make up the test set. We quantify performance via recall, which we define as the ratio between the number of correct step assignments (defined as falling into the correct ground-truth time interval) and the total number of steps over all videos. In other words, to get a perfect score, a method must correctly identify one instance of each step of the task in each test video. All methods make a single prediction per step, which prevents the trivial solution of assigning all frames to all actions.</p><p>We run experiments 20 times, each time making a train set of 30 videos per task and leaving the remaining 1850 videos for test. We report the average. Hyperparameters are set for all methods using a fixed validation set of 20 videos per primary task that are never used for training or testing. Baselines. Our goal is to examine whether our sharing approach can leverage related tasks to improve performance on our primary task. We compare our method to its version without sharing as well as to a number of baselines. (1) Uniform: simply predict steps at fixed time intervals. Since this predicts steps in the correct order and steps often break tasks into roughly equal chunks, this is fairly well-informed prior.</p><p>(2) Alayrac'16: the weakly supervised learning method for videos, proposed in <ref type="bibr" target="#b0">[1]</ref>. This is similar in spirit to our approach except it does not share and optimizes a L2-criterion via the DIFFRAC <ref type="bibr" target="#b2">[3]</ref> method. (3) Richard'18: the weakly supervised learning method <ref type="bibr" target="#b26">[27]</ref> that does not rely on the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Task</head><p>Make Tomato Rice Stir Rice</p><p>Grill Kabobs Cut Ingredients <ref type="figure">Figure 5</ref>. Components that share well and poorly: while stir shares well between steps of tasks, cut shares poorly when transferring from a food context to a home improvement context. known order of steps. (4) Task-Specific Steps: Our approach trained independently for each step of each task. In other words, there are separate models for pour egg in the contexts of making pancakes and making meringue. This differs from Alayrac in that it optimizes a cross-entropy loss using our proposed optimization method. It differs from our full proposed approach since it performs no sharing. Note, that the full method in <ref type="bibr" target="#b0">[1]</ref> includes automatic discovery of steps from narrations. Here, we only use the visual model of <ref type="bibr" target="#b0">[1]</ref>, while providing the same constraints as in our method. This allows for a fair comparison between <ref type="bibr" target="#b0">[1]</ref> and our method, since both use the same amount of supervision. At test time, the method presented in <ref type="bibr" target="#b26">[27]</ref> has no prior about which steps are present or the order in which they occur. To make a fair comparison, we use the trained classifier of the method in <ref type="bibr" target="#b26">[27]</ref>, and apply the same inference procedure as in our method. Qualitative results. We illustrate qualitative results of our full method in <ref type="figure">Figure 4</ref>. We show a parses of unseen videos of Build Shelves and Make Banana Ice Cream and failure modes. Our method can handle well a large variety of tasks and steps but may struggle to identify some details (e.g., vanilla vs. egg) or actions.</p><p>Quantitative results. <ref type="table" target="#tab_3">Table 2</ref> shows results summarized across steps. The uniform baseline provides a strong lower bound, achieving an average recall of 9.7% and outperforming <ref type="bibr" target="#b26">[27]</ref>. Note, however, that <ref type="bibr" target="#b26">[27]</ref> is designed to adress a different problem and cannot be fairly compared with other methods in our setup. While <ref type="bibr" target="#b0">[1]</ref> improves on this (13.3%), it does substantially worse than our task-specific step method (18.6%). We found that predictions from <ref type="bibr" target="#b0">[1]</ref> often had several steps with similar scores, leading to poor parse results, which we attribute to the convex relaxation used by DIFFRAC. This was resolved in the past by the use of narration at test time; our approach does not depend on this.</p><p>Our full approach, which shares across tasks, produces substantially better performance (22.4%) than the taskspecific step method. More importantly, this improvement is systematic: the full method improves on the task-specific step baseline in 17 tasks out of 18.</p><p>We illustrate some qualitative examples of steps benefiting and least benefiting from sharing in <ref type="figure">Figure 5</ref>. Typically, sharing can help if the component has distinctive appearance and is involved in a number of steps: steps involve stirring, for instance, have an average gain of 15% recall over independent training because it is frequent (in 30 steps) and distinctive. Of course, not all steps benefit: cut shelf is harmed (47% independent → 28% shared) because cut mostly occurs in cooking tasks with dissimilar contexts. Verifying optimizer on small-scale data. We now evaluate our approach on the smaller 5-task dataset of <ref type="bibr" target="#b0">[1]</ref>. Since here there are no common steps across tasks, we are able to test only the basic task-specific step-based version. To make a fair comparison, we use the same features, ordering constraints, as well as constraints from narration for every K as provided by the authors of <ref type="bibr" target="#b0">[1]</ref>, and we evaluate using the F1 metric as in <ref type="bibr" target="#b0">[1]</ref>. As a result, the two formulations are on par, where <ref type="bibr" target="#b0">[1]</ref> versus our approach result in 22.8% versus 21.8% for K=10 and 21.0% versus 21.1% for K=15, respectively. While these scores are slightly lower compared </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Add Strawberry</head><p>To Cake</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Steps From Related Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cut Steak</head><p>Cut Tomato</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Add Tomato</head><p>Add Cherries to Cake <ref type="figure">Figure 6</ref>. Examples of identified steps for an unseen task. While the model has not seen these steps and objects e.g., strawberries, its knowledge of other components leads to reasonable predictions.</p><p>to those obtained by the single-task probabilistic model in Sener <ref type="bibr" target="#b27">[28]</ref> (25.4% at K=10 and 23.6% at K=15), we are unable to compare using our full cross-task model on this dataset. Overall, these results verify the effectiveness of our optimization technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Evaluation of Cross-task Sharing</head><p>Having verified the framework and the role of sharing, we now more precisely evaluate how sharing is performed to examine the contribution of our proposed compositional model. We vary two dimensions. The first is the granularity, or at what level sharing occurs. We propose sharing at a component level, but one could share at a step level as well.</p><p>The second is what data is used, including (i) independently learning primary tasks; (ii) learning primary tasks together; (iii) learning primary plus related tasks together. <ref type="table" target="#tab_4">Table 3</ref> reveals that increased sharing consistently helps and component-based sharing extracts more from sharing than step-based (performance increases across rows). This gain over step-based sharing is because step-based sharing requires exact matches. Most commonality between tasks occurs with slight variants (e.g., cut is applied to steak, tomato, pickle, etc.) and therefore a componentbased model is needed to maximally enable sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Novel Task Transfer</head><p>One advantage of shared representations is that they can let one parse new concepts. For example, without any modifications, we can repeat our experiments from Section 6.1 in a setting where we never train on the 18 tasks that we test on but instead on the 65 related tasks. The only information given about the test tasks is an ordered list of steps. Setup. As in Section 6.1, we quantify performance with recall on the 18 primary tasks. However, we train on a subset of the 65 related tasks and never on any primary task. Qualitative results. We show a parse of steps of Make Strawberry Cake in <ref type="figure">Figure 6</ref> using all related tasks. The model has not seen cut strawberry before but has seen other forms of cutting. Similarly, it has seen add cherries to cake, and can use this step to parse add strawberries to cake.</p><p>Quantitative results. <ref type="figure" target="#fig_0">Figure 7</ref> shows performance as a function of the number of related tasks used for training. Increasing the number of training tasks improves performance on the primary tasks, and does not plateau even when 65 tasks are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have introduced an approach for weakly supervised learning from instructional videos and a new CrossTask dataset for evaluating the role of sharing in this setting. Our component model has been shown ability to exploit common parts of tasks to improve performance and was able to parse previously unseen tasks. Future work would benefit from improved features as well as from improved versions of sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline of supplementary material</head><p>This supplementary material provides more details on our method and our dataset, together with some additional results of our method.</p><p>In Section B we describe details of our narration-based temporal constraints and the optimization procedure. Section C provides more information about our dataset and video collection procedure, including a complete list of primary and related tasks and task-wise statistics. In Section D we illustrate additional quantitative and qualitative results of our method, including classifier scores and localized steps. We also provide more examples and analysis of failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modeling instructional videos B.1. Temporal text localization</head><p>In this section we explain in detail how we obtain temporal constraints from subtitles of the video. We assume that each step in the video occurs roughly at the same time as it is mentioned in the narration.</p><p>Step localization in narrations is challenging for several reasons. First, the same step may be described in different ways (e.g. cut steak and slice meat). It may contain a reference (e.g. cut it). Second, a mention of a step doesn't guarantee, that the step occurs at the same time (e.g. Let the steak rest before cutting it). Since most of the videos in our dataset are unprofessional, the narrator doesn't usually follow a strict scenario and often talks about unrelated topics. Finally, most of the subtitles are produced by YouTube automatic speech recognition, and, therefore, contain errors and lack punctuation.</p><p>As described in Section 5.1 of the main paper, we provide a short textual description of each step. These descriptions are matched to the text within a sliding window over the subtitles, in order to find where each step is mentioned. More formally, let f be a function, mapping a sequence of words of variable length into R D . Applying this function to the text within a sliding window of size w yields a matrix U ∈ R L×D , where L is the number of words in the subtitles. Applying the same function to the description of each step gives us a matrix V ∈ R K×D , where K is a total number of steps. Assuming that We find the best matching A ∈ {0, 1} L×K between the steps and the subtitles, that satisfies the ordering of the steps, by solving a linear problem</p><formula xml:id="formula_4">min A∈A l,k S l,k A l,k<label>(4)</label></formula><p>where A is a set of assignments that satisfy at-least-one and ordering constraints. This problem can be efficiently solved via dynamic programming, as described in Section B.2. Imposing the ordering constraints during step localization helps to avoid spurious mentions of steps, that don't follow the scenario of a task. We try different choices of mapping f . The first is TF-IDF representation of the text within sliding window. The second is a Word2Vec-like word embedding <ref type="bibr" target="#b37">[37]</ref>, followed by a max-pooling over sliding window. We obtain our word embedding by training the Fasttext model <ref type="bibr" target="#b38">[38]</ref> with dimension 100. The model is trained on a corpus of subtitles of 2 million YouTube videos for 6729 tasks from wikiHow.</p><p>Finally, we propose a way to learn a better aggregation function than max-pooling for the word vectors. Assume that we are given a set of sentences I of various lengths. Each sentence i is represented by X i ∈ R Mi×d0 , where M i is the number of words in a sentence and X i m ∈ R d0 is the feature vector corresponding to m-th word in the sentence. Assume that for each sentence i we are also given a set of sentences S i ⊂ I with similar meaning and a set of sentences D i ⊂ I with different meaning. f is learnt by minimizing loss</p><formula xml:id="formula_5">i∈I 1 |S i | j∈Si,k∈Di max[0, sim(f (X i ), f (X k )) − sim(f (X i ), f (X j )) + h],<label>(5)</label></formula><p>where sim(a, b) = a T b a b is the cosine similarity function, and, h is the margin constant. This can be understood as pushing representations of sentences with similar meaning closer together, as opposed to sentences with different meaning. We take f in the form of 1D convolution with kernel length 1 and a number of filters d, followed by the global max-pooling, and by a linear mapping R d → R d . We take d = 300 and h = 0.1.</p><p>We train our model on the set of sentences from wiki-How. Descriptions of the tasks on wikiHow are organized into step paragraphs, as shown on figure 1. We assume that sentences within the same paragraph describe similar concepts, while sentences from different steps of the same task have different meanings. For each sentence i, S i is defined as a set of sentences from the same paragraph, and D i is defined as a set of sentences from other paragraphs within the same wikiHow page.</p><p>We evaluate alternative text representations by comparing obtained constraints with the ground truth on the set of primary tasks. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Our aggregation function, trained on wikiHow outperforms TF-IDF Find a flat, stable and safe place to change your tire. You should have a solid, level surface that will restrict the car from rolling. If you are near a road, park as far from traffic as possible and turn on your emergency flashers (hazard lights). Avoid soft ground and hills.</p><p>Apply the parking brake and put car into "Park" position. If you have a standard transmission, put your vehicle in first or reverse.</p><p>Place a heavy object (e.g., rock, concrete, spare wheel, etc.) in front of the front and back tires. Place a heavy object (e.g., rock, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence representation</head><p>Embedding space <ref type="figure">Figure 1</ref>. Example of three wikiHow steps for the Change a Tire task. Our method learns a similarity function that pulls representations of the sentences from the same paragraphs closer together, and pushes the sentences from different paragraphs away from each other and max-pooled word vectors both in terms of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Constrained linear optimization</head><p>Our optimization procedure, inference and temporal text localization require solving a linear problem of the form</p><formula xml:id="formula_6">min Y ∈C t,k S tk Y tk ,<label>(6)</label></formula><p>where S ∈ R T ×K and C is the set of all assignments from {0, 1} T ×K that satisfy the ordering and at-least-once constraints. At-least-once constraints mean that every step k should be picked at least once: t Y t,k ≥ 1 for any k = 1 . . . K. Ordering constraints mean that the step k − 1 should precede step k for any k ≥ 2. This problem can be solved efficiently via dynamic programming. First, we rewrite the problem in the form:</p><formula xml:id="formula_7">min y∈C t S tyt ,<label>(7)</label></formula><p>where y t ∈ {0, 1, . . . , K} is the step label at time t (0 stands for background, i.e. when no step is selected). The ordering constraints impose that y t+1 ∈ {0, z t }, where z t = max(y 1 , . . . , y t ) is the last non-background step. We define state x t at time t as a pair (y t , z t ). Note, that for a given state x t , the only possible x t−1 that satisfies the constraints are (z t , z t ), (z t − 1, z t − 1) and (0, z t − 1) if y t = 0, and (z t − 1, z t − 1) and (0, z t − 1) otherwise. We denote this set of possible previous states as P(x t ). The minimum cumulative cost for state</p><formula xml:id="formula_8">x t = x at time t is V (x, t) = min x1,...,xt−1 | xt=x t τ =1 S τ yτ .<label>(8)</label></formula><p>Define C(x t , x t−1 ) = S tyt if x t−1 ∈ P(x t ) and C(x t , x t−1 ) = +∞ otherwise (for simplicity we denote x 0 = (0, 0)). This allows to rewrite <ref type="bibr" target="#b7">(8)</ref> in the recursive form:</p><formula xml:id="formula_9">V (x, t) = min x (C(x, x ) + V (x , t − 1)).<label>(9)</label></formula><p>We compute V (x, t) recursively for t = 1, . . . , T , using <ref type="bibr" target="#b8">(9)</ref>. In practice, computing V (x, t) given V (x , t − 1) for all x requires minimization only over x ∈ P(x) and can be done in O(1). Since there are 2K possible states, the complexity of computing V (x, t) for all x and t is O(KT ). To satisfy at-least-once constraints, the final state x T must be either (K, K), or (0, K). To get the optimal assignment, we take x * T = arg min</p><formula xml:id="formula_10">x∈{(K,K),(0,K)} V (x, T )</formula><p>and find x * t = arg min</p><formula xml:id="formula_11">x∈P(x * t+1 )</formula><p>V (x, t − 1) recursively for every t = T − 1, . . . , 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Optimization for discriminative clustering</head><p>The discriminative clustering problem</p><formula xml:id="formula_12">min Y ∈C,F ∈F − t,k Y t,k log( exp(f k (x t )) k exp(f k (x t )) ),<label>(10)</label></formula><p>introduced in Section 4.2 of the main paper can't be solved efficiently with standard techniques, such as projected gradient descent, because the projection over our constraint set C is computationally expensive.  Our optimization method can be applied to a broader class of problems of the form</p><formula xml:id="formula_13">min Y ∈C,θ∈R m tk Y tk F tk (θ).<label>(11)</label></formula><p>Given solution (Y l , θ l ) at l-th iteration, we define a quadratic upper bound for F (θ) in the neighbourhood of θ l : F tk (θ) ≤F tk (θ; θ l ), wherẽ</p><formula xml:id="formula_14">F tk (θ; θ l ) = F tk (θ l ) + ∇F tk (θ l )(θ − θ l ) + 1 2δK ||θ − θ l || 2 . (12) Note, that t,k Y tk F tk (θ) ≤ t,k Y tkFtk (θ; θ l ) for any (Y, θ) and that t,k Y l tk F tk (θ l ) = t,k Y l tkF tk (θ l ). This means, that for any (Y l+1 , θ l+1 ), s.t. t,k Y l+1 tkF tk (θ l+1 ) ≤ t,k</formula><p>Y l tkF tk (θ l ), the same inequality holds for F :</p><formula xml:id="formula_15">t,k Y l+1 tk F tk (θ l+1 ) ≤ t,k Y l tk F tk (θ l ).<label>(13)</label></formula><p>For the problem</p><formula xml:id="formula_16">min Y ∈C,θ∈R m t,k Y tkFtk (θ)<label>(14)</label></formula><p>it is possible to find a global minimum. The minimization with respect to θ yields</p><formula xml:id="formula_17">θ * (Y ) = θ l − δK t,k Y tk ∇F tk (θ l ) t,k Y tk .<label>(15)</label></formula><p>Since t,k Y tk = K, this expression can be simplified as</p><formula xml:id="formula_18">θ * (Y ) = θ l − δ t,k Y tk ∇F tk (θ l ).<label>(16)</label></formula><p>Substituting θ with θ * (Y ) in <ref type="bibr" target="#b13">(14)</ref> leads to the problem</p><formula xml:id="formula_19">min Y ∈C t,k [F tk (θ l ) − δ 2 ||∇F tk (θ l )|| 2 ]Y tk .<label>(17)</label></formula><p>This is a linear problem that can be solved as described in Section B.2. Denote Y * a solution of <ref type="bibr" target="#b16">(17)</ref>. We obtain θ * by substituting Y s with Y * in <ref type="bibr" target="#b15">(16)</ref>. The pair (Y * , θ * ) is a global minimum for <ref type="bibr" target="#b13">(14)</ref>. We take this pair as a new solution (Y l+1 , θ l+1 ).</p><p>The described optimization procedure can be seen as alternating between solving a linear problem <ref type="bibr" target="#b16">(17)</ref> for Y and a gradient descent step with the learning rate δ C. Dataset</p><formula xml:id="formula_20">θ l+1 = θ l − δ t,k Y l+1 tk ∇F tk (θ l ).<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Video collection</head><p>As described in Section 5.1 of the main paper, given a task, we collect top N videos from YouTube, by querying a title of the task. The choice of N relies on the following trade-off. Training the model on a large number of videos for a given task may lead to better performance. On the other hand, large N results in many videos being unrelated to the queried task, which may hurt the performance of the model. To investigate the influence of N on the purity of the data, we have annotated videos from YouTube search output as relevant or irrelevant to a task for all primary tasks. We define the average relevance as the ratio between the number of relevant videos and the total number of videos. <ref type="figure" target="#fig_5">Figure 3</ref> shows the average relevance for different values of N . The relevance rapidly decreases with N , making the data unusable without manual cleaning. For each related task we take top 30 videos from YouTube, which seems a reasonable compromise between the amount of data and the level of noise.</p><p>30 videos are clearly not enough to learn a task from scratch, as they feature only a few positive examples for each step in the task. Sharing knowledge across tasks as proposed in our paper is an essential mechanism to overcome this problem.</p><p>Since the videos are collected automatically for each task, they may be shared between tasks. The primary tasks have little in common and do not share any videos. The related tasks, however, may be similar to each other and to the primary tasks (e.g. Make Sourdough Pancakes and Make Pancakes). We found that the primary and related tasks share about 2.6% of videos. However, we stress the fact that such duplicate videos are provided with different supervision (different ordered list of steps) for each task. Our videos come from 3007 different YouTube channels, with 1.6 videos per channel on average. 70% of videos from primary tasks do not share channels with videos from related tasks. We, thus, conclude, that the difference between videos collected for primary and related tasks is sufficiently large, hence transferring related task models to primary tasks is not trivial. <ref type="figure" target="#fig_4">Figure 2</ref> shows all 83 primary and related tasks from our dataset. In order to illustrate the sharing between tasks, we define the distance between two tasks based on the number of common step components and make a 2D projection via t-SNE <ref type="bibr" target="#b39">[39]</ref>. The tasks mostly share within the same domain, forming different groups, such as Car Maintenance, Drinks and Woodworks. Car Maintenance and Home Woodwork tasks mostly share components within the same category, with some spurious sharing with cooking tasks (e.g. Pour and Oil components from Add Oil to Car task). This categories form two distinct clusters on the diagram. The rest of the tasks form a continuous Cooking cluster. <ref type="table" target="#tab_3">Table 2</ref> provides some statistics for the primary tasks. The order consistency, defined as in <ref type="bibr" target="#b36">[36]</ref>, shows how well the order is respected in the videos. For example, if the order of steps in a video is 2 → 1 → 3, the order consistency for this video would be equal to <ref type="bibr">2 3</ref> . The amount of background is defined as an average number of frames, which are not assigned to any step, divided by a total number of frames. The high amount of background (72% in average) motivates the use of methods, that aren't limited to a dense segmentation of a video and allow frames to remain unlabeled. The average order consistency is high (86%), thus justifying the use of hard ordering constraints. However, it varies across the tasks and has a relatively low value for some of the tasks (e.g. Make Kimchi Rice and Make Taco Salad). Similarly, the amount of missing steps is close to 50% for Grill Steak and Pickle Cucumber, making these tasks especially challenging for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Tasks and statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Comparison of evaluation metrics</head><p>At test time we predict one temporal unit per step and assume a correct detection if it falls within a ground truth interval for the corresponding step. This is motivated by the fact that in weakly supervised context, when no information about exact temporal extents of steps is given during the training, prediction of step time intervals is an ill-posed problem. Indeed, even people do not always agree on the action boundaries. Predicting punctual steps, defined as the most consistent and distinguishable frames in the videos, allows to avoid this problem. Although our model is trained for this punctual prediction, it may still be used to predict temporally extended steps, for example, by threshold-   <ref type="table" target="#tab_4">Table 3</ref> contains results, averaged over the primary tasks and compared to baselines. Here recall stands for the same evaluation procedure, as described in the main paper (global non-maximal suppression + recall). Note that both proposed ways of evaluation yield highly correlated results with recall roughly equal to mAP × 2. The only exception is Richard'18 that under-performs in the case of recall. This may be caused by the fact that, unlike other methods in <ref type="table" target="#tab_4">Table 3</ref>, it is trained to predict step intervals, and not punctual steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Importance of temporal text constraints</head><p>Temporal constraints, obtained from narration, provide a very noisy supervision. In case of our primary tasks, the intersection over union between the ground truth of the steps and the corresponding temporal text constraints is only 7.9%. Moreover, 61% of ground truth steps lie entirely outside of the constraint intervals. This means that our method is unable to assign a step to a correct frame even with a perfect classifier, if it is forced to satisfy these constraints. This is likely to be a disadvantage during training which tries to fit the step model to incorrect temporal inter-vals. Could it be better to learn our model without temporal constraints? We answer this question by training and evaluating our solution in the same setup as before, but without text constraints at the training time. The resulting recall is 17%, compared to 22.4%, when training with text constraints. This gain of 5.4% shows the importance of text guidance during training even at the presence of the high level of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Additional qualitative results</head><p>In this section we show some additional qualitative examples to provide a better intuition into our data and the method. <ref type="figure">Figures 4-9</ref> shows the outputs of our model for videos for several tasks. We show the outputs of classifiers for each step at each frame of the video, as well as the inferred solution and compare it with the ground truth. <ref type="figure">Figure  4</ref> illustrates two kinds of error, caused by our assumptions. First,the step Whisk Mixture is localized in the area of low confidence for this step. This is caused by the next step, Pouring Egg, that precedes Whisking Mixture in this particular video. Second, false detection for Topping Toast is due to the absence of this step in the video, while we assume that every step is present. Note that although step Top Toast doesn't appear in the video, the classifier puts high and well localized scores in the end of the video. This is because Making French Toastdefine positive as falling inside a ground truth interval for that step videos usually end with a demonstration of a final product on a plate. The model captures this visual consistency between the videos and takes it for the final step. <ref type="bibr">[s]</ref>   <ref type="bibr">[s]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAKE FRENCH TOAST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAKE STRAWBERRY CAKE</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 7 .</head><label>7</label><figDesc>Recall while transferring a learned model to unseen tasks as a function of the number of tasks used for training. Our component model approaches training directly on these tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 kd = 1</head><label>21</label><figDesc>for any k = 1 . . . K (i.e., the features are unit-norm), S = U V T ∈ R L×K is a matrix of cosine similarities between vector representations of subtitles and descriptions of steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 2 3 /</head><label>13</label><figDesc>How to Change a Tire Sentences Find a flat, stable and safe place ... You should have a solid, level ... If you are near a road, park as ... Avoid soft ground and hills Apply the parking brake and put ... If you have a standard ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>A t-SNE visualization of primary (in blue) and related (in green) tasks. The distance between two tasks is based on the number of components they share. Two well separable clusters on top correspond to Car Maintenance and Home Repairs categories, while most of the tasks belong to the Cooking category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Average relevance of videos as a function of the number of videos collected from YouTube. Taking top 30 videos per task results in 56% relevant videos. Attempting to collect more videos results in a noisy dataset with many irrelevant videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Example of obtained solution for Make French Toast task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red. Ground truth intervals for the steps are shown in yellow. Failure cases include false localization due to the ordering constraints (Pour milk, Whisk mixture and Dip bread) and due to a missing step (Top toast). Example of obtained solution for Build Floating Shelves task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red. Ground truth intervals for the steps are shown in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Example of obtained solution for Make Strawberry Cake task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red. Ground truth intervals for the steps are shown in yellow. Example of obtained solution for Make Irish Coffee task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red. Ground truth intervals for the steps are shown in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>| pour | milk | whisk | mixture | egg | spread | ... pour milk?</head><label></label><figDesc>Makepancakes Make meringue ..., pour milk, ..., whisk mixture, ... pour egg, ..., spread mixture, ...</figDesc><table><row><cell>...</cell><cell></cell><cell></cell></row><row><cell>Video</cell><cell></cell><cell></cell></row><row><cell cols="2">Alternate Optimization</cell><cell>U pdate F</cell><cell>Y U pdate</cell></row><row><cell>Constraints</cell><cell></cell><cell></cell><cell>Time</cell></row><row><cell>Narration</cell><cell cols="3">"[...] now I'm gonna pour some milk into the bowl and [...]"</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Num.</cell><cell cols="2">Total Num. Not only</cell><cell>Avail.</cell></row><row><cell></cell><cell>Vids</cell><cell cols="2">Length Tasks Cooking</cell><cell>Annots</cell></row><row><cell>[2]</cell><cell>150</cell><cell>7h</cell><cell>5</cell><cell>Windows</cell></row><row><cell>[29]</cell><cell cols="2">1.2K+85 100h</cell><cell>17</cell><cell>Windows</cell></row><row><cell>[35]</cell><cell>2K</cell><cell>176h</cell><cell>89</cell><cell>Windows</cell></row><row><cell>[24]</cell><cell>180K</cell><cell>3,000h</cell><cell></cell><cell>Recipes</cell></row><row><cell>CrossTask</cell><cell>4.7K</cell><cell>376h</cell><cell>83</cell><cell>Windows</cell></row></table><note>A comparison of CrossTask with existing instructional datasets. Our dataset is both large and more diverse while also having temporal annotations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.Failure Modes Predictions on Unseen Data</head><label></label><figDesc>Dataset. This results in a dataset containing 2750 videos Predictions on unseen data as well as typical failure modes. Our method does well on steps with distinctive motions and appearances. Failure modes include (top) features that cannot make fine-grained distinctions between e.g., egg and vanilla extract; and (bottom) models that overreact to particular nouns, preferring a more visible lemon over a less visible lemon actually being squeezed.</figDesc><table><row><cell cols="2">Build Shelves</cell><cell>Cut Shelf</cell><cell>Assemble Shelf</cell><cell>Sand Shelf</cell><cell>Paint Shelf</cell><cell>Attach Shelf</cell><cell>Make French Toast</cell><cell>Whisk Mixture</cell><cell>Pour Egg</cell></row><row><cell>Make Banana</cell><cell>Ice Cream</cell><cell>Peel Banana</cell><cell>Cut Banana</cell><cell>Put Bananas in Blender</cell><cell>Pour Milk</cell><cell>Mix Ingredients</cell><cell>Make Lemonade</cell><cell>Cut Lemon</cell><cell>Squeeze Lemon</cell></row><row><cell cols="6">Figure 4. of 18 primary tasks comprising 212 hours of video; and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">1950 videos of 65 related tasks comprising 161 hours of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">video. We contrast this dataset with past instructional video</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">datasets in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Weakly supervised recall scores on test set (in %). Our approach, which shares information across tasks, substantially and consistently outperforms non-sharing baselines. The standard deviation for reported scores does not exceed 1%.<ref type="bibr" target="#b24">25</ref>.3 38.0 37.5 25.7 28.2 54.3 25.8 18.3 31.2 47.7 12.0 39.5 23.4 30.9 41.1 53.4 17.3 31.6 Based 13.2 17.6 19.3 19.3 9.7 12.6 30.4 16.0 4.5 19.0 29.0 9.1 29.1 14.5 22.9 29.0 32.9 7.3 18.6 Proposed 13.3 18.0 23.4 23.1 16.9 16.5 30.7 21.6 4.6 19.5 35.3 10.0 32.3 13.8 29.5 37.6 43.0 13.3 22.4</figDesc><table><row><cell></cell><cell>Make</cell><cell>Kimchi Rice</cell><cell>Pickle</cell><cell>Cucumber</cell><cell>Make Banana</cell><cell>Ice Cream</cell><cell>Grill</cell><cell>Steak</cell><cell>Jack Up</cell><cell>Car</cell><cell>Make</cell><cell>Jello Shots</cell><cell>Change</cell><cell>Tire</cell><cell>Make</cell><cell>Lemonade</cell><cell>Add Oil</cell><cell>to Car</cell><cell>Make</cell><cell>Latte</cell><cell>Build</cell><cell>Shelves</cell><cell>Make</cell><cell>Taco Salad</cell><cell>Make</cell><cell>French Toast</cell><cell>Make</cell><cell>Irish Coffee</cell><cell>Make</cell><cell>Strawberry Cake</cell><cell>Make</cell><cell>Pancakes</cell><cell>Make</cell><cell>Meringue</cell><cell>Make</cell><cell>Fish Curry</cell><cell>Average</cell></row><row><cell cols="37">Supervised 19.1 Uniform 4.2 7.1 6.4 7.3 17.4 7.1 14.2 9.8 3.1 10.7 22.1 5.5 9.5 7.5 9.2 9.2 19.5 5.1</cell><cell>9.7</cell></row><row><cell>Alayrac'16 [1]</cell><cell cols="37">15.6 10.6 7.5 14.2 9.3 11.8 17.3 13.1 6.4 12.9 27.2 9.2 15.7 8.6 16.3 13.0 23.2 7.4 13.3</cell></row><row><cell>Richard'18 [27]</cell><cell cols="36">7.6 4.3 3.6 4.6 8.9 5.4 7.5 7.3 3.6 6.2 12.3 3.8 7.4 7.2 6.7 9.6 12.3 3.1</cell><cell>6.7</cell></row><row><cell>Task-Specific Step-Gain from Sharing</cell><cell cols="36">0.2 0.4 4.1 3.8 7.2 3.9 0.3 5.6 0.1 0.6 6.3 0.9 3.2 -0.7 6.6 8.7 10.1 6.0</cell><cell>3.7</cell></row><row><cell>Predicted</cell><cell cols="6">Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Make Lemonade</cell><cell cols="5">Make Lemonade</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stir Mixture</cell><cell></cell><cell cols="4">Stir Mixture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stir</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Make Shelves</cell><cell cols="5">Make Shelves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cut Shelf</cell><cell></cell><cell cols="3">Cut Shelf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cut</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Average recall scores on the test set for our method when changing the sharing settings and the model.</figDesc><table><row><cell></cell><cell>Unshared</cell><cell>Shared</cell><cell>Shared Primary</cell></row><row><cell></cell><cell>Primary</cell><cell>Primary</cell><cell>+ Related</cell></row><row><cell>Step-based</cell><cell>18.6</cell><cell>18.9</cell><cell>19.8</cell></row><row><cell>Component-based</cell><cell>18.7</cell><cell>20.2</cell><cell>22.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Unseen Task: Make</cell></row><row><cell></cell><cell></cell><cell cols="2">French Strawberry Cake</cell></row><row><cell></cell><cell></cell><cell cols="2">Cut Strawberry</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Precision and recall of the constraints obtained with our method, averaged over 18 primary tasks.</figDesc><table><row><cell></cell><cell cols="2">Precision (%) Recall (%)</cell></row><row><cell>Max-pooled word vectors</cell><cell>11.6</cell><cell>10.4</cell></row><row><cell>TF-IDF</cell><cell>13.3</cell><cell>11.4</cell></row><row><cell>Proposed</cell><cell>15.9</cell><cell>13.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Statistics for primary tasks. Task Number of videos Number of steps Average length Missing steps Background Order consistency</figDesc><table><row><cell>Make Kimchi Rice</cell><cell>120</cell><cell>6</cell><cell>4:47</cell><cell>21%</cell><cell>70%</cell><cell>0,69</cell></row><row><cell>Pickle Cucumber</cell><cell>106</cell><cell>11</cell><cell>5:35</cell><cell>48%</cell><cell>75%</cell><cell>0,85</cell></row><row><cell>Make Banana Ice Cream</cell><cell>170</cell><cell>5</cell><cell>4:04</cell><cell>38%</cell><cell>80%</cell><cell>0,98</cell></row><row><cell>Grill Steak</cell><cell>228</cell><cell>11</cell><cell>5:26</cell><cell>46%</cell><cell>75%</cell><cell>0,95</cell></row><row><cell>Jack Up Car</cell><cell>89</cell><cell>3</cell><cell>4:13</cell><cell>39%</cell><cell>81%</cell><cell>1,00</cell></row><row><cell>Make Jello Shots</cell><cell>182</cell><cell>6</cell><cell>4:15</cell><cell>21%</cell><cell>72%</cell><cell>0,87</cell></row><row><cell>Change Tire</cell><cell>99</cell><cell>11</cell><cell>4:52</cell><cell>27%</cell><cell>62%</cell><cell>0,97</cell></row><row><cell>Make Lemonade</cell><cell>131</cell><cell>8</cell><cell>3:44</cell><cell>28%</cell><cell>69%</cell><cell>0,80</cell></row><row><cell>Add Oil to Car</cell><cell>137</cell><cell>8</cell><cell>5:39</cell><cell>33%</cell><cell>85%</cell><cell>0,92</cell></row><row><cell>Make Latte</cell><cell>157</cell><cell>6</cell><cell>3:52</cell><cell>43%</cell><cell>71%</cell><cell>0,89</cell></row><row><cell>Build Floating Shelves</cell><cell>153</cell><cell>5</cell><cell>5:23</cell><cell>34%</cell><cell>58%</cell><cell>0,96</cell></row><row><cell>Make Taco Salad</cell><cell>170</cell><cell>8</cell><cell>4:44</cell><cell>41%</cell><cell>79%</cell><cell>0,66</cell></row><row><cell>Make French Toast</cell><cell>252</cell><cell>10</cell><cell>4:10</cell><cell>23%</cell><cell>68%</cell><cell>0,80</cell></row><row><cell>Make Irish Coffee</cell><cell>185</cell><cell>5</cell><cell>3:13</cell><cell>13%</cell><cell>74%</cell><cell>0,77</cell></row><row><cell>Make Strawberry Cake</cell><cell>86</cell><cell>9</cell><cell>5:36</cell><cell>25%</cell><cell>63%</cell><cell>0,82</cell></row><row><cell>Make Pancakes</cell><cell>182</cell><cell>8</cell><cell>4:34</cell><cell>19%</cell><cell>70%</cell><cell>0,89</cell></row><row><cell>Make Meringue</cell><cell>154</cell><cell>6</cell><cell>4:42</cell><cell>23%</cell><cell>67%</cell><cell>0,98</cell></row><row><cell>Make Fish Curry</cell><cell>149</cell><cell>7</cell><cell>5:31</cell><cell>25%</cell><cell>69%</cell><cell>0,74</cell></row><row><cell>Average</cell><cell>153</cell><cell>7</cell><cell>4:57</cell><cell>31%</cell><cell>72%</cell><cell>0,86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Results of cross-task learning, evaluated with mAP and recall metrics and averaged over primary tasks. Standard deviation does not exceed 0.3% for mAP and 1% for recall. s confidence of each step for each frame. Does this yield reasonable predictions? To answer this question we evaluate our model, using mAP metric.</figDesc><table><row><cell cols="4">Metric Random Richard'18 Alayrac'16</cell><cell>Ours (no sharing)</cell><cell>Ours (with sharing)</cell></row><row><cell>Recall</cell><cell>8.27</cell><cell>6.7</cell><cell>13.3</cell><cell>18.6</cell><cell>22.4</cell></row><row><cell>mAP</cell><cell>4.3</cell><cell>5.5</cell><cell>6.9</cell><cell>8.9</cell><cell>11.0</cell></row><row><cell cols="2">ing model'</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/DmZhukov/CrossTask</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cut strawberries (cake) <ref type="figure">Figure 10</ref>. Erroneous predictions, involving wrong objects and actions. Correct object/action is in green. Our method is not capable of distinguishing particular kinds of objects, especially liquids and powders, due to the nature of the features. Examples for the wrong action components show that in many cases the method captures a static context in which object occurs, rather than performed action.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint discovery of object states and manipulation actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DIFFRAC: A discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The EPIC-KITCHENS dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<editor>BMVA</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Demo2vec: Reasoning object affordances from online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised visual-linguistic reference resolution in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding &quot;it&quot;: Weakly-supervised reference-aware visual grounding in instructional video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVIU</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From Red Wine to Red Tomato: Composition with Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning and segmentation of complex activities from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Commonly uncommon: Semantic sparsity in situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chenliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Example of obtained solution for Change a Tire task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red</title>
		<imprint/>
	</monogr>
	<note>Figure 8. Ground truth intervals for the steps are shown in yellow</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
	<note>XX</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
