<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Pointer Network for Fast Extraction of Multiple Relations in a Sentence 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongsik</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kangwon National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harksoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Konkuk University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Pointer Network for Fast Extraction of Multiple Relations in a Sentence 1</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Appl. Sci</title>
						<imprint>
							<biblScope unit="volume">10</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date; Accepted: date; Published: date Featured Application: Ontology construction module for AI applications</note>
					<note>Article</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Relation extraction</term>
					<term>dual pointer network</term>
					<term>context-to-entity attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction is a type of information extraction task that recognizes semantic relationships between entities in a sentence. Many previous studies have focused on extracting only one semantic relation between two entities in a single sentence. However, multiple entities in a sentence are associated through various relations. To address this issue, we propose a relation extraction model based on a dual pointer network with a multi-head attention mechanism. The proposed model finds n-to-1 subject-object relations using a forward object decoder. Then, it finds 1-to-n subject-object relations using a backward subject decoder. Our experiments confirmed that the proposed model outperformed previous models, with an F1-score of 80.8% for the ACE-2005 corpus and an F1-score of 78.3% for the NYT corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relation extraction is a task that involves recognizing semantic relations (i.e., tuple structures; [subject, relation, object triples]) among entities in a sentence <ref type="bibr" target="#b0">[1]</ref>. Zeng et al. <ref type="bibr" target="#b1">[2]</ref> divided sentences into three types according to the triplet overlap degree: normal, entity pair overlap (EPO), and single entity overlap (SEO). In the normal type, the triples do not have overlapped entities; in the EPO type, some triples have an overlapped entity pair; and in the SEO type, some triplets have an overlapped entity, but these triplets do not have overlapped entity pairs. In this study, we focus on promptly extracting both the normal and SEO types because most relations are included in these types, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref>, [Lee, employed, ABC Mart], <ref type="bibr">[Lee, Family, his Father]</ref> and [his Father, Owner, ABC Mart] are SEO types. To promptly extract these relations, we adopt the concept of dependency parsing in which dependent words point to the head words by scanning each word in a sentence. We propose a This paper is an extended version of "Relation Extraction among Multiple Entities using a Dual Pointer Network with a Multi-Head Attention Mechanism" in Proceedings of the Second Workshop on Fact Extraction and VERification. dual pointer network model to efficiently extract multiple relations from a sentence through forward scanning (i.e., scanning from the first word to the last) and backward scanning (i.e., scanning from the last word to the first). The proposed model discovers an object of the current subject during forward scanning. Through forward scanning, all normal type relations can be found. However, SEO type relations are only partially found because a subject should point to only one object in the pointer network architecture. To address this limitation, the proposed model performs backward scanning to identify a subject of the current object.</p><p>The remainder of this paper is organized as follows. In Section 2, we review previous studies on relation extraction. Section 3 describes the proposed dual pointer network model. In Section 4, we elaborate on the experimental setup and results. Finally, we conclude the study in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Works</head><p>With the significant success of deep neural networks in the field of natural language processing, many researchers have proposed various relation extraction models based on convolutional neural networks (CNNs). These include the CNN model based on max-pooling <ref type="bibr" target="#b2">[3]</ref>, the CNN model based on multiple sizes of kernels <ref type="bibr" target="#b3">[4]</ref>, the combined CNN model <ref type="bibr" target="#b4">[5]</ref>, and the contextualized graph convolutional network (C-GCN) model <ref type="bibr" target="#b5">[6]</ref>. Relation extraction models based on recurrent neural network (RNNs) have also been proposed, including the long-short term memory (LSTM) model based on the dependency tree <ref type="bibr" target="#b6">[7]</ref> and the LSTM model using the position-aware attention technique <ref type="bibr" target="#b7">[8]</ref>. These models have focused on normal type extraction (i.e., extracting only one relation between two entities from a single sentence). However, many entities in a single sentence can form multiple relations. To resolve this problem, some studies have proposed multiple relation extraction. For example, Luan et al. <ref type="bibr" target="#b8">[9]</ref> treated triples in sentences as a graph and proposed a multiple relations extraction model that iteratively extracts spans between triples in the graph. In the present study, we propose a relation extraction model to simultaneously find all possible relations among multiple entities in a sentence. The proposed model is based on the pointer network <ref type="bibr" target="#b9">[10]</ref>. The pointer network is a sequence-to-sequence (Seq2Seq) model in which an attention mechanism <ref type="bibr" target="#b10">[11]</ref> is modified to learn the conditional probability of an output, whose values correspond to positions in a given input sequence. We modify the pointer network to include dual decoders, an object decoder (a forward decoder) and a subject decoder (a backward decoder). The object decoder extracts n-to-1 relations as shown in the following example: [Lee, employed, ABC Mart] and [his Father, Owner, ABC Mart] are extracted from the sentence. The subject decoder extracts 1-to-n relations as shown in the following example: [Lee, employed, ABC Mart] and [Lee, Family, his Father] are extracted from the sentence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dual Pointer Network Model for Relation Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Context and Entity Encoder</head><p>The context and entity encoder computes the degree of association between words and entities in a given sentence. For example, { 1 , 2 , …, } and { 1 , 2 , …, } refer to word and entity embedding vectors, respectively. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the process of word and entity embedding. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, the word embedding vectors are concatenations of two types of embeddings: word-level GloVe <ref type="bibr" target="#b11">[12]</ref> embeddings for representing the meaning of words and character-level CNN embeddings <ref type="bibr" target="#b12">[13]</ref> for addressing out-of-vocabulary problems. The entity embedding vectors are concatenations of three types of embeddings: word-level CNN embedding for representing the meaning of entities composed of multiple words, character-level CNN embedding for addressing out-of-vocabulary problems, and entity type embedding for representing the categorical information of input entities. Each word in the word-level CNN embedding is represented by word-level GloVe embeddings. The word embedding vectors are used as input for a bidirectional LSTM network to obtain contextual information as follows:</p><formula xml:id="formula_0">⃗ = LSTM( , ⃗ −1 ), ⃖ = LSTM( , ⃖ −1 ), = [ ⃗ ; ⃖ ],<label>(1)</label></formula><p>where is an embedding vector of the i-th word in a sentence, and [ ⃗ ; ⃖ ] is a concatenation of ⃗ and ⃖ that represents the output vectors of a forward LSTM and a backward LSTM, respectively. The entity embedding vectors are used as input for a forward LSTM network because the entities are listed in the order they appear in a sentence, as shown below.</p><formula xml:id="formula_1">= LSTM( , −1 ),<label>(2)</label></formula><p>where is an embedding vector of the t-th one among all entities occurring in a sentence, and is an output vector encoded by a forward LSTM. The output vectors of the bidirectional LSTM network { 1 , 2 , … , } and the forward LSTM network { 1 , 2 , … , } are used as input for the context-to-entity attention layer (as shown in <ref type="figure" target="#fig_1">Figure 2</ref>), to compute the relative degrees of association between words and entities. This is similar to the well-known multi-head attention mechanism <ref type="bibr" target="#b13">[14]</ref> as shown below.</p><formula xml:id="formula_2">= * ( , ) , = ( √ ), ℎ = , = relu( [ℎ 0 ; ℎ 1 ; ℎ 2 ; … ; ℎ ] + ),<label>(3)</label></formula><p>where the query is set to , the key and the value are set to C's. The query is split into vectors, where is the number of heads. The attention score is calculated by a scaled-dot product, where is a normalization factor. The context-to-entity layer output is determined through a fully-connected neural network (FNN) using a concatenation of heads as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual Pointer Network Decoder</head><p>In a pointer network, attentions show the position distributions of an encoding layer. Because an attention is highlighted at only one position, the pointer network has a structural limitation when one entity forms relations with several entities (for instance, "Lee" in <ref type="figure" target="#fig_0">Figure 1</ref>). The proposed model adopts a dual pointer network decoder (see <ref type="figure" target="#fig_1">Figure 2</ref>) to overcome this limitation. The first decoder, called an object decoder, learns the position distribution from subjects to objects as follows:</p><formula xml:id="formula_3">ℎ = [ ; ], = LSTM(ℎ , −1 ), = tanh( [O; ]), = softmax� �, ̂= argmax( ), ̂= argmax( tanh( [ ; ])),<label>(4)</label></formula><p>where ℎ is a concatenation of the entity embedding vector and the LSTM-encoded entity embedding vector , and the decoding vector (i.e., the t-th entity to determine its objects) is calculated by the forward LSTM. Then, is the position distribution based on the attention scores between and the other entities 1 , … , −1 , +1 , … , in the context-to-entity attention layer. ̂ and ̂ represent a position and a relation name of 's object, respectively. The weighting parameters , , , and are set during the training phase. Conversely, the second decoder, called a subject decoder, learns the position distribution from objects to subjects in the same manner as the object decoder, as shown below. </p><p>where ̂ and ̂ represent a position and a relation name of 's subject, respectively. In <ref type="figure" target="#fig_0">Figure 1</ref>, "Lee" should point to both "ABC mart" and "his father." This problem cannot be solved using the conventional forward decoder because it cannot point to multiple targets. However, the subject decoder (a backward decoder) resolves this problem, because "ABC mart" and "his father" can point to "Lee."</p><p>Additionally, we adopt a multi-head attention mechanism to improve the performance of the dual pointer network; this is shown in the following equation.</p><formula xml:id="formula_5">= * ( , ) , = ( √ ), ℎ = , ̂= argmax( 1 ∑ =0 ), ̂= argmax(relu( [ℎ 0 ; ℎ 1 ; ℎ 2 ; … ; ℎ ] + )),<label>(6)</label></formula><p>where the query is set to , the key and the value are set to 's. The position distribution ̂ is calculated by an average of multi-head attention vectors, and the relation name ̂ is determined through an FNN using a concatenation of heads as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation detail</head><p>The context and entity encoder comprised 256 hidden units in each layer, and the dual pointer network decoder comprised 512 hidden units. We adopted a 0.1 drop-out probability for all the LSTM cells. We used 8 heads, with 32 units per head, for the multi-head attention. The vocabulary size and word-embedding size was set to 16,925 and 300, respectively. The filter size of the CNNs for character and word embeddings were 3, 4, and 5. The total number of filters was 100. 50-dimensional random initialized vectors were used for the character and entity embeddings. A cross-entropy function was used as a cost function to maximize the log-probability as follows:</p><formula xml:id="formula_6">( , �) = − ∑ log( � ), = α 2 � ( , � ) + ( , � )� + (1−α) 2 ( ( ,̃) + ( ,̃)),<label>(7)</label></formula><p>where is the target answer, � is the score distribution of the model prediction, and is the number of target classes. The loss is calculated by the cross-entropy combination of all targets and predictions. The weighting factor α was experimentally set to 0.6 as a scalar value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Setting</head><p>We evaluated the proposed model using the following benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE-05 corpus [15]:</head><p>The ACE dataset includes seven major entity types and six major relation types. The ACE-05 corpus does not properly evaluate models that extract multiple triples from a sentence. Therefore, if some triples in the ACE-05 corpus share a sentence (i.e., some triples occur in the same sentence), the triples were merged, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>. As a result, we obtained a dataset annotated with multiple triples. We divided the new dataset into a training set (5,023 sentences), a development set (629 sentences), and a test set (627 sentences) by a ratio of 8:1:1.</p><p>NYT corpus <ref type="bibr" target="#b15">[16]</ref>. This is a news corpus sampled from news articles published in the NYT. The training data is automatically labeled using distant supervision. The NYT corpus was manually converted to a relation extraction dataset by Zheng et al. <ref type="bibr" target="#b16">[17]</ref>. We excluded sentences without relation facts from Zheng's corpus. Finally, we obtained 66,202 sentences in total. We used 59,581 sentences for training and 6,621 for testing. We adopted the standard micro precision, recall, and F1 score to evaluate the results:</p><formula xml:id="formula_7">Recall = # # ℎ Precision = # # ℎ F1 = 2 * * + (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>In the first experiment, we evaluated the effectiveness of the multi-head attention in the dual pointer network decoder; the results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The evaluation was performed using the ACE-05 corpus. In <ref type="table" target="#tab_0">Table 1</ref>, single-head refers to a conventional attention mechanism proposed by Bahdanau et al. <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the multi-head attention mechanism used in the proposed model demonstrated better performance than the single-head one. Then, using the ACE-05 corpus, we evaluated the effectiveness of multi-head attention in the context and entity encoder; the results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. In <ref type="table" target="#tab_1">Table 2</ref>, BIDAF <ref type="bibr" target="#b17">[18]</ref> refers to a machine reading and comprehension (MRC) model based on a co-attention mechanism between a query and a context. C2Q and Q2C are refer to mean context-to-query attention and query-to-context attention used in the BIDAF model, respectively. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the multi-head attention mechanism used in the proposed model showed the best F1-score.</p><p>In the second experiment, we compared the proposed model with previous state-of-the-art models. <ref type="table" target="#tab_2">Table 3</ref> compares the performance of the proposed model and with other models for the ACE-2005 dataset. In <ref type="table" target="#tab_2">Table 3</ref>, SPTree <ref type="bibr" target="#b5">[6]</ref> is a model that applies the dependency information between the entities. In FCM <ref type="bibr" target="#b18">[19]</ref>, handcrafted features are combined with word embeddings. DYGIE <ref type="bibr" target="#b8">[9]</ref> dynamically generates spans between entities and spans' representations. Span-Level <ref type="bibr" target="#b19">[20]</ref> jointly performs entity mention detection and relation extraction. HRCNN <ref type="bibr" target="#b20">[21]</ref> is a hybrid model of CNN, RNN, and FNN. Walk-Based <ref type="bibr" target="#b21">[22]</ref> is a graph-based neural network model. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the proposed model outperformed all models across all metrics. <ref type="table" target="#tab_3">Table 4</ref> compares the performance of the proposed model with existing models for the NYT Corpus. In <ref type="table" target="#tab_3">Table 4</ref>, NovelTag <ref type="bibr" target="#b16">[17]</ref> is an end-to-end model that extracts entities and their relations based on a novel tagging scheme designed for relation extraction. MultiDecoder <ref type="bibr" target="#b1">[2]</ref> is a Seq2Seq-based model that combines the entity and relation extraction using a decoder with copy mechanism. GraphRE <ref type="bibr" target="#b22">[23]</ref> is a joint model that extracts entities and their relation using graph convolutional networks (GCN) <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the proposed model outperformed all models. It is not reasonable to directly compare the proposed model with these models because it requires gold-labeled entities, while the other models automatically extract entities from sentences. Although direct comparison is unfair, the proposed model exhibited considerably better performance. If we adopt a state-of-the-art named entity tagger based on BERT <ref type="bibr" target="#b24">[25]</ref> with F1-scores of 0.9 or more, the proposed model is expected to show F1-scores of 0.662 or more based on simple multiplication. The cases where the proposed model incorrectly extracted relations were also grouped in <ref type="table" target="#tab_4">Table  5</ref>. Most incorrect predictions included cases where the decoders incorrectly pointed out subjects or objects, and these incorrect entities lead to incorrect relation names, as shown in the first and third sentences in <ref type="table" target="#tab_4">Table 5</ref>. In some cases, the decoder did not point out subjects or objects. As a result, any triples in a sentence were not omitted, as shown in the second sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a relation extraction model to find all possible relations among multiple entities in a sentence simultaneously. The proposed model is based on pointer networks with multi-head attention mechanisms. To extract all possible relations from a sentence, we modified a single decoder into a dual decoder. In the dual decoder, the object decoder extracts n-to-1 subject-object relations, and the subject decoder extracts 1-to-n subject-object relations. The results from the experiments with the ACE-05 corpus and the NYT corpus confirmed that the proposed model shows an improvement in performance. Our future work will focus on an end-to-end model that directly extracts entities and their relations. In addition, we will focus on a method for improving performance using a large-scale language model like BERT <ref type="bibr" target="#b24">[25]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Subject-relation-object triples in a sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of dual pointer networks for relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>illustrates the architecture of the proposed model. This consists of two parts, a context and entity encoder, and a dual pointer network decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Word embedding process (b) Entity embedding process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>ACE-2005 data preprocess.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Author Contributions:</head><label></label><figDesc>Conceptualization, H. Kim.; methodology, H. Kim.; software, S. Park.; validation, S. Park.; formal analysis, H. Kim.; investigation, H. Kim.; resources, S. Park.; data curation, S. Park.; writing-original draft preparation, S. Park.; writing-review and editing, H. Kim.; visualization, H. Kim.; supervision, H. Kim.; project administration, H. Kim.; funding acquisition, H. Kim. Funding: This work was supported by the Institute for Information &amp; communications Technology Planning &amp; Evaluation(IITP) grant funded by the Korea government (MSIT) (No. 2013-0-00131, Development of Knowledge Evolutionary WiseQA Platform Technology for Human Knowledge Augmented Services).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance for different attention mechanisms in the dual pointer network decoder</figDesc><table><row><cell>Model</cell><cell>Recall</cell><cell>Precision</cell><cell>F1</cell></row><row><cell>Single-head</cell><cell>0.800</cell><cell>0.759</cell><cell>0.779</cell></row><row><cell>Multi-head</cell><cell>0.832</cell><cell>0.787</cell><cell>0.808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance for different attention mechanisms in the context and entity encoder</figDesc><table><row><cell>Model</cell><cell>Recall</cell><cell>Precision</cell><cell>F1</cell></row><row><cell>BIDAF-C2Q</cell><cell>0.819</cell><cell>0.766</cell><cell>0.792</cell></row><row><cell>BIDAF-C2Q&amp;Q2C</cell><cell>0.821</cell><cell>0.792</cell><cell>0.806</cell></row><row><cell>Multi-head</cell><cell>0.832</cell><cell>0.787</cell><cell>0.808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on ACE-2005 dataset</figDesc><table><row><cell>Model</cell><cell>Recall</cell><cell>Precision</cell><cell>F1</cell></row><row><cell>SPTree</cell><cell>0.540</cell><cell>0.572</cell><cell>0.556</cell></row><row><cell>FCM</cell><cell>0.493</cell><cell>0.715</cell><cell>0.582</cell></row><row><cell>DYGIE</cell><cell>0.567</cell><cell>0.643</cell><cell>0.602</cell></row><row><cell>Span-Level</cell><cell>0.584</cell><cell>0.680</cell><cell>0.628</cell></row><row><cell>Walk-Based</cell><cell>0.595</cell><cell>0.697</cell><cell>0.642</cell></row><row><cell>HRCNN</cell><cell>-</cell><cell>-</cell><cell>0.741</cell></row><row><cell>Our model</cell><cell>0.832</cell><cell>0.787</cell><cell>0.808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance comparisons on NYT corpus</figDesc><table><row><cell>Model</cell><cell>Recall</cell><cell>Precision</cell><cell>F1</cell></row><row><cell>NovelTag</cell><cell>0.414</cell><cell>0.615</cell><cell>0.495</cell></row><row><cell>MultiDecoder</cell><cell>0.566</cell><cell>0.610</cell><cell>0.587</cell></row><row><cell>GraphRE</cell><cell>0.600</cell><cell>0.639</cell><cell>0.619</cell></row><row><cell>Our model</cell><cell>0.820</cell><cell>0.749</cell><cell>0.783</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Main reasons for errors (underline denotes incorrect results)</figDesc><table><row><cell>Input sentence</cell><cell>Correct relation</cell><cell>Predicted relation</cell></row><row><cell>Iraqi forces responded with</cell><cell>[Iraqi forces, Art, artillery]</cell><cell>[Iraqi forces, Part-whole, Iraqi]</cell></row><row><cell>artillery fire</cell><cell>[Iraqi forces, Gen-aff, Iraqi]</cell><cell>[Iraqi forces, Gen-aff, Iraqi]</cell></row><row><cell>It is the first time they have</cell><cell></cell><cell></cell></row><row><cell>had freedom of movement with cars and weapons since</cell><cell>[they, Art, cars] [they, Art, weapons]</cell><cell>[they, Art, cars]</cell></row><row><cell>the start of the intifada</cell><cell></cell><cell></cell></row><row><cell>It was in northern Iraq today that an eight artillery round hit the site occupied by Kurdish fighters near Chamchamal</cell><cell>[Kurdish fighters, Phys, the site] [the site, Phys, Chamchamal] [Kurdish, Gen-aff, Kurdish fighters] [the site, Part-whole, northern Iraq]</cell><cell>[Kurdish fighters, Phys, the site] [the site, Phys, Chamchamal] [the site, Part-whole, northern Iraq] [Kurdish fighters, Art, artillery]</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We especially thank the members of the NLP laboratory at Kangwon National University and Konkuk University for their technical support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extraction of Instances with Social Relations for Automatic Construction of a Social Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of KIISE : Computing Practices and Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="548" to="552" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pairwise Relation Classification with Mirror Instances and a Combined Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2366" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Positionaware Attention and Supervised Data Improve Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A General Framework for Information Extraction using Dynamic Span Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Named Entity Recognizer Using Gloval Vector and Convolutional Neural Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of KITI : Telecommunication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="30" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<ptr target="https://catalog.ldc.upenn.edu/LDC2006T06" />
		<title level="m">ACE 2005 Multilingual Training Corpus</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cotype</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International World Wide Web Conference</title>
		<meeting>International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics 2017</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics 2017</meeting>
		<imprint>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bi-Directional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved Relation Extraction with Feature-Rich Compositional Embedding Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Span-Level Model for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics 2019</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics 2019</meeting>
		<imprint>
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation Extraction using Hybrid Convolutional and Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="619" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Walk-based Model on Entity Graphs for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics 2019</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics 2019</meeting>
		<imprint>
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technolog</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics on Human Language Technolog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
