<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reformulating HOI Detection as Adaptive Set Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Zhiyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reformulating HOI Detection as Adaptive Set Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Determining which image regions to concentrate is critical for Human-Object Interaction (HOI) detection. Conventional HOI detectors focus on either detected human and object pairs or pre-defined interaction locations, which limits learning of the effective features. In this paper, we reformulate HOI detection as an adaptive set prediction problem, with this novel formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with parallel instance and interaction branches. To attain this, we map a trainable interaction query set to an interaction prediction set with transformer. Each query adaptively aggregates the interaction-relevant features from global contexts through multi-head co-attention. Besides, the training process is supervised adaptively by matching each ground-truth with the interaction prediction. Furthermore, we design an effective instance-aware attention module to introduce instructive features from the instance branch into the interaction branch. Our method outperforms previous stateof-the-art methods without any extra human pose and language features on three challenging HOI detection datasets. Especially, we achieve over 31% relative improvement on a large scale HICO-DET dataset. Code is available at https://github.com/yoyomimi/AS-Net. (a) Union boxes: verb "direct" in yellow, "drive" in purple, matched anchor in red.</p><p>(c) Our adaptive set prediction method: verb "drive" in purple, "direct" in yellow. Interaction vectors point from human centers to object centers. The features aggregated by queries are visualized at left. Adaptive Feature Aggregation Adaptive Set-based Ground-truth Matching drive direct Aggregated Features Ground-truth … … (b) Interaction midpoints: verb "direct" in yellow, "drive" in purple, matched point in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Query Set Interaction Prediction Set</head><p>pair-wise proposals. Two-stage methods have made great progress in HOI detection, however, their efficiency and effectiveness are limited by their serial architectures. With the development of one-stage object detectors, one-stage HOI detectors <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b25">27]</ref> have raised a new fashion. Existing onestage HOI detectors formulate HOI detection as a parallel detection problem, which detects the HOI triplets from an image directly. One-stage methods have delivered great improvements in both efficiency and effectiveness.</p><p>Determining which regions to concentrate on is critical and challenging for HOI detectors. To obtain essential features for interaction prediction, conventional two-arXiv:2103.05983v2 [cs.CV] 6 May 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection aims to identify HOI triplets &lt;human, verb, object&gt; from a given image, it is an important step toward the high-level semantic understanding <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b42">44]</ref>. Conventional HOI methods can be divided into two-stage methods <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b33">35]</ref> and one-stage methods <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b25">27]</ref>. Most two-stage methods detect instances (humans and objects), and match the detected humans and objects one by one to form pair-wise proposals in the first stage. Next, in the second stage, such methods infer the interactions based on the features of cropped human-object <ref type="figure">Figure 1</ref>. Both the anchor-based (a) and point-based (b) one-stage methods infer two different interactions "drive" and "direct" are at similar location and concentrate on the similar features. Our set prediction method (c) maps an interaction query set to an interaction prediction set by an interaction decoder. Then, interaction predictions are adaptively matched with ground-truth. To attain this, we first train a set of learnable embeddings as an interaction query set. Next, each interaction query adaptively aggregates the interaction-relevant features by co-attention. Finally, we match each ground-truth with prediction for adaptive supervision. This mechanism empowers our method to accurately predict two interactions for "drive" and "direct". Best viewed in color. stage methods usually involve extra features, e.g., human pose <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b12">14]</ref> and language <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b19">21]</ref>. However, even with extra features, two-stage methods still focus on the detected instances that might be inaccurate, which are less adaptive and limited by the detected instances. Onestage methods partially alleviate these issues by inferring interactions directly from the whole image. Such methods intuitively define a location-relative medium to predict interactions, and can be mainly divided into anchor-based methods and point-based methods. Anchor-based methods <ref type="bibr" target="#b18">[20]</ref> predict the interactions based on the union box of each pair-wise human and object instances. While pointbased methods <ref type="bibr" target="#b25">[27]</ref> infer the interaction midpoint of each corresponding human-object pair. However, we argue that it is sub-optimal to predict the interaction through a predefined interaction location. <ref type="figure">Figure 1</ref> illustrates an example. The interaction "direct" (in yellow) and "drive" (in purple) are quite different and thus require different visual features for interaction prediction. However, their union boxes are considerably overlapped <ref type="figure">(Figure 1 (a)</ref>), and their interaction midpoints are very close <ref type="figure">(Figure 1 (b)</ref>). Therefore, these one-stage methods concentrate on similar visual features for the two different interactions.</p><p>To further address the limitation of interaction location in one-stage methods, we reformulate interaction detection as a set-based prediction problem. We define an interaction query set with several learnable embeddings, and an interaction prediction set. Each embedding in the query set is mapped by a transformer based interaction decoder to an interaction prediction set. By feeding the interaction query set into a multi-head co-attention module, we are able to adaptively aggregate features from global contexts. Our proposed method matches each ground-truth with the resembling interaction prediction for adaptive supervision. Therefore, our proposed method adaptively concentrates on the most suitable features for each prediction, free from the location limitation of conventional one-stage methods. As demonstrated in <ref type="figure">Figure 1</ref> (c), our method aggregates arm features of the left person and pose features of the right person to make two different interaction predictions. The predictions are then matched with the ground-truth interaction "direct" and "drive" respectively.</p><p>To this end, we propose a novel Adaptive Set-based onestage framework, namely AS-Net. Our AS-Net consists of two parallel branches: an instance branch and an interaction branch. Both branches leverage a transformer encoderdecoder structure, which utilize global features to perform set predictions. The instance branch predicts location and category for each instance, while the interaction branch predicts interaction vectors and their corresponding categories. The interaction vectors point from the centers of the human instances to the centers of the object instances. We obtain the predicted interaction triplets by matching each interac-tion vector from the interaction branch with the detected instances from the instance branch. Besides, we exploit an instance-aware attention module in a co-attention manner to perform branch aggregation. Specifically, this module aggregates information in the instance branch and introduces the aggregated features into the interaction branch. We also utilize semantic embeddings to perform more accurate human-object matching.</p><p>We test our proposed AS-Net on three datasets, i.e., HICO-Det <ref type="bibr" target="#b32">[34]</ref>, V-COCO <ref type="bibr" target="#b11">[13]</ref>, and HOI-A <ref type="bibr" target="#b25">[27]</ref>. Our proposed AS-Net outperforms all the other algorithms among all datasets. In specific, our proposed AS-Net has gained 31% relative improvement comparing to the previous stateof-the-art one-stage method <ref type="bibr" target="#b25">[27]</ref> on HICO-DET.</p><p>Our contributions can be concluded in the following three aspects:</p><p>• We formulate HOI detection as a set prediction problem, which breaks the instance-centric limitation and location limitation of the existing methods. Thereby, our method can adaptively concentrate on the most suitable features to improve the predicting accuracy.</p><p>• We propose a novel one-stage transformer-based HOI detection framework, namely AS-Net. We also design an instance-aware attention module to introduce the information in the instance branch into the interaction branch.</p><p>• Without introducing any extra features, our method outperforms all the previous state-of-the-art methods, achieving 31% relative improvement over the second best one-stage method on the HICO-DET dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage Methods. Most conventional HOI detectors are in a two-stage manner. In the first stage, an object detector <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b2">4]</ref> is applied to detect the instances. In the second stage, the cropped instance features are classified to obtain the interaction categories. In addition to the cropped instance features, previous methods leverage combined spatial features <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b45">47]</ref>, union box features <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b38">40]</ref>, or context features <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b28">30]</ref> to improve the accuracy of HOI detection. In order to concentrate on more interaction-relevant features, some methods utilize extra features, such as human pose <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b12">14]</ref>, human parts <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b21">23]</ref> and language features <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b19">21]</ref>. However, the serial architectures of such two-stage methods impair the efficiency of HOI detection. Moreover, the prediction accuracy is usually limited by the results of instance detection.</p><p>One-stage Methods. Recently, one-stage HOI detection methods with higher efficiency <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b25">27]</ref> have attracted increasing attention. Most one-stage methods extract features with a bottom-up structure <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b43">45]</ref>, and detect the  <ref type="figure">Figure 2</ref>. Overview of the proposed framework. First, a CNN and a transformer encoder are applied to extract the feature sequence with global contexts. Then two branches are built on the transformer decoder layers: a) the instance branch transforms a set of learnable instance queries to an instance prediction set one by one b) the interaction branch utilizes an interaction query set to estimate an interaction prediction set. The instance-aware attention module is designed to introduce the interaction-relevant instance features from the instance branch to the interaction branch. At the end, the detected instances are matched with the interaction predictions to infer the HOI triplets.</p><p>HOI triplets in parallel from an image directly. Specifically, the one-stage methods can be divided into anchor-based methods <ref type="bibr" target="#b18">[20]</ref> and point-based methods <ref type="bibr" target="#b25">[27]</ref> according to the manners of their interaction prediction. The anchorbased methods predict the interactions based on each union box. The point-based methods perform inference at each interaction key point, such as the midpoint of each corresponding human-object pair. Though breaking the limitation from instance detection, such methods which preassign each ground-truth interaction to the predictions, are still non-adaptive and limited by the interaction locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>HOI detection aims to predict the triplet of &lt;human, verb, object&gt;, which contains a pair of bounding-boxes for a human and an object, and a corresponding verb category. In this paper, we reformulate HOI detection as a set prediction problem, and propose an Adaptive Set-based one-stage Network (AS-Net).</p><p>Our AS-Net builds on a transformer encoder-decoder architecture and makes parallel set-based predictions for the HOI triplets. As illustrated in <ref type="figure">Figure 2</ref>, our proposed AS-Net consists of four parts. We first utilize a backbone (Section 3.1) to extract the visual feature sequence with global contexts. The instance (Section 3.2) and interaction branches (Section 3.3) following the backbone parallelly detect an instance and interaction prediction set from the feature sequence respectively. In order to intensify the instance features that are valuable for interaction inference, we design an instance-aware attention module (Section 3.4) to perform branch aggregation. Specifically, we introduce semantic embeddings (Section 3.5) in instance and interaction branches for more accurate triplet prediction. At the end, we match detected instances and interactions to obtain the final HOI triplets (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone</head><p>We define the backbone by combining a CNN and a transformer encoder to extract the image features. The encoder is in a multi-layer manner, where each layer comprises a multi-head self-attention module and a two-layer Feed-Forward Network (FFN). For a given image, we first extract a visual feature map I ∈ R W ×H×C using the CNN. Then we utilize a 1×1 convolution to reduce the channel dimension of the visual feature map from C to d, and reshape such feature map as a feature sequence I s ∈ R W H×d . Next, we feed the feature sequence to the encoder which refines the feature sequence by introducing global contexts into the output feature sequence I s ∈ R W H×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Branch</head><p>The instance branch is designed to localize and classify the instances. Following the detector DETR <ref type="bibr" target="#b0">[2]</ref>, our instance branch consists of a multi-layer transformer decoder and several FFN heads. Each layer of the decoder is comprised of a self-attention module, and a multi-head co-attention module. The input of each decoder layer is the summation of a learnable positional embedding sequence Q d ∈ R N d ×d and the output of last layer. Except for the first layer where there is no output of last layer, we added zeros to the learnable positional embedding sequence. We first feed the input into the self-attention module. Then the multi-head co-attention module adaptively aggregates the key contents from I s to F d ∈ R N d ×d , where we take Q d with the output of self-attention as queries, and I s with the corresponding fixed positional encodings <ref type="bibr" target="#b31">[33]</ref> as keys. There is an FFN head on top of each decoder layer to decode a set of instance predictions from F d . The FFN head comprises three independent sub-branches. One to predict the normalized bounding-box in (cx, cy, w, h) format for each detected instance. Another to infer a (L d + 1)-dimensional scores for L d categories, where the last dimension refers to the no-object (∅) category. The other to generate a distinctive semantic embedding ε ∈ R K for each instance, which will be explained in Section 3.5. Each sub-branch constitutes of one or several perception layers. The FFN head of each decoder layer shares the same weights. Training. For the set-based training process, we first find a one-to-one bipartite matching between the detected instance setŷ and the ground-truth y (padded with no-instance ∅ to a set of size N d ). To this end, we deploy a matching loss, which is the summation of bounding-box loss and category semantic distance between instance and all groundtruth bounding-boxes. Following <ref type="bibr" target="#b0">[2]</ref>, the bounding-box loss is composed of a l 1 loss and a GIoU loss <ref type="bibr" target="#b35">[37]</ref>. The category semantic distance is the negative of summation of the predicted scores for each ground-truth category.</p><p>The universal index permutation set of N d predictions is denoted as S N d . We considerσ d ∈ S N d that minimizes the summation of all the matching cost L match (ŷ σ d (i) , y i ) as the optimal index permutation of the detected instance set, which we adopt the Hungarian algorithm <ref type="bibr" target="#b20">[22]</ref> to calculate. The i-th element of the index permutation σ d ∈ S N d is defined as σ d (i), and theσ d is formulated as:</p><formula xml:id="formula_0">σ d = arg min σ d ∈S N d N d i=1 1 y i =∅ L match (ŷ σ d (i) , yi).</formula><p>(1)</p><p>For the instance prediction with index permutationσ d (i), the predicted bounding-box and category are represented asbσ d (i) andpσ d (i) respectively. We follow the DETR detector <ref type="bibr" target="#b0">[2]</ref> to construct the set-based instance detection loss L ins : <ref type="bibr" target="#b0">(2)</ref> where b i and c i denotes the bounding-box and category of the matched ground-truth instance respectively,pσ d (i) (c i ) is the confidence score for category c i .</p><formula xml:id="formula_1">Lins = N d i=1 [− logpσ d (i) (ci) + 1 {c i =∅} L box (bi,bσ d (i) )],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Interaction Branch</head><p>The interaction branch predicts the interaction vectors and categories for each interaction. Its architecture is similar to the instance branch, which constitutes a multi-layer transformer decoder and several FFN heads. Each decoder layer utilizes several interaction query set Q r to aggregate the corresponding key contents F r ∈ R Nr×d from the shared feature sequence I s . Each decoder layer is equipped with a FFN head as the instance branch. Each FFN head is also split into three sub-branches. For each interaction prediction, we predict a 4-dimensional interaction vector with categories, and two semantic embeddings, i.e., ε h ∈ R K and ε o ∈ R K for the corresponding human and object instances respectively. The interaction vector points from the normalized human center (x h ct , y h ct ) to the object center (x o ct , y o ct ). Considering there might exist multiple interactions for the same human-object pair, we use a multi-label classifier to predict a score for each verb category respectively.</p><p>Training. We denote the ground-truth interaction as t = (v, z), where v is the interaction vector of t, and z indicates the L ground-truth interaction categories of t. We compute the matching loss between t and each predicted interaction t = (v,ẑ), wherev refers to the predicted interaction vector andẑ indicates the confidence scores of the interaction categories. The matching cost L match (t σr(i) , t i ) can be computed by:</p><formula xml:id="formula_2">L match (t σr(i) , t i ) = v i −v σr(i) 1 + L l=1 − 1 1 + e −ẑ σr (i) (z l ) ,<label>(3)</label></formula><p>whereẑ σr(i) (z l ) refers to the score for the l-th ground-truth interaction category z l of t. Similar to the set-based training process for the instance branch, we utilize the Hungarian algorithm <ref type="bibr" target="#b20">[22]</ref> to find the optimal index assignmentσ r for the predicted interaction set w.r.t. the ground-truth.</p><p>For the interaction prediction with indexσ r (i), we define the predicted interaction vector and categories asvσ r (i) andẑσ r (i) respectively, and the matched target interaction vector and categories are v i and z i respectively. To balance the ratio between the positive and negative samples for each classifier, we apply Focal loss <ref type="bibr" target="#b26">[28]</ref>, denoted as L cls , for the training of interaction classification. Besides, we adopt l 1 loss, denoted as L reg , for the regression of interaction vectors. The interaction loss L int is calculated as:</p><formula xml:id="formula_3">L int = Nr i=1 [λ cls L cls (z i ,ẑσ r (i) ) + 1 {zi =∅} λ reg L reg (v i ,vσ r (i) )],<label>(4)</label></formula><p>where λ cls and λ reg are the weight coefficients of L cls and L reg respectively. Analysis. Adaptation is involved in the interaction prediction from two aspects. First, for each interaction query, we apply multi-head co-attention to aggregate information from each element in the feature sequence. Hence, each query can adaptively aggregate the interaction-relevant visual features. Second, instead of pre-assigning each ground-truth to the corresponding prediction, we consider both the predicted interaction vectors and categories to match each ground-truth interaction with the resembling prediction. Therefore, each interaction prediction can be supervised by the most suitable ground-truth more adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Instance-aware Attention</head><p>We construct an instance-aware attention module between each instance and interaction layer to emphasize rel-evant instance features for interaction prediction.</p><p>First, we compute an affinity score map A ∈ R (Nr×N d ) between the instance features F d and the interaction features F r :</p><formula xml:id="formula_4">A = (W r F r + b r )(W d F d + b d ) √ d .<label>(5)</label></formula><p>We then apply Softmax to obtain the instance-aware attention weight matrix M ∈ [0, 1] (Nr×N d ) :</p><formula xml:id="formula_5">M ij = exp(A ij ) N d j=1 exp(A ij ) ,<label>(6)</label></formula><p>where M ij refers to the attention weight of the j-th detected instance with respect to the i-th predicted interaction. The final output interaction features F r ∈ R (Nr×d) of the instance-aware attention module is formulated as:</p><formula xml:id="formula_6">F r = M (W d F d + b d ) + F r .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Semantic Embedding</head><p>The interaction vectors are not pointing to a instance directly, instead, they point to a region. Instead of matching which only employs location indication from the interaction vectors, we introduce semantic embeddings inferred by an MLP block in our matching strategy. We infer semantic embeddings ε from F d for each detected instance in instance branch. And in the interaction branch, two semantic embeddings ε h and ε o are inferred from F r , one for the human instance and another for the object instance for each prediction.</p><p>In the training process, the semantic embeddings of different instances are pushed away from each other. The push procedure can be described as:</p><formula xml:id="formula_7">L push = |σ d |−1 i=1 |σ d | j=i+1 [max(0, t − εσ d (i) − εσ d (j) )] 2 ,<label>(8)</label></formula><p>where |σ d | refers to the total number of the ground-truth instances, and εσ d (i) refers to the semantic embedding of the predicted instance matched to the i-th target instance. If the l 2 distance between two semantic embeddings are more than a threshold t, we consider two embeddings are separate enough and set L push to 0.</p><p>We pull the semantic embeddings that refer to the same instance towards each other:</p><formula xml:id="formula_8">L pull = |σr | i=1 ( ε ĥ σr (i) − εσ d (h i ) 2 + ε ô σr (i) − εσ d (o i ) 2 ),<label>(9)</label></formula><p>where we denote the predicted human semantic embedding as ε ĥ σr(i) and the object embedding as ε ô σr(i) for the interaction prediction with indexσ r (i). The semantic embedding εσ d (hi) and ε ĥ σr(i) refer to the same human instance in the instance and interaction branches respectively. Similarly, εσ d (oi) and ε ô σr(i) refer to the same object instance. |σ r | refers to the total number of the ground-truth interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training Loss and Post-processing</head><p>The target loss is the weighted sum of the losses mentioned above:</p><formula xml:id="formula_9">L = Lins + Lint + λ emb (L pull + L push ),<label>(10)</label></formula><p>where λ emb is a hyper-parameter to balance different loss.</p><p>During the post-processing, we first match the detected human instances with object instances based on our predicted interaction vectors and semantic embeddings. A good human-object interaction match should meet the following three requirements: 1) the normalized center of the matched human/object instances is close to the start and the end point of the interaction vector respectively; 2) the matched instances have high confidence scores on their predicted categories; 3) the semantic embedding referring to the same matched instances are similar to each other.</p><p>We consider all detected instances as object instances. For each predicted interaction vectorv = (x h ct ,ŷ h ct ,x o ct ,ŷ o ct ), the matching distance D can be calculated as:</p><formula xml:id="formula_10">D =(|x h ct −x h ct | + 1)(|ỹ h ct −ŷ h ct | + 1) (|x o ct −x o ct | + 1)(|ỹ o ct −ŷ o ct | + 1),<label>(11)</label></formula><p>where (x h ct ,ỹ h ct ), (x o ct ,ỹ o ct ) refers to the center of the detected human and object instances, with a confidence score of s h and s o , respectively.</p><p>When the semantic embeddings are introduced for matching, given the human (ε h ) and object (ε o ) semantic embedding of the instance branch, the embedding matching distance R for the predicted human (ε h ) and object (ε o ) semantic embedding of the interaction branch can be defined as:</p><formula xml:id="formula_11">R = ( ε h −ε h + 1)( εo −εo + 1).<label>(12)</label></formula><p>The final matching cost is calculated as DR s h s o . We match the detected instances with the minimum matching cost to each interaction prediction. The HOI confidence score for each predicted triplet is the product of the interaction category score, and the matched instance scores s h and s o . Triplets with top N confidence scores are preserved as the final HOI triplet predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Datasets. To verify the effectiveness of our model, we conduct experiments on three HOI detection datasets HICO-DET <ref type="bibr" target="#b1">[3]</ref>, V-COCO <ref type="bibr" target="#b11">[13]</ref> and HOI-A <ref type="bibr" target="#b25">[27]</ref>. HICO-DET contains 38, 118 images for training and 9, 658 images for testing, contains the same 80 object categories as MS-COCO <ref type="bibr" target="#b27">[29]</ref> and 117 verb categories. The objects and verbs form 600 classes of HOI triplets. V-COCO provides 2, 533 images for training, 2, 867 images for validating and 4, 946 images for testing. V-COCO is derived from MS-COCO dataset, annotated with 29 action categories. HOI-A dataset consists of 38, 668 annotated images, 11 kinds of objects and 10 action categories. Metrics. Following <ref type="bibr" target="#b1">[3]</ref>, the mean average precision (mAP) is adopted as evaluation metric. For one positive predicted HOI triplet human, verb, object , both the predicted human and object bounding-boxes have IoUs greater than 0.5 w.r.t. the ground-truth boxes, with the correct predicted verb simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone Extra mAP role</head><p>Two-stage Method: InteractNet <ref type="bibr" target="#b10">[12]</ref> ResNet-50-FPN 40.0 GPNN et al. <ref type="bibr" target="#b32">[34]</ref> Res-DCN-152 44.0 iCAN <ref type="bibr" target="#b8">[10]</ref> ResNet-50 45.3 DRG <ref type="bibr" target="#b7">[9]</ref> ResNet-50-FPN L 51.0 IP-Net <ref type="bibr" target="#b40">[42]</ref> Hourglass-104 51.0 VSGNet <ref type="bibr" target="#b37">[39]</ref> ResNet-152 51.8 PMFNet <ref type="bibr" target="#b38">[40]</ref> ResNet-50-FPN P 52.0 PD-Net <ref type="bibr" target="#b45">[47]</ref> ResNet-152-FPN L 52.6 FCMNet <ref type="bibr" target="#b28">[30]</ref> ResNet-50 53.1 One-stage Method:</p><p>UnionDet <ref type="bibr" target="#b18">[20]</ref> ResNet-50-FPN 47.5 AS-Net* ResNet-50 53.9 <ref type="table">Table 2</ref>. Performance comparison on the V-COCO test set. The 'P', 'L' represent the human pose information and the language feature, respectively. * denotes freezing the instance detection related parameters pretrained on the MS-COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our implementation is based on two parallel 6-layer transformer decoders with a shared backbone, where the backbone is built on ResNet-50 <ref type="bibr" target="#b13">[15]</ref> with a 6-layer selfattention encoder. Following the detector DETR <ref type="bibr" target="#b0">[2]</ref>, we infer N d = 100 instances based on the aggregated interactionrelevant contents F d ∈ R 100×256 on the instance branch. ResNet-50 P 60.26 C-HOI <ref type="bibr" target="#b47">[49]</ref> ResNet-50 P 66.04 One-stage Method: PPDM-DLA <ref type="bibr" target="#b25">[27]</ref> DLA-34 67.03 PPDM-Hourglass <ref type="bibr" target="#b25">[27]</ref> Hourglass-104 71.23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AS-Net</head><p>ResNet-50 72.19 <ref type="table">Table 3</ref>. Performance comparison on the HOI-A test set. The 'P' represents the extra human pose or body parts information.</p><p>On the interaction branch, we infer a set of N r = 16 interaction vectors with categories from F r ∈ R 16×256 . Moreover, the predicted semantic embeddings are all with a dimension of K = 8. After the matching process 3.6, the top N = 100 predictions are finally preserved.</p><p>During training, we resize the shortest side of the input image to the range [480, 800], and the longest side is no more than 1, 333. We set the weight coefficients λ cls , λ reg and λ emb in Section 3.6 to 1, 2 and 0.1 respectively. The model is trained with AdamW <ref type="bibr" target="#b29">[31]</ref> for 90 epochs on the HICO-DET and HOI-A dataset, and for 75 epochs on the V-COCO dataset, with a learning rate of 10 −4 decreased by 10 times at the 70th epoch. All the instance detection related parameters (backbone and the instance decoder layers) which are pretrained on the MS-COCO dataset, are frozen on the V-COCO dataset and trained with a learning rate of 10 −5 on the other two datasets. Our experiments are all conducted on the GeForce GTX 1080Ti GPU and CUDA 9.0, with a batchsize of 64 on 32 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full</head><p>Rare Non-Rare  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing to State-of-the-art</head><p>We conduct experiments on three HOI detection benchmarks to verify the effectiveness of our AS-Net. It is shown in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref> that our AS-Net has achieved state-of-the-art across all the three benchmarks. Specifically, on the HICO-DET dataset, comparing to the previous state-of-the-art one-stage method PPDM <ref type="bibr" target="#b25">[27]</ref> which adopts Hourglass-104 as backbone, our AS-Net has achieved a 31% performance gain with a relatively light-weight backbone, i.e., ResNet-50. Since the object detectors in twostage methods are purely trained on MS-COCO, which does not fine-tune on HICO-DET, thus we also show the result when only training the interaction branch for fair comparison. In this setting, our AS-Net* has achieved 24.40% mAP, which is superior to all existing two-stage methods, and has achieved above 3% mAP improvements.</p><p>We compare our results on the V-COCO dataset with other state-of-the-art methods. Freezing the instance detection related parameters pretrained on the MS-COCO dataset, we only train the remaining parameters of our model. As shown in <ref type="table">Table 2</ref>, our model achieves 53.9% on mAP role , outperforms the previous works. Considering the relatively small scale of the V-COCO dataset may impair the representation capability of the trained semantic embeddings, we test the results using the matching strategy without the semantic embeddings.</p><p>The <ref type="table">Table 3</ref> also illustrates our effectiveness on the HOI-A test set. We reach a mAP of 72.19%, better than all the previous methods, including the method which adopts a relatively heavy-weight Hourglass-104 as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Matching Strategy. Two variants of inference matching methods are implemented. As shown in <ref type="table" target="#tab_3">Table 4a</ref>, when only using the vector matching distance D, or only using the semantic embedding distance R in Section 3.6, the effectiveness are both compromised. Semantic Embedding Settings. To explore the suitable semantic embedding setting, we evaluate the models with different embedding dimension K and weight coefficient λ emb of the training losses L pull and L push . As shown in <ref type="table" target="#tab_3">Table 4b</ref>, the effectiveness of our model is not sensitive to the embedding dimension. As K changes from 4 to 32, the changing of the mAP result is only 0.66 point. The embedding dimension K is set to 8 regarding the trade-off for both effectiveness and computational cost. As illustrated in <ref type="table" target="#tab_3">Table 4c</ref>, the model performs best when training with λ emb = 0.1, while the effectiveness will be impaired when λ emb is increased or decreased.</p><p>Single Branch Variant. We implemented a single branch variant to detect instances along their interactions while keeping all hyper-parameters. As shown in <ref type="table" target="#tab_3">Table 4d</ref>, the variant achieves 25.91% mAP on the HICO-DET dataset, which is 2.96% lower than our AS-Net. Especially, the Rare mAP is 17.88%, which is 6.37% lower than ours. We consider it is because detection and interaction rely on some different features. Lacking the interaction-related features such as human postures, the single branch variant is more likely to infer actions that frequently appear in the presence of the detected objects. Basic Model. To verify the effectiveness of the basic framework, we implement a variant consists of one 6-layer instance detection branch and one 6-layer interaction detection branch, without the instance-aware interaction attention module and semantic embedding. <ref type="table" target="#tab_3">Table 4d</ref> articulates that our basic model (Basic Model, Int×6) achieves 27.52% mAP on the HICO-DET dataset, which outperforms the previous methods by a large margin. Instance-aware Attention. Two other variants are evalu-ated by utilizing the instance-aware attention module to verify the contribution of branch aggregation. As presented in <ref type="table" target="#tab_3">Table 4d</ref>, the instance-aware attention module on our basic model (+ IA Attn×6, Int w/o emb×6) improves mAP by 0.44 point. For the basic model with the semantic embeddings (+ Int w/ emb×6), the improvements are 1.12 points using the instance-aware attention module (+ IA Attn×6, Int w/ emb×6). Therefore, we conclude that the instanceaware attention features from the instance branch are valuable for the interaction prediction. Semantic Embedding &amp; Instance-aware Attention. The basic model with the semantic embeddings (+ Int×6 w/ emb) improves slightly comparing to the basic model (Basic Model, Int×6) without the instance-aware attention as shown in <ref type="table" target="#tab_3">Table 4d</ref>. As the bridge connecting predicted instances and interaction vectors, the semantic embeddings also contribute to the training. However, the semantic embedding is less powerful than the instance-aware interaction attention module from the results. Based on the basic model with the semantic embedding, several variants are implemented, which consist of different interaction decoder layers or attention modules additionally:1) 3 instanceaware attention modules with 6-layer interaction decoder (+ IA Attn×3, Int w/ emb×6), performs attention every other layer; 2) 3 instance-aware attention modules with 3-layer interaction decoder (+ IA Attn×3, Int w/ emb×3). From <ref type="table" target="#tab_3">Table 4d</ref>, the performance is improved by about 1 point utilizing the attention module and the semantic embedding jointly. Besides, it's better to use the instance-aware modules and the decoder layers with the same number of times. The effectiveness reduces slightly when we utilize the two modules with less times, while the amount of the model parameters is reduced significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>As shown in the first three rows of the <ref type="figure" target="#fig_2">Figure 3</ref>, we visualize the interaction decoder attention for some interaction pairs in our basic model, the basic model with the semantic embeddings (+ Int w/ emb×6) and the model with both the instance-aware attention module as well as the semantic embeddings (+ IA Attn×6, Int w/ emb×6), respectively. We also visualize the instance-aware attention in the last row for each example interaction pairs to present how the attention module contributes to the interaction prediction.</p><p>From the <ref type="figure" target="#fig_2">Figure 3 (a)</ref>, the basic model without any branch aggregation focuses on some scattered redundant feature regions and leave out some interaction-relevant features. From the <ref type="figure" target="#fig_2">Figure 3</ref>   the body of the girl, while still concentrates on an irrelevant person. When we involve the instance-aware attention module, as shown in <ref type="figure" target="#fig_2">Figure 3</ref> (c) and <ref type="figure" target="#fig_2">Figure 3 (d)</ref>, the interaction branch concentrates on the whole umbrella and some body parts which are close to the umbrella, and the instanceaware attention module focuses on the body and head of the girl. In such a separated focus mechanism, our model can concentrate on the features more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we reformulate HOI detection as an adaptive set prediction problem and propose a novel one-stage HOI detection framework, namely AS-Net. By aggregating interaction-relevant features from global contexts, and matching each ground-truth with the interaction prediction, our method demonstrates adaptive ability on both feature aggregation and supervision. Moreover, the designed instance-aware attention module contributes to intensify the instructive instance features, and we also introduce semantic embeddings to improve performance. The ablation studies verify the effectiveness of each key component of our model. Our AS-Net outperforms all existing methods on three HOI detection datasets. In the future, we plan to extend AS-Net to handle more general association problems, e.g., visual relationship detection and multi-object tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b), the model with semantic embeddings only partially alleviates the problem. For example, there is a girl holding an umbrella in the figures in the first column. To predict such interaction, the basic model concentrates on the head of the girl and the body of an irrelevant person. Correspondingly, the model with semantic embeddings pays attention to the edge of the umbrella and (a) Visual attention of interaction decoder in (Basic Model, Int× ). (b) Visual attention of interaction decoder in (+ Int w/ emb× ). (c) Visual attention of interaction decoder in (+ IA Attn× , Int w/ emb× ). (d) Visual attention of instance-aware attention in (+ IA Attn× , Int w/ emb× ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the interaction-relevant attention. In each sub-figure, the interaction vector in red is pointing from the corresponding human center to the object center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the HICO-DET test set. The 'P', 'L' represent human pose information and the language feature, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Finetune</cell><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell cols="2">Know Object</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Detection Extra Time (ms) / FPS</cell><cell>Full</cell><cell>Rare</cell><cell>Non-Rare</cell><cell>Full</cell><cell>Rare</cell><cell>Non-Rare</cell></row><row><cell>Two-stage Method:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InteractNet [12]</cell><cell>ResNet-50-FPN</cell><cell></cell><cell>145 / 6.90</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GPNN [34]</cell><cell>Res-DCN-152</cell><cell></cell><cell>-</cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iCAN [10]</cell><cell>ResNet-50</cell><cell></cell><cell>204 / 4.90</cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell cols="2">16.26 11.33</cell><cell>17.73</cell></row><row><cell>No-Frills [14]</cell><cell>ResNet-152</cell><cell>P</cell><cell>494 / 2.02</cell><cell cols="2">17.18 12.17</cell><cell>18.68</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PMFNet [40]</cell><cell>ResNet-50-FPN</cell><cell>P</cell><cell>253 / 3.95</cell><cell cols="2">17.46 15.65</cell><cell>18.00</cell><cell cols="2">20.34 17.47</cell><cell>21.20</cell></row><row><cell>DRG [9]</cell><cell>ResNet-50-FPN</cell><cell>L</cell><cell>200 / 5.00</cell><cell cols="2">19.26 17.74</cell><cell>19.71</cell><cell cols="2">23.40 21.75</cell><cell>23.89</cell></row><row><cell>IP-Net [42]</cell><cell>Hourglass-104</cell><cell></cell><cell>-</cell><cell cols="2">19.56 12.79</cell><cell>21.58</cell><cell cols="2">22.05 15.77</cell><cell>23.92</cell></row><row><cell>VSGNet [39]</cell><cell>ResNet-152</cell><cell></cell><cell>312 / 3.21</cell><cell cols="2">19.80 16.05</cell><cell>20.91</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PD-Net [47]</cell><cell>ResNet-152-FPN</cell><cell>L</cell><cell>-</cell><cell cols="2">20.81 15.90</cell><cell>22.28</cell><cell cols="2">24.78 18.88</cell><cell>26.54</cell></row><row><cell>DJ-RN [23]</cell><cell>ResNet-50</cell><cell>P</cell><cell>-</cell><cell cols="2">21.34 18.53</cell><cell>22.18</cell><cell cols="2">23.69 20.64</cell><cell>24.60</cell></row><row><cell>One-stage Method:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UnionDet [20]</cell><cell>ResNet-50-FPN</cell><cell></cell><cell>78 / 12.82</cell><cell cols="2">17.58 11.72</cell><cell>19.33</cell><cell cols="2">19.76 14.68</cell><cell>21.27</cell></row><row><cell>PPDM-Hourglass [27]</cell><cell>Hourglass-104</cell><cell></cell><cell>71 / 14.08</cell><cell cols="2">21.94 13.97</cell><cell>24.32</cell><cell cols="2">24.81 17.09</cell><cell>27.12</cell></row><row><cell>AS-Net*</cell><cell>ResNet-50</cell><cell></cell><cell>71 / 14.08</cell><cell cols="2">24.40 22.39</cell><cell>25.01</cell><cell cols="2">27.41 25.44</cell><cell>28.00</cell></row><row><cell>AS-Net</cell><cell>ResNet-50</cell><cell></cell><cell>71 / 14.08</cell><cell cols="2">28.87 24.25</cell><cell>30.25</cell><cell cols="2">31.74 27.07</cell><cell>33.14</cell></row></table><note>* denotes freezing the instance detection related parameters pretrained on the MS-COCO dataset. Our one-stage model with a high inference speed of 71 ms / 14.08 FPS outperforms all previous work by a large margin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of our proposed model on the HICO-DET test set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Turbo learning framework for human-object interactions recognition and human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01435,2020.1</idno>
		<title level="m">Recapture as you want</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jie Cao, Haoqian He, Ran He, and Shuicheng Yan. Interactgan: Learning to generate human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defa</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic structure guided context modeling for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Psgan: Pose and expression robust spatial-aware gan for customizable makeup transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transferable interactiveness prior for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene graph generation with hierarchical context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep contextual attention for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Greedynas: Towards fast oneshot nas with greedy supernet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-modal omni interaction modeling for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
