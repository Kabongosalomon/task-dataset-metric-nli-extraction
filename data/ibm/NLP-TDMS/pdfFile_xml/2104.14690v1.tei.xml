<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entailment as Few-Shot Learner</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
							<email>sinongwang@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>USA</roleName><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Entailment as Few-Shot Learner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent improvements in NLP models relied largely on the pre-training and fine-tuning paradigm, wherein a language model is first pre-trained on a massive text corpora, followed by finetuning on the downstream tasks <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Liu et al., 2019;</ref><ref type="bibr" target="#b31">Raffel et al., 2020;</ref><ref type="bibr" target="#b22">Lewis et al., 2020)</ref>. The performance of the model varies depending on the tasks, and the number of available training examples. However, in practice, there is an overly large number of domains, tasks, and languages, and scaling to a new problem space will require additional labeled data. This leads to an important research area, few-shot learning, which assumes accessing to only a small number of labeled examples.</p><p>Different from this pre-training/fine-tuning paradigm, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> showed that pretraining alone combined with a sufficiently large parameter size yields a model that is capable of achieving nearly SOTA results when prompted with the training examples. More importantly, GPT-3 demonstrated that the number of examples needed for promoting can be as small as one, and performance can be significantly improved when using 16 examples (per class) in a few-shot manner. However, the performance of the model depended largely on the number of parameters, and scaling to 175 billion parameters was imperative to achieve these results. Unfortunately, this scale introduces significant challenges for both training and serving.</p><p>Various methods have been proposed to equip smaller language models with few-shot capabilities. One alternative approach is to follow the Masked Language Modeling (MLM) paradigm <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, and reformulate downstream tasks as similar cloze questions (e.g., by appending phrases such as "the correct answer is "), allowing pre-trained LMs to predict the correct label by reusing MLM head <ref type="bibr">(Schick &amp; Schütze, 2020a,b)</ref>. The performance of these techniques are limited when the downstream tasks have different data distribution from the pre-trained text corpora. For example, the work <ref type="bibr" target="#b12">(Gao et al., 2020)</ref> shows that pre-trained LM performs worse in some tasks such as linguistic acceptability task and natural language inference task since pre-trained LM barely saw this data distribution. Textual Entailment</p><p>Label descriptions (c) Entailment-based method (our approach). Each class has a label description, and we choose the class with maximum probability of entailment between input sentence and label description. <ref type="figure">Figure 1</ref>: An illustration of (a) standard fine-tuning of a classification problem; (b) prompt-based method; (c) our proposed method using entailment-based fine-tuning. Compared to prompt-based methods, the key difference of this approach is reformulating tasks as entailment task instead of cloze questions and design fine-grained label descriptions instead of a single task description.</p><p>In this paper, we propose a new approach, in which NLP tasks are first reformulated as a textual entailment task.</p><p>The assumption here is that entailment can be used as unified method to model all classification tasks 1 . As illustrated in <ref type="figure">Figure 1</ref>(c), the key idea is to convert the class label into a natural language sentence which can be used to describe the label, and determine if the example entails the label description. By converting tasks into this entailment style, we demonstrate that standard pre-trained language models are very effective few-shot learners. Another benefit is that, since various tasks are reformulated as a sentence-pair entailment task, we can utilize contrastive learning to construct pairwise augmented data to further improve the few-shot performance. Experiments on wide variety of tasks including 8 tasks from GLUE benchmark, SNLI, BoolQ from superGLUE, and 8 other popular sentence classification tasks show that such off-the-shelf entailment models can improve few-shot performance by 12% compared to various few-shot learning methods. Our methods also established 1.9pt absolute improvement in full training dataset compared to standard fine-tuning of RoBERTa model. We further extend this method to multilingual few-shot learning, which also shows average 19pt improvement compared to standard fine-tuning method.</p><p>The model's striking ability to perform few-shot learning could be due to entailment being a true language understanding task, that once the model is capable of performing it correctly, it can easily apply this knowledge to other tasks which are framed as such. While the proposed approach still requires fine-tuning the model for each task, it does not require a prohibitively large language model to achieve strong performance. Moreover, entailment models are widely accessible to everyone to download and fine-tune through various repositories, thus it democratizes few-shot learners and does not limit it to commercial black-box APIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been many research studies on improving the few-shot learning performance of a pretrained language model:</p><p>Language modeling with demonstrations: The series of GPT works <ref type="bibr" target="#b30">(Radford et al., 2019;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref> proposed to add a task description (prompt) and annotated examples as demonstration to enable few-shot learning, which has been commonly applied to classification <ref type="bibr" target="#b29">(Puri &amp; Catanzaro, 2019)</ref>, QA, commonsense knowledge mining <ref type="bibr" target="#b9">(Davison et al., 2019)</ref>. It is also explored for probing the knowledge contained within pre-trained LMs.  <ref type="figure">Figure 2</ref>: An illustration of intermediate training: first fine-tune the pre-trained LM on a source task with rich annotated data, then further fine-tune the model with 8 annotated samples per class in target task. Figure (a) illustrates the accuracy heatmap between 11 tasks. It shows that sentiment-related task and NLI task have better transferability among each other and non-trivial few-shot results, while other task type such as topic classification, subjectivity task has marginal improvements. <ref type="figure">Figure (</ref> <ref type="bibr">b)</ref> illustrates the diagram of this approach, which requires 110 times fine-tuning runs to fine the best results among 11 tasks.</p><p>Task reformulation: Language model is usually pre-trained with a Masked Language Modeling (MLM) objective, which is motivated by Cloze task in <ref type="bibr" target="#b37">(Taylor, 1953)</ref>. There have been several works reformulating few-shot learning tasks as cloze questions to reuse pre-trained LM such as LM prompt <ref type="bibr" target="#b19">(Jiang et al., 2020)</ref>, PET <ref type="bibr" target="#b30">(Radford et al., 2019;</ref><ref type="bibr" target="#b34">Schick &amp; Schütze, 2020a)</ref>, and recent LM-BFF <ref type="bibr" target="#b12">(Gao et al., 2020)</ref>. It shows a pre-trained LM can achieve non-trivial performance with few annotated samples. There are also some other works transforming NLP tasks as generative QA tasks <ref type="bibr" target="#b29">(Puri &amp; Catanzaro, 2019)</ref>.</p><p>Intermediate training: The work by <ref type="bibr" target="#b28">(Phang et al., 2018)</ref> shows that supplementing pre-trained LMs with further training on data-rich supervised tasks can obtain additional performance improvements on the GLUE benchmark. More recently, this approach has been further improved by a matchingbased few-shot learning method <ref type="bibr" target="#b44">(Yin et al., 2020)</ref>.</p><p>General techniques: There are several general techniques to improve the few-shot learning performance, including: (i) optimization and regularization techniques during the fine-tuning <ref type="bibr" target="#b16">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b45">Zhang et al., 2020)</ref>, (ii) semi-supervised learning to augment training data <ref type="bibr" target="#b43">(Xie et al., 2020)</ref>, and (iii) supervised contrastive learning as additional objective <ref type="bibr" target="#b14">(Gunel et al., 2021)</ref>. We anticipate that these studies are largely complementary to ours.</p><p>Comparing to existing prompt-based few-shot learning methods <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b34">Schick &amp; Schütze, 2020a;</ref><ref type="bibr" target="#b12">Gao et al., 2020)</ref>, the key differences of our proposed method are: (i) our approach reformulates NLP tasks as textual entailment instead of cloze questions; (ii) provide label-specific descriptions for each class instead of single task description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework of Entailment Training</head><p>In this section, we introduce our motivation and framework of entailment training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>We assume there exists a pre-trained language model M and a new downstream task T with label space Y. Suppose that the training dataset</p><formula xml:id="formula_0">D train = {D 1 , D 2 , . . . , D |Y| }, where D k = {x i , y i } K i=1</formula><p>, k ∈ Y and K is the number of training data for each class. In this paper, we set K = 8 as default. Note that label space Y can be both discrete and continuous space, which corresponds to  In the standard NLP tasks, the input x in is usually a single sentence S 1 (classification, regression) or a sentence pair (S 1 , S 2 ) (natural language inference, QA). In the standard fine-tuning approach, given a pre-trained language model M, we usually take</p><formula xml:id="formula_1">x in = [CLS]S 1 [EOS] or x in = [CLS]S 1 [SEP]S 2 [EOS]</formula><p>, and map x in into a sequence of hidden vectors M(x in ). Then we can predict the output classes via a downstream classification head softmax(Wh <ref type="bibr">[CLS]</ref> ), where h <ref type="bibr">[CLS]</ref> is the hidden vector of [CLS] token. During the fine-tuning phase, the parameters of both language model M and classification head W will be updated to minimize the cross entropy between softmax output and correct label 2 .</p><p>The key challenge of few-shot learning is that we have limited training data, while a huge amount of model parameters need to be updated, e.g., BERT-large has 340M parameters. To resolve this challenge, one alternative approach is to reuse the language model parameters if the model is already pre-trained with a similar task. For example, in <ref type="figure">Figure 2</ref>, we conduct a two-stage experiment across 11 NLP tasks (details of data stats can be seen in Appendix 2). We first fine-tune a RoBERTa-Large <ref type="bibr" target="#b23">(Liu et al., 2019</ref>) model on the full dataset of one task, then we load the model parameters M and continue fine-tune on another task in a few-shot setting (8 training data per label class in each task, and fine-tuned with 5 random split). We report the average accuracy on full D test . As illustrated in <ref type="figure">Figure 2</ref>(a), we can achieve reasonable few-shot learning performance, i.e. 85% accuracy, on most sentiment tasks such as fine-tuning on IMDB first, then fine-tune on MR. However, in other tasks such as OS, CoLA and NLI tasks, all the source tasks brings marginal improvement.</p><p>Moreover, the above approach is not task-agnostic. It requires substantial amount of sweeping over available source tasks <ref type="figure">(Figure 2(b)</ref>), and the domain-knowledge of the downstream task. One natural question is that can we build a single language model M that is task-agnostic and generalize well in few-shot setting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>To answer this question, we propose a new framework in which NLP tasks are transformed as a textual entailment task. We refer to this framework as EFL, short for Entailment as Few-shot Learner. For instance, we can reformulate a sentiment classification task as a textual entailment one with an input sentence S 1 as</p><formula xml:id="formula_2">x in = [CLS]S 1 [SEP]S 2 [EOS], where S 2 = This indicates positive user sentiment,<label>(1)</label></formula><p>and let the language model M to determine the if input sentence S 1 entails the label description S 2 . It is important to note that this approach consolidates all the one-sentence classification/regression tasks and sentence-pair tasks into a unified template softmax[WM(x in ) <ref type="bibr">[CLS]</ref> ], where x in is defined in (1). Then we can reduce the gap between traditional MLM pre-training and fine-tuning by further pre-training the language model with existing entailment tasks or reformulating other tasks as entailment task such that we can reuse the model parameters W and M.</p><p>In the binary classification/regression task, we simply choose a label description p as second sentence S 2 , and fine-tune the model on   <ref type="bibr" target="#b35">(Schick &amp; Schütze, 2020b)</ref>, we hand-craft the label descriptions for each task. Details can be seen in <ref type="table" target="#tab_4">Table 10</ref>. The general design principle is to intuitively choose some simple words to describe the labels. For instance, in the sentiment classification task, we can choose This movie is great or This review indicates positive user experience as label description. In <ref type="table" target="#tab_4">Table 1</ref>, we show that our method actually requires minimum domain knowledge to choose the label descriptions. Note that in sentence-pair task such as NLI task, this approach is degenerating to the standard fine-tuning method.</p><formula xml:id="formula_3">D train = {(x i , p, y i )} K|Y| i=1 .</formula><p>In the multi-class classification task, we choose a label description p k for each class k ∈ Y. Suppose that the dataset</p><formula xml:id="formula_4">D train = {D i } |Y| i=1 , where D i = {(x j , y j )} k j=1</formula><p>is the data for ith class. Based on the designed label descriptions {p k } k∈Y , we reshape each dataset as an entailment datasetD k = {(x j , p k , y j )} K j=1 . To accommodate the entailment training, we reshape the training dataset aŝ</p><formula xml:id="formula_5">D train = {(D i ,D −i )} |Y| i=1 ,<label>(2)</label></formula><p>whereD −i is a random subset of {D j } j =i,j∈Y with |D −i | = K. For instance, in AG news classification <ref type="bibr" target="#b46">(Zhang et al., 2015)</ref>, we can formulate a simple set of label descriptions {This is worlds news, This is business news, This is sports news, This is science news} for total 4 classes. Note that this approach requires |Y| forward passes during inference time, while standard fine-tuning only requires one forward pass to get the prediction of all the classes. Actually, the inference speed can be easily improved if we reuse the siamese transformer networks <ref type="bibr" target="#b33">(Reimers &amp; Gurevych, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we present experiment results of our proposed approach and various existing fewshot learning methods. We also evaluate the impact of training data size and pre-trained model size on those methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Statistics</head><p>We conduct a comprehensive study across 18 NLP tasks (see <ref type="table" target="#tab_6">Table 2</ref>), which covers sentiment analysis, topic classification, natural language inference, paraphrases, sentence similarity and QA. Our evaluation consists of 8 tasks from the GLUE benchmark <ref type="bibr" target="#b38">(Wang et al., 2018)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, BoolQ <ref type="bibr" target="#b4">(Clark et al., 2019)</ref>    We divide our tasks into two categories: (i) One-sentence task: The input of this task is a single sentence, the goal is to predict the binary or multi-class label. For example, predicting whether user sentiment is positive or negative. (ii) Two-sentence task: The goal of this task is to make prediction based on a sentence pair. For example, predicting similarity of two input sentences .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation and Training Protocol</head><p>It is well-known that fine-tuning and evaluating on small datasets can suffer from instability and results may change dramatically given a new split of data. To address this, existing work <ref type="bibr" target="#b12">(Gao et al., 2020)</ref> proposed to measure the average performance across 5 different randomly sampled train and dev splits, using a fixed set of seeds. To further reduce the instability of evaluation, we measure the performance on full test dataset instead of randomly downsampled 5 small dev splits. In all of our experiment, we use 8 training samples per class, and the size of evaluation data is listed in TABLE 2. We also follows the principle in <ref type="bibr">(Schick &amp; Schütze, 2020a,b)</ref> and assume no access to a development set and adopt a fixed set of hyper-parameters based on practical considerations.</p><p>For fair comparison, we use the same label descriptions for both prompt-based fine-tuning and entailment-based fine-tuning. For each experiment, we run 5 experiments with 5 different training split and report the average results and standard deviations. We experiment all the methods using a RoBERTa-large model <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, which is a 24-layer, 16-heads transformer model with 355M model parameters. Details of label descriptions and hyper-parameter setup can be seen in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>We conduct both experiments on full-shot and few-shot scenarios, and compare our proposed methods with various baselines, including:</p><p>Majority: simply take the most frequent class (measured on the full test dataset).</p><p>Standard FT: standard fine-tuning of pre-trained language model on full/few-shot dataset with cross entropy loss (or mean square error for regression task).</p><p>STILTS <ref type="bibr" target="#b28">(Phang et al., 2018)</ref>: STILTS is widely used technique to improve the performance of a target problem by creating intermediate training task. Traditionally, people pre-train M on MNLI and then fine-tune on target data. We also create a new stronger version, named as STILTS-close, which pre-train M across the tasks in <ref type="table" target="#tab_6">Table 2</ref>, and report average results of best 4 source tasks.</p><p>LM-BFF <ref type="bibr" target="#b12">(Gao et al., 2020)</ref>: add prompts in each input sentence (pair) and replace keywords by masked tokens, then predict masked tokens by reusing the output layer of pre-trained mask language model. This method has show superior performance across GPT-3 in-context learning and PET <ref type="bibr" target="#b34">(Schick &amp; Schütze, 2020a)</ref>.  EFL: Our proposed method, which transforms label descriptions as a input sentence and reformulate original classification/regression task as a entailment task. We also create two version of this methods: (i) EFL wo PT refers to directly fine-tune pre-trained language model; (ii) EFL refers to first train on MNLI task, then fine-tune on downstream tasks. <ref type="table" target="#tab_8">Table 3</ref> and <ref type="table" target="#tab_10">Table 4</ref> shows the main results of 15 NLP tasks. We observe that our proposed method EFL greatly outperforms standard fine-tuning, LM-BFF, Stilts-NLI and even a very strong baseline Stilts-Close, which assumes the access of development set to sweep the choice of upstream task. In few-shot scenario, K=8, EFL achieves average 8.2% (up to 55%) improvements across 15 tasks among those existing methods. The only exception is the CoLA (the linguistic acceptability task). It is might due to that MLM pre-trainng and entailment training did not see this type of data distribution before. In the full training dataset, EFL shows around 1.9pt average improvements compared to standard fine-tuning of RoBERTa-Large 3 . These results identify the effectiveness of the proposed entailment-based method as a better approach for few-shot learning, moreover, a unified approach for various NLP tasks.</p><p>Furthermore, we observe that if we remove the entailment training step of EFL and directly fine-tune a language model as an entailment task, the performance drops significantly in 8-shot scenario, even worse compared to the standard fine-tuning. This is an interesting case as we will show that, in the next section, EFL without entailment training can also perform well if we increase K to 16 or 32. Note that the overall label description we used in our experiments is default choice of prompt-based method <ref type="bibr" target="#b12">(Gao et al., 2020)</ref>. As shown in <ref type="table" target="#tab_4">Table 1</ref>, we observe that our method can achieve even better performance if we optimize the label descriptions.  <ref type="table">Table 5</ref>: Our main few-shot learning results using RoBERTa-large on multi-class tasks. Besides accuracy metric, we also report macro-F1 metric since these multi-class datasets are unbalanced across different classes. <ref type="table">Table 5</ref> shows the main results of three multi-class benchmarks. Both LM-BFF and our proposed method have better performance than standard fine-tuning. Each multi-class benchmark has unbalanced data across different classes, using accuracy metric is unrepresentative in this scenario. An interesting observation is that, when we use Macro-F1 metric (average F1 across each class) to measure the performance, our proposed method has significantly better performance than other methods. It implies that EFL has better performance in some low-resource classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Training Data Scale</head><p>We first investigate how our proposed method and other methods such as standard fine-tuning and prompt-based method LM-BFF scale as number of training data K increases. In <ref type="figure">Figure 3</ref>, we plot the trends for SST-2 and QNLI. We observe that EFL has bigger improvements when number of annotated samples are small for both benchmarks. For simple tasks such as SST-2, when K=8, EFL already performs as good as K=256. For the harder tasks such as QNLI, EFL keep improving as K increases, and perform best among all the methods. Another interesting observation is that, although EFL without entailment training performs poorly when K = 8, it starts performing better than LM-BFF and standard fine-tuning when K is larger than 16. Based on this observation, we further run ablation studies of our proposed methods without entailment training on two benchmarks: Subj and OS. In <ref type="table">Table 6</ref>, we observe that the gap between EFL and EFL without entailment training is further reduced as K increases and performs similarly when K = 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Few  <ref type="table">Table 6</ref>: Ablation studies of proposed EFL method without entailment training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Model Size</head><p>We further investigate the impact of model size on different methods. We evaluate standard finetuning, LM-BFF and our proposed method on two different-sized pre-trained LMs: RoBERTa-base  <ref type="figure">Figure 5</ref>: An illustration of our proposed unsupervised contrastive learning-based data augmentation method for entailment-based training.</p><p>(12 layer, 768 hidden dimension, 125M parameters) and RoBERTa-Large (24 layer, 1024 hidden dimension, 355M parameters). In <ref type="figure">Figure 4</ref>, we experiment with three benchmarks: SST-2, MR and QNLI. The performances of both LM-BFF and our proposed method improve when the size of pretrained language models increases. Another interesting observation is that standard fine-tuning has better performance on SST-2 and QNLI dataset when uses a smaller language models. We suspect that this phenomenon is due to smaller LM has less number of parameters to be updated in standard fine-tuning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Optimizations</head><p>In this section, we discuss two further optimizations of our proposed framework: (i) a natural combination with unsupervised contrastive learning-based data augmentation method; (ii) multilingual few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unsupervised Contrastive Learning</head><p>In our proposed framework, we reformulate various NLP tasks as a textual entailment task, which takes sentence pair as input. This approach facilitates leveraging unsupervised techniques to construct pairwise data to augment existing limited annotated data. Based on this observation, we propose a new data augmentation strategies, UCA, Unsupervised Contrastive data Augmentation.</p><p>The <ref type="figure">Figure 5</ref> illustrates our main procedure of UCA. In EFL, each training sample has the format of S 1 [SEP]S 2 , where S 1 is the original input sentence, S 2 is either label descriptions for onesentence task or 2nd input sentence for sentence-pair task. In one-sentence task, we can construct a new instance S 1 from S 1 via sentence augmentation, and add S 1 [SEP]S 1 or S 1 [SEP]S 2 as a new training sample with positive label. Similarly, in sentence-pair task, we can also create a new positive instance S 1 [SEP]S 1 or S 2 [SEP]S 2 . In practical implementation, we can alternate between these two augmentations in a probabilistic way. The negative sampling in UCA is straightforward: (i) we randomly sample two sentences R 1 , R 2 from different instances of D train and construct a negative data R 1 [SEP]R 2 ; (ii) we use multiple sentence augmentation methods to largely change the original sentence meaning and add it as a negative sample. The rest relies on how to construct positive sentence pair from existing data.</p><p>We have four basic sentence augmentations: word deletion, span deletion, reordering and substitution. The basic intuition of deletion is that small portion of deletion in a sentence wouldn't affect too much of the original semantic meaning. In word deletion, we randomly remove p del percent of words in the input sentence, while in span deletion, we randomly pick d span consecutive words and directly delete them. Sentence reordering is originally proposed in BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref> for auto-denoising, i.e., restoring original sentence from random reordered sentence. We randomly sample d re pairs of span and switch them pairwise to construct the reordering augmentation in our implementation. In synonym substitution <ref type="bibr" target="#b18">(Jia et al., 2019)</ref>, we sample d sub words and replace them with synonyms to construct one augmentation. This approach is known to be able to improve model's robustness, which is beneficiary for some grammatical acceptability task such as CoLA <ref type="bibr" target="#b40">(Warstadt et al., 2019)</ref>.  <ref type="table">Table 7</ref>: Few-shot learning results (K = 8) with EFL and unsupervised contrastive data augmentation. Note that bold numbers refer to the new best results achieved compared to results in <ref type="table" target="#tab_8">Table 3</ref> and <ref type="table" target="#tab_10">Table 4</ref>.  <ref type="table">Table 8</ref>: Results of combining unsupervised contrastive data augmentation (UCA) with different methods. We report average ∆ across 5 different training sets. Note that bold numbers refer to the new best results achieved compared to results in <ref type="table" target="#tab_8">Table 3 and Table 4</ref>.</p><p>As illustrated in <ref type="table">Table 7</ref>, UCA further brings significant improvement across 12 of 15 tasks, and average 2.7pt improvements across 15 tasks. In particular, the improvement is even larger in the sentence-pair task. For example, we have seen 14pt improvement of QQP task. This is intuitive since UCA creates more similarly distributed data via sentence-level augmentation. In the linguistic acceptability task CoLA, the UCA can create some negative data by aggressively delete or reorder words of sentences, which matches the data format of this task.</p><p>It is also interesting to see how our proposed UCA method impacts other few-shot or fine-tuning methodologies. In <ref type="table">Table 8</ref>, we further make a direct comparison of combining UCA with standard fine-tuning, LM-BFF and our proposed method on 5 single-sentence task and 3 sentence-pair task.</p><p>As it is shown, the UCA method improves all 8 tasks for EFL, 5 of 8 task for standard fine-tuning, 6 of 8 tasks for LM-BFF, and brings average 3 to 4pt improvement. It implies our proposed method can benefit different training methods and may not be limited to EFL. Further, we observe that combining UCA method with standard fine-tuning and LM-BFF mostly improves sentence-pair task. The major reason is that only sentence-pair task in standard fine-tuning and LM-BFF has similar input format of UCA data, instead, the training sample created by EFL is similar format as UCA data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multilingual Few-shot Learning</head><p>When we scale an existing problem to a new language, one challenge is how to obtain the annotated data in the target domain, which leads to an even important research area, multilingual few-shot learning (MFL). In MFL, suppose that we have language space L = {l 1 , . . . , l m } with m different languages. The training dataset is similarly defined as D train as in previous monolingual scenario.</p><p>The key difference is the test dataset,</p><formula xml:id="formula_6">D X test = {D l1 test , D l2 test , . . . , D lm test }, where D lm</formula><p>test is the test dataset in language l m . This problem is even challenging compared to previous scenario since we not only need to generalize to the original unseen test data, but also generalize to unseen languages.</p><p>There have been existing works leveraging multilingual masked language modeling to develop crosslingual LM such as XLM <ref type="bibr" target="#b20">(Lample &amp; Conneau, 2019)</ref>. However, there are no existing works on exploring MFL. As shown in <ref type="figure">Fig 6,</ref> our proposed method can be straightforwardly extended to this scenario. We follow the same steps when we construct the training dataset in monolingual scenario. In test dataset D X test , we don't need to translate label description into target language, instead, simply use original English label description for various languages.</p><p>[CLS] I am in love with these characters This is great movie <ref type="bibr">[EOS]</ref> Entailment head <ref type="bibr">(Feed Forward Layer)</ref>   <ref type="figure">Figure 6</ref>: Generalization of our proposed method to multilingual few-shot learning: we first finetune on English data with few annotated examples and then test it on multilingual data.  <ref type="table">Table 9</ref>: Our main multilingual few-shot learning results using XLM-R. All the results are evaluated on full test sets and averaged across 5 different training sets. We add M-in front of each benchmark to represent the multilingual testing.</p><formula xml:id="formula_7">Method M-SST-2 M-MR M-CR M-MPQA M-Subj M-OS M-CoLA Average (Acc) (Acc) (Acc) (Acc) (Acc) (Acc) (Acc)<label>(Acc)</label></formula><p>We follow the same evaluation protocol in the monolingual scenario. For evaluation benchmark, we reuse the previous monolingual benchmark by using the translation test <ref type="bibr" target="#b25">(Ott et al., 2018)</ref>. We also use XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020)</ref> as our pre-trained LM. For our proposed method EFL, we fine-tune the model on XNLI dataset <ref type="bibr" target="#b5">(Conneau et al., 2018)</ref> instead of MNLI. <ref type="table">Table 9</ref> shows the main results of 7 NLP tasks. We observe that EFL performs significantly better in the multilingual scenarios compared to standard fine-tuning, i.e, improved average accuracy from 61.9 to 80.7 across 7 tasks. This result demonstrates the effectiveness of EFL on multilingual scenario. We further experiment with translate training method: translate few-shot English annotated samples into target languages and add them into training data. Interestingly, we observe standard fine-tuning method improves from 61.9 to 67.0, while EFL only improves from 80.7 to 81.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a set of simple and effective few-shot learning methods: (i) reformulates traditional classification/regression tasks as a textual entailment task; (ii) unsupervised contrastive learning-based data augmentations. We show through a series of systematic evaluations that our method outperforms various few-shot learning methods by up to 55% (and 12% on average). In the future, we will explore several new directions based on this approach: (i) how to choose the best label descriptions based on reinforcement learning; (ii) how to create a more effective entailment training task instead of MNLI tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head><p>The label descriptions we use for each method are the same and defined in <ref type="table" target="#tab_4">Table 10</ref>. For model hyperparameters, in full data experiment, we use the learning rate 1e-5, batch size 32, weight decay 0.1, max epochs 10 and linear learning rate decay with a warm up ratio 0.06. In few-shot experiments and ablation studies, we use a constant learning rate 1e-5, batch size 8, max epochs 10, and standard Adam optimizer.</p><p>For unsupervised contrastive data augmentation (UCA), we use the following default setups: we randomly augment 8 data for each class.</p><p>• Positive generation: We alternate sentence augmentations with 10% probability in deleting character, 10% probability in reordering words, 40% probability in deleting words, and 40% probability in reordering words. In deleting characters, we delete 15% characters of input sentence. In reordering spans, each span cross 5% consecutive characters of input sentence, and we consider switching of 3 pairs of spans. In deleting words, we randomly delete 15% words of input sentence. In reordering words, we randomly switch 2 pairs of words.</p><p>• Negative generation: It has same probability distribution across different sentence augmentations but higher probability of reordering and deletion: In deleting characters, we delete 40% characters of input sentence; In reordering spans, each span cross 25% consecutive characters of input sentence, and we consider switching of 3 pairs of spans. In deleting words, we randomly delete 40% words of input sentence. In reordering words, we randomly switch 2 pairs of words and each contains 40% words.</p><p>For one-sentence task, suppose p 1 is the label description, we use the above rule to generate positive sample S 1 from input sentence S 1 and add augmented data: S 1 [SEP]S 1 or S 1 [SEP]p 1 with equal probability. The negative augmented data consists of 70% of randomly pairwise down-sampled data and and 30% UCA constructed negative samples. In pairwise down-sampling, we randomly sample the first sentence S i1 from existing positive sample, and then randomly sample another first sentence S j1 from existing negative sample, and construct S i1 [SEP]S j1 as new sample. For sentence-pair task, suppose that we have one annotated sample S 1 [SEP]S 2 , we will add augmented data S 1 [SEP]S 2 or S 1 [SEP]S 2 with equal probability. The negative data follows the similar rule of one-sentence task.   <ref type="bibr" target="#b12">(Gao et al., 2020)</ref> and our method used in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Benchmark</head><p>Our benchmark includes 8 datasets from GLUE <ref type="bibr" target="#b38">(Wang et al., 2018)</ref>: CoLA <ref type="bibr" target="#b40">(Warstadt et al., 2019)</ref>, SST-2 <ref type="bibr" target="#b36">(Socher et al., 2013)</ref>, <ref type="bibr">MPRC (Dolan &amp; Brockett, 2005)</ref>, QQP 4 , STS-B <ref type="bibr" target="#b3">(Cer et al., 2017)</ref>, MNLI <ref type="bibr" target="#b42">(Williams et al., 2017)</ref>, QNLI <ref type="bibr" target="#b32">(Rajpurkar et al., 2016)</ref>, <ref type="bibr">RTE (Dagan et al., 2005;</ref><ref type="bibr" target="#b15">Haim et al., 2006;</ref><ref type="bibr" target="#b13">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b0">Bentivogli et al., 2009)</ref>, IMDB <ref type="bibr" target="#b24">(Maas et al., 2011)</ref>, Yelp, AG News <ref type="bibr" target="#b46">(Zhang et al., 2015)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>. For the datasets which require a crossvalidation evaluation, we follow similar processing path <ref type="bibr" target="#b12">(Gao et al., 2020)</ref>: MR <ref type="bibr" target="#b27">(Pang &amp; Lee, 2005)</ref>, CR <ref type="bibr" target="#b17">(Hu &amp; Liu, 2004)</ref>, MPQA <ref type="bibr" target="#b41">(Wiebe et al., 2004)</ref>, Subj <ref type="bibr" target="#b26">(Pang &amp; Lee, 2004</ref>)-we simply randomly sample 2,000 examples as the testing set and leave them out from training. BoolQ <ref type="bibr" target="#b4">(Clark et al., 2019)</ref> is from SuperGlue <ref type="bibr" target="#b39">(Wang et al., 2019)</ref>, and OS is twitter offensive speech data <ref type="bibr" target="#b8">(Davidson et al., 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fine</head><label></label><figDesc>58.6 (6.7) 55.1 (1.1) 64.0 (5.6) 60.4 (3.7) 77.9 (6.1) 75.5 (9.6) 71.2 (3.2) 70.4 (1.9) EFL 90.8 (1.0) 86.2 (0.8) 92.3 (0.4) 87.0 (0.6) 80.0 (5.4) 87.1 (1.2) 76.6 (3.5) 69.4 (0.9)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Sample efficiency of standard fine-tuning, LM-BFF, our proposed EFL and EFL without entailment pre-training as a function of K (number of instances per class). Impact of pre-trained Language model size on standard fine-tuning, LM-BFF and our proposed method EFL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2104.14690v1 [cs.CL] 29 Apr 2021 [CLS] The IAU downgrade Pluto as a dwarf planet[EOS] Predict The IAU downgrade Pluto as a dwarf planet[SEP]What is news about? [MASK] The IAU downgrade Pluto as a dwarf planet This is science news [EOS]</figDesc><table><row><cell>(Feed Forward Layer) CLS head</cell><cell>-business news Label: √ science news</cell><cell></cell><cell>(Feed Forward Layer) MLM head</cell><cell>Predict</cell><cell>-business Label: √ science</cell></row><row><cell></cell><cell>-sports news</cell><cell></cell><cell></cell><cell></cell><cell>-sports</cell></row><row><cell cols="2">(a) Standard finetuning</cell><cell></cell><cell cols="2">(b) Prompt-based method</cell></row><row><cell>(Feed Forward Layer) [CLS] Entailment head</cell><cell>Predict</cell><cell>Science: √ Entail -Not entail</cell><cell>Business: -Entail -Not entail This is business news [EOS]</cell><cell cols="2">Sports: -Entail -Not entail This is sports news [EOS]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The impact of label descriptions on our proposed EFL method with K = 8.</figDesc><table><row><cell>The perfor-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The datasets evaluated in this work. |L|: # of classes for classification tasks. Note that we only sample D train with K = 8 examples from the original training set and evaluate using full dev dataset in our few-shot experiments choose the label descriptions for different tasks. Similar as the existing work</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>from SuperGLUE and 8 other popular sentence classification Fine-tuning 96.4 (0.2) 91.6 (0.2) 91.8 (0.7) 89.4 (0.9) 97.4 (0.1) 96.1 (0.2) 94.3 (0.1) 86.2 (1.6)</figDesc><table><row><cell>Method</cell><cell>SST-2</cell><cell>MR</cell><cell>CR</cell><cell>MPQA</cell><cell>Subj</cell><cell>IMDB</cell><cell>OS</cell><cell>CoLA</cell></row><row><cell></cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc.)</cell></row><row><cell cols="2">Full training dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Majority</cell><cell>50.9</cell><cell>50.0</cell><cell>50.0</cell><cell>50.0</cell><cell>50.0</cell><cell>50.0</cell><cell>66.8</cell><cell>69.1</cell></row><row><cell>EFL</cell><cell cols="8">96.9 (0.2) 92.5 (0.1) 92.5 (0.4) 90.8 (0.4) 97.1 (0.2) 96.1 (0.2) 95.1 (0.1) 86.4 (0.5)</cell></row><row><cell cols="2">Few-shot with K=8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Our main few-shot learning results using RoBERTa-large on one sentence task. All the results are evaluated on full test sets and averaged across 5 different training sets.</figDesc><table /><note>tasks (Yelp, MR, CR, MPQA, Subj, IMDB, AG News, OS, Trec). See Appendix A for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Our main few-shot learning results using RoBERTa-large on NLI, paraphrase, similarity and QA tasks. All the results are evaluated on full dev sets and averaged across 5 different training sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>I am in love with actors [SEP]This is great movie [EOS] [CLS] This style matches my taste[SEP]This is great movie [EOS] [CLS] I in love with actors [SEP]This is great movie [EOS] [CLS] In love with these actors, I am [SEP]I in love with actors [EOS]</figDesc><table><row><cell>I am in love with these actors</cell><cell>(CL sample, label des')</cell><cell>(Feed Forward Layer) [CLS] Entailment head</cell><cell>Predict</cell><cell>-Entail -Not entail</cell><cell>data Annotated</cell></row><row><cell>I am in love with these actors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I am in love with these characters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Augmented</cell></row><row><cell>In love with these actors, I am</cell><cell>(CL sample, sample)</cell><cell></cell><cell></cell><cell></cell><cell>positive data</cell></row><row><cell>CL augmentations</cell><cell>(aggressive CL sample, label des') random sample</cell><cell cols="3">[CLS] I am actors [SEP]This is great movie [EOS] [CLS] I am in love with actors [SEP] This style matches my taste[EOS]</cell><cell>Augmented negative data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>UCA 90.3 (1.2) 86.3 (1.0) 92.8 (0.3) 87.8 (0.5) 83.3 (1.2) 87.4 (1.2) 79.8 (3.3) 71.2 (2.1)</figDesc><table><row><cell>Method</cell><cell>SST-2</cell><cell>MR</cell><cell>CR</cell><cell>MPQA</cell><cell>Subj</cell><cell>IMDB</cell><cell>OS</cell><cell>CoLA</cell></row><row><cell></cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc.)</cell></row><row><cell>EFL</cell><cell cols="8">90.8 (1.0) 86.2 (0.8) 92.3 (0.4) 87.0 (0.6) 80.0 (5.4) 87.1 (1.2) 76.6 (3.5) 69.4 (0.9)</cell></row><row><cell>EFL + Method</cell><cell>QQP</cell><cell>QNLI</cell><cell>SNLI</cell><cell>RTE</cell><cell>MRPC</cell><cell>STS-B</cell><cell>BoolQ</cell><cell>Average</cell></row><row><cell></cell><cell>(F1)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(Acc)</cell><cell>(F1)</cell><cell>(Pear.)</cell><cell>(Acc)</cell><cell></cell></row><row><cell>EFL</cell><cell cols="7">67.3 (2.6) 68.0 (3.4) 81.0 (1.1) 85.8 (0.9) 76.2 (1.3) 71.0 (1.3) 73.9 (1.8)</cell><cell>79.5</cell></row><row><cell cols="8">EFL + UCA 81.5 (0.8) 74.6 (0.8) 80.9 (1.1) 87.2 (0.9) 80.8 (0.8) 75.7 (1.7) 73.7 (1.9)</cell><cell>82.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>[SEP]It was[MASK]. (great / terrible) sentence 1 [SEP]It was great MR sentence 1 [SEP]It was[MASK]. (great / terrible) sentence 1 [SEP]It was great CR sentence 1 [SEP]It was[MASK]. (great / terrible) sentence 1 [SEP]It was great MPQA sentence 1 [SEP]It was[MASK]. (positive / negative) sentence 1 [SEP]It was positive Subj sentence 1 [SEP]It was[MASK]. (subjective / objective) sentence 1 [SEP]It was objective OS sentence 1 [SEP]It was[MASK]. (hatespeech / benign) sentence ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2 MRPC sentence 1 ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2 QNLI sentence 1 ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2 MNLI sentence 1 ?[MASK], sentence 2 . (yes / maybe / no) sentence 1 [SEP]sentence 2 SNLI sentence 1 ?[MASK], sentence 2 . (yes / maybe / no) sentence 1 [SEP]sentence 2 RTW sentence 1 ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2 STS-B sentence 1 ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2 BoolQ sentence 1 ?[MASK], sentence 2 . (yes / no) sentence 1 [SEP]sentence 2</figDesc><table><row><cell cols="2">Dataset Template of Prompt Finetuning</cell><cell>Template of EFL</cell></row><row><cell>SST-2</cell><cell cols="2">sentence 1 1 [SEP]It was hatespeech</cell></row><row><cell>IMDB</cell><cell>sentence 1 [SEP]It was[MASK]. (great / terrible)</cell><cell>sentence 1 [SEP]It was great</cell></row><row><cell>CoLA</cell><cell>sentence 1 [SEP]It was[MASK]. (correct / incorrect)</cell><cell>sentence 1 [SEP]It was correct</cell></row><row><cell>Yelp</cell><cell>sentence 1 [SEP]It was[MASK] news. (Great/good/ok/bad/terrible)</cell><cell>sentence 1 [SEP]It was (great/good/ok/bad/terrible).</cell></row><row><cell cols="2">AG news sentence 1 [SEP]It was[MASK] news. (World/sports/business/science)</cell><cell>sentence 1 [SEP]It is (World/sports/ business/science) news.</cell></row><row><cell>Trec</cell><cell>sentence 1 [SEP]It was[MASK] news. (expression/entity/description/human/location/number)</cell><cell>sentence 1 [SEP]It is (expression/ entity/description/human/location/number).</cell></row><row><cell>QQP</cell><cell>sentence 1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Prompts and label descriptions of prompt-based finetuning method</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This approach does not extend to generative tasks, such as translation or summarization</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For regression task, we will minimize the mean square error.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that we have very large improvement on SNLI, if we remove that benchmark, the average improvement is 1.0pt.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.quora.com/share/First-Quora-Dataset-Release-Question-Pairs</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xnli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">Evaluating cross-lingual sentence representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated hate speech detection and the problem of offensive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Macy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>R Bar Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial word substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerem</forename><surname>Göksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4120" to="4133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Cross-lingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>cs/0409058</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>cs/0506075</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10165</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Universal natural language processing with limited annotations: Try few-shot textual entailment as a start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Revisiting fewsample bert fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01626</idno>
		<title level="m">Character-level convolutional networks for text classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
