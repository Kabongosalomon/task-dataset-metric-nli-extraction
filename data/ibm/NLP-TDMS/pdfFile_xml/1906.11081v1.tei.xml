<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Molecular Property Prediction: A Multilevel Quantum Interactions Modeling Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Anhui Province Key Lab. of Big Data Analysis and Application</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Quantum Information</orgName>
								<orgName type="institution" key="instit1">University of S&amp;T of China</orgName>
								<orgName type="institution" key="instit2">University</orgName>
								<address>
									<country>S&amp;T of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Molecular Property Prediction: A Multilevel Quantum Interactions Modeling Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting molecular properties (e.g., atomization energy) is an essential issue in quantum chemistry, which could speed up much research progress, such as drug designing and substance discovery. Traditional studies based on density functional theory (DFT) in physics are proved to be time-consuming for predicting large number of molecules. Recently, the machine learning methods, which consider much rule-based information, have also shown potentials for this issue. However, the complex inherent quantum interactions of molecules are still largely underexplored by existing solutions. In this paper, we propose a generalizable and transferable Multilevel Graph Convolutional neural Network (MGCN) for molecular property prediction. Specifically, we represent each molecule as a graph to preserve its internal structure. Moreover, the well-designed hierarchical graph neural network directly extracts features from the conformation and spatial information followed by the multilevel interactions. As a consequence, the multilevel overall representations can be utilized to make the prediction. Extensive experiments on both datasets of equilibrium and off-equilibrium molecules demonstrate the effectiveness of our model. Furthermore, the detailed results also prove that MGCN is generalizable and transferable for the prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Predicting molecular properties, such as atomization energy, is one of the fundamental issues in quantum chemical science. Indeed, it has attracted much attention in relevant fields of physics, chemistry and computer science, since it speeds up the societal and technological progress in the application of discovering substances with desired characteristics, such as drug design with specific target and new material manufacture <ref type="bibr" target="#b0">(Becke 2007;</ref><ref type="bibr" target="#b18">Oglic, Garnett, and Gärtner 2017)</ref>.</p><p>In the literature, density functional theory (DFT) plays an important role in physics for molecular property prediction. It holds a common statement that the quantum interactions between particles (e.g., atom) create the correlation and entanglement of molecules which are closely related to their inherent properties <ref type="bibr" target="#b23">(Thouless 2014)</ref>. Along this line, many quantum mechanical methods based on DFT have been developed to model the quantum interactions of molecules for the prediction (Hohenberg and <ref type="bibr" target="#b8">Kohn 1964;</ref><ref type="bibr" target="#b13">Kohn and Sham 1965)</ref>. However, DFTs are computationally costly since they usually use specific functions to determine the interactions of particles, which proves to be extraordinarily time consuming. For example, experimental results indicated that it took nearly an hour to predict the properties of merely one molecule with 20 atoms . Obviously, it is unacceptable to make prediction on large number of molecules in chemical compound space. Therefore, it is necessary to find more effective solutions.</p><p>Recently, inspired by the remarkable success of machine learning in many tasks including computer vision, natural language processing, natural and social science <ref type="bibr" target="#b11">(Karpathy et al. 2014;</ref><ref type="bibr" target="#b8">He et al. 2016;</ref><ref type="bibr" target="#b10">Huang et al. 2017;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref>, researchers have shown the potentials of these data-driven techniques for molecular property prediction <ref type="bibr" target="#b5">(Faber et al. 2017;</ref><ref type="bibr" target="#b21">Schütt et al. 2017a)</ref>. Generally, these studies mainly rely on rule-based feature engineering (e.g., bag of atom bonds) or treat molecules as grid-like structures (e.g., 2D images or text). However, few of them directly take the inherent quantum interactions of molecules into consideration, causing severe information loss, which makes the molecular property prediction problem pretty much open.</p><p>Unfortunately, there are many technical and domain challenges along this line. First, there are highly complex quantum interactions, such as distracted attraction, exchange repulsion and electrostatic interaction in molecules, especially in the large molecules <ref type="bibr" target="#b14">(Kollman 1985)</ref>. It is hard to model them with analytical methods. Second, compared with traditional tasks including computer vision, the amount of labeled molecule data is significantly limited, which requires a generalizable approach for the prediction. Last but not least, in practice, we are often provided with labeled data of small and medium molecules except large molecules since the calculation of them are expensive. Thus, it is necessary to notice this unbalancedness to propose a transferable solution for property prediction of large molecules using the model trained on smaller ones.</p><p>To address these challenges, in this paper, we propose a well-designed Multilevel Graph Convolutional Neural Network (MGCN) for predicting properties of molecules by directly incorporating their quantum interactions. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the process of our approach. Specifically, we first represent each molecule as an interaction graph, which could preserve its internal structure without information loss. Then we propose a hierarchical graph convolutional neural network to model the multilevel quantum interactions based on the graph-like molecular structures. Here, we follow the DFT theory that the quantum interactions could be transformed at different levels, i.e., atom-wise refers to the inherent influence of each atom (e.g., oxygen), atompair refers to the interaction between two atoms, atom-triple means the correlation among three atoms, and so on. Thus, our proposed graph network incorporates hierarchical layers of point-wise, pair-wise, triple-wise, etc to extract representations of the multilevel interactions, respectively. Finally, the overall interaction representation from all levels could be utilized to make the property (e.g., atomization energy) prediction. We conduct extensive experiments on both datasets of equilibrium and off-equilibrium molecules, where the experimental results shows the effectiveness of our proposed approach. Moreover, as MGCN could naturally pass the interaction information of molecules level by level, which also proves the superior ability of generalizability and transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Generally, the related work of our research could be classified into the following three categories.</p><p>Density Functional Theory. Molecular property prediction problem has been studied for a long time in physics, chemistry and material science (Wang and Hou 2011). In the literature, density functional theory (DFT) is the most popular method, which plays a vital role in making the prediction, and could date back to 1960s <ref type="bibr" target="#b8">(Hohenberg and Kohn 1964;</ref><ref type="bibr" target="#b13">Kohn and Sham 1965;</ref><ref type="bibr" target="#b14">Lawless and Chandrasekara 2002)</ref>. Generally, it states that the quantum interactions between particles (e.g., atoms) create the correlation and entanglement of molecules which are closely related to their inherent properties (Thouless 2014). Following this theory, many DFT based methods, such as B3LYP, were proposed, which mapped the quantum interactions of molecules onto every single particles, for predicting the properties <ref type="bibr" target="#b24">(Yanai, Tew, and Handy 2004)</ref>. However, the complexity of DFT could be approximated as O(N 3 ), where N denotes the number of particles. Therefore, it is time-consuming in the experiments and unacceptable for the prediction when facing large number of molecules .</p><p>Traditional Machine Learning Methods. To find more efficient solutions for molecular property prediction, researchers have attempted to leverage various machine learning models, such as kernel ridge regression, random for-est and Elastic Net <ref type="bibr" target="#b5">(Faber et al. 2017;</ref><ref type="bibr" target="#b26">Zou and Hastie 2005;</ref><ref type="bibr" target="#b16">McDonagh et al. 2017)</ref>. Generally, they rely on rulebased hand crafted features using the domain knowledge of physics and chemistry, including bag of bonds, coulomb matrix, and histogram of distances, angles and dihedral angles <ref type="bibr" target="#b9">(Huang and von Lilienfeld 2016;</ref><ref type="bibr" target="#b7">Hansen et al. 2015;</ref><ref type="bibr" target="#b17">Montavon et al. 2012)</ref>. Although some superior experimental results have been achieved, these traditional machine learning methods take manual feature engineering, which requires much domain expertise. Thus, they are often restricted in practice.</p><p>Deep Neural Networks. Compared to traditional machine learning models, deep neural networks hold a superiority of automatic feature learning, which have achieved great success in many applications, such as speech recognition <ref type="bibr" target="#b25">(Zhu et al. 2016)</ref>, computer vision (LeCun, Bengio, and others 1995) and natural language processing <ref type="bibr" target="#b4">(Collobert and Weston 2008)</ref>. With this ability, researchers have noticed the potentials of these deep methods for molecular property prediction. Along this line, convolutional neural network based models were proposed, where they represented each molecule as grid-like structures, such as image <ref type="bibr" target="#b6">(Goh et al. 2017</ref><ref type="bibr">), string (Gómez-Bombarelli et al. 2018</ref>, and sphere (Boomsma and Frellsen 2017). For example, <ref type="bibr" target="#b6">Goh et al. (2017)</ref> converted molecular diagrams into 2D RGB images and proposed the ChemNet for the prediction. However, this grid-like transformation usually caused information loss of the molecules which lied in non-Euclidean space, where the internal spatial and distance information of atoms were not fully considered <ref type="bibr" target="#b2">(Bronstein et al. 2017)</ref>. Therefore, some works operated the molecule as a atom graph and developed graph convolutional neural networks for the property prediction <ref type="bibr" target="#b22">(Schütt et al. 2017b;</ref>. For instance, <ref type="bibr">Schutt et al. (2017b)</ref> proposed the deep tensor neural network that captured the representation of each atom node in molecules. <ref type="bibr" target="#b23">Shang et al. (2018)</ref> further introduced attention mechanism for characterizing the edge information to improve the prediction.</p><p>Our work improves the previous studies as follows. First, we propose the multilevel graph network to directly model the multilevel quantum interactions of molecules from hierarchical perspectives (i.e., atom-wise, pair-wise, triplewise, etc), which developed the graph modeling for molecular property prediction. Second, our work could pass the interaction information level-by-level, which benefits more practical scenarios, i.e., generalizability of limited data and transferability of unbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilevel Graph Convolutional Network</head><p>In this section, we first formally introduce the molecular property prediction problem. Then we describe our Multilevel Graph Convolutional Network in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>Given a molecule, it is natural to represent it as graphs without the loss of information, where vertices represent atoms and edges represent chemical bonds. Thus, a molecule is denoted by G V, E , and in the setting of molecular structure, V is a set of atoms with |V| = N . We regard the graph as a complete undirected graph following the assumption that every atom has the interactions with others so that the set of edges satisfies that |E| = N (N − 1)/2. Here each E contains two kinds of information, namely edge type and spatial information, respectively. Our target is to construct a regressor to predict the properties of molecules. Formally, we can define the problem as:</p><formula xml:id="formula_0">g(f (G)) = y,<label>(1)</label></formula><p>where y is the target property to predict and the middle function f : G → R N ×D is used to learn representations of atoms. Then g converts the obtained features to final result. The multilevel interactions widely exist in the graph structures. In the field of molecule, physical experts design the different symmetry functions to describe the atomic environment by considering the interactions at varied levels <ref type="bibr" target="#b1">(Behler 2014)</ref>. Inspired by this idea, we model quantum interactions in molecules by representing the interactions between two, three, and more atoms level by level to demystify the complexity of molecular interactions. In the next subsection, we will introduce our Multilevel Graph Convolutional Network (MGCN) in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>Overview. The entire architecture could be split into three parts in a high-level discussion except for thse input. The initial input is a graph which consists of a list of atoms and a Euclidean distance matrix of the molecules. The preprocessing part includes embedding layer and Radial Basis Function (RBF) layer. The embedding layer generates atom and edge embeddings while Radial Basis Function (RBF) layer converts the distance matrix to a distance tensor. The next part of MGCN are several interaction layers that aim to learn different node representations in different levels. The last phase is the readout layer that outputs the final result.</p><p>Embedding Layer. Atoms and bonds are the basic elements in a molecule. Thus, to model interactions with as less information loss as possible, we present an embedding layer to directly embed vertices and edges of a graph into vectors. Each atom in a molecule is represented as a vector a 0 ∈R D initially. Therefore, the vertices in the entire molecular are denoted as a matrix A 0 ∈R N ×D and a 0 i indicates the atom embedding of i-th atom in a molecule. The atoms that have the same number of protons in their atomic nuclei share the same initial representation which is called the atom embedding here. Taking CH 2 O 2 as an example, there are five atoms and different kinds of atoms are labeled with different colors in the input part of <ref type="figure" target="#fig_1">Figure 2</ref>. After the process of embedding layer, we get a 5×D matrix and the rows that are related to the atoms of the same type share the same value. The atom embeddings of all chemical elements are generated randomly before training. The initialization of pair-wise embeddings e∈R is similar to atom embeddings (see the preprocess part of <ref type="figure" target="#fig_1">Figure 2</ref>). Thus we get E∈R N ×N ×D , and the edges connecting the same set of atoms have the same initial edge embedding. Specifically, e ij indicates the edge embedding of the bond between i-th atom and j-th atom. The representations generated by the embedding layer are only related to the inherent property of isolated atoms and bonds. The interaction terms are modeled in later subnetwork.</p><p>Radial Basis Function Layer. The spatial information influences the degree of interactions between nodes and we use the RBF Layer to convert these information to robust distance tensors for further utilization. First of all, we reform the raw coordinates of atoms to distance matrix to remove the disturbance of selection of coordinate frame. Secondly, Radial Basis Functions are applied to convert the distance matrix to a distance tensor.</p><p>RBF is a widespread kernel method which originally was invented to generate function interpolation <ref type="bibr" target="#b3">(Broomhead and Lowe 1988)</ref>. Its variant was proved to be advantageous to create fingerprint-like descriptor of molecules <ref type="bibr" target="#b16">(Li, Han, and Wu 2018)</ref>. Here we use RBF to spread the 2D inter-atomic distance matrix to a 3D representation. Given a set of K central points {µ 1 , . . . , µ K }, the single data point x, namely one pair-wise distance in the molecule, will be processed as:</p><formula xml:id="formula_1">RBF (x) = K i=1 h( x − µ i ).</formula><p>(2)</p><p>Here the notation means concatenation, and we take Euclidean distance as the norm. As for radial basis function h, we take Gaussian exp(−β x − µ i 2 ) following the suggestion in <ref type="bibr" target="#b21">(Schütt et al. 2017a)</ref> to avoid the long plateau on the initial phases of the training procedure. k central points are picked evenly in the range from the shortest to the longest edge among the entire dataset. Therefore all distances in the dataset will be covered.</p><p>Through the non-linear transformation, the representations of distances between nodes become more robust. Furthermore, more additional interpretation is introduced by radial basis function layer than simple multi-layer perceptron. After the RBF layer, we create the pair-wise distance tensors D∈R N ×N ×K , and d ij denotes the distance tensor between i-th atom and j-th atom.</p><p>Interaction Layer. To model the multilevel molecular structure with all the conformation and spatial information embedded through previous layers, we construct the interaction layer which is a crucial component of our model. Considering that the quantum interactions in molecules could be transformed at different levels (i.e., atom-wise, atom-pair, atom-triple, etc), our interaction layer is designed by the hierarchical architecture level by level. Specifically, in the l-th interaction layer, we define the edge representation e l+1 ij and atom representation a l+1 i as:</p><formula xml:id="formula_2">e l+1 ij = h e (a l i , a l j , e l ij ),<label>(3)</label></formula><formula xml:id="formula_3">a l+1 i = N j=1,j =i h v (a l j , e l ij , d ij ),<label>(4)</label></formula><p>where h e is used to update edge representation and h v is the function that collects the message from the neighbours of the i-th atom to generate a l+1 i . With this hierarchical modeling, MGCN could effectively preserve the structure of each molecules and describe its quantum interactions. Specifically, in the first layer, a 0 i denotes the atom embedding that show the inherent properties of certain chemical elements. As the forward inference steps, a 1 i involves the first-order neighbour node and spatial information with the message passed by a 0 , e and d. In a similar way, a 2 i represents the triple-wise interactions, a 3 i indicates the interactions between four nodes and so on. As shown in <ref type="figure">Figure.</ref>2, after each interaction layer, we obtain the representations of atoms that reflect the higher-order interactions thanks to decomposition of molecule. The update function h e is calculated as:</p><formula xml:id="formula_4">h e = ηe l ij ⊕ (1 − η)W ue a l i a l j .</formula><p>(5) where η is the hyper-parameter that controls the influence of former pair-wise information (default value is 0.8). Here and ⊕ denote the element-wise dot and plus respectively. In this way, The edge embedding is corrected by the related atomic representations of former interaction layer.</p><p>The function h v applies the message passing operation to create the atom representation at a higher order. The distance tensor d ij here controls the magnitude of impact in each pair of atoms and the edge embedding e ij provides the extra bond information. Thus, it combines the information of nodes, edges, and space, more formally:</p><formula xml:id="formula_5">h v = σ(W uv (M f a (a l j ) M f d (d ij ) ⊕ M f e (e ij )),<label>(6)</label></formula><p>where σ is a tanh activation function, W uv is a weight matrix. The notation M(x) refers to a dense layer that M(x) = W x + b with the input x for simplicity. M f a ,M f d ,M f e are dense layers here.</p><p>Readout Layer. After the interaction layers, we get atom representations at different levels. In the last phase, we construct a readout layer to make the final prediction utilizing these features more clearly.</p><p>First of all, we aggregate the various atom representations to obtain the final vertex feature map as following:</p><formula xml:id="formula_6">a i = T k=0 a k i ,<label>(7)</label></formula><p>where T indicates the number of interaction layers and means concatenation. Secondly, we need to predict the property of molecule with the multilevel representations of each atoms. Fortunately, the molecular properties satisfy additivity and locality. For example, to predict the energetic property, we can model potential energy surfaces as follows <ref type="bibr" target="#b1">(Behler 2014;</ref><ref type="bibr" target="#b4">Cubuk et al. 2017)</ref>:</p><formula xml:id="formula_7">E = N i N j E ij ,<label>(8)</label></formula><p>where E is the total energy and E ij indicates the part of energy related to the bond between i-th and j-th atom (i = j). Besides, E ii could be regarded as the partial energy that mapped to i-th atom. Along this line, we can process the representations separately and then sum them up:</p><formula xml:id="formula_8">y = N i=1 W r a 2 σ(M r a 1 (a i ))+ N i=1 N j=1 i =j W r e 2 σ(M r e 1 (e ij )),<label>(9)</label></formula><p>where σ is the activation function, more specifically, the softplus function. The former term refers to the contribution of quantum interactions that mapped to each atom. Additionally, the latter term denotes the edge-related contribution that can not be mapped to single particle. Since the atom-related interactions account for vast majority of molecular interactions, the latter term is nuanced. Therefore, when the amount of data is small, we tend to ignore the latter term. To train this model, we use the Root-Mean-Square Error (RMSE) as our loss function:</p><formula xml:id="formula_9">(ŷ, y) = |ŷ − y| 2 ,<label>(10)</label></formula><p>whereŷ denotes the predictive value and y is the true value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on MGCN</head><p>Generalizability. In the field of chemistry, the set of all possible molecules in unexplored regions is called chemical space. One of the famous chemical space project (Ruddigkeit et al. 2012) collected 166.4 billion molecules while merely 134k samples of them were labeled . Therefore, the generalization ability of enabling accurate prediction with the limited dataset is indeed essential in our task.</p><p>In the design of our model, we decide to use the distance tensors D as the form of spatial information instead of coordinates of atom. Accordingly, MGCN enforces rotation and translation invariance. Henceforth, the representation learned by MGCN is more general and would not be confused with the same molecule in different orientations.</p><p>Moreover, we perform element-wise operations in interaction layers (equation <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref>) to generate representations and process the representations of each atom respectively. Under those circumstances, the prediction made by our model is irrelevant to sequence of atoms. The index invariance enhances the generalization ability of MGCN.</p><p>Additionally, we utilize some normalization techniques such as dropout to prevent overfitting, which also benefits generalizability of our model.</p><p>In brief, our model is generalizable which is particular important for molecular property prediction where the amount of training data is limited.</p><p>Transferability. Since the expensive computational cost is a critical bottleneck which limits capabilities to calculate the properties of large molecules, most open data are small and medium molecules and the amount of large molecules is small. Therefore, the ability of transferring the knowledge learned from small molecules to larges ones could help us deal with the data-hungry of big molecules.</p><p>The atom/edge embeddings generated by embedding layer are only in regard to the type of atoms and edges and irrelevant to the specific molecular structure and spatial information. As a result, the chemical-domain knowledge learned in the embeddings is universal in the molecular system no matter small or large molecules. Then, in our mulitilevel phase, we use the embeddings to generate the representation in deeper level, e.g., pair-wise and triple-wise. Although small and large molecules are different in the distribution of atoms and bonds, their interactions in different levels are similar. Consequently, with the general embeddings and similar interaction mechanism, MGCN could infer the higher-level representations to predict property and maintain a certain accuracy. Therefore, our model that trained on the small molecules could obtain competitive performance in the prediction of larger molecules.</p><p>Rather than applying the model trained on small molecules to big molecules directly, another way to transfer the knowledge is using pre-trained embeddings. To train a model in large molecules, we could initialize this model with the atom and edge embeddings of another model that was trained on small molecules. The pre-trained embeddings could speed up the convergence and improve the accuracy, because the domain knowledge in embeddings learned from small molecules is still meaning suitable to big molecules.</p><p>Along this line, this model is capable of transferring the knowledge of small molecules to large molecules and tackle the structural shortage of data.</p><p>Time Complexity. The time complexity of our MGCN model is O(N 2 a ) since the calculation of h e and h v (equation <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref>) are independent of molecular size. Here N a indicates the number of atoms in a molecule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We conduct experiments to demonstrate the effectiveness of MGCN from various aspects: 1) predictive performance; 2) the effectiveness of multilevel structure; 3) the validation of generalizability; 4) the verification of transferability; 5) the influence of varied number of interaction layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>QM9. The QM9 1 dataset <ref type="bibr" target="#b19">(Ramakrishnan et al. 2014</ref>) is perhaps the most well-known benchmark dataset which contains 134k equilibrium molecules with their 13 different properties. All of the relaxed geometries and properties for all the 134k molecules are calculated by DFT. The DFT error is the empirical inaccuracy estimation of DFT based approaches <ref type="bibr" target="#b5">(Faber et al. 2017)</ref>. The QM9 dataset also provides the chemical accuracy which is generally accepted by the chemistry community as a relatively ideal accuracy. ANI-1. The ANI-1 2 dataset provides access to the total energies of 20 million off-equilibrium molecules which is 100 times larger than QM9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We use mini-batch stochastic gradient descent (mini-batch SGD) with the Adam optimizer (Kingma and Ba 2014) to train our MGCN. The batch size is set to 64 and the initial learning rate is 1e −5 . For all 13 properties of QM9, we pick 110k out of 130k molecules randomly as our training set that accounts for about 84.7% of the entire dataset. With the rest of the data, we choose half of them as the validation set and the other half as the testing set. As for the much larger ANI-1, we randomly choose 90% samples for training, 5% samples for validation and 5% for testing. We select Mean Absolute Error (MAE) as our evaluation metrics for the convenience of comparison with baselines <ref type="bibr" target="#b5">(Faber et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our model with the 7 baseline methods that could be categorized into two groups.</p><p>The first group consist of 3 traditional ML models using hand-engineered features derived from the molecular literature <ref type="bibr" target="#b5">(Faber et al. 2017;</ref><ref type="bibr" target="#b9">Huang and von Lilienfeld 2016;</ref><ref type="bibr" target="#b7">Hansen et al. 2015)</ref>. These ML models include Random Forest (RF) and Kernel Ridge Regression (KRR). The hand-craft features include Bag of Bonds (BOB), Bond-Angle Machine Learning (BAML) and "Projected Histograms" (HDAD). We imply the combination of X regressor and Y representation with the notation X+Y. Thus, these  three baselines are denoted by RF+BAML, KRR+BOB, KRR+HDAD. These models achieve the best performance in the prediction of one or more properties among all 30 combinations of regressors and features <ref type="bibr" target="#b5">(Faber et al. 2017</ref>).</p><p>The second group contain 4 deep neural networks. They are gated graph network (GG, <ref type="bibr" target="#b12">Kearnes et al. 2016)</ref>, edge neural network with set-to-set (enn-s2s, , deep tensor neural network (DTNN, <ref type="bibr" target="#b22">Schütt et al. 2017b</ref>) and SchNet <ref type="bibr" target="#b21">(Schütt et al. 2017a</ref>). These models are proved to be competitive in the molecular property prediction. Noting that DTNN and SchNet only provide their experimental results in the prediction of property U 0 for the molecules in QM9, thus we complete the rest of the experiments. Besides, all of other numerical results of the baselines are extracted from their works directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Predictive performance. We compare our model with the baseline models mentioned above in two datasets. In <ref type="table" target="#tab_0">Table 1</ref>, we provide the MAE of baselines and our approach as well as DFT error and chemical accuracy for all 13 properties. <ref type="table" target="#tab_1">Table 2</ref> shows the performance comparison in ANI-1.</p><p>As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, MGCN gets the best performance in 11 out of 13 properties, and 11 of them exceed the chemical accuracy. Our model is able to improve the performance upon state-of-the-art. Another observation is that the deep neural networks (GG, enn-s2s, DTNN, SchNet and MGCN) outperform the models that using hand-craft features comprehensively. In the experiment in ANI-1, we choose the state-of-the-art models (DTNN and SchNet) as comparison. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the accuracies in ANI-1 is lower than in QM9, and there are possible two reasons. First, the force in equilibrium molecules of QM9 is negligible, while in off-equilibrium molecules of ANI-1, this factor increases the complexity of quantum interactions. Second, the 100 times larger size of ANI-1 than QM9 makes it more difficult to t. Even though, our model still achieves satisfac-tory accuracy and outperform other methods.</p><p>In brief, our model attains the best performance benefiting from the mulitilevel interaction modeling, and the results prove that our model could handle both equilibrium and offequilibrium molecules and is capable to fit the large dataset. Considering that most previous work have no experiment in the ANI-1 dataset, we chose QM9 as our default dataset in the rest of our experiments.</p><p>Effectiveness of multilevel interactions. In the molecular system, with the increase of the number of atoms in a molecule, the complexity of quantum interactions will grow exponentially. In consequence, it is much harder to model the interactions of molecules if the size of them is larger. <ref type="figure" target="#fig_2">Figure 3</ref> shows the MAE of predictions as the function of the number of atoms. We select the state-of-the-art work SchNet as a comparison for better illustration. Four representative and prevalent properties (µ, ε HOMO , U , Cv) are picked in this experiment. The dotted horizontal line in each subplot is the chemical accuracy of each property. <ref type="figure" target="#fig_2">Figure 3</ref> shows that MGCN assesses more accurate and stable performance than SchNet. Furthermore, as the number of atoms increases, the advantage of MGCN becomes more apparent due to the multilevel modeling. Our model simplifies the interactions by dividing them into different levels and represent them respectively using the mulitilevel structure and decomposition of molecular quantum interactions. Along this line, our model performs better comparatively when the number of atoms increases. In addition, the significant fluctuations appearing in the front and end of the curves derive from the lack of molecules that contain less than 10 atoms or more than 24 atoms in dataset.</p><p>To investigate this further, we construct a control model that blends all levels of interactions in a single embedding rather than construct representation level by level. Taking the prediction of U property for example, the result of this model is not as well as our MGCN with an MAE of 0.03683 on average. It implies the molecular representations modeled by multilevel interaction layers are more robust, which validates the effectiveness of our multilevel modeling.</p><p>Generalizability. The potential molecules in chemical space is numerous extra, but the amount of labeled data is quite small <ref type="bibr" target="#b22">(Schütt et al. 2017b</ref>). Due to the limitation of the magnitude of existing datasets, the ideal model should be able to perform well even trained with a small amount of  data. Thus, the generalizability is another essential aspect to evaluate these models. We train our model in three training sets with the different size that consist of 50k, 100k and 110k samples respectively and test them in the same test set that contains 10k molecules. In addition to the three baselines mentioned above (DTNN, SchNet, enn-s2s), the ensemble model of five enn-s2s models is also listed in the comparison. In <ref type="table" target="#tab_2">Table 3</ref>, the MGCN gets the lowest MAE in three training sets. Regarding that the readout phase of MGCN and SchNet are similar, the representation of molecular learned by MGCN is more generalizable when the accessible data is smaller thanks to the modeling of multilevel interactions.</p><p>Transferability. As mentioned before, the data in existing datasets are unbalanced. For instance, relative large molecules that contain more than 20 atoms account for merely 20.7% of total amount in QM9 and only occupy  8.5% in ANI-1. Therefore, the transferability is quite important to an approach.</p><p>We conduct experiments to validate the transferability of MGCN. Specifically, we sample 50k small and 50k large molecules from QM9 respectively. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>.a, there are four experimental settings. The first two models are trained and tested on the small and large molecules respectively (noted by "Small" and "Large"). The latter two approaches are different ways to transfer the knowledge. The "Develop" denotes the develop model which is trained on the small molecules and applied to the large ones directly. The last model (labeled with "Pre-train") utilizes the embeddings learned on the small molecules as initialization, and then refine itself during the training on the large molecules. <ref type="figure" target="#fig_3">Figure 4</ref>.a illustrates the performance of MGCN in four settings. In the first place, The MAE of small molecules is lower relatively due to the higher complexity of large molecules. Secondly, we observe that as we feed in more small data, the MAE of develop model keeps decreasing. The performance is fairly decent because we did not feed any large molecules to this model. The numerical results show that our model is capable to learn knowledge from small data and then transfer them to larger molecules. Thirdly, the pre-trained model outperforms all of other models when the size of training set is small with the universal domain knowledge learned before. This technique helps address the structural shortage of data.</p><p>Influence of interaction layers. In DFT, physicists usually use 4 to 5 different empirical symmetry functions for molecular property prediction. Each symmetry could be mapped to the interaction layer in each level. <ref type="figure" target="#fig_3">Figure 4</ref>.b shows the relationship between the number of interaction layers and the MAE of property U 0 . We randomly pick 50k molecules as our training set and test on remaining data. As <ref type="figure" target="#fig_3">Figure 4</ref>.b illustrates, too many or few interaction layers could cause higher MAE. The network with less than 4 interaction layers does not have enough capacity to learn the representations of molecules and using the deeper model that contain more than 5 interaction layers will widen the generalization gap. The empirical results indicate that four is the best number of interaction layers which conform to the number of symmetry functions mentioned previously.</p><p>Summary.</p><p>Through the experiments, MGCN shows the superiority of incorporating multilevel modeling of molecular interactions. Moreover, the experimental results prove that our model is generalizable and transferable. Besides, in theory, the time complexity of MGCN is O(N 2 ) compared with O(N 3 ) of DFT. Experimentally, with the same setting (a single core of a Xeon E5-2660), our model spends 2.4×10 −2 second predicting the property of one molecule, which is nearly 1.5×10 5 times faster than DFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduced a Multilevel Graph Convolutional Network (MGCN) for molecular property prediction. The well-designed model utilized the multilevel structure in molecular system to learn the representations of the quantum interactions level by level, and then made prediction with overall interaction representation. The experimental results on two prevalent datasets demonstrated the competency of our approach. Furthermore, our model was proved to be generalizable and transferable.</p><p>We believe future research should concentrate efforts on enhancing the generalization of the atom representation because the predictive accuracy is quite high in small samples and it is tough to obtain the dataset of sufficient large molecules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the process of a molecule (CH 2 O 2 ) via our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the entire MGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>MAE of prediction in different size molecules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>a(left). Performance comparison in the training set with different size. b(right). Predictive performance of models with different number of interaction layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Predictive accuracy of different models in QM9</figDesc><table><row><cell>Properties</cell><cell>U0</cell><cell>U</cell><cell>G</cell><cell>H</cell><cell>Cv</cell><cell>εHOMO</cell><cell>εLUMO</cell><cell>∆ε</cell><cell>ω1</cell><cell>ZPVE</cell><cell>R 2</cell><cell>µ</cell><cell>α</cell></row><row><cell>Unit</cell><cell>eV</cell><cell>eV</cell><cell>eV</cell><cell>eV</cell><cell cols="2">cal/molK eV</cell><cell>eV</cell><cell>eV</cell><cell cols="2">cm −1 eV</cell><cell cols="3">Bohr 2 Debye Bohr 3</cell></row><row><cell>DFT Error</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.34</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28</cell><cell>0.0097</cell><cell>-</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell>Chemical Acc.</cell><cell>0.043</cell><cell>0.043</cell><cell>0.043</cell><cell>0.043</cell><cell>0.05</cell><cell>0.043</cell><cell>0.043</cell><cell>0.043</cell><cell cols="2">10 0.00122</cell><cell>1.2</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>RF+BAML</cell><cell>0.2000</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">0.451 0.1070 0.1180 0.1410</cell><cell cols="2">2.71 0.01320</cell><cell>51.10</cell><cell>0.434</cell><cell>0.638</cell></row><row><cell>KRR+BOB</cell><cell>0.0667</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">0.092 0.0948 0.1220 0.1480 13.20 0.00364</cell><cell>0.98</cell><cell>0.423</cell><cell>0.298</cell></row><row><cell>KRR+HDAD</cell><cell>0.0251</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">0.044 0.0662 0.0842 0.1070 23.10 0.00191</cell><cell>1.62</cell><cell>0.334</cell><cell>0.175</cell></row><row><cell>GG</cell><cell>0.0421</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">0.084 0.0567 0.0628 0.0877</cell><cell cols="2">6.22 0.00431</cell><cell>6.30</cell><cell>0.247</cell><cell>0.161</cell></row><row><cell>enn-s2s</cell><cell cols="4">0.0194 0.0194 0.0168 0.0189</cell><cell cols="4">0.040 0.0426 0.0374 0.0688</cell><cell cols="2">1.90 0.00152</cell><cell>0.18</cell><cell>0.030</cell><cell>0.092</cell></row><row><cell>DTNN</cell><cell cols="4">0.0364 0.0377 0.0385 0.0357</cell><cell cols="4">0.089 0.0982 0.1053 0.1502</cell><cell cols="2">4.23 0.00312</cell><cell>0.30</cell><cell>0.257</cell><cell>0.131</cell></row><row><cell>SchNet</cell><cell cols="4">0.0134 0.0189 0.0196 0.0182</cell><cell cols="4">0.067 0.0507 0.0372 0.0795</cell><cell cols="2">3.83 0.00172</cell><cell>0.27</cell><cell>0.071</cell><cell>0.073</cell></row><row><cell>MGCN</cell><cell cols="4">0.0129 0.0144 0.0146 0.0162</cell><cell cols="4">0.038 0.0421 0.0574 0.0642</cell><cell cols="2">1.67 0.00112</cell><cell>0.11</cell><cell>0.056</cell><cell>0.030</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Predictive accuracy of different models in ANI-1</cell></row><row><cell cols="4">Methods DTNN SchNet MGCN</cell></row><row><cell>MAE</cell><cell>0.113</cell><cell>0.108</cell><cell>0.078</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: Performance comparison in varied size training set</cell></row><row><cell>N</cell><cell cols="3">SchNet DTNN enn-s2s MGCN</cell></row><row><cell cols="3">50,000 0.0256 0.0408</cell><cell>0.0249 0.0229</cell></row><row><cell cols="3">100,000 0.0147 0.0364</cell><cell>-0.0142</cell></row><row><cell cols="2">110,462 0.0134</cell><cell>-</cell><cell>0.0194 0.0129</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.quantum-machine.org/datasets/#qm9 2 https://www.nature.com/articles/sdata2017193</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The quantum theory of atoms in molecules: from solid state to DNA and drug design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Becke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representing potential energy surfaces by high-dimensional neural network potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3436" to="3446" />
		</imprint>
	</monogr>
	<note>Spherical convolutions and their application in molecular modelling</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Radial basis functions, multi-variable functional interpolation and adaptive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Signals and Radar Establishment Malvern</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Broomhead and Lowe</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Onat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waterland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page">24104</biblScope>
		</imprint>
	</monogr>
	<note>Representations in neural network based empirical potentials</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction errors of molecular machine learning models lower than hybrid dft error</title>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a datadriven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02734</idno>
	</analytic>
	<monogr>
		<title level="m">Chemnet: A transferable and generalizable deep neural network for small-molecule property prediction</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ACS central science</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning predictions of molecular properties: Accurate many-body potentials and nonlocality in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of physical chemistry letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2326" to="2331" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<idno>770- 778</idno>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">3B</biblScope>
			<biblScope unit="page">864</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
	<note>Inhomogeneous electron gas</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question difficulty prediction for reading problems in standard tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Karpathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kearnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computeraided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Computer Science</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-consistent equations including exchange and correlation effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Sham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">4A</biblScope>
			<biblScope unit="page">1133</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Theory of complex molecular interactions: computer graphics, distance geometry, molecular mechanics, and quantum mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kollman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandrasekara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings AAAI Fall Conference</title>
		<meeting>AAAI Fall Conference</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
	<note>Information density functional theory: A quantum approach to intent</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine learning of dynamic electron correlation energies from topological atoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M ;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Popelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
	<note>Deeper insights into graph convolutional networks for semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning invariant representations of molecules for atomization energy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Montavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Many molecular properties from one kernel in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename><surname>Oglic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oglic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="182" to="186" />
		</imprint>
	</monogr>
	<note>Active search in intensionally specified structured spaces</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="2864" to="75" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Ruddigkeit et al. 2012</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
	<note>Schütt et al. 2017a</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantumchemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Schütt et al. 2017b</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Application of molecular dynamics simulations in molecular property prediction ii: diffusion coefficient</title>
		<idno type="arXiv">arXiv:1802.04944</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3505" to="3519" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Edge attention-based multirelational graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new hybrid exchange-correlation functional using the coulomb-attenuating method (cam-b3lyp)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tew</forename><surname>Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Handy ; Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Tew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Handy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Physics Letters</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2837" to="2846" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>and Hastie</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
