<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViDeNN: Deep Blind Video Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Claus</surname></persName>
							<email>claus.michele@hotmail.it</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision lab Delft</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ViDeNN: Deep Blind Video Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the noise distribution (blind denoising). The CNN architecture uses a combination of spatial and temporal filtering, learning to spatially denoise the frames first and at the same time how to combine their temporal information, handling objects motion, brightness changes, low-light conditions and temporal inconsistencies. We demonstrate the importance of the data used for CNNs training, creating for this purpose a specific dataset for lowlight conditions. We test ViDeNN on common benchmarks and on self-collected data, achieving good results comparable with the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image and video denoising aims to obtain the original signal X from available noisy observations Y . Noise influences the perceived visual quality, but also segmentation <ref type="bibr" target="#b0">[1]</ref> and compression <ref type="bibr" target="#b1">[2]</ref> making denoising an important step. With X as the original signal, N as the noise and Y as the available noisy observation, the noise degradation model can be described as Y = X + N , for an additive type of noise. In low-light conditions, noise is signal dependent and more sensitive in dark regions, modeled as Y = H(X)+N , with H as the degradation function.</p><p>Imaging noise is due to thermal effects, sensor imperfections or low-light. Hand tuning multiple filter parameters is fundamental to optimize quality and bandwidth of new cameras for each gain level, taking much time and effort. Here, we automate the denoising procedure with a CNN for flexible and efficient video denoising, capable to blindly remove noise. Having a noise removal algorithm working in "blind" conditions is essential in a real-world scenario where color and light conditions can change suddenly, producing a different noise distribution for each frame.</p><p>Solutions based on statistical models include Markov Random Field models <ref type="bibr" target="#b2">[3]</ref>, gradient models <ref type="bibr" target="#b3">[4]</ref>, sparse models <ref type="bibr" target="#b4">[5]</ref> and Nonlocal Self-Similarity (NSS) currently used in state-of-the-art techniques such as BM3D <ref type="bibr" target="#b5">[6]</ref>, LSSC <ref type="bibr" target="#b6">[7]</ref>, NCSR <ref type="bibr" target="#b7">[8]</ref> and WNNM <ref type="bibr" target="#b8">[9]</ref>. Even though they achieve respectable denoising performance, most of those algorithms have some drawbacks. Firstly, they are generally designed to tackle specific noise models and levels, limiting their usage in blind denoising. Secondly, they involve timeconsuming hand-tuned optimization procedures.</p><p>Much work has been done on image denoising while few algorithms have been specifically designed for videos. The key assumption for video denoising is that video frames are strongly correlated. The most basic video denoising technique consists of the temporal average over various subsequent frames. While giving excellent results for steady scenes, it blurs motion, causing artifacts. The VBM4D method <ref type="bibr" target="#b9">[10]</ref> is the state-of-the-art in video denoising. It extends BM3D <ref type="bibr" target="#b5">[6]</ref> single image denoising by the search of similar patches, not only in spatial but also in temporal domain. Searching for similar patches in more frames drastically increases the processing time.</p><p>In this paper we propose ViDeNN, illustrated in <ref type="figure" target="#fig_0">Fig 1:</ref> a convolutional neural network for blind video denoising, capable to denoise videos without prior knowledge over the noise model and the video content. For comparison, experiments have been run on publicly available and on self captured videos. The videos, the low-light dataset and the code will be published on the project's GitHub page.</p><p>The main contributions of our work are: (i) a novel CNN architecture capable to blind denoise videos, combining spatial and temporal information of multiple frames with one single feed-forward process; (ii) Flexibility tests on Additive White Gaussian Noise and real data in low-light condition; (iii) Robustness to motion in challenging situations; (iv) A new low-light dataset for a specific Bosch security camera, with sample pairs of noise-free and noisy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs for Image Denoising. From the 2008 CNN image denoising work of Jain and Seung <ref type="bibr" target="#b10">[11]</ref> there have been huge improvements thanks to more computational power and high quality datasets. In 2012, Burger et al. <ref type="bibr" target="#b11">[12]</ref> showed how even a simple Multi Layer Perceptron can obtain comparable results with BM3D <ref type="bibr" target="#b5">[6]</ref>, even though a huge dataset was required for training <ref type="bibr" target="#b12">[13]</ref>. Recently, in 2016, Zhang et al. <ref type="bibr" target="#b13">[14]</ref> used residual learning and Batch Normalization <ref type="bibr" target="#b14">[15]</ref> for image denoising in their DnCNN architecture. With its simple yet effective architecture, it has shown to be flexible for tasks as blind Gaussian denoising, JPEG deblocking and image inpainting. FFDNet <ref type="bibr" target="#b15">[16]</ref> extends DnCNN by handling an extended range of noise levels and has the ability to remove spatially variant noise. Ulyanov et al. <ref type="bibr" target="#b16">[17]</ref> showed how, with their Deep Prior, they can enhance a given image with no prior training data other than the image itself, which can be seen as a "blind" denoising. There have been also some works on CNNs directly inspired by BM3D such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, Ying et al. propose a deep persistent memory network called MemNet that obtains valid results, introducing a memory block, motivated by the fact that human thoughts are persistent. However, the network structure remains complex and not easily reproducible. A U-Net variant has been successfully used for image denoising in the work of Xiao-Jiao et al. <ref type="bibr" target="#b20">[21]</ref> and in the most recent work on image denoising of Guo et al. <ref type="bibr" target="#b21">[22]</ref> called CBDNet. With their novel approach, CBDNet reaches extraordinarily results in real world blind image denoising. The recently proposed Noise2Noise <ref type="bibr" target="#b22">[23]</ref> model is based on an encoder-decoder structure, obtains almost the same result using only noisy images for training, instead of cleannoisy pairs, which is particularly useful for cases where the ground truth is not available.</p><p>Video and Deep Neural Networks. Video denoising using deep learning is still an under-explored research area. The seminal work of Xinyuan et al. <ref type="bibr" target="#b23">[24]</ref>, is currently the only one using neural networks (Recurrent Neural Networks) to address video denoising. We differ by addressing color video denoising, and offer comparable results to the state-of-art. Other video-based tasks addressed using CNNs include Video Frame Enhancement, Interpolation, Deblurring and Super-Resolution, where the key component is how to handle motion and temporal changes. For frame interpolation, Niklaus et al. <ref type="bibr" target="#b24">[25]</ref> use a pre-computed optical flow to feed motion information to a frame interpolation CNN. Meyer et al. <ref type="bibr" target="#b25">[26]</ref> use instead phase based features to describe motion. Caballero et al. <ref type="bibr" target="#b26">[27]</ref> developed a network which estimate the motion by itself for video super resolution. Similarly, in Multi Frame Quality Enhancement (MFQE), Yang et al. <ref type="bibr" target="#b27">[28]</ref> use a Motion Compensation Network and a Quality Enhancement Network, considering three non-consecutive frames for H265 compressed videos. Specifically for video deblurring, Su et al. <ref type="bibr" target="#b28">[29]</ref> developed a network called DeBlurNet: a U-Net CNN which takes three frames stacked together as input. Similarly, we also use three stacked frames in our ViDeNN. Additionally, we have also investigated variations in the number of input frames.</p><p>Real World Datasets. An image or video denoising algorithm, has to be effective on real world data to be successful. However, it is hard to obtain the ground truth for real pictures, since perfect sensors and channels do not exist. In 2014, Anaya and Barbu, created a dataset for low-light conditions called RENOIR <ref type="bibr" target="#b29">[30]</ref>: they use different exposure times and ISO levels to get noisy and clean images of the same static scene. Similarly, in 2017, Plotz and Roth created a dataset called DND <ref type="bibr" target="#b30">[31]</ref>. In this case, only the noisy samples have been released, whereas the noise free ones are kept undisclosed for benchmarking purposes. Recently, two other related papers have been published. The first, written by Abdelhamed et al. <ref type="bibr" target="#b31">[32]</ref> concerns the creation of a smartphone image dataset of noisy and clean images, which at the time of writing is not yet publicly available. The second, written by Chen et al. <ref type="bibr" target="#b32">[33]</ref>, presents a new CNN based algorithm capable to enhance the quality of low-light raw images. They created a dedicated dataset of two camera types similarly to <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ViDeNN: Video DeNoising Net</head><p>ViDeNN has two subnetworks: Spatial and temporal denoising CNN, as illustrated in <ref type="figure">Fig 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Denoising CNN</head><p>For spatial denoising we build on <ref type="bibr" target="#b13">[14]</ref>, which showed great flexibility tackling multiple degradation types at the same time, and experimented with the same architecture for blind spatial denoising. It is shown, that this architecture can achieve state-of-art results for Gaussian denoising. A first layer of depth 128 helps when the network has to handle different noise models at the same time. The network depth is set to 20 and Batch Normalization (BN) <ref type="bibr" target="#b14">[15]</ref> is used. The activation function is ReLU (Rectified Linear Unit). We also investigated the use of Leaky ReLU as activation function, which can be more effective <ref type="bibr" target="#b33">[34]</ref>, without improvement over ReLU. Comparison results are provided in the supplementary material. Our Spatial-CNN uses Residual Learning, which has been firstly introduced in [14] <ref type="figure">Figure 2</ref>: The architecture of the proposed ViDeNN network. Every frame will go through a spatial denoising CNN. The temporal CNN takes as input three spatially denoised frames and outputs the final estimate of the central frame. Both CNNs estimate first the noise residual, i.e. the unwanted values noise adds to an image, and then subtracts them from the noisy input (⊕ means addition of the two signals, and "-" the negation). ViDeNN is composed only by Convolutional Layers. The number of feature maps is written at the bottom of each layer.</p><p>to tackle image denoising. Instead of forcing the network to output directly the denoised frame, the residual architecture predicts the residual image, which consist in the difference between the original clean image and the noisy observation. The loss function L is the L2-norm, also known as least squares error (LSE) and is the sum of the square of the differences S between the target value Y and the estimated values Y est . In this case the difference S represents the noise residual image estimated by the Spatial-CNN, and is given</p><formula xml:id="formula_0">by L = x y Y (x, y) − Y est (x, y) Noise Residual 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">A Realistic Noise Model</head><p>The denoising performance of a spatial denoising CNN depends greatly on the training data. Real noise distribution differs from Gaussian, since it is not purely additive but it contains a signal dependent part. For this reason, CNN models trained only on Additive White Gaussian Noise (AWGN) fail to denoise real world images <ref type="bibr" target="#b21">[22]</ref>. Our goal is to achieve a good balance between performance and flexibility, training a single network for multiple noise models. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our Spatial-CNN can handle blind Gaussian denoising: we will further investigate its generalization capabilities, introducing a signal dependent noise model. This specific noise model, in equation 1, is composed by two main contributions, the Photon Shot Noise (PSN) and the Read Noise. The PSN is the main noise source in low-light condition, where N sat accounts the saturation number of electrons. The Read Noise is mainly due to </p><formula xml:id="formula_1">M = Ag * Dg N sat * s PSN + Dg 2 * (Ag * CT 1 n + CT 2 n ) 2 Read Noise ,<label>(1)</label></formula><formula xml:id="formula_2">Noisy Image = s + N (0, 1) * M (s),<label>(2)</label></formula><p>where the relevant terms for the considered Sony sensor are: Ag (Analog Gain), in range [0,64], Dg (Digital Gain), in range [0,32] and s, the image that will be degraded. The remaining values are CT 1 n =1.25 −4 , CT 2 n =1.11 −4 and N sat =7489. The noisy image is generated by multiplying observations of a normal distribution N (0, 1) with the same shape of the reference image s, with the Noise Model M in equation 2. In <ref type="figure" target="#fig_1">Figure 3</ref> we illustrate that AWGN-based algorithms such as CBM3D and DnCNN do not generalize to realistic noise. CBM3D, in its blind version, i.e. with the supposed AWGN standard deviation σ set to 50, oversmooths the image, getting a low SSIM (Structural Similarity, the higher the better) score, whereas DnCNN preserves more structure. Our result shows that, to better denoise real world images, a realistic noise model has to be used for the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temp3-CNN: Temporal Denoising CNN</head><p>The temporal denoising part of ViDeNN is similar in structure to the spatial one, having the same number of layers and feature maps. However, it stacks frames together as input. Following other work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> we stack 3 frames, which is efficient, while our preliminary results show no improvements for stacking more than 3 frames. Consider- ing a frame with dimensions w×h×c, the new input will have dimension of w×h×3c. Similar to the Spatial-CNN, the temporal denoising also uses residual learning and will estimate the noise residual image of the central input frame, combining the information of other frames allowing it to learn temporal inconsistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we present the dataset creation and the training/validation, performing insightful experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Low-Light Dataset Creation</head><p>An image denoising dataset has pairs of clean and noisy images. For realistic low-light conditions, creating pairs of noisy and clean images is challenging and the publicly available data is scarce. We used Renoir <ref type="bibr" target="#b29">[30]</ref> and our selfcollected dataset. Renoir <ref type="bibr" target="#b29">[30]</ref> proposes to use two different ISO values and exposure times to get reference and distorted images, demanding many camera settings and parameters. We use a simpler process: grabbing many noisy images of a static scene and then simply averaging to get an estimated ground truth. We used a Bosch Autodome IP 5000 IR, a security camera capable to record raw images, i.e. without any type of processing. The setting involved a static scene and a light source with color temperature 3460K, which has variable intensity between 0 and 255. We varied the light intensity in 12 steps, from the lowest acceptable light condition of value 46, below of which the camera showed noise only, up to the maximum with value 255. For every different light intensity, we recorded 200 raw images in a row. Additionally, we recorded six test video sequences with the stop-motion technique in different light conditions, consisting in three or four frames with moving objects or light changes: for each frame we recorded 200 images, which results in a total of 4200 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial CNN Training</head><p>The training is divided in two parts: (i) we train the spatial denoising CNN and (2) after convergence, we train the temporal denoising CNN.  Our ideal model has to tackle multiple degradation types at the same time, such as AWGN and real noise model 2 including low-light conditions. During the training phase, our neural network will learn how to estimate the residual noise content of the input noisy image, using the clean one as reference. Therefore, we require couples of clean and noisy images. which are easily created for AWGN and the real noise model in equation 2.</p><p>We use the Waterloo Exploration Dataset <ref type="bibr" target="#b35">[36]</ref>, containing 4,744 clean images. The amount of available images helps greatly to generalize and allows us to keep a good part of it for testing. The dataset is randomly divided in two parts, 70% for training and 30% for testing. Half of the images are being added with AWGN with σ=[0,55]. The second half are processed with equation 2 which is the realistic noise model, with Analog Gain Ag=[0,64] and Digital Gain Dg=[0,32].</p><p>Following <ref type="bibr" target="#b13">[14]</ref>, the network is trained with 50 × 50 × 3 patches. We obtained 120,000 patches from the Waterloo training set, containing AWGN and real noise type, using data augmentation such as rotating, flipping and mirroring. For low-light conditions, we used five noisy images for each light level from our own training dataset, obtaining 60 pairs of noisy-clean images for training. The patches extracted are 80,000. From the Renoir dataset, we used the subset T3 and randomly cropped 40,000 patches. For low-light testing, we will use 5 images from our camera of a different scene, not present in the training set, and part of the Renoir T3 set. We trained for 100 epochs, using a batch of 128 and Adam Optimizer <ref type="bibr" target="#b36">[37]</ref> with a learning rate of 10 −3 for the first 20 epochs and 10 −4 for the latest 80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Validation of static Image Denoising</head><p>We compared blind Gaussian denoising with the original implementation of DnCNN, on which ours is based. From our test in <ref type="table" target="#tab_0">Table 1</ref> on the BSD68 test set, we notice how the result of our blind model and the one proposed by the paper <ref type="bibr" target="#b13">[14]</ref> are comparable.</p><p>To further validate on real-world images, we evaluate the sRGB DND dataset <ref type="bibr" target="#b30">[31]</ref> and submitted for evaluation. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR [dB] SSIM</head><p>Spatial-CNN 37.0343 0.9324 CBDNet <ref type="bibr" target="#b21">[22]</ref> 38.0564 0.9421 DnCNN+ <ref type="bibr" target="#b13">[14]</ref> 37.9018 0.943 FFDNet+ <ref type="bibr" target="#b15">[16]</ref> 37.6107 0.9415 BM3D <ref type="bibr" target="#b34">[35]</ref> 34.51 0.8507 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Temp3-CNN: Temporal CNN Training</head><p>For video evaluation we need pairs of clean and noisy videos. For artificially added noise as Additive White Gaussian Noise (AWGN) or the real noise model in equation 2, is easy to create such couples. However, for real-world and low-light conditions videos it is almost impossible. For this reason, this kind of video dataset, offering pairs of noisy and noise-free sequences, are not available. Therefore, we decided to proceed according to this sequence: We followed the same training procedure as the Spatial-CNN, even though now the network will be trained with patches of dimension 50 × 50 × 9, containing three patches coming from three subsequent frames. The 31 selected videos contain 8922 frames, which means 2974 sequences of three frames and a final number of patches of 300032. We ran the training for 60 epochs with a batch size of 128, Adam optimizer with learning rate of 10 −4 and LeakyReLU as activation function. It is shown LeakyReLU can outperform ReLU <ref type="bibr" target="#b33">[34]</ref>. However, we did not use Leaky Relu in the spatial CNN, because ReLU performed better. We present the comparison result in the supplementary material. In the final version of Temp3-CNN, Batch Normalization (BN) was not used: experiments show it slows down the training and denoising process. BN did not improve the final result in terms of PSNR. Moreover, denoising without BN requires around 5% less time. <ref type="figure" target="#fig_3">Figure 5</ref> represents the evolution of the L2-loss for the Temp3-CNN: avoiding the normalization step makes the loss starting immediately at a low value. We trained also with Leaky ReLU, Leaky ReLU+BN and ReLU+BN and present the results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Exp 1: The Video Denoising CNN Architecture</head><p>The final proposed version of ViDeNN consists in two CNNs in a pipeline, performing first spatial and then temporal denoising. To get the final architecture, we trained Vi-DeNN with different structures and tested it on two famous benchmarking videos and on one we personally recorded with a Blackmagic Design URSA Mini 4.6K, capable to record raw videos. The videos have various levels of Additive White Gaussian Noise (AWGN). We will answer to three critical questions.</p><p>Q1: Is Temp3-CNN able to learn both temporal and spatial denoising?</p><p>We compare the Spatial-CNN with the Temp3-CNN, which in this case tries to perform spatial and temporal denoising at the same time. Answer: No, Temp3 is not enough. Referring to <ref type="table" target="#tab_3">Table 3</ref>, we notice how using Temp3-CNN alone leads to a worse result compared to the simpler Spatial-CNN.</p><p>Q2: Ordering of spatial and temporal denoising? Knowing that using Temp3-CNN alone is not enough, we now have to compare different combination of spatial and temporal denoising. Answer: looking at <ref type="table" target="#tab_4">Table 4</ref>, we can confirm that using temporal denoising improves the result over spatial denoising, with the best performing combination as Spatial-CNN followed by Temp3-CNN.    <ref type="table">Table 5</ref>: Comparison of architectures using 3 or 5 frames. Using a bigger time window, i.e. five frames, may slightly improve the final result or even worsen it. Hence, we decided to proceed using a 3-frames architecture. Results expressed in terms of PSNR[dB].</p><p>Q3: How many frames to consider? We compare now the introduced Temp3-CNN with Temp5-CNN, which considers a time window of 5 frames. Answer: Results in <ref type="table">Table 5</ref> shows that considering more frames could improve the result, but this is not guaranteed. Therefore, since using a bigger time window means more memory and time needed, we decided to use the three frames model for a better trade-off. For comparison, using the Temp5-CNN on the video Foreman took 6.5% more time than using the Temp3-CNN, 21.17s vs 19.85s on GPU. The difference does not seem much here, but will enhance for realistic large videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Exp 2: Sensitivity to Temporal Inconsistency</head><p>We investigate temporal consistency with a simple experiment where we remove an object from the first frame and the last frame where we denoise on the middle frame, see <ref type="figure">Figure 6</ref>. Specifically, (i) on the video Tennis from <ref type="bibr" target="#b39">[41]</ref>, add Gaussian noise with standard deviation σ=40; (ii) Manually remove the white ball on the first and last frame; (iii) Denoise the middle frame. In <ref type="figure">Figure 6</ref> we show the modified input frames and ViDeNN output. In terms of PSNR value, we got the same value for both normal and experimental case: 27.28dB. This is an illustration that the network does what we expected: it uses part of the secondary frames and combine them with the reference, but only where the pixel content is similar enough: the ball is not removed from frame 10.  <ref type="figure">Figure 6</ref>: ViDeNN achieves the same PSNR value of 27.28dB for frame 10 of the video Tennis with AWGN σ=40, even if we manually cancel the white ball from the secondary frames. The network understands which part has to take into consideration and which not, i.e. the ball area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of temporal filters</head><p>To understand what our model detects, we show in <ref type="figure" target="#fig_7">Figure 7</ref> the output of two of the 128 filters in the first layer of Temp3-CNN. In <ref type="figure" target="#fig_7">Figure  7a</ref>, we see in black the table-tennis ball of the current frame, whereas the ones in the previous and subsequent frame are in white. In <ref type="figure" target="#fig_7">Figure 7b</ref> instead, we see how this filter highlights flat areas with similar colors and shows mostly the ball of the current frame in white. Therefore, Temp3-CNN gives different importance to similar and different areas among the three frames. This is a simple indication on how the CNN handles motion and temporal inconsistencies. We used frames number 9, 10 and 11 from the video Tennis as input. Filter 59 highlights the reference ball and other areas with similar colors, whereas filter 90 seems to highlight mostly contours and the ball at the reference position in frame 10.  <ref type="table">Table 6</ref>: Comparison of ViDeNN with a video denoising algorithm, VBM4D <ref type="bibr" target="#b9">[10]</ref>, and two image denoising algorithms, DnCNN <ref type="bibr" target="#b13">[14]</ref> and CBM3D <ref type="bibr" target="#b34">[35]</ref>. ViDeNN-G is the model trained specifically for blind Gaussian denoising. Test videos have different length, size and level of Additive White Gaussian Noise. ViDeNN performs better than blind denoising algorithms CBM3D, DnCNN and VBM4D, which has been used with the low complexity setup due to our memory limitations. Best results are highlighted in bold. Original videos are publicly available here <ref type="bibr" target="#b39">[41]</ref>. Results expressed in terms of PSNR[dB]. We show the result of two competitors, VBM4D and CBM3D, which scored respectively second and third (see <ref type="table">Table 6</ref>) on this test video. ViDeNN performs well in challenging situations, even if the previous frame is completely different 8a, thanks to the temporal inconsistency detection. VBM4D suffers from the change of view, creating artifacts. Results in brackets are referred to the single frame 149 (PSNR [dB]/SSIM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Exp 3: Evaluating Gaussian Video Denoising</head><p>Currently, most of the video and image denoising algorithms have been developed to tackle Additive White Gaussian Noise (AWGN). We will compare ViDeNN with the state-of-art algorithm for Gaussian video denoising VBM4D <ref type="bibr" target="#b9">[10]</ref> and additionally with CBM3D <ref type="bibr" target="#b34">[35]</ref> and DnCNN <ref type="bibr" target="#b13">[14]</ref> for single frame denoising. We used the algorithms in their blind version: for VBM4D we activated the noise estimation, for CBM3D we set the sigma level to 50 and for DnCNN we use the blind model provided by the authors. We compare two versions of ViDeNN, where ViDeNN-G is the model trained specifically for AWGN denoising and ViDeNN is the final model tackling multiple noise models, including low-light conditions. The videos have been stored as uncompressed png frames, added with AWGN and saved again in loss-less png format. From the results in <ref type="table">Table 6</ref> we notice that VBM4D achieves superior results compared to its spatial counterpart CBM3D, which is probably due to the effectiveness of the noise estimator implemented in VBM4D. CBM3D suffers from the wrong noise std. deviation (σ) level for low noise intensities, whereas for high levels achieves comparable results. Overall, our implemented ViDeNN in its Gaussian specific version performs better than the general blind model, even though the difference is limited. ViDeNN-G scores the best results, as highlighted in bold in <ref type="table">Table 6</ref>, confirming our blind video denoising network as a valid approach, which achieves state-of-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Exp 4: Evaluating Low-Light Video Denoising</head><p>Along with the low-light dataset creation, we also recorded six sequences of three or four frames each:</p><p>• Two sequences of the same scene, with a moving toy train, in two different light intensities. • A sequence of an artificial mountain landscape with increasing light intensity. • Three sequences of the same scene, with a rotating toy windmill and a moving toy truck, in three different light conditions. Those sequences are not part of the training set and have been recorded separately, with the same Bosch Autodome IP 5000 IR camera. In <ref type="table" target="#tab_7">Table 7</ref> we present the results of ViDeNN highlighted in bold, in comparison with other state-of-art denoising algorithms on the low-light test set. We compare our method with VBM4D <ref type="bibr" target="#b9">[10]</ref>, CBM3D <ref type="bibr" target="#b34">[35]</ref>, DnCNN <ref type="bibr" target="#b13">[14]</ref> and CBDNet <ref type="bibr" target="#b21">[22]</ref>. ViDeNN outperforms the competitors, especially for the lowest light intensities. Surprisingly, the single frame denoiser CBM3D performs better than the video version VBM4D: the difference may be   <ref type="figure">Figure 9</ref>: Detailed comparison of denoising algorithms on the low-light video Train with light intensity at 50/255. Our ViDeNN shows good performance in this light condition, preserving edges and correctly smoothing flat areas. Results referred to frame 2, expressed in terms of (PSNR [dB]/SSIM). because CBM3D in its blind version uses σ = 50, whereas VBM4D has a built-in noise level estimator, which may perform worse with a completely different noise model from the supposed Gaussian one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we presented a novel CNN architecture for Blind Video Denoising called ViDeNN. We use spatial and temporal information in a feed-forward process, combining three consecutive frames to get a clean version of the middle frame. We perform temporal denoising in simple yet efficient manner, where our Temp3-CNN learns how to handle objects motion, brightness changes, and temporal inconsistencies. We do not address camera motion in videos, since the model was designed to reduce the bandwidth usage of static security cameras keeping the network as simple and efficient as possible. We define our model as Blind, since it can tackle different noise models at the same time, without any prior knowledge nor analysis of the input signal. We created a dataset containing multiple noise models, showing how the right mix of training data can improve image denoising on real world data, such as on the DND Benchmarking Dataset <ref type="bibr" target="#b30">[31]</ref>. We achieve state-of-art results in blind Gaussian video denoising, comparing our outcome with the competitors available in the literature. We show how it is possible, with the proper hardware, to address lowlight video denoising with the use of a CNN, which would ease the tuning of new sensors and camera models. Collecting the proper training data would be the most time consuming part. However, defining an automatic framework with predefined scenes and light conditions would simplify the process, allowing to further reduce the needed time and resources. Our technique for acquiring clean and noisy lowlight image pairs has proven to be effective and simple, requiring no specific exposure tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Limitations and Future Works</head><p>The largest real-world limitations of ViDeNN is the required computational power. Even with an high-end graphic card as the Nvidia Titan X, we were able to reach a maximum speed of only ∼ 3fps on HD videos. However, most of the current cameras work with Full HD or even UHD (4K) resolutions with high frame rates. We did not try to implement ViDeNN on a mobile device supporting Tensorflow Lite, which converts the model to a lighter version more suitable for handled devices. This could be new development and challenging question to investigate on, since every week the available hardware in the market improves.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ViDeNN approach to Video Denoising: combining two networks performing first Single Frame Spatial Denoising and subsequently Temporal Denoising over a window of three frames, all in a single feed-forward process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of spatial denoising of an image from the CBSD68 dataset corrupted with 1, with Ag=64 and Dg=4. AWGN based method as CBM3D and DnCNN does not achieve optimal result. The first blurs excessively the image. Using the proper noise model for training leads to a better result. (PSNR [dB]/SSIM) the quantization process in the Analog to Digital Converter (ADC), used to transform the analog light signal into a digital image. CT 1 n represents the normalized value of the noise contribution due to the Analog Gain, whereas CT 2 n represents the additive normalized part. The realistic noise model is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample detail of noisy-clean image pairs of our own low-light dataset, collected with a Bosch Autodome IP 5000 IR security camera. The ground truth is obtained averaging 200 raw images collected in the same light conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>σ = 5 σ</head><label>5</label><figDesc>= 10 σ = 15 σ = 25 σ = 35 σ = 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 .</head><label>1</label><figDesc>Select 31 publicly available videos from [41]. 2. Divide videos in sequences of 3 frames. 3. Added either Gaussian noise with σ=[0,55] or real noise 2 with Ag=[0,64] and Dg=[0,32]. 4. Apply Spatial-CNN 5. Train on pairs of spatially-denoised and clean video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Evolution of the L2-Loss during the training of the Temporal-CNN. Batch Normalization (BN) does not help, adding a computation overhead without any improvement. With Leaky ReLU as activation function and with no BN, the loss starts immediately around 1 and decreases to 0.5 after 60 epochs. Denoising without BN takes around 5% less time. First 1800 steps visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of filters 59 and 90 output of Temp3-CNN first convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Blind video denoising comparison on Tennis [41] corrupted with AWGN σ=40 and values clipped between [0,255].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of blind Gaussian denoising on the CBSD68 dataset. Our modified version of DnCNN for spatial denoising has comparable results with the original one. The values represent PSNR[dB], the higher the better. DnCNN results obtained with the provided Matlab implementation<ref type="bibr" target="#b37">[38]</ref>. CBSD68 available here[39].</figDesc><table><row><cell>Spatial-CNN*</cell><cell>39.73</cell><cell>35.92</cell><cell>33.66</cell><cell>30.99</cell><cell>29.34</cell><cell>27.63</cell></row><row><cell cols="2">DnCNN-B* [14] 39.79</cell><cell>35.87</cell><cell>33.57</cell><cell>30.69</cell><cell>28.74</cell><cell>26.53</cell></row><row><cell>DnCNN-B [14]</cell><cell>40.62</cell><cell>36.14</cell><cell>33.88</cell><cell>31.22</cell><cell>29.57</cell><cell>27.91</cell></row></table><note>*Noisy images clipped in range [0,255].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the DND benchmark<ref type="bibr" target="#b30">[31]</ref> on real-world noisy images. It shows that our dataset, containing different noise models, is valid for real-world image denoising, placing our Spatial-CNN in the top 10 for sRGB denoising.</figDesc><table><row><cell>result [40] are encouraging, since our trained model (called</cell></row><row><cell>128-DnCNN Tensorflow on the DND webpage) scored an</cell></row><row><cell>average of 37.0343dB for the PSNR and 0.9324 for the</cell></row><row><cell>SSIM, placing it in the first 10 positions. Interestingly, the</cell></row><row><cell>authors of DnCNN submitted their result of a fine-tuned</cell></row><row><cell>model, called DnCNN+, a week later, achieving the overall</cell></row><row><cell>highest score for SSIM, which further validates its flexibil-</cell></row><row><cell>ity, see Table 2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Spatial-CNN 32.18 28.27 29.46 26.15 32.73 Temp3-CNN 31.56 27.45 29.32 25.63 31.13</figDesc><table><row><cell></cell><cell cols="2">Foreman</cell><cell></cell><cell>Tennis</cell><cell>Strijp-S *</cell></row><row><cell cols="6">Res./Frames 288×352 / 300 240×352 / 150 656×1164/787</cell></row><row><cell>σ</cell><cell>25</cell><cell>55</cell><cell>25</cell><cell>55</cell><cell>25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Spatial-CNN and Temp3-CNN over videos with different levels of AWGN. The Temp3-CNN alone can not outperform the Spatial-CNN. Results expressed in terms of PSNR[dB]. *Self-recorded Raw video converted to RGB.</figDesc><table><row><cell></cell><cell cols="2">Foreman</cell><cell></cell><cell>Tennis</cell><cell>Strijp-S</cell></row><row><cell cols="6">Res./Frames 288×352 / 300 240×352 / 150 656×1164/787</cell></row><row><cell>σ</cell><cell>25</cell><cell>55</cell><cell>25</cell><cell>55</cell><cell>25</cell></row><row><cell>Spatial-CNN</cell><cell cols="4">32.18 28.27 29.46 26.15</cell><cell>32.73</cell></row><row><cell>Temp3-CNN &amp; Spatial-CNN</cell><cell cols="4">32.09 28.37 29.21 25.98</cell><cell>32.28</cell></row><row><cell>Spatial-CNN &amp; Temp3-CNN</cell><cell cols="4">33.12 29.56 30.36 27.18</cell><cell>34.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The combination of Spatial-CNN + Temp3-CNN is the best performing, showing consistent improvements of ∼ 1dB over the spatial-only denoising. Results expressed in terms of PSNR[dB].</figDesc><table><row><cell></cell><cell cols="2">Foreman</cell><cell></cell><cell>Tennis</cell><cell>Strijp-S</cell></row><row><cell cols="6">Res./Frames 288×352 / 300 240×352 / 150 656×1164/787</cell></row><row><cell>σ</cell><cell>25</cell><cell>55</cell><cell>25</cell><cell>55</cell><cell>25</cell></row><row><cell>Spatial-CNN</cell><cell cols="4">32.18 28.27 29.46 26.15</cell><cell>32.73</cell></row><row><cell>Spatial-CNN &amp; Temp3-CNN</cell><cell cols="4">33.12 29.56 30.36 27.18</cell><cell>34.07</cell></row><row><cell>Spatial-CNN &amp; Temp5-CNN</cell><cell cols="4">33.03 29.87 30.72 27.70</cell><cell>33.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>ViDeNN 35.51 29.97 28.00 32.15 30.91 29.41 31.04 28.44 25.97 32.06 29.23 24.63 ViDeNN-G 37.81 30.36 28.44 32.39 31.29 29.97 31.25 28.72 26.36 32.37 29.59 25.06 VBM4D [10] 34.64 29.72 27.49 32.40 31.21 29.57 29.99 27.90 25.84 29.90 27.87 23.83 CBM3D [35] 27.04 26.37 25.62 28.19 27.95 27.35 24.75 24.46 23.71 26.19 25.89 24.18 DnCNN [14] 35.49 27.47 25.43 31.47 30.10 28.35 30.66 27.87 25.20 32.20 29.29 24.51</figDesc><table><row><cell></cell><cell></cell><cell>Tennis</cell><cell></cell><cell cols="3">Old Town Cross</cell><cell></cell><cell>Park Run</cell><cell></cell><cell></cell><cell>Stefan</cell><cell></cell></row><row><cell>Res./Frames</cell><cell></cell><cell cols="2">240×352 / 150</cell><cell cols="3">720×1280 / 500</cell><cell cols="3">720×1280 / 504</cell><cell cols="3">656×1164 / 300</cell></row><row><cell>σ</cell><cell>5</cell><cell>25</cell><cell>40</cell><cell>15</cell><cell>25</cell><cell>40</cell><cell>15</cell><cell>25</cell><cell>40</cell><cell>15</cell><cell>25</cell><cell>55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of state-of-art denoising algorithms over six low-light sequences recorded with a Bosch Autodome IP 5000 IR in raw mode, without any type of filtering activated. Every sequence is composed of 4 or 3 frames, with ground truths obtained averaging over 200 images. The Windmill sequences has been recorded with a different light source, where we were able to measure the light intensity. Highlighted in bold our ViDeNN results, which performs well. Results expressed in terms of PSNR[dB].</figDesc><table><row><cell>(a) Noisy frame 2</cell><cell>(b) DnCNN [14]</cell><cell>(c) VBM4D [10]</cell></row><row><cell>(22.54/0.4402)</cell><cell>(24.30/0.5323)</cell><cell>(29.08/0.7684)</cell></row><row><cell>(d) CBDNet [22]</cell><cell>(e) CBM3D[35]</cell><cell>(f) ViDeNN</cell></row><row><cell>(30.75/0.8710)</cell><cell>(31.11/0.8982)</cell><cell>(34.14/0.9158)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mri segmentation of the human brain: Challenges, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Despotovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Philips</surname></persName>
		</author>
		<idno>1-23. 05 2015. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A study on the influence of image dynamics and noise on the jpeg 2000 compression performance for medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Nabil</forename><surname>Belbachir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Truppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Approaches to Medical Image Analysis: Second International ECCV Workshop</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4241</biblScope>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries in wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<idno>754-758. 09 2009. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Image and Graphics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2080" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<idno>22. 12 2012. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3952" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Labelme: A database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatis</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5882" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bm3d-net: A convolutional neural network for transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Journals &amp; Magazines</title>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep rnns for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang Xinyuan Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">9971</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Phasenet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiframe quality enhancement for compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Renoir a dataset for real low-light image noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminance-chrominance space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">I -313-I -316</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Waterloo Exploration Database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page" from="12" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dncnn matlab implementation on github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dnd Benchmark</forename><surname>Darmstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Results</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Xiph.org Video Test Media</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>derf&apos;s collection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
