<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Interactive Technologies</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Interactive Technologies</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep ConvNets have been shown to be effective for the task of human pose estimation from single images. However, several challenging issues arise in the video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples in training data sets. Temporal information can provide additional cues about the location of body joints and help to alleviate these issues. In this paper, we propose a deep structured model to estimate a sequence of human poses in unconstrained videos. This model can be efficiently trained in an end-to-end manner and is capable of representing appearance of body joints and their spatio-temporal relationships simultaneously. Domain knowledge about the human body is explicitly incorporated into the network providing effective priors to regularize the skeletal structure and to enforce temporal consistency. The proposed end-to-end architecture is evaluated on two widely used benchmarks (Penn Action dataset and JHMDB dataset) for video-based pose estimation. Our approach significantly outperforms the existing state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating human poses is one of the core problems in computer vision and has many applications in the lifesciences, computer animation and the growing fields of robotics, augmented and virtual reality. Accurate pose estimates can also drastically improve the performance of activity recognition and high-level analysis of videos(cf. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>). Recent pose estimation methods have exploited deep convolutional networks (ConvNets) for bodypart detection in single, fully unconstrained images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. While demonstrating the feasibility of detection-based pose estimation from images taken under general conditions, such methods still struggle with several challenging aspects including the diversity of human appearance and self-symmetries. Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref> have explicitly incorporated geometric constraints among body parts into such frameworks, ensuring spatial consistency and penalizing physically impossible solutions (cf. <ref type="figure">Figure 1</ref>. Our method incorporates spatio-temporal information into a single end-to-end trainable network architecture, aiming to deal with challenging problems such as (self-)occlusions, motion blur, and uncommon poses. Taking fully unconstraind images as input (a), we regress body-part locations with standard ConvNet layers (b). Spatial inference helps in overcoming confusion due to symmetric body parts (c). Our spatio-temporal inference layer (d) can deal with extreme cases where spatial information only fails (cf. 11 vs 12, 15 vs 16) and improves prediction accuracies for unary terms due to repeating measurements by temporal propagation of joint position estimates <ref type="bibr">(3 vs 4)</ref>. <ref type="figure">Figure 1, (c)</ref>).</p><p>In this paper we consider the comparatively less studied problem of human pose estimation from unconstrained videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. While inheriting many properties from image-based pose estimation, it also brings new challenges. In particular, unconstrained videos such as those found in online portals, contain many frames with occlusions, unusual poses, and motion blur (see <ref type="figure">Figure 1</ref>). These issues continue to limit the accuracy of joint detection even if taking priors about the spatial configuration of the human skeleton into consideration, and often result in visible jitter if such models are applied directly to video sequences.</p><p>To tackle these problems, we propose to incorporate spatial and temporal modeling into deep learning architec-tures. The proposed model is based on a simple observation: human motion exhibits high temporal consistency, which could be captured by optical flow warping <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> and spatio-temporal inference <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, we leverage a spatio-temporal relational model into the ConvNet framework and develop a new deep structured architecture, called Thin-Slicing Network. Our deep structured model allows for end-to-end training of body part regressors and spatio-temporal relational models in a unified framework. This enables improving generalization performance by regularizing the learning process both spatially and temporally across adjacent frames. We deploy fully ConvNet for initial part detection. Furthermore, via a flow warping layer which propagates joint prediction heat maps temporally and a novel inference layer, message passing on arbitrary loopy graphs along both spatial and temporal edges is performed.</p><p>In consequence, our approach can deal with many challenging situations arising in unconstrained video, and outperform both the original joint-position estimation methods and those incorporating spatial priors only. <ref type="figure">Figure 1</ref> illustrates how our approach can accurately predict joint positions in difficult situations of full occlusion (3 rd row, given visibility in adjacent frames) or severe motion blur (4 th row, by exploiting temporal consistency). Last but not least, the model also improves predictions in relatively simple cases (see <ref type="figure">Figure 1</ref>, 1 st and 2 nd row). This can be explained by optimizing of several correlated but different frames through the entire architecture jointly, which not only learns weights of the inference layers, but also refines the underlying ConvNet-based part regressors, resulting in more accurate joint detections.</p><p>In summary our main contributions are: (i) A structured model captures the inherent consistency of human poses in video sequences based on a loopy spatio-temporal graph. Our approach does not rely on explicit human motion priors but leverages dense optical flow to exploit image evidence from adjacent frames. (ii) An efficient and flexible inference layer performs message passing along the spatial and temporal graph edges and significantly reduces joint position uncertainty. (iii) The entire architecture well integrates the ConvNet-based joint regressors and the high-level structured inference model in a unified framework, which could be optimized in an end-to-end manner. (iv) Our method significantly improves the state-of-the-art performance on two widely used video based pose estimation benchmarks: the Penn Action dataset <ref type="bibr" target="#b39">[40]</ref>, the JHMDB dataset <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Pose estimation from single images has benefitted tremendously from leveraging structural models such as tree-structured pictorial models <ref type="bibr" target="#b0">[1]</ref> and part-based models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref>, encoding the relationships between articulated joints. While capturing kinematic correlations, such models are prone to errors such as double-counting part evidence. More expressive loopy graph models, allowing for cyclic joint dependencies have been proposed to better capture symmetry and long-range correlation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. Since exact inference in cyclic graphs is generally speaking intractable, approximate inference methods like loopy belief propagation are typically used.</p><p>The above methods are based on hand-crafted features and are sensitive to (the limits of) their representative power. More recently, convolutional deep learning architectures have been deployed to learn richer and more expressive features directly from data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>, outperforming prior work. Toshev et al. <ref type="bibr" target="#b31">[32]</ref> directly regress the joint coordinates from images. Follow-up work suggests that regressing full image confidence maps as intermediate representation can be more effective <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref>. While multi-stage convolutional operations can capture information in large receptive fields, they still lack the ability to fully model skeletal structure in their predictions.</p><p>Several approaches to refine confidence maps have been proposed. First, additional convolutional layers taking joint heat-maps as input can be added to learn implicit spatial dependencies without requiring explicit articulated human body priors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4]</ref>. Second, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref> explicitly resort to graphical models to post-process regressed confidence maps. However, the parameters of part regression networks and spatial inference are learned independently in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. In <ref type="bibr" target="#b36">[37]</ref> an end-to-end trainable framework, combining convolutional operations and spatial refinement is proposed. Our work not only incorporates spatial information but also models temporal dependencies.</p><p>Pose estimation in videos brings new challenges (illustrated in <ref type="figure">Figure 1</ref>) and requires the coupling of parts across frames to ensure accurate and temporally stable predictions. Early work initializes a temporal tracker from few predicted poses in the sequence's initial frames <ref type="bibr" target="#b26">[27]</ref> but suffers from pose drift. Tracking-by-detection schemes have been used to more robustly estimate poses in videos <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. Researchers have also attempted to design spatio-temporal graphs to capture motion in short video sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. However, modeling spatial and temporal dependencies explicitly results in highly interconnected models (i.e., loopy graphs with large tree-width) and exact inference becomes again intractable. One solution is to resort to approximate inference, for instance using sampling based approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref> or loopy belief propagation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>. Alternatively, approximating the original large loopy model into one or multiple simplified tree-based models allows for efficient exact inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Some recent deep learning methods aide predictions in the current frame with information from its neighbors <ref type="bibr" target="#b12">[13]</ref>. Similar to our approach, <ref type="bibr" target="#b19">[20]</ref> directly propagates joint position estimates from previous to the current frame via opti- cal flow. Warped heatmaps from multiple nearby frames are combined as weighted average. Chain models <ref type="bibr" target="#b10">[11]</ref> can capture longer temporal dependencies but makes assumptions about regular motion patterns. Our approach also incorporates spatio-temporal models into deep ConvNets but differs in that it (i) explicitly models the spatial configuration of human poses; (ii) regularizes temporal joint positions using dense optical flow via (iii) a novel inference layer, performing message passing on general loopy spatio-temporal graphs; (iv) and is end-to-end trainable. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our proposed network architecture, consisting of several interconnected layers. Given a thin-slice of a video sequence (i.e., a small number of adjacent frames), a spatial fully ConvNet first regresses joint confidence maps (heat-maps) of joint positions for each input frame <ref type="figure" target="#fig_0">(Figure 2</ref>, (c)). These heat-maps are sent into a flow warping layer and a spatio-temporal inference layer. The flow warping layer ( <ref type="figure" target="#fig_0">Figure 2, (d)</ref>) warps the body part heat-maps by pixel-wise dense optical flow tracks so that they align with its neighboring frame. Finally, both the warped heat-maps and the part heat-maps of the current frame pass through the spatio-temporal inference layer ( <ref type="figure" target="#fig_0">Figure 2</ref>, (e)). This layer conducts inference between body parts spatially and temporally, producing the final joint position estimates <ref type="figure" target="#fig_0">(Figure 2</ref>, (f)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Thin-Slicing Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully convolutional joint regression layer</head><p>Several recent works regress heat-maps of body joints via ConvNets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref>. Such models are usually consist entirely of convolutional operations combined with spatial pooling layers. We leverage such a Conv-Net <ref type="bibr" target="#b34">[35]</ref> as basis for our architecture. More specifically as joint detection layers shown in <ref type="figure" target="#fig_0">Figure 2</ref>, (b). Such models have already demonstrated the ability to capture local appearance properties and outperform hand-designed shallow features by large margins, but occlusions, (self-)symmetries and motion blur still pose significant challenges (cf. <ref type="figure">Figure 1)</ref>. In order to alleviate these problems, a novel spatiotemporal message passing layer (Sec. 3.3) is proposed and incorporated into the network for end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Flow warping layer</head><p>While our goal is to improve temporal stability of joint predictions, we do not incorporate an explicit motion pattern (since human motion tends to be too unpredictable) but instead rely on dense optical flow to propagate information temporally. The joint detection heat-maps, produced by fully convolutional layers, is passed through the flow warping layer to align heat-maps from one frame to the targeted neighbor ( <ref type="figure" target="#fig_0">Figure 2, (d)</ref>). Pixel-wise flow vectors are used to align confidence estimates in neighboring frames to the target frame by shifting confidence values along the track directions. Next, these warped heat-maps serve as input to the spatio-temporal inference layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatio-temporal inference layer</head><p>Incorporating domain specific knowledge into deep networks has been proven to be effective in many vision tasks such as object detection <ref type="bibr" target="#b9">[10]</ref> and semantic segmentation <ref type="bibr" target="#b40">[41]</ref>. In this work, we propose to explicitly incorporate spatio-temporal dependencies into an end-to-end trainable framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling</head><p>Let G = (V, E) represent a graph as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (e), with vertices V and edges E ⊆ V × V denoting the spatio-temporal structure of a human pose. K = |V | is the number of body parts, and i ∈ {1, ..., K} is the i th part. Each vertex corresponds to one of the body parts (i.e., head, shoulders), and each edge represents a connection between two of these parts spatially (blue arrows in <ref type="figure" target="#fig_0">Figure 2</ref>, (e)) or between the same part but distributed temporally (yellow arrows in <ref type="figure" target="#fig_0">Figure 2</ref>, (e)). We denote these edges as E s and E f respectively. Given an image I, a pose p with respect to this graph G is defined as a set of 2D coordinates in the image space representing the positions of the different body parts:</p><formula xml:id="formula_0">p = {p i = (x i , y i ) ∈ R 2 : ∀i ∈ V }.</formula><p>The singleimage pose estimation problem then can be formulated as the maximization of the following score S(I, p) for a pose p given an image I:</p><formula xml:id="formula_1">S(I, p) = i∈V φ i (p i |I) + (i,j)∈Es ψ i,j (p i , p j ), (1) where φ i (p i |I)</formula><p>is the unary term for the body part i at the position p i in image I and ψ i,j (p i , p j ) is the pairwise term modeling the spatial compatibility of two neighboring parts i and j. The unary term provides confidence values of part i based on the local appearance and it is modeled by the fully ConvNet (Sec. 3.1). For pairwise term we use a spring energy model to measure the deformation cost, where</p><formula xml:id="formula_2">ψ i,j (p i , p j ) is defined as w i,j · d(p i − p j ).</formula><p>With standard quadratic deformation constraints d(p i − p j ) = [∆x ∆x 2 ∆y ∆y 2 ] T , where ∆x = x i − x j and ∆y = y i −y j are the relative positions of part i with respect to part j. The parameter w i,j encodes rest location and rigidity of each spring, which can be learned together with the whole network.</p><p>Given a slice of a video sequence I = (I 1 , I 2 , ..., I T ) as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (a), the temporal links (yellow arrows in <ref type="figure" target="#fig_0">Figure 2</ref>, (e)) are introduced among neighboring frames in order to impose temporal consistency for estimating poses P = (p 1 , p 2 , ..., p T ). The objective score function of the entire slice with temporal constrains is then given by:</p><formula xml:id="formula_3">S(I, P) slice = T t=1 S(I t , p t ) + (i,i * )∈E f ψ i,i * (p i , p i * ).</formula><p>(2) Here S(I t , p t ) is the score function for each frame as defined in Eq. (1). The pairwise term ψ i,i * (p i , p i * ) regularizes the temporal consistency of the part i in neighboring frames. Specifically, here p i * = p i * + f i * ,i (p i * ) and f i * ,i (p i * ) is the optical flow evaluated at p i * . This is the flow warping process in which pixel-wise flow tracks are applied to align confidence values in neighboring frames to the target frame. We use the same quadratic spring model to penalize the estimation drift between these neighboring frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>Inference corresponds to maximizing S slice defined in Eq. (2) over p for the image sequence slice. When the relational graph G = (V, E) is a tree-structured graph, exact belief propagation can be applied efficiently by one pass of dynamic programming in polynomial time. However, for cases in which the factor graph is not tree-structured but contains cycles, the belief propagation algorithm is not applicable as no leaf-to-root order can be established. However, loopy belief propagation algorithms such as the Max-Sum algorithm make approximate inference possible in intractable loopy models <ref type="bibr" target="#b8">[9]</ref>. Empirical performance has consistently been reported to be excellent across various problems <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28]</ref>. More precisely, in our case at each iteration a part i sends a message to its neighbors and also receives reciprocal messages along the edges in G:</p><formula xml:id="formula_4">score i (p i ) ← φ i (p i |I) + k∈child(i) m ki (p i ),<label>(3)</label></formula><p>where child(i) is defined as the set of children of part i. The local score i (p i ) is the sum of the unary terms φ i (p i |I) and the messages collected from its all children. The messages m ki (p i ) sent from body part k to part i are given by:</p><formula xml:id="formula_5">m ki (p i ) ← max p k (score k (p k ) + ψ k,i (p k , p i ))<label>(4)</label></formula><p>Eq. (4) computes for every location of part i the best scoring location of its child k, based on the score of part k and the spring model between i and k. This cost maximization process can be efficiently solved via the generalized distance transforms <ref type="bibr" target="#b5">[6]</ref>, reducing the computational complexity to be linear in the number of possible part locations, which is the size of the regressed heat-map from the fully ConvNet (Sec. 3.1). This inference process could be operated by several iterations till convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>In our implementation of the spatio-temporal message passing layer, for the first iteration, the local score for each part is initialized by its corresponding unary term obtained from the regressor layers <ref type="figure" target="#fig_0">( Figure 2,(c)</ref>). The inference process is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>,(e). The children of one node could be either adjacent parts in the same frame or the same part in the neighboring frames. For the first case, the heat-maps of other parts are directly taken as input to the generalized distance transform, while for the second case the score k (p k ) is the heat-map after flow warping <ref type="figure" target="#fig_0">(Figure 2,(d)</ref>). We implement message passing in a broadcasting style where messages are passed simultaneously across every edge in both directions.</p><p>Specifically, for each part i, Eq. (4) computes the best score from its child k. The forward of this maximization process is efficiently solved via the generalized distance transform. The resulting Max location p * for each pixel is stored. Similar to the Max Pooling operation, the backpropagation of Eq. (4) is achieved through sub-gradient decent:</p><formula xml:id="formula_6">∂m ki (p i ) ∂score k (p k ) = 1 if p k = p * , 0 otherwise. ∂m ki (p i ) ∂ψ k,i (p k , p i ) = 1 if p k = p * , 0 otherwise.</formula><p>The gradient for the parameter of the spring model w ki is calculated by ∂m ki (pi)</p><formula xml:id="formula_7">∂w ki = ∂m ki (pi) ∂ψ k,i (p k ,pi) d(p k − p i ), where d(p k − p i ) is the quadratic displacement.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning</head><p>The learning of Thin-Slicing Network is decomposed into two stages: (1) Training fully convolutional layers and (2) Joint training with flow warping and inference layers.</p><p>Training fully convolutional layers As discussed in Sec. 3.1, we deploy fully convolutional layers as the basic regressor to produce the belief maps for all the body parts in the sequence. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>,(c), every pixel position has a confidence value for each joint. The ground truth heat-map for a part i is written as b i * (Y i = p), which is created by placing a Gaussian peak at the center location of the part. In our implementation, we set peak values as 1 and the background as 0. We aim to minimize the l 2 distance between the predicted and ideal belief maps for each part, yielding the loss function:</p><formula xml:id="formula_8">f = K i=1 p b i (p) − b i * (p) 2 .<label>(5)</label></formula><p>We use the stochastic gradient descent algorithm to train these fully convolutional layers with dropouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint training with flow warping and inference layers</head><p>For the second stage of training, the unified end-to-end model ( <ref type="figure" target="#fig_0">Figure 2)</ref> is jointly trained by initializing the weights of the fully convolutional layers with the pre-trained parameters. In this training stage, instead of using l 2 distance loss, we use the hinge loss during optimization. The final loss is defined in Eq. (6), I i (p) is an indicator which is equal to 1 if the pixel lies within a circle of radius r centered on the ground truth joint position, otherwise it is equal to -1:</p><formula xml:id="formula_9">f = K i=1 p max(0, 1 − b i (p) · I i (p)).<label>(6)</label></formula><p>The parameters in the inference layer are differentiable so they can be end-to-end trained alongside the other weights in the network by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we present results from our experimental evaluation of the proposed architecture performed on standard datasets. First we introduce the datasets and the implementation details as used during our experiments. Furthermore, we compare performance of our method with two separate baselines: a fully convolutional network and a ConvNet with spatial inference only. Finally, we compare our results with other state-of-the-art approaches across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct experiments on the Penn Action <ref type="bibr" target="#b39">[40]</ref> and JHMDB <ref type="bibr" target="#b13">[14]</ref> datasets, both standard datasets to evaluate video-based pose estimation.</p><p>Penn Action dataset the Penn Action dataset <ref type="bibr" target="#b39">[40]</ref> is one of the largest datasets with full annotations of human joints in videos,containing 2326 unconstrained videos depicting 15 different action categories and the annotations include 13 human joints for each image. An additional occlusion label for each joint is also provided. We follow the original paper <ref type="bibr" target="#b39">[40]</ref> to split the data into training and testing subsets in a roughly half-half manner. In total there are around 90k images for training and 80k images for testing.</p><p>JHMDB dataset The JHMDB dataset <ref type="bibr" target="#b13">[14]</ref> contains 928 videos and 21 action classes. The dataset provides three different splits of training and testing, and we report the average performance over these three splits for all evaluations on this dataset. The experiments on a subset of this dataset (sub-JHMDB dataset) are also conducted to compare with other state-of-the-art methods. This subset contains 316 clips with 12 action categories. In this subset the whole human body is inside the image so all joints are annotated with ground truth positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Data augmentation to introduce more variation in the training data and thus reducing overfitting, we augment the data by rotating images between -90 to 90 degrees chosen randomly and by scaling by a random factor between 0.5 to 2. When pre-training the fully convolutional layers, the inputs to the network are the cropped image patch around the center of persons with random shifts. For end-to-end training with the flow warping and spatio-temporal message passing layer, the input patches for the sequence are controlled to have the same pre-processing.</p><p>Network parameter settings for the fully convolutional layers, we deploy the network structure based on <ref type="bibr" target="#b34">[35]</ref>. This model has a multiple-stage structure which is designed to alleviate the problem of vanishing gradients. We use an input size of 368 × 368 px in order to cover sufficient context. The batch size is set to 20 for pre-training the convolutional layers and 6 for jointly training the unified network respectively when the thin-slicing is 5 frames. The learning rates are initialized as 0.0005 for the first stage of training and dropped by a factor of 3 every 20k iterations. For end-toend training, the learning rate is set to be lower (0.0001) and is dropped every 5k iterations also by a factor of 3. The dropout rate is set to 0.5 for the first stage and increased to 0.7 for the second stage with flow warping and message passing layers to reduce potential effects of overfitting. The fully ConvNet is trained for 10 epochs for initialization. The unified end-to-end model typically converges after 3-4 epochs. The flow warping layer takes resized optical flow images of the same size as the heat-maps as input with their values rescaled by the same scaling factor. For the spatio-temporal message passing layer, we initialize the weight of the quadratic term to 0.01 and the first-order term to 0 for the generalized distance transform algorithm <ref type="bibr" target="#b5">[6]</ref>. Please note that setting the normalization terms when collecting messages sent from children can help stabilize the training process. A similar observation is also reported in <ref type="bibr" target="#b36">[37]</ref>. We find that three iterations of approximate inference already provides satisfactory results and if not specified otherwise message passing is stopped after three iterations in our experiments.</p><p>Edge connections in the graph The spatio-temporal loopy structure used in this implementation is visualized in <ref type="figure" target="#fig_0">Figure 2, (e)</ref>. Spatially, the structured model has edges coinciding with body limbs and it additionally connects symmetric body parts (e.g., left wrist and right wrist, left knee and right knee) to alleviate the double counting issue. Temporal edges connect the same body parts across two adjacent frames. However, our implementation of the inference layer is flexible and can perform approximate inference on arbitrary loopy graph configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Protocol</head><p>For consistent comparison with prior work on both the Penn Action dataset and the JHMDB dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref>, we use a metric referred to as PCK, introduced in <ref type="bibr" target="#b37">[38]</ref>. A candidate keypoint prediction is considered to be correct if it falls within α · max(h, w) pixels of the ground-truth keypoint, where h and w are the height and width of the bounding box of the instance in question, and α controls the relative threshold for considering correctness. We report results from different settings of α. We also report results that plot accuracy vs normalized distance from ground truth in pixels, where a joint is deemed correctly located if it is within a set distance of d pixels from a ground-truth joint center, where d is normalized by the size of the instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Head Shou Elbo Wris Hip Knee Ankl Mean <ref type="bibr" target="#b18">[19]</ref> 62  <ref type="table">Table 1</ref>. Comparison of PCK@0.2 on Penn Action dataset. We compare our proposed model with baseline model, baseline model with spatial inference and other state-of-the-art methods. We also investigate the performance of independent training ( ), the baseline ConvNet after end-to-end training ( * ) and temporal connection across 2 frames (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Result Analysis for Penn Action Dataset</head><p>Baseline comparison: <ref type="table">Table 1</ref> shows the relative performance on the Penn Action test set. For consistent comparison with previous work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>, the metric PCK@0.2 is used, which means a prediction is considered correct if it lies within (α = 0.2) × max(s h , s w ). We first compare the results from the pure ConvNet baseline model, the spatialonly model and finally our spatio-temporal inference model. The baseline model corresponds to the pure fully ConvNet as described in Sec. 3.1 and is trained with loss Eq. <ref type="bibr" target="#b4">(5)</ref>. We also report the result after only applying spatial inference on top of the heat-maps obtained from the ConvNet, coresponding to only the blue arrows in <ref type="figure" target="#fig_0">Figure 2</ref>,(e). Please note that these two settings essentially treat video-based pose estimation as pure concatenation of single image predictions. Finally, we report the performance of our proposed end-toend trainable network with full spatio-temporal inference. Our baseline setting achieves 87.0% average accuracy for all 13 body parts. Spatial inference with geometric constraints among human body parts in individual images increases the overall result by 4.4%. By incorporating temporal consistency across frames, we observe an additional accuracy gain of 5.1% over spatial inference only. Body parts like head and shoulders are usually visible and less flexible, so even with the baseline model very high detection accuracies can be achieved. However, parts such as elbows and wrists are the most flexible joints of our body. This flexibility can yield configurations with very large variation and these joints are also prone to be occluded by other parts of the body. This is shown by the low detection rates from the baseline part-regression model. With spatial message passing, the accuracy increases, and our proposed model boosts this again by roughly 10%. Note that predictions for shoulders can be negatively influenced by sending or receiving messages from elbows through spatial inference only. However, deploying temporal information helps  in recovering from such errors. <ref type="figure" target="#fig_2">Figure 3</ref> plots the normalized distance to the ground truth annotations. Generally, our proposed model outperforms the baseline model and the one with spatial inference over all levels of the evaluation and across all joints. Interestingly, even for stable (and hence easy to predict) joints like the head, we can still see improvements. In particular when the metric gets more strict (i.e., smaller d). In the cases of more flexible pody parts such as elbows, wrists and knees, a constant improvement for both loose and strict metric can be observed. Especially over the 0.05 to 0.1 region, we can clearly observe more accurate predictions. This further suggests that back-propagating the error from several frames through our spatio-temporal network architecture benefits both unary and pairwise terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of normalized distance curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further evaluations</head><p>We also test the effectiveness of joint training of convolutional layers with message passing. Keeping the weights of convolutional layers fixed, we just train the parameters in the spatio-temporal inference layer. The overall performance is 92.8% <ref type="table">(Table 1</ref>, row annotated by ( )). It improves over the baseline model by 5.8% but could not reach the performance of joint training. As mentioned previously, the end-to-end training helps the fully convolutional layers to capture appearance features better. To validate this claim we conduct the same evaluation using the convolutional layers from the end-to-end trained model (removing the spatio-temporal inference layers) and compare the result with the baseline model (trained standalone). An overall 4% performance increase ( <ref type="table">Table 1</ref>, row annotated by ( * )) can be observed. We also perform the experiment with temporal edges across not only 1 frame but 2  <ref type="formula">(2)</ref>). However, here we do not observe a significant increase of mean accuracy.</p><p>Comparison with state-of-the-art <ref type="table">Table 1</ref> also lists the comparison between the results of previous methods and ours. We first compare with shallow hand-crafted features based works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="bibr" target="#b18">[19]</ref> is based on N-best algorithm and <ref type="bibr" target="#b35">[36]</ref> employs different action specific models. We use the figures reported in <ref type="bibr" target="#b35">[36]</ref> for comparison. We outperform them by a large margin for all body parts. <ref type="bibr" target="#b10">[11]</ref> incorporates deep features with a recurrent structure to model longterm dependency between frames. While only propagating information over short periods of time (thin-slices of the sequence), we still attain an overall performance boost of 4.7% on this dataset. Please note that ours consistently localizes all joints better than prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Result Analysis for JHMDB dataset</head><p>We also conduct a systematic evaluation on the JHMDB dataset <ref type="bibr" target="#b13">[14]</ref>. The average result of three splits on this dataset is illustrated in <ref type="table" target="#tab_1">Table 2</ref>. The first three rows summarize the performance under the PCK@0.2 metric. The same three models and settings as previously are evaluated and we observe results consistent with the experiments conducted on <ref type="figure">Figure 4</ref>. Qualitative results on Penn Action dataset. We visualize connections among challenging limbs (arms and legs). Some failure cases are listed. Our method may miss limbs due to significant occlusions and heavy blur (last row).</p><p>the Penn Action dataset. The proposed end-to-end model boosts the overall performance by a relatively large margin. We also provide results for PCK@0.1 ( <ref type="table" target="#tab_1">Table 2</ref>, row marked with * ). To consistently compare with other state-of-the-art Method Head Shou Elbo Wris Hip Knee Ankl Mean <ref type="bibr" target="#b18">[19]</ref> 79.0 60. <ref type="bibr" target="#b2">3</ref>   <ref type="table">Table 3</ref>. PCK@0.2 results on sub-JHMDB dataset. We compare with other previous methods and our own baselines.</p><p>results, we perform further experiments on a subset of the JHMDB dataset. These subsets remove sequences with incomplete bodies. The comparison is listed in <ref type="table">Table 3</ref>. We outperform shallow feature based methods by a large gap <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, features are taken from the deep ConvNet and a graphical model based inference is conducted independently to refine the result. Our proposed method also provides better performance across all body parts. <ref type="figure">Figure 4</ref> illustrates results from representative sequences taken from our experiments. Our method can capture articulated poses with strong pose changes across several frames. Cases with cluttered background, occlusion, and blur are in-cluded. Failure cases, shown in the bottom row of <ref type="figure">Figure 4</ref>, are often linked to extended periods of motion blur or occlusion across frames. This hinders the ConvNet from capturing local appearance properties and impacts the estimation of dense optical flow. In these cases temporal inference over longer distances may be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed an end-to-end trainable network taking spatio-temporal consistency into consideration to estimate human poses in natural, unconstrained video sequence. We have experimentally shown that leveraging such a unified structured prediction approach outperforms multiple baselines and state-of-the art methods across datasets. Training regression layers jointly with the spatiotemporal inference layer benefits cases that display motion blur and occlusions but also improves predictions of unary terms due to the iterative back-propagation of errors. Interesting directions for future work include incorporation of long-range temporal dependencies and handling of groups of people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Schematic overview of Thin-Slicing Network architecture. Our model takes a small number of adjacent frames as input (a) and fully convolutional layers (b) regress initial body joint position estimates (c). We compute dense optical flow between neighboring frames to propagate joint position estimates through time. A flow based warping layer aligns joint heat-maps to the current frame (d). A spatio-temporal inference layer performs iterative message passing along both spatial and temporal edges of the loopy pose configuration graph (e) and computes final joint position estimates (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>PCK curve for Penn Action dataset. We compare our proposed model with two baselines -ConvNet-only and spatial inferenceonly. Ours yields consistent accuracy improvements across the entire range of strictness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on full JHMDB dataset. The first three rows are based on PCK@0.2 while the results with ( * ) are with PCK@0.1.</figDesc><table><row><cell>Method</cell><cell cols="4">Head Shou Elbo Wris Hip Knee Ankl Mean</cell></row><row><cell>baseline</cell><cell>93.2</cell><cell>72.4 57.3 61.9 88.4 63.6</cell><cell>48.6</cell><cell>70.9</cell></row><row><cell>S-infer</cell><cell>93.6</cell><cell>85.1 72.9 70.1 87.2 66.2</cell><cell>52.2</cell><cell>76.5</cell></row><row><cell>ST-infer</cell><cell>93.6</cell><cell>94.7 84.8 80.2 87.7 68.8</cell><cell>55.2</cell><cell>81.6</cell></row><row><cell cols="2">baseline( * ) 86.2</cell><cell>50.2 42.9 47.4 61.4 43.4</cell><cell>34.1</cell><cell>54.5</cell></row><row><cell>S-infer( * )</cell><cell>86.1</cell><cell>62.8 55.2 51.9 68.3 48.1</cell><cell>36.7</cell><cell>60.2</cell></row><row><cell cols="2">ST-infer( * ) 85.4</cell><cell>77.6 69.4 62.6 76.9 57.4</cell><cell>42.9</cell><cell>68.7</cell></row></table><note>frames (Table 1, row annotated by</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by the ERC Advanced Grant VarCity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2353" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distance transforms of sampled functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose from flow and flow from pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2059" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A revolution: Belief propagation in graphs with cycles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="479" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02346</idno>
		<title level="m">Chained predictions using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pose for action-action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04037</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human pose tracking in monocular sequence using multilevel structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2329" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2627" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06645</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to parse images of articulated bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1129" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strike a pose: Tracking people by finding stylized poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing human motion with stretchable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1281" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3d human figures using 2d image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="702" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measure locally, reason globally: Occlusion-sensitive articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2041" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating articulated human motion with covariance scaled sampling. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="371" to="391" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2012" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimating human pose with flowing puppets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3312" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
