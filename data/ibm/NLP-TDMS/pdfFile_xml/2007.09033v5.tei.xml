<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region-based Non-local Operation for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<email>adrian.bors@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Region-based Non-local Operation for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) model longrange dependencies by deeply stacking convolution operations with small window sizes, which makes the optimizations difficult. This paper presents region-based non-local (RNL) operations as a family of self-attention mechanisms, which can directly capture long-range dependencies without using a deep stack of local operations. Given an intermediate feature map, our method recalibrates the feature at a position by aggregating the information from the neighboring regions of all positions. By combining a channel attention module with the proposed RNL, we design an attention chain, which can be integrated into the off-the-shelf CNNs for end-to-end training. We evaluate our method on two video classification benchmarks. The experimental results of our method outperform other attention mechanisms, and we achieve state-of-the-art performance on the Something-Something V1 dataset. The code is available at: https://github.com/guoxih/regionbased-non-local-network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the rapid development of the Internet, videos have become the main multimedia resource of information, and the analysis of video information is in high demand. Video classification attracts increasing research interest, given the numerous applications for this area. As Convolutional Neural Networks (CNNs) demonstrated high capability for learning visual representations in the image domain, it is natural to attempt to apply CNNs to the video area. An effective way to extend CNN from image to video domain is by changing the convolution kernels from 2D to 3D, aka 3D CNN <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or by adding recurrent operations to CNNs <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>The models based on convolutional or recurrent operations capture long-range dependencies by deeply stacking local operations with small window sizes. However, the deep stack of local operations limits the efficiency of message delivery to distant positions, and makes the optimization difficult <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. To mitigate the optimization difficulties, Wang et al. proposed the non-local (NL) operation <ref type="bibr" target="#b6">[7]</ref> that works as a self-attention mechanism <ref type="bibr" target="#b7">[8]</ref> to capture long-range dependencies directly by exploiting the inner-interactions between positions regardless of their positional distance, which we revisit in Section III-A. However, in the NL operation, the calculation of the relation between two positions only relies on the information from these two positions while not fully utilizing the information around them. As a result, its calculation of positional relationships is not robust to noise or unrelated features, especially in high resolution, which has been emphasized in <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this paper, we investigate the non-local operation <ref type="bibr" target="#b6">[7]</ref> and propose a region-based non-local (RNL) operation based on the non-local mean concept <ref type="bibr" target="#b8">[9]</ref>, which enhances the calculation of positional relationships by fully utilizing the information from neighboring regions. The proposed RNL operation endows CNNs with a global view of input features without needing a deep stack of local operations to ease the optimization difficulties. In <ref type="figure" target="#fig_0">Figure 1</ref>, we illustrate an example to demonstrate that the proposed RNL operation can better capture positional relationships than NL operation. There are two advantages of the proposed RNL compared with the original NL: first of all, RNL is more robust to noise or unrelated features; secondly, the RNL is more computationally efficient. Meanwhile, we present various instantiations of the RNL operation to meet different application requirements. By adding RNL operation into the off-the-shelf CNNs, we obtain a new video classification architecture named regionbased non-local network. In order to evaluate the effectiveness of our method, we conduct video classification experiments on two large-scale video benchmarks, Kinetics-400 <ref type="bibr" target="#b1">[2]</ref> and Something-Something V1 <ref type="bibr" target="#b9">[10]</ref>. Our models outperform the baseline and other popular attention mechanisms, and achieve state-of-the-art performance on Something-Something V1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Spatio-temporal Networks. With the tremendous success of CNNs on image classification tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Some research studies have attempted to extend the applications of CNNs to video-based classification tasks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Among them, the two-stream model <ref type="bibr" target="#b18">[19]</ref> and its variant <ref type="bibr" target="#b21">[22]</ref> learn temporal evolution by using jointly the optical flow stream and the RGB stream for video classification. The recent video models <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref> leverage long short-term memory (LSTM) to fuse frame-level CNN representations for modeling long-term temporal relationships. However, 2D CNN+LSTM <ref type="bibr" target="#b1">[2]</ref> empirically shows lower performance than two-stream architectures. CNNs employing 3D convolution processing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref> represent a promising research direction for spatio-temporal representation learning, but the training of 3D CNNs has huge computational demands. Some research studies have devoted to simplifying 3D CNNs, such as P3D <ref type="bibr" target="#b23">[24]</ref>, TSM <ref type="bibr" target="#b24">[25]</ref>, S3D <ref type="bibr" target="#b25">[26]</ref>, CSN <ref type="bibr" target="#b26">[27]</ref>, X3D <ref type="bibr" target="#b27">[28]</ref>. serious, and there is not much research on this problem, which is the main theme of this paper.</p><p>Attention Mechanisms. Attention mechanisms have been initally used for machine translation <ref type="bibr" target="#b28">[29]</ref>. Recent works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> would embed task-specific attention mechanisms to CNNs to boost up performance and robustness in visual tasks.</p><p>In computer vision, attention mechanisms can be decomposed into two components, channel attention -focusing on 'what' is meaningful, and spatial (or spatio-temporal) attentionfocusing on 'where' is informative <ref type="bibr" target="#b31">[32]</ref>. For example, The Squeeze-and-Excitation (SE) module is a representative channel attention mechanism, which utilizes global average-pooled features to exploit the inter-channel relationships. Inspired by the classic non-local mean algorithm <ref type="bibr" target="#b8">[9]</ref> for image denoising, Wang, et al. <ref type="bibr" target="#b6">[7]</ref> introduced the self-attention concept <ref type="bibr" target="#b7">[8]</ref> from machine translation to large-scale visual classification tasks, and proposed non-local (NL) operation for video classification. The NL operation was initially designed to learn spatiotemporal attention. However, Cao et al. <ref type="bibr" target="#b32">[33]</ref> observe that NL can only capture the global context of channels, aka channel attention. Moreover, they demonstrate that the intrinsic natures of the NL operation and SE module <ref type="bibr" target="#b29">[30]</ref> are the same while the implementation of the SE module is rather economical.</p><p>In this paper, we redesign the non-local operation and propose the region-based non-local operation which increases the effectiveness and efficiency in capturing the spatio-temporal attention. Yue et al. <ref type="bibr" target="#b33">[34]</ref> also aimed to improve the NL operation, proposing a compact generalized version of the NL operation by integrating channel attention and spatio-temporal attention into a compact module. However, their work do not improve the effectiveness of NL operation. Instead of simplifying the NL, we focus on improving the effectiveness of NL for better capturing the spatio-temporal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NON-LOCAL METHODS FOR VIDEO CLASSIFICATION A. Revisiting the Non-local (NL) Operation</head><p>Intuitively, the non-local operation <ref type="bibr" target="#b6">[7]</ref>, illustrated in Figure 2 (b), strengthens the feature in a certain position via aggregating the information from other positions. The estimated value for a position, is computed as a weighted sum of the feature values of all other positions. Formally, we denote x, y ∈ R T HW ×C as the input and output of an NL operation, flattened along the space-time directions, where T , H, W and C are temporal length (depth), height, width and the number of channels, respectively. Then, the NL operation can be described as:</p><formula xml:id="formula_0">y i = 1 C(x) ∀j w i,j W g x j , w i,j = f (x i , x j ),<label>(1)</label></formula><p>where x i , x j ∈ R C are the i-th and j-th element of x, i is the index of a reference position, and j enumerates all possible positions. W g is a learnable weight matrix that computes a representation of x j , and C(x) is the normalization factor. Meanwhile, w i,j is a weight, representing the relationship between positions i and j, which is calculated by pairwise similarity function f (·, ·). Regarding the form for f (·, ·), Wang et al. <ref type="bibr" target="#b6">[7]</ref> propose four instantiations for the non-local operation, of which the embedded Gaussian form is described</p><formula xml:id="formula_1">as f (x i , x j ) = e θ(xi) T φ(xj ) , C(x) = Σ ∀j f (x i , x j ), where θ and φ represent linear transformations, implemented with 1 × 1 × 1 convolutions.</formula><p>Attention Maps of the Non-local Operation. In the NL operation, each output element y i is a weighted average of the input features over all positions x j , and therefore each y i has a corresponding attention weight map calculated by f (·, ·), highlighting the areas related to position i. In <ref type="figure" target="#fig_0">Figure 1</ref> (b), we randomly pick one video from Kinetics-400 and visualize the attention maps of NL at two different reference positions, one of which is located in the background area while the other is located in the region of the moving object. In the original NL operation, its attention maps with different reference positions are almost the same, which indicates that this fails to capture the positional relations. The NL operation realistically learns channel-wise attention rather than spatio-temporal attention.</p><p>We redesign the non-local operation as a spatio-temporal attention mechanism, namely the region-based non-local operation (RNL). <ref type="figure" target="#fig_0">Figure 1</ref> (a) shows that our RNL operation only highlights the regions related to the reference position, which indicates that the proposed RNL operation can effectively learn spatio-temporal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Region-based non-local (RNL) Operation</head><p>The initial idea for the RNL operation is that the relation between two positions in a video representation should not rely on just their own features but also on those features from their neighborhoods. Therefore, for each position i of input sample x, we define a cuboid region N i of fixed size centered at position i. The calculation of the relationship w i,j between positions i and j is redefined as:</p><formula xml:id="formula_2">w i,j = f (θ(N i ), θ(N j )),<label>(2)</label></formula><p>where, θ(·) denotes an information aggregation function that separately summarizes the features in a region for each channel. Function θ(·) is given by</p><formula xml:id="formula_3">θ(N i ) = k∈Ni u k x k ,<label>(3)</label></formula><p>where denotes element-wise multiplication and u k denotes a vector shared by all cuboid regions N i . As there is no channel interaction in θ(·), it can be implemented as channel-wise 1 convolutions <ref type="bibr" target="#b34">[35]</ref>, or as average/max pooling. By replacing the expression of w i,j from equation (1) with the expression from (2), the RNL operation can be written as:</p><formula xml:id="formula_4">y i = 1 C(x) ∀j f (θ(N i ), θ(N j ))x j .<label>(4)</label></formula><p>From equation <ref type="formula" target="#formula_4">(4)</ref>, we can see that by employing the RNL operation, the new feature of each position is a weighted sum of the old features from all positions, where the weights are calculated by the similarity function f (·, ·) according to the similarity between the target region, and all the other regions. The proposed RNL operation enhances the calculation of positional relations by fully utilizing the information from the neighboring regions, which increases the robustness to noise or unrelated features, Hence, the RNL operation can learn more meaningful representations in comparison with NL.</p><p>For the form of function f (·, ·), in addition to adopting the Gaussian version and the Dot product version as in <ref type="bibr" target="#b6">[7]</ref>, we also propose a new form, called the Cosine version. Specifically, the Gaussian form of f (·, ·) is given by</p><formula xml:id="formula_5">f (θ(N i ), θ(N j )) = e θ(Ni) T θ(Nj ) .<label>(5)</label></formula><p>The Dot product form of f (·, ·) measures the relation between two regions by using the dot-product similarity:</p><formula xml:id="formula_6">f (θ(N i ), θ(N j )) = θ(N i ) T θ(N j ).<label>(6)</label></formula><p>However, the dot-product similarity takes into account both the vector angle and the magnitude, as θ(N i ) T θ(N j ) = θ(N i ) θ(N j ) cos ψ i,j , where ψ i,j is the angle between vectors θ(N i ) and θ(N j ). It is preferable to replace dotproduct similarity with the cosine similarity, ignoring the vector magnitude and resulting in a value within the range</p><formula xml:id="formula_7">[−1, 1]. The Cosine form of f (·, ·) is expressed as: f (θ(N i ), θ(N j )) = ReLU( θ(Ni) T θ(Nj ) θ(Ni) θ(Nj ) ) = ReLU(cos ψ i,j ).<label>(7)</label></formula><p>When f (θ(N i ), θ(N j )) &lt; 0, it indicates that the features in positions i and j are not related. As the new feature in a cetrain position should only be determined by those related features, we use the ReLU function to restrict the output of f (·, ·) to be non-negative. The normalization factor is set as (a) conventional convolution (b) channel-wise convolution <ref type="figure">Fig. 3</ref>. Illustrations of the conventional convolution (a) and the channelwise convolution (b). The total number of connections of the channel-wise convolution <ref type="bibr" target="#b34">[35]</ref> is reduced to 1 C of that of the conventional convolution.</p><formula xml:id="formula_8">C(x) = ∀j f (θ(N i ), θ(N j ))</formula><p>for the Gaussian version from <ref type="bibr" target="#b4">(5)</ref>, and set as C(x) = T HW for the Dot-product and Cosine versions from equations <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Region-based non-local Block</head><p>In order to embed the RNL operation into the off-the-shelf CNNs without influencing the results provided by the pretrained kernels, we embed the RNL operation into a residual style block <ref type="bibr" target="#b4">[5]</ref>, named the RNL block. The Gaussian RNL block, defined by <ref type="formula" target="#formula_5">(5)</ref>, is written as a matrix form as:</p><formula xml:id="formula_9">z = yW z + x,<label>(8)</label></formula><formula xml:id="formula_10">y = sof tmax(F θ (xW g )(F θ (xW g )) T )xW g ,<label>(9)</label></formula><p>where z is the output that represents the feature after recalibration, W z ∈ R C 2 ×C and W g ∈ R C× C 2 are learnable weight matrices, which are implemented as 1 × 1 × 1 convolutions, and '+x' denotes a residual term. F θ denotes the operation that corresponds to the matrix form of function θ(·) from equation <ref type="formula" target="#formula_3">(3)</ref>. We present the architectures of the Gaussian RNL block and the Gaussian embedding version of the original NL block in <ref type="figure" target="#fig_1">Figure 2</ref>. We can observe that the original NL block illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (b) uses four 1 × 1 × 1 convolutions, while the proposed RNL block shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a) uses two 1 × 1 × 1 convolutions and one channel-wise convolution, which reduces the computational complexity significantly.</p><p>Next, we explain two main implementations of the region information aggregation function F θ in RNL operation.</p><p>1) Channel-wise Convolutions. It is worthwhile to note that, in principle, the candidates for implementing F θ should not fuse together information across channels. Otherwise, the new feature embedding might fail to represent its original information, which is why we cannot adopt conventional convolutions. In contrast, channel-wise convolution <ref type="bibr" target="#b34">[35]</ref>, exemplified in <ref type="figure">Figure 3</ref>, is a perfect candidate for the implementation of F θ , as there is no interaction between the channels. An additional benefit that the channel-wise convolution brings is that it reduces the computation and the parameters by a factor of C, compared with the conventional convolution. The kernel size of the channel-wise convolution has a significant impact on performance, as it corresponds to how large a region N i is considered for information aggregation. We will explore the effectiveness of various kernel sizes, in Section IV-A. </p><formula xml:id="formula_11">res2   1 × 1 × 1, 64 1 × 3 × 3, 64 1 × 1 × 1, 256   × 3 8 × 56 × 56 res3   1 × 1 × 1, 128 1 × 3 × 3, 128 1 × 1 × 1, 512   RNL × 4 8 × 28 × 28 res4   1 × 1 × 1, 256 1 × 3 × 3, 256 1 × 1 × 1, 1024   RNL × 6 8 × 14 × 14 res5   1 × 1 × 1, 512 1 × 3 × 3, 512 1 × 1 × 1, 2048   × 3 8 × 7 × 7</formula><p>2) Average/Max Pooling. The other implementation options for F θ are the average pooling and max pooling, which have been widely adopted for information aggregation. Although it shows a relatively weaker capability than the implementation of channel-wise convolution, average/max pooling adds no extra parameters to the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention Chain</head><p>When the proposed RNL block can learn the long-range dependencies for each position in the spatio-temporal dimension, the squeeze-excitation (SE) block <ref type="bibr" target="#b29">[30]</ref> can learn the long-range dependencies in the channel dimension. In order to capture both spatio-temporal attention and channel-wise attention in a single module, we embed the SE block <ref type="bibr" target="#b29">[30]</ref> together with the RNL block to form an attention chain module (SE+RNL). Firstly, we modify the SE block <ref type="bibr" target="#b29">[30]</ref>, where the squeeze operation F sq is expressed as:</p><formula xml:id="formula_12">s = F sq (x) = 1 T HW T HW i=1 x i ,<label>(10)</label></formula><p>and the excitation operation F ex is expressed as:</p><formula xml:id="formula_13">s = F ex (s ) = W 2 ReLU(BN(W 1 s )),<label>(11)</label></formula><p>where W 1 ∈ R C 2 ×C and W 2 ∈ R C× C 2 are learnable weights, which can be implemented with fully-connected (FC) layers. In the excitation operation F ex , we add a batch normalization (BN) layer <ref type="bibr" target="#b35">[36]</ref> right after the FC layer W 1 to reduce the internal covariate shift. Subsequently, we reshape s ∈ R C into R 1×C . The output of the SE block is given by:</p><formula xml:id="formula_14">v = x ⊕ s,<label>(12)</label></formula><p>where ⊕ refers to the element-wise addition broadcasting in unmatched dimensions (replicate x to match the dimension of s). After that, we place the RNL block after the SE block to form an attention chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Network Architecture</head><p>The RNL block is designed to be compatible with most existing CNNs. It can be plugged into a CNN at any processing stage, resulting in an RNL network. For the implementation, we use ResNet-50 <ref type="bibr" target="#b4">[5]</ref> with the temporal shift modules (TSM) <ref type="bibr" target="#b24">[25]</ref> as the backbone network to build our model (RNL TSM), and its structures is provided in <ref type="table" target="#tab_0">Table I</ref>. The TSM is a lightweight module enabling 2D CNNs to achieve temporal modeling by shifting part of the channels along the temporal dimension, which facilitates the information exchange among neighboring frames. In this architecture, we keep the temporal size constant, which means all the layers in the network only reduce the spatial size of the input features. The backbone network is also the baseline for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We perform video classification experiments on two standard video benchmarks, Kinetics-400 <ref type="bibr" target="#b1">[2]</ref> and Something-Something V1 <ref type="bibr" target="#b9">[10]</ref>. Kinetics-400 is a large-scale video classification benchmark that consists of ∼300K video clips, classified into 400 categories. Something-Something V1 consists of ∼108K videos from 174 categories. We report Top-1, Top-5 accuracy on the validation sets and the computational cost (in GFLOPs) of a single, spatially center-cropped clip to comprehensively evaluate the effectiveness and efficiency. <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_3">Figure 5</ref> visualize some examples of the attention maps of RNL operation, which shows RNL operation can correctly learn the relations between positions. Training and Inference. Our models are pretrained on Im-ageNet <ref type="bibr" target="#b36">[37]</ref>. For the training, we follow the setting from <ref type="bibr" target="#b6">[7]</ref> and use a spatial size of 224 × 224, which is randomly cropped from a resized video frame. The temporal size is set as 8 frames unless otherwise specified. In order to prevent overfitting, we add a dropout layer after the global pooling layer. We optimize our models using the Stochastic Gradient Descent, and train the models for 50 epochs with a cosine decay learning rate schedule. The batch size is set at 64 across multiple GPUs. For Kinetics, the initial learning rate, weight decay and dropout rate are set to 0.01, 1e-4 and 0.5 respectively; for Something-Something, these hyperparameters are set to 0.02, 8e-4, and 0.8 respectively. In the inference, we follow the common setting in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Unless stated otherwise, we uniformly sample 10/2 clips for Kinetics-400/Something-Something V1, and perform spatially fully convolutional inference (three crops of size 256 × 256 to cover the spatial dimensions) for all clips, and the videolevel prediction is obtained by averaging all the clip prediction scores of a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Studies</head><p>We explore the most efficient and effective form of RNL operation on Kinetics-400. By default, the function f (·, ·) of RNL operation is implemented by using the equation <ref type="bibr" target="#b4">(5)</ref>, and F θ is implemented by a channel-wise convolution with a kernel size of 3 × 7 × 7, unless otherwise specified. Following the results from <ref type="bibr" target="#b6">[7]</ref>, we add RNL blocks to the res3 and res4 stages in the architecture shown in <ref type="table" target="#tab_0">Table I</ref>. Our exploration is organized in three parts. First, we search for the effective kernel size of F θ in RNL blocks. Next, we evaluate the performance of various instantiations of RNL and find out the efficient and effective one. Finally, we combine the selected version of RNL with an SE block to form an attention chain module. Kernel Size. The kernel size of F θ (determining the size of region N i ) in the RNL block has a significant impact on the performance as it affects what the RNL operation would learn. Large kernels are supposed to be robust to noise, while small kernels would consider the details and fine structures from video sequences. By considering that the features learned by the kernel from the temporal and spatial dimensions are different, we separately explore the temporal and spatial sizes of the kernel by fixing one while varying the other. The results are shown in <ref type="table" target="#tab_0">Table II</ref> (a). We observe that in the temporal dimension, the size of 3 surpasses other options regardless of the spatial size of the kernel, while in the spatial dimension, the size of 7 is the best option. Therefore, we expect the kernel of 3×7×7 is the best option in space and time, and it has been verified through our grid search. Concurrently, we evaluate the influence of the kernel size of F θ to the model performance by visualizing the attention maps of the RNL operation, shown in <ref type="figure" target="#fig_2">Figure 4</ref>, where the RNL operation considers the highlighted areas to have strong relations with the reference position, indicated by a red point. <ref type="figure" target="#fig_2">Figure 4</ref> shows that a kernel of a small size spatially, such as 1 × 1, tends to incorrectly interpret the relations between some background areas and the foreground areas. In contrast, a kernel with larger spatial size can learn more precise relations between such positions. For example, the kernel of size 7 × 7 precisely highlights the moving object in in <ref type="figure" target="#fig_2">Figure 4</ref> when the reference position is located at the moving object. However, too large kernels could also lead to performance degradation. For example, the kernel of size 3 × 9 × 9 has a lower accuracy than the kernel of size 3 × 7 × 7 (73.51% vs. 73.66%), and the kernel of 7 × 7 × 7 shows a lower performance than the kernel of size 3 × 7 × 7 (73.11% vs. 73.66%). The kernel of size 1 × 1 × 1 has a lower accuracy than the others except for 7×1×1 and 7×7×7, which verifies our assumption that the relation between two positions should not rely on just their own features but also on features from their neighborhoods. Instantiations. There are various solutions for f (·, ·) from equation (4) and for F θ from equation <ref type="formula" target="#formula_10">(9)</ref>, as discussed in Section III-B and Section III-C, respectively. In the following, we conduct ablation studies on the instantiations by fixing a specific choice for either f (·, ·) or F θ while changing the other. The operation F θ can be implemented as a channel-wise convolution or as the average/max pooling, the stride of which is set as 1, and the padding of which is half of the kernel size. From the results shown in <ref type="table" target="#tab_0">Table II</ref> (c), we can see that the channel-wise convolution implementation achieves a higher accuracy with +0.44% and +0.19% than the average and max pooling, respectively. However, the implementation of average/max pooling is more efficient and adds fewer parameters (-  </p><formula xml:id="formula_15">1 × 1 3 × 3 5 × 5 7 × 7 9 × 9</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>In order to evaluate the efficiency and effectiveness of our method in comparison with other attention mechanisms, we reimplement the original NL network <ref type="bibr" target="#b6">[7]</ref>, GCNet <ref type="bibr" target="#b32">[33]</ref> (a simplified NL network), SE network <ref type="bibr" target="#b29">[30]</ref> and CBAM network <ref type="bibr" target="#b31">[32]</ref>. <ref type="table" target="#tab_0">Table III</ref> presents the results on Kinetics and Something-Something. We can see that the proposed RNL block achieves higher performance than other attention mechanisms. Notably, the network with 5 RNL blocks outperforms the network with 5 NL blocks with +0.27% on Kinetics and +1% on Something-Something, while the computational complexity required in FLOPs of the RNL network is 8.23G less than that of the NL network. Furthermore, by adding 5 blocks of the attention chain (SE + RNL), as described in Section III-D, to the backbone network, the performance is further improved (74.97% on Kinetics and 49.47% on Something-Something). In the visualization examples of the RNL and NL blocks, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we observe that the attention maps of the RNL block would only highlight those regions related to the reference positions. However, the attention maps of the original NL block always highlight the same regions for different reference positions. The observation demonstrates that the RNL block can capture the spatio-temporal attention while the NL block only captures the channel attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with the State-of-the-Art</head><p>We compare the proposed method with the state-of-the-art methods on Kinetics-400 and Something-Something V1. In order to achieve the best performance on Kinetics-400, we increase the number of training epochs from 50 to 100. The performance comparisons are summarized in <ref type="table" target="#tab_0">Tables IV and V,</ref> where RNL TSM refers to the model with 5 attention chain blocks. Note that using the same approach, the models with deeper backbone networks or longer clips as training inputs would consistently result in better performance in comparison with shallower backbone networks. on Kinetics, we use a shallower network, such as ResNet-50, as the backbone, and  the length of our input video clips is at least 8 times shorter than other methods, yet our results are highly competitive with those of the other approaches. On Something-Something V1, when using ResNet-50 as the backbone, the ensemble version of our model, the RNL TSM En , using {8, 16} frames as inputs, achieves a higher accuracy than other approaches, w.r.t., single-clip &amp; centercrop (Top-1: 51.3%) and multi-clip &amp; multi-crop (Top-1: 52.7%). When adopting ResNet-101 as the backbone, we gain extra performance boost (Top-1: 50.8% vs. 49.5%). Moreover, the ensemble of the deep model of 8 frame inputs and the shallow model of 16 frame inputs achieves the best accuracy (Top-1: 54.1%). All these results further demonstrate the effectiveness and efficiency of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we presented the region-based non-local operation (RNL), a novel self-attention mechanism that effectively captures long-range dependencies by exploiting pairwise region relationships. The RNL blocks can be easily embedded into the off-the-shelf CNNs architectures for end-toend training. We have performed ablation studies to investigate the effectiveness of the proposed RNL operation in various settings. To verify the efficiency and effectiveness of the proposed methodology, we conducted experiments on two video benchmarks, Kinetics-400 and Something-Something V1. The results of the proposed method are shown to outperform the baseline and other recently proposed attention methods. Furthermore, we achieve state-of-the-art performance on Something-Something V1, which has demonstrated the powerful representation learning ability of our models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Nevertheless, the inefficiency of message delivery caused by the deep stacking of local operations in 3D CNNs remains arXiv:2007.09033v5 [cs.CV] 2 Feb 2021 Examples of visualizing the attention maps of RNL and NL operations in the res4 stage of ResNet on a video clip from Kinetics-400. Given a reference position, an ideal non-local operation should only highlight the regions related to the reference position. In the same video clip, the NL operation has almost the same attention maps at different reference positions while the proposed RNL operation presents query-specific attention maps, which demonstrate that the proposed RNL operation can better compute the relationships between positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Diagrams of implementing the NL and RNL operations in (b) and (a), respectively, indicating the shaping and the reshaping operations of a tensor together with the connections. ⊗ denotes matrix multiplication while ⊕ denotes element-wise addition. The blue boxes denote 1 × 1 × 1 convolutions, and the red box F θ denotes a 3 × 7 × 7 channel-wise convolution or an average/max pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization the attention maps of the RNL block when considering different kernel sizes in the res3 stage by giving the reference position (red point). When the reference point is located at the moving object, the RNL operation with proper kernel size should just highlight the related moving regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of attention maps of the RNL in the res3 stage, with different reference positions on frames from Kinetics (1st row) and Something-Something (2nd row). Given a video clip, the RNL operation only highlights those regions related to the reference position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>ARCHITECTURE OF THE RNL NETWORK. THE KERNEL SIZE AND THE OUTPUT SIZE ARE SHOWN IN THE SECOND AND THIRD COLUMNS, RESPECTIVELY. THE RNL BLOCKS ARE INSERTED AFTER THE RESIDUAL BLOCKS SHOWN IN BRACKETS, WHERE THE TEMPORAL SHIFT MODULES [25] ARE EMBEDDED INTO THE CONVOLUTIONAL LAYERS. × 7, 64, stride 1,2,2 8 × 112 × 112 pool1 1 × 3 × 3, 64, stride 1,2,2 8 × 56 × 56</figDesc><table><row><cell>Layer</cell><cell>Operation</cell><cell>Output size</cell></row><row><cell>conv1</cell><cell>1 × 7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EXPLORATION</head><label>II</label><figDesc>OF THE EFFECTIVENESS AND EFFICIENCY OF VARIOUS RNL MODULES ON KINETICS-400. FOR THE MODELS IN (A) AND (C), WE INSERT ONE GAUSSIAN RNL BLOCK INTO THE RES3 STAGE OF RESNET-50.</figDesc><table><row><cell cols="4">Kernel size Top-1 (%) Kernel size Top-1 (%)</cell><cell cols="3"># RNL Method(f (·, ·)) Top-1 (%)</cell><cell>Method (F θ )</cell><cell cols="3">Top-1 (%) GFLOPs Params</cell></row><row><cell>1 × 1 × 1</cell><cell>73.28</cell><cell>3 × 3 × 3</cell><cell>73.53</cell><cell></cell><cell>Dot-product</cell><cell>73.22</cell><cell>channel-wise conv</cell><cell>73.66</cell><cell>1.65</cell><cell>2.67M</cell></row><row><cell>3 × 1 × 1</cell><cell>73.41</cell><cell>3 × 5 × 5</cell><cell>73.27</cell><cell>1</cell><cell>Gaussian</cell><cell>73.66</cell><cell>average pooling</cell><cell>73.22</cell><cell>1.65</cell><cell>0.26M</cell></row><row><cell>7 × 1 × 1</cell><cell>73.12</cell><cell>3 × 7 × 7</cell><cell>73.66</cell><cell></cell><cell>Cosine</cell><cell>73.46</cell><cell>max pooling</cell><cell>73.47</cell><cell>1.65</cell><cell>0.26M</cell></row><row><cell>1 × 3 × 3</cell><cell>73.32</cell><cell>3 × 9 × 9</cell><cell>73.51</cell><cell></cell><cell>dot-product</cell><cell>74.16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 × 7 × 7</cell><cell>73.43</cell><cell>7 × 7 × 7</cell><cell>73.11</cell><cell>5</cell><cell>Gaussian</cell><cell>74.68</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 × 9 × 9</cell><cell>73.32</cell><cell>7 × 9 × 9</cell><cell>73.30</cell><cell></cell><cell>Cosine</cell><cell>74.40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(a) RNL blocks with different kernel sizes of</cell><cell cols="3">(b) Instantiations of the RNL with</cell><cell cols="4">(c) Instantiations of RNL with different</cell></row><row><cell>F θ .</cell><cell></cell><cell></cell><cell></cell><cell cols="2">different form of f (·, ·).</cell><cell></cell><cell cols="2">implementations of F θ .</cell><cell></cell><cell></cell></row><row><cell cols="6">2.4M) to the model compared to the channel-wise convolution.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">We instantiate three versions of the RNL operation, such as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Gaussian, Dot-product and Cosine, provided in equations (5),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(6) and (7) respectively. The results are shown in Table II (b).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">By adding a single RNL block into the backbone network, the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">result of the Gaussian RNL outperforms the Dot-product and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Cosine versions. Moreover, the performance of all installations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">of the RNL operation can be further improved by stacking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">more RNL blocks. The model with 5 Gaussian RNL blocks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(3 in the res4 stage and 2 in the res3 stage) gains an additional</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">1.02% accuracy increase in comparison with adding a single</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RNL block.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISONS</head><label>III</label><figDesc>BETWEEN VARIOUS VISUAL ATTENTION MECHANISMS ON KINETICS-400 AND SOMETHING-SOMETHING V1.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">Top-1 (%) FLOPs (G) # Param (M)</cell></row><row><cell></cell><cell>baseline</cell><cell>72.80</cell><cell>32.89</cell><cell>24.33</cell></row><row><cell></cell><cell>+ 5 SE</cell><cell>73.70</cell><cell>32.89</cell><cell>24.79</cell></row><row><cell>Kinetics-</cell><cell>+ 5 CBAM</cell><cell>73.99</cell><cell>32.90</cell><cell>24.80</cell></row><row><cell>400</cell><cell>+ 5 GC</cell><cell>73.76</cell><cell>32.90</cell><cell>24.79</cell></row><row><cell></cell><cell>+ 5 NL</cell><cell>74.41</cell><cell>49.38</cell><cell>31.69</cell></row><row><cell></cell><cell>+ 5 RNL</cell><cell>74.68</cell><cell>41.15</cell><cell>35.48</cell></row><row><cell></cell><cell>+ 5 [SE+RNL]</cell><cell>74.97</cell><cell>41.16</cell><cell>35.95</cell></row><row><cell cols="2">Something-baseline</cell><cell>46.63</cell><cell>32.89</cell><cell>24.33</cell></row><row><cell cols="2">Something + 5 NL</cell><cell>48.25</cell><cell>49.38</cell><cell>31.69</cell></row><row><cell>V1</cell><cell>+ 5 RNL</cell><cell>49.24</cell><cell>41.15</cell><cell>35.48</cell></row><row><cell></cell><cell>+ 5 [SE+RNL]</cell><cell>49.47</cell><cell>41.16</cell><cell>35.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>ON KINETICS-400.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Training</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell>Frames</cell><cell></cell><cell></cell></row><row><cell>I3D RGB [2]</cell><cell>Inception</cell><cell>64</cell><cell>72.1</cell><cell>90.3</cell></row><row><cell>S3D-G RGB [26]</cell><cell>Inception</cell><cell>64</cell><cell>74.7</cell><cell>93.4</cell></row><row><cell>TSM [25]</cell><cell>ResNet-50</cell><cell>8</cell><cell>74.1</cell><cell>91.2</cell></row><row><cell>TSM [25]</cell><cell>ResNet-50</cell><cell>16</cell><cell>74.7</cell><cell>-</cell></row><row><cell>NL I3D [7]</cell><cell>ResNet-50</cell><cell>32</cell><cell>74.9</cell><cell>91.6</cell></row><row><cell>Slow [38]</cell><cell>ResNet-50</cell><cell>8</cell><cell>74.9</cell><cell>91.5</cell></row><row><cell>SlowFast [38]</cell><cell>ResNet-50</cell><cell>4+32</cell><cell>75.6</cell><cell>92.1</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>8</cell><cell>75.6</cell><cell>92.3</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>16</cell><cell>77.2</cell><cell>93.1</cell></row><row><cell>RNL TSM En (ours)</cell><cell>ResNet-50</cell><cell>8+16</cell><cell>77.4</cell><cell>93.2</cell></row><row><cell>NL I3D [7]</cell><cell>ResNet-50</cell><cell>128</cell><cell>76.5</cell><cell>92.6</cell></row><row><cell>NL I3D [7]</cell><cell>ResNet-101</cell><cell>128</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>SlowFast [38]</cell><cell>ResNet-101</cell><cell>16+64</cell><cell>78.9</cell><cell>93.5</cell></row><row><cell>LGD-3D RGB [39]</cell><cell>ResNet-101</cell><cell>128</cell><cell>79.4</cell><cell>94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V RESULTS</head><label>V</label><figDesc>ON SOMETHING-SOMETHING V1.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="3">Frame×Clip×Crop Top-1 Top-5</cell></row><row><cell>I3D [40]</cell><cell>ResNet-50</cell><cell>192=32×2×3</cell><cell>41.6</cell><cell>72.2</cell></row><row><cell>NL I3D [40]</cell><cell>ResNet-50</cell><cell>192=32×2×3</cell><cell>44.4</cell><cell>76.0</cell></row><row><cell cols="2">NL I3D + GCN [40] ResNet-50</cell><cell>192=32×2×3</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>TSM [25]</cell><cell>ResNet-50</cell><cell>8=8×1×1</cell><cell>45.6</cell><cell>74.2</cell></row><row><cell>TSM [25]</cell><cell>ResNet-50</cell><cell>16=16×1×1</cell><cell>47.2</cell><cell>77.1</cell></row><row><cell>TSM En [25]</cell><cell>ResNet-50</cell><cell>24=(8+16)×1×1</cell><cell>49.7</cell><cell>78.5</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>8=8×1×1</cell><cell>47.3</cell><cell>-</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>16=16×1×1</cell><cell>49.4</cell><cell>-</cell></row><row><cell cols="2">RNL TSM En (ours) ResNet-50</cell><cell>24=(8+16)×1×1</cell><cell>51.3</cell><cell>80.6</cell></row><row><cell>SmallBig [41]</cell><cell>ResNet-50</cell><cell>48=8×2×3</cell><cell>48.3</cell><cell>78.1</cell></row><row><cell>SmallBig [41]</cell><cell>ResNet-50</cell><cell>96=16×2×3</cell><cell>50.0</cell><cell>79.8</cell></row><row><cell>SmallBig En [41]</cell><cell>ResNet-50</cell><cell>144=(8+16)×2×3</cell><cell>51.4</cell><cell>80.7</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>48=8×2×3</cell><cell>49.5</cell><cell>78.4</cell></row><row><cell>RNL TSM (ours)</cell><cell>ResNet-50</cell><cell>96=16×2×3</cell><cell>51.0</cell><cell>80.3</cell></row><row><cell cols="2">RNL TSM En (ours) ResNet-50</cell><cell>144=(8+16)×2×3</cell><cell>52.7</cell><cell>81.5</cell></row><row><cell>RNL TSM (ours)</cell><cell cols="2">ResNet-101 48=8×2×3</cell><cell>50.8</cell><cell>79.8</cell></row><row><cell cols="3">RNL TSM En (ours) R101 + R50 144=(8+16)×2×3</cell><cell>54.1</cell><cell>82.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Also referred to as "depth-wise". We use the term "channel-wise" to avoid confusions with the network depth.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Rep. (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for comp. vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comp. Vision and Pattern Recog</title>
		<meeting>IEEE Comp. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representations with temporal squeeze pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal . (ICASSP)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCV-w)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Workshops (ICCV-w)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6510" to="6519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML</title>
		<meeting>Int. Conf. Mach. Learn. (ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
