<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Monocular Scene Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country>TU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country>TU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Monocular Scene Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Results of our monocular scene flow approach on the KITTI dataset <ref type="bibr" target="#b10">[11]</ref>. Given two consecutive images (left), our method jointly predicts depth (middle) and scene flow (right). (x,z)-coordinates of 3D scene flow are visualized using an optical flow color coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -obtaining 3D structure and 3D motion from two temporally consecutive images -is a highly illposed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene flow estimation is the task of obtaining 3D structure and 3D motion of dynamic scenes, which is crucial to environment perception, e.g., in the context of autonomous navigation. Consequently, many scene flow approaches have been proposed recently, based on different types of input data, such as stereo images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>, 3D point clouds <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, or a sequence of RGB-D images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref>. However, each sensor configuration has its own limitations, e.g. requiring stereo calibration for a stereo rig, expensive sensing devices (e.g., LiDAR) for measuring 3D points, or being limited to indoor usage (i.e., RGB-D camera). We here consider monocular 3D scene flow estimation, aiming to overcome these limitations.</p><p>Monocular scene flow estimation, however, is a highly ill-posed problem since both monocular depth (also called single-view depth) and per-pixel 3D motion need to be estimated from consecutive monocular frames, here two consecutive frames. Comparatively few approaches have been suggested so far <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b57">58]</ref>, none of which achieves both reasonable accuracy and real-time performance.</p><p>Recently, a number of CNN approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64]</ref> have been proposed to jointly estimate depth, flow, and camera ego-motion in a monocular setup. This makes it possible to recover 3D motion from the various outputs, however with important limitations. The depth-scale ambiguity <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b63">64]</ref> and the impossibility of estimating depth in occluded regions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref> significantly limit the ability to obtain accurate 3D scene flow across the entire image.</p><p>In this paper, we propose a monocular scene flow approach that yields competitive accuracy and real-time performance by exploiting CNNs. To the best of our knowledge, our method is the first monocular scene flow method that directly predicts 3D scene flow from a CNN. Due to the scarcity of 3D motion ground truth and the domain over-fitting problem when using synthetic datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">33]</ref>, we train directly on the target domain in a self-supervised manner to leverage large amounts of unlabeled data. Optional semi-supervised fine-tuning on limited quantities of ground-truth data can further boost the accuracy.</p><p>We make three main technical contributions: (i) We propose to approach this ill-posed problem by taking an inverse problem view. Noting that optical flow is the 2D projection of a 3D point and its 3D scene flow, we take the in-verse direction and estimate scene flow in the monocular setting by decomposing a classical optical flow cost volume into scene flow and depth using a single joint decoder. We use a standard optical flow pipeline (PWC-Net <ref type="bibr" target="#b44">[45]</ref>) as basis and adapt it for monocular scene flow. We verify our architectural choice and motivation by comparing with multi-task CNN approaches. (ii) We demonstrate that solving the monocular scene flow task with a single joint decoder actually simplifies joint depth and flow estimation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64]</ref>, and yields competitive accuracy despite a simpler network. Existing multi-task CNN methods have multiple modules for the various tasks and often require complex training schedules due to the instability of training multiple CNNs jointly. In contrast, our method only uses a single network that outputs scene flow and depth (as well as optical flow after projecting to 2D) with a simpler training setup and better accuracy for depth and scene flow. (iii) We introduce a self-supervised loss function for monocular scene flow as well as a suitable data augmentation scheme. We introduce a view synthesis loss, a 3D reconstruction loss, and an occlusion-aware loss, all validated in an ablation study. Interestingly, we find that the geometric augmentations of the two tasks conflict one another and determine a suitable compromise using an ablation study.</p><p>After training on unlabeled data from the KITTI raw dataset <ref type="bibr" target="#b9">[10]</ref>, we evaluate on the KITTI Scene Flow dataset <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> and demonstrate highly competitive accuracy compared to previous unsupervised/self-supervised learning approaches to monocular scene flow <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, increasing the accuracy by 34.0%. The accuracy of our fine-tuned network moves even closer to that of the semi-supervised method of <ref type="bibr" target="#b2">[3]</ref>, while being orders of magnitude faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene flow. Scene flow is commonly defined as a dense 3D motion field for each point in the scene, and was first introduced by Vedula et al. <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. The most common setup is to jointly estimate 3D scene structure and 3D motion of each point given a sequence of stereo images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>. Early approaches were mostly based on standard variational formulations and energy minimization, yielding limited accuracy and incurring long runtime <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>. Later, Vogel et al. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> introduced an explicit piecewise planar surface representation with a rigid motion model, which brought significant accuracy improvements especially in traffic scenarios. Exploiting semantic knowledge by means of rigidly moving objects yielded further accuracy boosts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Recently, CNN models have been introduced as well. Supervised approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b41">42]</ref> rely on large synthetic datasets and limited in-domain data to achieve stateof-the-art accuracy with real-time performance. Un-/selfsupervised learning approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref> have been de-veloped to circumvent the difficulty of obtaining groundtruth data, but their accuracy has remained behind.</p><p>Another category of approaches estimates scene flow from a sequence of RGB-D images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref> or 3D points clouds <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, exploiting the given 3D structure cues. In contrast, our approach is based on a more challenging setup that jointly estimates 3D scene structure and 3D scene flow from a sequence of monocular images. Monocular scene flow. Xiao et al. <ref type="bibr" target="#b57">[58]</ref> introduced a variational approach to monocular scene flow given an initial depth cue, but without competitive accuracy. Brickwedde et al. <ref type="bibr" target="#b2">[3]</ref> proposed an integrated pipeline by combining CNNs and an energy-based formulation. Given depth estimates from a monocular depth CNN, trained on pseudo-labeled data, the method jointly estimates 3D plane parameters and the 6D rigid motion of a piecewise rigid scene representation, achieving state-of-the-art accuracy. In contrast to <ref type="bibr" target="#b2">[3]</ref>, our approach is purely CNN-based, runs in real-time, and is trained in an end-to-end self-supervised manner, which allows to exploit a large amount of unlabeled data (cf . <ref type="bibr" target="#b57">[58]</ref>). Joint estimation of optical flow and depth. Given two depth maps and optical flow between two temporally consecutive frames, 3D scene flow can be simply calculated <ref type="bibr" target="#b42">[43]</ref> by relating two 3D points from optical flow. However, this pipeline has a critical limitation; it cannot estimate the 3D motion for occluded pixels since their depth value in the second frame is not known. Several recent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref> utilized multi-task CNN models to jointly estimate depth, optical flow, camera motion, and moving object masks from a monocular sequence in an unsupervised/self-supervised setting. While it may be possible to reconstruct scene flow from their outputs, these methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60]</ref> yield limited scene flow accuracy due to being limited to non-occluded regions. In contrast, our method directly estimates 3D scene flow with a CNN so that we naturally bypass this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Supervised Monocular Scene Flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>For each pixel p = (p x , p y ) in the reference frame I t , our main objective is to estimate the corresponding 3D point P = (P x , P y , P z ) and its (forward) scene flow s = (s x , s y , s z ) to the target frame I t+1 , as illustrated in <ref type="figure" target="#fig_1">Fig. 2a</ref>. The scene flow is defined as 3D motion with respect to the camera, and its projection onto the image plane becomes the optical flow f = (f x , f y ).</p><p>To estimate scene flow in the monocular camera setting, we take an inverse problem approach: we use CNNs to estimate a classical optical flow cost volume as intermediate representation, which is then decomposed with a learned decoder into 3D points and their scene flow. Unlike scene flow with a stereo camera setup <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">53]</ref>, it is challeng-  ing to determine depth on an absolute scale due to the scale ambiguity. Yet, relating per-pixel correspondence between two images can provide a cue for estimating depth in the monocular setting. Also, given an optical flow estimate, back-projecting optical flow into 3D yields many possible combinations of depth and scene flow, see <ref type="figure" target="#fig_1">Fig. 2b</ref>, which makes the problem much more challenging.</p><formula xml:id="formula_0">( , , ) ′ ( ′ , ′ , ′ ) ( , , s z ) ′ ( ′ , ′ ) ( , ) ( , ) ( , , ) ′ ( ′ , ′ , ′ ) ′ ( ′ , ′ ) ( , ) ( , ) (a) Projecting scene flow into 2D space. ( , , ) ′ ( ′ , ′ , ′ ) ( , , s z ) ′ ( ′ , ′ ) ( , ) ( , ) ( , , ) ′ ( ′ , ′ , ′ ) ′ ( ′ , ′ ) ( , ) ( , )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>In contrast to previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> that uses separate networks for each task (e.g., optical flow, depth, and camera motion), our method only uses one single CNN model that outputs both 3D scene flow and disparity 1 through a single decoder. We argue that having a single decoder is more sensible in our monocular setting than separate decoders, because when decomposing evidence for 2D correspondence into 3D structure and 3D motion, their interplay need to be taken into account (cf . <ref type="figure" target="#fig_1">Fig. 2b</ref>).</p><p>The first technical basis of our CNN model is PWC-Net <ref type="bibr" target="#b44">[45]</ref>, one of the state-of-the-art optical flow networks, which we modify for our task. <ref type="figure">Fig. 3</ref> illustrates our monocular scene flow architecture atop PWC-Net. PWC-Net has a pyramidal structure that constructs a feature pyramid and incrementally updates the estimation across the pyramid levels. The yellow-shaded area shows one forward pass for each pyramid level.</p><p>While maintaining the original structure, we modify the decoder of each pyramid level to output disparity and scene <ref type="bibr" target="#b0">1</ref> Even though we do not have stereo images at test time, we still estimate disparity of a hypothetical stereo setup following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which can be converted into depth given the assumed stereo configuration. flow together by increasing the number of output channels from 2 to 4 (i.e., 3 for scene flow and 1 for disparity). Following the benefit of residual motion estimation in the context of optical flow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45]</ref>, we estimate residual scene flow at each level. In contrast, we observe that residual updates hurt disparity estimation, hence we estimate (nonresidual) disparity at all levels. To have more discriminate features, we increase the number of feature channels in the pyramidal feature extractor from <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">96,</ref><ref type="bibr">128,</ref><ref type="bibr">196]</ref> to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">96,</ref><ref type="bibr">128,</ref><ref type="bibr">192,</ref><ref type="bibr">256</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Addressing the scale ambiguity</head><p>When resolving the 3D ambiguities, it is not possible to determine the depth scale from a single correspondence in two monocular images. In order to estimate depth and scene flow on an absolute scale, we adopt the monocular depth estimation approach of Godard et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> as our second basis, which utilizes pairs of stereo images with their known stereo configuration and camera intrinsics K for training; at test time, only monocular images and known intrinsics are needed. The images from the right camera guide the CNN to estimate the disparity d on an absolute scale by exploiting semantic and geometric cues indirectly <ref type="bibr" target="#b6">[7]</ref> through a selfsupervised loss function. Then the depthd can be trivially recovered given the baseline distance of a stereo rig b and the camera focal length f focal asd = b · f focal /d. We also use stereo images only for training; at test time our approach is purely monocular. In our context, estimating depth on an absolute scale helps to disambiguate scene flow on an absolute scale as well (cf . <ref type="figure" target="#fig_1">Fig. 2b</ref>). Moreover, tightly coupling temporal correspondence and depth actually helps to identify the appropriate absolute scale, which allows us to avoid unrealistic testing settings that other monocular methods rely on (e.g., <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> use ground truth to correctly scale their predictions at test time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A proxy loss for self-supervised learning</head><p>Similar to previous monocular structure reconstruction methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>, we exploit a view synthesis loss to guide the network to jointly estimate disparity and scene flow. For better accuracy in both tasks, we exploit occlusion cues through bi-directional estimation <ref type="bibr" target="#b33">[34]</ref>, here of disparity and scene flow. Given a stereo image pair of the reference and target frame {I l t , I l t+1 , I r t , I r t+1 }, we input a monocular sequence from the left camera (I l t and I l t+1 ) to the network and obtain a disparity map of each frame (d l t and d l t+1 ) as well as forward and backward scene flow (s l fw and s l bw ) by simply switching the temporal order of the input. The two images from the right camera (I r t and I r t+1 ) are used only as a guidance in the loss function and are not used at test time. Our total loss is a weighted sum of a disparity loss L d and a scene flow loss L sf ,  <ref type="figure">Figure 3</ref>. Our monocular scene flow architecture based on PWC-Net <ref type="bibr" target="#b44">[45]</ref>: while maintaining the overall original structure of PWC-Net, we modify the decoder to output residual scene flow and (non-residual) disparity together. After the residual update of scene flow, we project the scene flow back to optical flow using depth. Then, the optical flow is used for warping the feature map (only 3 of 7 levels shown for ease of visualization) in the next pyramid level. The light-yellow shaded region shows one forward pass for each pyramid level.</p><formula xml:id="formula_1">L total = L d + λ sf L sf .<label>(1)</label></formula><p>Disparity loss. Based on the approach of Godard et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we propose an occlusion-aware monocular disparity loss, consisting of a photometric loss L d_ph and a smoothness loss L d_sm ,</p><formula xml:id="formula_2">L d = L d_ph + λ d_sm L d_sm ,<label>(2)</label></formula><p>with regularization parameter λ d_sm = 0.1. The disparity loss is applied to both disparity maps d l t and d l t+1 . For brevity, we only describes the case of d l t . The photometric loss L d_ph penalizes the photometric difference between the left image I l t and the reconstructed left imageĨ l,d t , which is synthesized from the output disparity map d l t and the given right image I r t using bilinear interpolation <ref type="bibr" target="#b22">[23]</ref>. Different to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we only penalize the photometric loss for non-occluded pixels. Following standard practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we use a weighted combination of an L 1 loss and the structural similarity index (SSIM) <ref type="bibr" target="#b54">[55]</ref>:</p><formula xml:id="formula_3">L d_ph = p 1 − O l,disp t (p) · ρ I l t (p),Ĩ l,d t (p) q 1 − O l,disp t (q) (3a) with ρ(a, b) = α 1 − SSIM(a, b) 2 + (1 − α) a − b 1 ,<label>(3b)</label></formula><p>where α = 0.85 and O l,disp t is the disparity occlusion mask (0 -visible, 1 -occluded). To obtain the occlusion mask O l,disp t , we feed the right image I r t into the network to obtain the right disparity d r t and take the inverse of its disocclusion map, which is obtained by forward-warping the right disparity map <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>To encourage locally smooth disparity estimates, we adopt an edge-aware 2 nd -order smoothness <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b56">57]</ref>, with β = 10 and N being the number of pixels. Scene flow loss. The scene flow loss consists of three terms -a photometric loss L sf_ph , a 3D point reconstruction loss L sf_pt , and a scene flow smoothness loss L sf_sm ,</p><formula xml:id="formula_4">L d_sm = 1 N p i∈{x,y} ∇ 2 i d l t (p) · e −β ∇iI l t (p) 1 , (4) ′ ′ fw l ( ) � d +1 l ( ′ ) +1 ′ ′ ′ fw l ( ) � d l ( ) (a) Photometric loss. ′ ′ fw l ( ) � d +1 l ( ′ ) +1 ′ ′ ′ fw l ( ) � d l ( ) (b) 3D point reconstruction loss.</formula><formula xml:id="formula_5">L sf = L sf_ph + λ sf_pt L sf_pt + λ sf_sm L sf_sm ,<label>(5)</label></formula><p>with regularization parameters λ sf_pt = 0.2 and λ sf_sm = 200. The scene flow loss is applied to both forward and backward scene flow (s l fw and s l bw ). Again for brevity, we only describe the case of forward scene flow s l fw . The scene flow photometric loss L sf_ph penalizes the photometric difference between the reference image I l t and the reconstructed reference imageĨ l,sf t , synthesized from the disparity map d l t , the output scene flow s l fw , and the target image I l t+1 . To reconstruct the image, the corresponding pixel coordinate p in I l t+1 of each pixel p in I l t is calculated by back-projecting the pixel p into 3D space using the camera intrinsics K and estimated depthd l t (p), translating the points using the scene flow s l fw (p), and then re-projecting them to the image plane (cf . <ref type="figure" target="#fig_2">Fig. 4a</ref>),</p><formula xml:id="formula_6">p = K d l t (p) · K −1 p + s l fw (p) ,<label>(6)</label></formula><p>assuming a homogeneous coordinate representation. Then, we apply the same occlusion-aware photometric loss as in the disparity case (Eq. 3a),</p><formula xml:id="formula_7">L sf_ph = p 1 − O l,sf t (p) · ρ I l t (p),Ĩ l,sf t (p) q 1 − O l,sf t (q) ,<label>(7)</label></formula><p>where O l,sf t is the scene flow occlusion mask, obtained by calculating disocclusion using the backward scene flow s l bw . Additionally, we also penalize the Euclidean distance between the two corresponding 3D points, i.e. the translated 3D point of pixel p from the reference frame and the matched 3D point in the target frame (cf . <ref type="figure" target="#fig_2">Fig. 4b</ref>):</p><formula xml:id="formula_8">L sf_pt = p 1 − O l,sf t (p) · P t − P t+1 2 q 1 − O l,sf t (q) ,<label>(8a)</label></formula><p>with</p><formula xml:id="formula_9">P t =d l t (p) · K −1 p + s l fw (p) (8b) P t+1 =d l t+1 (p ) · K −1 p ,<label>(8c)</label></formula><p>and p as defined in Eq. <ref type="bibr" target="#b5">(6)</ref>. Again, this 3D point reconstruction loss is only applied on visible pixels, where the correspondence should hold. Analogous to the disparity loss in Eq. (4), we also adopt edge-aware 2 nd -order smoothness for scene flow to encourage locally smooth estimation:</p><formula xml:id="formula_10">L sf_sm = 1 N p i∈{x,y} ∇ 2 i s l fw (p) · e −β ∇iI l t (p) 1 . (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data augmentation</head><p>In many prediction tasks, data augmentation is crucial to achieving good accuracy given limited training data. In our monocular scene flow task, unfortunately, the typical geometric augmentation schemes of the two tasks (i.e., monocular depth estimation, scene flow estimation) conflict each other. For monocular depth estimation, not performing geometric augmentation is desirable as it enables learning the scene layout under a fixed camera configuration <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. On the other hand, the scene flow necessitates geometric augmentations to match corresponding pixels better <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">33]</ref>.</p><p>We investigate which type of (geometric) augmentation is suitable for our monocular scene flow task and method. Similar to previous multi-task approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">64]</ref>, we prepare a simple data augmentation scheme, consisting of random scales, cropping, resizing, and horizontal image  <ref type="table">Table 1</ref>. Impact of geometric augmentations (Aug.) and CAM-Convs (CC.) <ref type="bibr" target="#b8">[9]</ref> on monocular depth and scene flow estimation (on KITTI split, see text): the accuracy of monocular depth estimation improves only when using CAM-Convs while that of monocular scene flow estimation improves when only using augmentation without CAM-Convs.</p><p>flipping. Upon the augmentation, we also explore the recent CAM-Convs <ref type="bibr" target="#b8">[9]</ref>, which facilitate depth estimation irrespective of the camera intrinsics. After applying augmentations on the input images, we calculate the resulting camera intrinsics and then input them in the format of CAM-Convs (see <ref type="bibr" target="#b8">[9]</ref> for technical details). We conjecture that using geometric augmentation will improve the scene flow accuracy. Yet, at the same time adopting CAM-Convs <ref type="bibr" target="#b8">[9]</ref> could prevent the depth accuracy from dropping due to the changes in camera intrinsics of the augmented images. We conduct our empirical study on the KITTI split <ref type="bibr" target="#b12">[13]</ref> of the KITTI raw dataset <ref type="bibr" target="#b9">[10]</ref> (see Sec. 4.1 for details).</p><p>Empirical study for monocular depth estimation. We use a ResNet18-based monocular depth baseline <ref type="bibr" target="#b12">[13]</ref> using our proposed occlusion-aware loss. <ref type="table">Table 1</ref> (left hand side) shows the results. As we can see, geometric augmentations deteriorate the depth accuracy, since they prevent the network from learning a specific camera prior by inputting augmented images with diverse camera intrinsics; this observation holds with and without CAM-Convs. This likely explains why some multi-task approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref> only use minimal augmentation schemes such as image flipping and input temporal-order switching. Only using CAM-Convs <ref type="bibr" target="#b8">[9]</ref> works best as the test dataset contains images with different intrinsics, which CAM-Convs can handle.</p><p>Empirical study for monocular scene flow estimation.</p><p>We train our full model with the proposed loss from Eq. (1). Looking at the right side of <ref type="table">Table 1</ref> yields different conclusions for monocular scene flow estimation: using augmentation improves the scene flow accuracy in general, but using CAM-Convs <ref type="bibr" target="#b8">[9]</ref> actually hurts the accuracy. We conjecture that the benefit of CAM-Convs -introducing a testtime dependence on input camera intrinsics -may be redundant for correspondence tasks (i.e. optical flow, scene flow) and can hurt the accuracy. We also observe that CAM-Convs lead to slight over-fitting on the training set, yielding marginally lower training loss (e.g., &lt; 1%) but with higher error on the test set. Therefore, we apply only geometric augmentation without CAM-Convs in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Dataset. For evaluation, we use the KITTI raw dataset <ref type="bibr" target="#b9">[10]</ref>, which provides stereo sequences covering 61 street scenes. For the scene flow experiments, we use the KITTI Split <ref type="bibr" target="#b12">[13]</ref>: we first exclude 29 scenes contained in KITTI Scene Flow Training <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> and split the remaining 32 scenes into 25 801 sequences for training and 1684 for validation. For evaluation and the ablation study, we use KITTI Scene Flow Training as test set, since it provides groundtruth labels for disparity and scene flow for 200 images.</p><p>After training on KITTI Split in a self-supervised manner, we optionally fine-tune our model using KITTI Scene Flow Training <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> to see how much accuracy gain can be obtained from annotated data. We fine-tune our model in a semi-supervised setting by combining a supervised loss with our self-supervised loss (see below for details).</p><p>Additionally for evaluating monocular depth accuracy, we also use the Eigen Split [8] by excluding 28 scenes that the 697 test sequences cover, splitting into 20 120 training sequences and 1338 validation sequences.</p><p>Data augmentation. We adopt photometric augmentations with random gamma, brightness, and color changes. As discussed in Sec. 3.5, we use geometric augmentations consisting of horizontal flips <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref>, random scales, random cropping <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">64]</ref>, and then resizing into 256 × 832 pixels as in previous work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Self-supervised training. Our network is trained using Adam <ref type="bibr" target="#b24">[25]</ref> with hyper-parameters β 1 = 0.9 and β 2 = 0.999. Our initial learning rate is 2 × 10 −4 , and the mini-batch size is 4. We train our network for a total of 400k iterations. <ref type="bibr" target="#b1">2</ref> In every iteration, the regularization weight λ sf in Eq. (1) is dynamically determined to make the loss of the scene flow and disparity be equal in order to balance the optimization of the two joint tasks <ref type="bibr" target="#b20">[21]</ref>. Our specific learning rate schedule, as well as details on hyper-parameter choice and data augmentation are provided in the supplementary material.</p><p>Unlike previous approaches requiring stage-wise pretraining <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b63">64]</ref> or iterative training <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref> of multiple CNNs due to the instability of joint training, our approach does not need any complex training strategies, but can just be trained from scratch all at once. This highlights the practicality of our method.</p><p>Semi-supervised fine-tuning. We optionally fine-tune our trained model in a semi-supervised manner by mixing the two datasets, the KITTI raw dataset <ref type="bibr" target="#b9">[10]</ref> and KITTI Scene Flow Training <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, at a ratio of 3 : 1 in each batch of 4. The latter dataset provides sparse ground truth of the disparity map of the reference image, disparity information at the 2 Code is available at https://github.com/visinf/self-mono-sf.  <ref type="table">Table 2</ref>. Ablation study on the loss function: based on the Basic 2D loss consisting of photometric and smoothness loss, the 3D point reconstruction loss (3D points) improves scene flow accuracy, especially when discarding occluded pixels in the loss (Occ.).</p><p>target image mapped into the reference image, as well as optical flow. We apply our self-supervised loss to all samples and a supervised loss (L 2 for optical flow, L 1 for disparity) only for the sample from KITTI Scene Flow Training after converting the scene flow into two disparity maps and optical flow. Through semi-supervised fine-tuning, the proxy loss can guide pixels that the sparse ground truth cannot supervise. Moreover, the model can be prevented from heavy over-fitting on the only 200 annotated images by leveraging more data. We train the network for 45k iterations with the learning rate starting at 4 × 10 −5 (see supplemental).</p><p>Evaluation metric. For evaluating the scene flow accuracy, we follow the evaluation metric of KITTI Scene Flow benchmark <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. It evaluates the accuracy of the disparity for the reference frame (D1-all) and for the target image mapped into the reference frame (D2-all), as well as of the optical flow (F1-all). Each pixel that exceeds a threshold of 3 pixels or 5% w.r.t. the ground-truth disparity or optical flow is regarded as an outlier; the metric reports the outlier ratio (in %) among all pixels with available ground truth. Furthermore, if a pixel satisfies all metrics (i.e., D1-all, D2all, and F1-all), it is regarded as valid scene flow estimate from which the outlier rate for scene flow (SF1-all) is calculated. For evaluating the depth accuracy, we follow the standard evaluation scheme introduced by Eigen et al. <ref type="bibr" target="#b7">[8]</ref>. We assume known test-time camera intrinsics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>To confirm the benefit of our various contributions, we conduct ablation studies based on our full model using the KITTI split with data augmentation applied. Proxy loss for self-supervised learning. Our proxy loss consists of three main components: (i) Basic: a basic combination of 2D photometric and smoothness losses, (ii) 3D points: the 3D point reconstruction loss for scene flow, and (iii) Occ.: whether applying the photometric and point reconstruction loss only for visible pixels or not. <ref type="table">Table 2</ref> shows the contribution of each loss toward the accuracy.</p><p>The 3D points loss significantly contributes to more accurate scene flow by yielding more accurate disparity on the target image (D2-all). This highlights the importance of penalizing the actual 3D Euclidean distance between two corresponding 3D points (cf . <ref type="figure" target="#fig_2">Fig. 4b</ref>  <ref type="table">Table 3</ref>. Single decoder vs. separate decoders: using a single decoder yields stable training and comparable accuracy on both tasks to models that target each individual task separately. functions in 2D space (i.e. Basic loss) as in previous work <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60]</ref> cannot. Taking occlusion into account consistently improves the scene flow accuracy further. The main objective of our proxy loss is to reconstruct the reference image as closely as possible, which can lead to hallucinating potentially incorrect estimates of disparity and scene flow in the occluded areas. Thus, discarding occluded pixels in the loss is critical to achieving accurate predictions.</p><p>Single decoder vs. separate decoders. To verify the key motivation of decomposing optical flow cost volumes into depth and scene flow using a single decoder, we compare against a model with separate decoders for each task, which follows the conventional design of other multi-task methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64]</ref>. We also prepare two baselines that estimate either monocular depth or optical flow only, to assess the capacity our modified PWC-Net for each task. <ref type="table">Table 3</ref> demonstrates our ablation study on the network design. First, our model with a single decoder achieves comparable or even higher accuracy on the depth and optical flow tasks, compared to using the same network only for each individual task. We thus conclude that solving monocular scene flow using a single joint network can substitute the two individual tasks given the same amount of training resources and network capacity.</p><p>When separating the decoders, we find that the network cannot be trained stably, yielding trivial solutions for disparity. This is akin to issues observed by previous multi-task approaches, which require pre-training or iterative training for multiple CNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64</ref>]. In contrast, having a single decoder resolves the imbalance and stability problem by virtue of joint estimation. We include a more comprehensive analysis in the supplemental, gradually splitting the decoder to closely analyze its behavior. <ref type="table">Table 4</ref> demonstrates the comparison to existing monocular scene flow methods on KITTI Scene Flow Training. We compare against state-of-the-art multi-task CNN methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> on the scene flow evaluation metric. Our model significantly outperforms these methods by a large margin, confirming our method as the most accurate monocular scene flow method using CNNs to date. For example, our method yields more than 40.1% accuracy gain for esti- mating the disparity on the target image (D2-all). Though the two methods, EPC <ref type="bibr" target="#b59">[60]</ref> and EPC++ <ref type="bibr" target="#b29">[30]</ref>, do not provide scene flow accuracy numbers (SF1-all), we can conclude that our method clearly outperforms all four methods in SF1-all, since SF1-all is lower-bounded by D2-all. Our self-supervised learning approach (Self-Mono-SF) is outperformed only by Mono-SF <ref type="bibr" target="#b2">[3]</ref>, which is a semisupervised method using pseudo labels, semantic instance knowledge, and an additional dataset (Cityscapes <ref type="bibr" target="#b5">[6]</ref>). However, our method runs more than two orders of magnitude faster. We also provide the accuracy of our fine-tuned model (Self-Mono-SF-ft) on the training set for reference. <ref type="table" target="#tab_4">Table 5</ref> shows the comparison with stereo and monocular scene flow methods on the KITTI Scene Flow 2015 benchmark. <ref type="figure" target="#fig_3">Fig. 5</ref> provides a visualization. Our semi-supervised fine-tuning further improves the accuracy, going toward that of Mono-SF <ref type="bibr" target="#b2">[3]</ref>, but with a more than 400× faster runtime. For further accuracy improvements, e.g. rigidity refinement <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>, exploiting an external dataset <ref type="bibr" target="#b5">[6]</ref> for pretraining, or pseudo ground truth <ref type="bibr" target="#b2">[3]</ref> can be applied on top of our self-supervised learning and semi-supervised finetuning pipeline without affecting run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Monocular scene flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Monocular depth and optical flow</head><p>Finally, we provide a comparison to unsupervised multitask CNN approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref>   <ref type="table">Table 7</ref>. Optical flow estimation on the KITTI split: our method demonstrates comparable accuracy to both monocular and stereo-based multi-task methods. port methods that use extra datasets (e.g., the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>) for pre-training or online fine-tuning <ref type="bibr" target="#b4">[5]</ref>, which is known to give an accuracy boost. For monocular depth estimation in <ref type="table" target="#tab_5">Table 6</ref>, our monocular scene flow method outperforms all published multi-task methods on the KITTI Split <ref type="bibr" target="#b12">[13]</ref> and demonstrates competitive accuracy on the Eigen split <ref type="bibr" target="#b7">[8]</ref>. Note that some of the methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> use ground truth to correctly scale their predictions at test time, which gives them an unfair advantage, but are still outperformed by ours.</p><p>For optical flow estimation in <ref type="table" target="#tab_5">Table 6</ref>, our method demonstrates comparable accuracy to existing state-of-theart monocular <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref> and stereo methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, in part outperforming them.</p><p>One reason why our flow accuracy may not surpass all previous methods is that we use a 3D scene flow regularizer and not a 2D optical flow regularizer. This is consistent with our goal of estimating 3D scene flow, but it is known that using a regularizer in the target space is critical for achieving best accuracy <ref type="bibr" target="#b51">[52]</ref>. While our choice of 3D regularizer is not ideal for optical flow estimation, its benefits manifest in 3D. For example, while we do not outperform EPC++ <ref type="bibr" target="#b29">[30]</ref> in terms of 2D flow accuracy, we clearly surpass it in terms of scene flow accuracy (see <ref type="table">Table 4</ref>). Consequently, our approach is not only the first CNN approach to monocular scene flow estimation that directly predicts the 3D scene flow, but also outperforms existing multi-task CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a CNN-based monocular scene flow estimation approach based on PWC-Net that predicts 3D scene flow directly. A crucial feature is our single joint decoder for depth and scene flow, which allows to overcome the limitations of existing multi-task approaches such as complex training schedules or lacking occlusion handling. We take a self-supervised approach, where our 3D loss function and occlusion reasoning significantly improve the accuracy. Moreover, we show that a suitable augmentation scheme is critical for competitive accuracy. Our model achieves stateof-the-art scene flow accuracy among un-/self-supervised monocular methods, and our semi-supervised fine-tuned model approaches the accuracy of the best monocular scene flow method to date, while being orders of magnitude faster. With competitive accuracy and real-time performance, our method provides a solid foundation for CNN-based monocular scene flow estimation as well as follow-up work. <ref type="bibr" target="#b7">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Monocular Scene Flow Estimation -Supplementary Material -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Junhwa Hur</head><p>Stefan Roth Department of Computer Science, TU Darmstadt In this supplementary material, we provide further details on the learning rate schedules, data augmentation, and the hyper-parameter settings. Afterwards, we provide a more comprehensive study of the decoder design, qualitative examples for the loss ablation study, and a qualitative comparison with the state-of-the-art Mono-SF approach <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates the learning rate schedules for both selfsupervised learning and semi-supervised fine-tuning. When first training our model in a self-supervised manner for 400k iterations, the initial learning rate starts from 2 × 10 −4 and is halved at 150k, 250k, 300k, and 350k iteration steps. When fine-tuning in a semi-supervised manner afterwards, the training schedule consists of 45k iterations; the initial learning rate starts from 4 × 10 −5 and is halved at 10k, 20k, 30k, 35k, and 40k iteration steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Rate Schedule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on Data Augmentation</head><p>As discussed in the main paper, we perform photometric and geometric augmentations at training time. Here we provide more details on our augmentation setup for both self-supervised training and semi-supervised fine-tuning. Augmentations for self-supervised training. We apply photometric augmentations with 50% probability. Specifically, we adopt random gamma adjustments, uniformly sampled from [0.8, 1.2], brightness changes with a multiplication factor that is uniformly sampled in [0.5, 2.0], and random color changes with a multiplication factor that is uniformly sampled in [0.8, 1.2] for each color channel.</p><p>For geometric augmentations, we first randomly crop the input images with a random scale factor uniformly sampled in [93%, 100%] and apply random translations uniformly sampled from [−3.5%, 3.5%] w.r.t. the input image size. Then we resize the cropped image to 256 × 832 pixels as in previous work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref>. We also apply a horizontal flip <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref> with 50% probability. Because the geometric augmentations have an effect on the camera intrinsics, we adjust the intrinsic camera matrix accordingly by calculating the corresponding camera center and focal  Augmentations for semi-supervised fine-tuning. Likewise, we also apply the same photometric augmentations with 50% probability. For geometric augmentations, we only apply random cropping without scaling and then resize to 256 × 832 pixels. Not performing scaling is to avoid changes to the ground truth, which may happen if zooming and interpolating the sparse ground truth. The crop size s · h 0 × s · w 0 is determined by the cropping factor s that is uniformly sampled in [94%, 100%], where h 0 and w 0 is height and width of the original input resolution. At testing time, the same augmentation scheme as during self-supervised training applies: resizing the input images to 256 × 832 pixels without photometric augmentation. However, we note that better augmentation protocols can likely be discovered with further investigation <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyper-Parameter Settings</head><p>Our self-supervised proxy loss in Eq. (1) of the main paper has a total of 6 hyper-parameters, which could make it difficult to achieve satisfactory results without careful tuning. In this section, we thus discuss how we choose the hyper-parameters and provide an analysis on how sensitive the scene flow accuracy is depending on the hyperparameter choices.</p><p>First, as discussed in the main paper, the balancing weight λ sf between the two joint tasks in Eq. (1) is dynamically determined to make the loss of the scene flow and disparity be equal in every iteration <ref type="bibr" target="#b20">[21]</ref>. For the disparity loss, we simply adopt the same hyper-parameters (i.e., λ d_sm , α, and β in Eqs. (2), (3b) and (4), respectively) as in previous work <ref type="bibr" target="#b12">[13]</ref>, which leaves only two hyper-parameters, λ sf_sm and λ sf_pt , to tune in the scene flow loss, Eq. (5). We perform grid search on the two parameters. <ref type="table">Table 8</ref> gives the grid search results regarding the two hyper-parameters, reporting the accuracy for monocular depth, optical flow, and scene flow. In the upper half of the table, we fix the smoothness parameter λ sf_sm and control the 3D point reconstruction loss parameter λ sf_pt to see its effect on the accuracy. The bottom half of the table is set up the other way around. Note that the lower the better for all metrics.</p><p>We find that λ sf_pt is important for best scene flow accuracy, specifically settings that yield accurate disparity information on the target frame, D2-all. This observation follows our design of the 3D point reconstruction loss, which penalizes the 3D distance between corresponding points, encouraging more accurate 3D scene flow in 3D space. However, as a trade-off, having a higher value of λ sf_pt leads to lower accuracy for 2D estimation, i.e. of depth and optical flow. On the other hand, we find that the parameter for the 3D smoothness loss, λ sf_sm , does not strongly affect the accuracy in general. That is, once λ sf_pt is in the right range, the results are not particularly sensitive to the parameter choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. In-Depth Analysis of the Decoder Design</head><p>With the decoder ablation study in <ref type="table">Table 3</ref> of the main paper, we demonstrate that having separate decoders for disparity and scene flow yields instable, unbalanced outputs in contrast to having our proposed single decoder design. For a more comprehensive analysis, we conduct an empirical study by gradually splitting the decoder consisting of 5 convolution layers and studying the behavior of the  <ref type="table">Table 8</ref>. Grid search results on the two hyper-parameters, λ sf_sm and λ sf_pt based on the accuracy of monocular depth, optical flow, and scene flow. The 3D point reconstruction parameter λsf_pt contributes to more accurate disparity information on the target frame, D2-all, yielding more accurate scene flow SF1-all in the end. The overall results are not very sensitive to the choice of the 3D smoothness parameter λsf_sm.</p><p>networks for each configuration. Our backbone network, PWC-Net <ref type="bibr" target="#b44">[45]</ref>, has context networks at the end of the decoder, which are fed the output and the last feature map from the decoder as input and perform post-processing for better accuracy. In our splitting study, we also separate the context networks for each separated decoder so that the two decoders at the end of the networks do not share information. <ref type="figure">Fig. 7</ref> illustrates each configuration. From our single decoder design in <ref type="figure">Fig. 7a</ref>, we first split the context network for disparity and scene flow respectively, as shown in <ref type="figure">Fig. 7b</ref>. Then, we begin to split the decoder from the last convolution layer (i.e., <ref type="figure">Fig. 7c</ref>), the 2 nd -to-last layer (i.e., <ref type="figure">Fig. 7d</ref>), and so on until eventually completely splitting into two separate decoders (i.e., <ref type="figure">Fig. 7e</ref>). To ensure the same network capacity, we adjust the number of filters so that all configurations have network parameter numbers in a similar range. All configurations are trained on the KITTI Split of KITTI raw <ref type="bibr" target="#b9">[10]</ref> in our self-supervised manner. <ref type="table" target="#tab_7">Table 9</ref> shows the disparity, optical flow, and scene flow accuracy of each configuration on KITTI Scene Flow Train-  <ref type="figure">Figure 7</ref>. Gradually splitting the single decoder into two separate decoders: we gradually split the single decoder (a) by first splitting the context network (b), and then splitting from the last layer of the decoder (c), the 2 nd -to-last layer (d), and so on until completely splitting into two separate decoders(e). For ease of visualization, we omit showing the convolution operation between the neighboring feature maps in the decoder.</p><p>ing <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. We first observe that splitting the context network yields a significant 32.73% decrease in scene flow accuracy (i.e., SF1-all), which mainly stems from the less accurate disparity estimates (i.e., D1-all and D2-all) although the optical flow accuracy remains almost the same. This provides an important outlook: given the same optical flow accuracy, the scene flow accuracy depends crucially on how well one can decompose the optical flow cost volume into depth and scene flow, where using the single decoder model works better. When further splitting the decoder starting from the last convolution layer, the networks (i) cannot be trained stably anymore, (ii) output trivial solutions for the disparity, and (iii) even decrease the optical flow accuracy. This observation again confirms the benefits of using our proposed single decoder design in terms of both accuracy and training stability. <ref type="table">Table 2</ref> in the main paper provides an ablation study of our self-supervised proxy loss. For better understanding of how each loss term affects the results, we provide qualitative examples of disparity, optical flow, and scene flow estimation. <ref type="figure">Fig. 8</ref> displays the results for each loss configuration: (a) the basic loss where only the brightness and smoothness terms are active; (b) with occlusion handling, which discards occluded pixels in the loss; (c) with the 3D point reconstruction loss; and (d) the full loss. Each con- figuration is trained in the proposed self-supervised manner using the KITTI Split and evaluated on KITTI Scene Flow Training <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Analysis of Loss Ablation Study</head><p>Without the 3D point reconstruction loss for scene flow (i.e., columns (a) and (b) in <ref type="figure">Fig. 8</ref>), the networks output inaccurate disparity information for the target frame (D2) especially in the road area, which yields inaccurate scene flow results (SF1) in the end. Applying the 3D point reconstruction loss but without occlusion handling (i.e., column (c) in <ref type="figure">Fig. 8</ref>) results in inaccurate estimates and some artifacts appearing on out-of-bound pixels, still leading to an unsatisfactory final scene flow accuracy. These artifacts happen when the 3D point reconstruction loss tries to minimize the 3D Euclidean distance between incorrect pixel correspon-13 dences, such as for occlusions or out-of-bound pixels. Discarding those occluded regions in the proxy loss eventually yields better estimates in the occluded region as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Comparison</head><p>We provide some qualitative examples of our monocular scene flow estimation by comparing with the state-of-theart Mono-SF method <ref type="bibr" target="#b2">[3]</ref>, which uses an integrated pipeline of CNNs and an energy-based model. <ref type="figure">Figs. 9 and 10</ref> show successful qualitative results as well as some failure cases of our fine-tuned model on the KITTI 2015 Scene Flow public benchmark <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, respectively.</p><p>In <ref type="figure">Fig. 9</ref>, our model outputs more accurate disparity and optical flow estimation results than Mono-SF <ref type="bibr" target="#b2">[3]</ref> without using an explicit planar surface representation or a rigid motion assumption, which would be beneficial for achiev-ing better accuracy on the KITTI 2015 Scene Flow public benchmark. <ref type="figure">Fig. 10</ref>, in contrast, shows some of the failure cases, where our model outputs less accurate results for scene flow estimation than Mono-SF <ref type="bibr" target="#b2">[3]</ref>. Although our model can estimate optical flow with an accuracy comparable to Mono-SF, inaccurate disparity estimation eventually leads to less accurate scene flow. The gap in terms of the disparity accuracy of ours vs. Mono-SF <ref type="bibr" target="#b2">[3]</ref> can be explained by the fact that Mono-SF exploits over 20 000 instances of pseudo groundtruth depth data to train their monocular depth model, while our method uses only 200 images for fine-tuning.   <ref type="figure">Figure 8</ref>. Qualitative examples on the loss ablation study. For each scene in the first row we show two input images, the reference and the target image. From the second to the last row, we show a qualitative comparison of each loss configuration: (a) basic loss, (b) with occlusion handling, (c) with 3D point reconstruction loss, and the (d) our full loss. Each row visualizes the disparity map of the reference image (D1) with its error map (D1 Error), disparity estimation at the target image mapped into the reference frame (D2) along with its error map (D2 Error), optical flow (F1) with its error map (F1 Error), and the scene flow error map (SF1 Error). The outlier rates are overlayed on each error map. The last column shows (e) the ground truth for each estimate.  <ref type="figure">Figure 9</ref>. Some successful cases and qualitative comparison with the state of the art on the KITTI 2015 Scene Flow public benchmark <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. In the first row, we show two input images, the reference and target image. From the second to the last row, we give a qualitative comparison with Mono-SF <ref type="bibr" target="#b2">[3]</ref>: the disparity map of the reference image (D1) with its error map (D1 Error), disparity estimation at the target image mapped into the reference frame (D2) along with its error map (D2 Error), optical flow (F1) with its error map (F1 Error), and the scene flow error map (SF1 Error). The outlier rates are overlayed on each error map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16</head><p>Reference image  <ref type="figure">Figure 10</ref>. Failure cases and qualitative comparison with the state of the art on the KITTI 2015 Scene Flow public benchmark <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. In the first row, we show two input images, the reference and target image. From the second to the last row, we give a qualitative comparison with Mono-SF <ref type="bibr" target="#b2">[3]</ref>: the disparity map of the reference image (D1) with its error map (D1 Error), disparity estimation at the target image mapped into the reference frame (D2) with its error map (D2 Error), optical flow (F1) with its error map (F1 Error), and the scene flow error map (SF1 Error). The outlier rates are overlayed on each error map.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Back-projecting optical flow into 3D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Relating monocular scene flow estimation to optical flow: (a) Projection of scene flow into the image plane yields optical flow [59]. (b) Back-projection of optical flow leaves an ambiguity in jointly determining depth and scene flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Scene flow losses: (a) Finding corresponding pixels given depth and scene flow for the photometric loss Lsf_ph (Eq. 7). (b) Penalizing 3D distance (dashed, red) between corresponding 3D points by the point reconstruction loss Lsf_pt (Eq. 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of our monocular scene flow results (Self-Mono-SF-ft) on KITTI 2015 Scene Flow Test: each scene shows (a) two input images, (b) monocular depth, (c) optical flow, and (d) a 3D visualization of estimated depth, overlayed with the reference image, and colored with the (x, z)-coordinates of the 3D scene flow using the standard optical flow color coding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 ×10 - 5 (</head><label>45</label><figDesc>Learning rate schedule for self-supervised learning. b) Learning rate schedule for fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Learning rate schedules for (a) self-supervised learning and (b) semi-supervised fine-tuning. length of each augmented image. At testing time, we only resize the input image to 256 × 832 pixels without photometric augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Splitting from the 2 nd -to-last layer. Splitting into two separate decoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Warping Correlation Scene Flow + Disparity Decoder Residual Scene Flow Output Disparity Output Scene Flow Projecting to Optical Flow Feature Pyramid Feature Pyramid Target Image Reference Image</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Monocular depth Monocular scene flow Aug. CC. [9] Abs. Rel. Sq. Rel. D1-all D2-all F1-all SF1-all 0.113 1.118 32.06 36.46 24.68 49.89 0.122 1.172 31.25 34.86 23.49 47.05 0.112 1.089 37.24 39.26 24.82 54.83 0.121 1.155 33.25 36.21 24.73 49.12</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="4">D1-all D2-all F1-all SF1-all Runtime</cell></row><row><cell>DF-Net [64]</cell><cell cols="2">46.50 61.54 27.47 73.30</cell><cell>-</cell></row><row><cell>GeoNet [61]</cell><cell cols="2">49.54 58.17 37.83 71.32</cell><cell cols="2">0.06 s</cell></row><row><cell>EPC [60]</cell><cell cols="2">26.81 60.97 25.74 (&gt;60.97)</cell><cell cols="2">0.05 s</cell></row><row><cell>EPC++ [30]</cell><cell cols="2">23.84 60.32 19.64 (&gt;60.32)</cell><cell cols="2">0.05 s</cell></row><row><cell>Self-Mono-SF (Ours)</cell><cell cols="2">31.25 34.86 23.49 47.05</cell><cell cols="2">0.09 s</cell></row><row><cell>Mono-SF [3]</cell><cell cols="2">16.72 18.97 11.85 21.60</cell><cell>41</cell><cell>s</cell></row><row><cell>Self-Mono-SF-ft (Ours)</cell><cell cols="2">(2.89) (3.91) (6.19) (7.53)</cell><cell cols="2">0.09 s</cell></row><row><cell cols="5">Table 4. Monocular scene flow evaluation on KITTI Scene Flow</cell></row><row><cell cols="5">Training: our self-supervised learning approach significantly out-</cell></row><row><cell cols="5">performs all multi-task CNN methods (upper rows) on the scene</cell></row><row><cell cols="5">flow metric, SF1-all. Lower rows provide the accuracy of a semi-</cell></row><row><cell cols="2">supervised method [3] and our fine-tuned model.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">D1-all D2-all F1-all SF1-all Runtime</cell></row><row><cell>DRISF [32]</cell><cell>2.55 4.04 4.73</cell><cell>6.31</cell><cell cols="2">0.75 s</cell></row><row><cell>SENSE [24]</cell><cell>2.22 5.89 7.64</cell><cell>9.55</cell><cell cols="2">0.32 s</cell></row><row><cell>PWOC-3D [42]</cell><cell cols="2">5.13 8.46 12.96 15.69</cell><cell cols="2">0.13 s</cell></row><row><cell>UnOS [53]</cell><cell cols="2">6.67 12.05 18.00 22.32</cell><cell cols="2">0.08 s</cell></row><row><cell>Mono-SF [3]</cell><cell cols="3">16.32 19.59 12.77 23.08 41</cell><cell>s</cell></row><row><cell>Self-Mono-SF (Ours)</cell><cell cols="2">34.02 36.34 23.54 49.54</cell><cell cols="2">0.09 s</cell></row><row><cell>Self-Mono-SF-ft (Ours)</cell><cell cols="2">22.16 25.24 15.91 33.88</cell><cell cols="2">0.09 s</cell></row></table><note>. Scene flow evaluation on KITTI Scene Flow Test: we compare our method with stereo (top) and monocular (bottom) scene flow methods. Despite the difficult setting, our fine-tuned model demonstrates encouraging results in real-time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Monocular depth comparison: our method demonstrates superior accuracy on the KITTI split and competitive accuracy on the Eigen split compared to all published multi-task methods. § method using stereo sequences for training.</figDesc><table><row><cell>regard-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Scene flow accuracy of each decoder configuration: splitting the context network already decreases the scene flow accuracy by 32.73%. Further splitting the decoder yields training instability with trivial solutions for the disparity output.</figDesc><table><row><cell>Configuration</cell><cell cols="2">D1-all D2-all F1-all SF1-all</cell></row><row><cell>Single decoder</cell><cell cols="2">31.25 34.86 23.49 47.05</cell></row><row><cell>Splitting the context network</cell><cell cols="2">44.19 45.02 23.51 62.45</cell></row><row><cell>Splitting at the last layer</cell><cell>100</cell><cell>97.22 26.46 100</cell></row><row><cell>Splitting at the 2 nd -to-last layer</cell><cell>100</cell><cell>97.22 26.39 100</cell></row><row><cell>Splitting at the 3 rd -to-last layer</cell><cell>100</cell><cell>97.22 26.94 100</cell></row><row><cell>Splitting at the 4 th -to-last layer</cell><cell>100</cell><cell>97.22 28.68 100</cell></row><row><cell>Splitting into two separate decoders</cell><cell>100</cell><cell>97.22 27.63 100</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view scene flow estimation: A view centered variational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahum</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3D scene flow estimation in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">Abu</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2574" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mono-SF: Multi-view geometry meets single-view depth for monocular scene flow estimation of dynamic traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Brickwedde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How do neural networks see depth in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Tom Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2014</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CAM-Convs: Camera-aware multi-scale convolutions for singleview depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11826" to="11835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HPLFlowNet: Hierarchical permutohedral lattice FlowNet for scene flow estimation on large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3254" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kinecting the dots: Particle based scene flow from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2290" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SphereFlow: 6 DoF scene flow from RGB-D pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hornáček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3526" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualization of convolutional neural networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3869" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A variational method for scene flow estimation from stereo sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Devernay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lite-FlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MirrorFlow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2015</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SENSE: A shared encoder network for scene-flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bridging stereo matching and optical flow via spatiotemporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ying</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning residual flow as dynamic motion from stereo videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of scene flow estimation fusing with local rigidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FlowNet3D: Learning scene flow in 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="529" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning rigidity in dynamic scenes with a moving camera for 3D motion field estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep rigid instance scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint 3D estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Workshop on Image Sequence Analysis (ISA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning scene flow from RGB-D images with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Lue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ze</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sf-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dense semi-rigid scene flow estimation from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Devernay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="567" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cascaded scene flow prediction using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Sudderth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PWOC-3D: Deep occlusion-aware end-to-end scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining stereo disparity and optical flow for basic scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Commercial Vehicle Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SceneFlowFields: Dense interpolation of sparse scene flow correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kuschk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1056" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SceneED-Net: A deep learning approach for scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehasis</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundar</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundar</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="480" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3D scene flow estimation with a rigid motion prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1291" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Viewconsistent 3D scene flow estimation over multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="263" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Piecewise rigid scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1377" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3D scene flow estimation with a piecewise rigid scene model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">UnOS: Unified unsupervised optical-flow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stereoscopic scene flow computation for 3D motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="51" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Global stereo reconstruction under second order smoothness priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Monocular scene flow estimation via variational method. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="10575" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Scene flow estimation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zike</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02590[cs.CV],2016.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3D motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On 3D scene flow and structure estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3526" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robustness meets deep learning: An end-to-end hybrid pipeline for unsupervised learning of egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ScopeFlow: Dynamic scene scoping for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
