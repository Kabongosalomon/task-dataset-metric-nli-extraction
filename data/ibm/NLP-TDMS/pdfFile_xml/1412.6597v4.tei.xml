<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN ANALYSIS OF UNSUPERVISED PRE-TRAINING IN LIGHT OF RECENT ADVANCES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Le Paine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
							<email>pkhorra2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<email>weihan3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN ANALYSIS OF UNSUPERVISED PRE-TRAINING IN LIGHT OF RECENT ADVANCES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a workshop contribution at ICLR 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We analyze the benefits of unsupervised pre-training in the context of recent deep learning innovations including: rectified linear units, data augmentation, and dropout. Recent work shows that convolutional neural networks (CNNs) can achieve state-of-the-art performance for object classification ) and object detection <ref type="bibr" target="#b7">(Girshick et al. (2013)</ref>), when there is enough training data. However, in many cases there is a dearth of labeled data. In these cases regularization is necessary for good results. The most common types of regularization are data augmentations ; <ref type="bibr" target="#b5">Dosovitskiy et al. (2014)</ref>) and dropout ). Another form of regularization, unsupervised pre-training <ref type="bibr" target="#b10">(Hinton et al. (2006)</ref>; <ref type="bibr" target="#b0">Bengio et al. (2007)</ref>; <ref type="bibr" target="#b6">Erhan et al. (2010)</ref>), has recently fallen out of favor.</p><p>While there has been significant work in unsupervised learning, most of these works came before rectified linear units, which significantly help training deep supervised neural networks, and before simpler regularization schemes for unsupervised learning, such as zero-bias with linear encoding for auto-encoders <ref type="bibr" target="#b22">(Memisevic et al. (2014)</ref>).</p><p>We train an unsupervised method that takes advantage of these improvements we call Zero-bias Convolutional Auto-encoders (CAEs). Previous work showed that pre-trained tanh CAEs achieved an increase in performance over randomly initialized tanh CNNs. We conduct this experiment with our zero-bias CAE and observe a larger boost in performance.</p><p>We analyze the effectiveness of our technique when combined with the popular regularization techniques used during supervised training on CIFAR-10 while varying the ratio of unsupervised to supervised samples. We do this comparing against randomly initialized CNNs without any additional regularization. We find that, when ratio is large, unsupervised pre-training provides useful regularization, increasing test set performance. When the ratio is small, we find that unsupervised pre-training hurts performance.</p><p>Accepted as a workshop contribution at ICLR 2015</p><p>We verify our finding that unsupervised pre-training can boost performance when the ratio of unsupervised to supervised samples is high by running our algorithm on the STL-10 dataset, which has a ratio of 100:1. As expected, we observe an improvement (3.87%). When combined with additional color augmentation, we achieve near state-of-the-art results. Our unsupervised regularization still yields an improvement of (1.69%).</p><p>We will begin by reviewing related work on fully-connected and convolutional auto-encoders. In Section 3, we will present our method and how it is trained both during unsupervised pre-training and supervised fine-tuning. We present our results on the CIFAR-10 and STL-10 datasets in Section 4, and in Section 5 we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many methods have used unsupervised learning to learn parameters, which are subsequently used to initialize a neural network to be trained on supervised data. These are called unsupervised pretraining, and supervised fine-tuning respectively. We will highlight some of the unsupervised learning methods related to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AUTO-ENCODERS</head><p>One of the most widely-used models for unsupervised learning, an auto-encoder is a model that learns a function that minimizes the squared error between the input x ∈ R n and its reconstruction r(x):</p><formula xml:id="formula_0">L = x − r(x) 2 2 (1) r(x) = W T d f (W e x + b) + c<label>(2)</label></formula><p>In the above equation, W e represents the weight matrix that transforms the input, x into some hidden representation, b is vector of biases for each hidden unit and f (·) is some nonlinear function.</p><p>Commonly chosen examples for f (·) include the sigmoid and hyperbolic tangent functions. Meanwhile, W d is the weight matrix that maps back from the hidden representation to the input space and c is a vector of biases for each input (visible) unit. These parameters are commonly learned by minimizing the loss function over the training data via stochastic gradient descent.</p><p>When no other constraints are imposed on the loss function, the auto-encoder weights tend to learn the identity function. To combat this, some form of regularization must imposed upon the model so that the model can uncover the underlying structure in the data. Some forms of regularization include adding noise to the input units ) and requiring the hidden unit activations be sparse  or have small derivatives <ref type="bibr" target="#b23">(Rifai et al. (2011)</ref>). These models are known as de-noising, sparse, and contractive auto-encoders respectively. A more recent work by <ref type="bibr" target="#b22">Memisevic et al. (2014)</ref> showed that training an auto-encoder with rectified linear units (ReLU) caused the activations to form tight clusters due to having negative bias values. They showed that using thresholded linear (TLin) or thresholded rectifier (TRec) activations with no bias can allow one to train an auto-encoder without the need for additional regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONVOLUTIONAL AUTO-ENCODERS</head><p>While the aforementioned fully-connected techniques have shown impressive results, they do not directly address the structure of images. Convolutional neural networks (CNNs) <ref type="bibr" target="#b15">(LeCun et al. (1998)</ref>; <ref type="bibr" target="#b17">Lee et al. (2009)</ref>) present a way to reduce the number of connections by having each hidden unit only be responsible for a small local neighborhood of visible units. Such schemes allow for dense feature extraction followed by pooling layers which when stacked could allow the network to learn over larger and larger receptive fields. Convolutional auto-encoders (CAEs) combined aspects from both auto-encoders and convolutional neural nets making it possible to extract highly localized patchbased information in an unsupervised fashion. There have been several works in this area including <ref type="bibr" target="#b12">Jarrett et al. (2009) and</ref><ref type="bibr" target="#b26">Zeiler et al. (2010)</ref>. Both rely on sparse coding to force their unsupervised learning to learn non-trival solutions. <ref type="bibr" target="#b27">Zeiler et al. (2011)</ref> extended this work by introducing pooling/unpooling and visualizing how individual feature maps at different layers influenced specific portions of the reconstruction. These sparse coding approaches had limitations because they used an iterative procedure for inference. A later work by <ref type="bibr" target="#b21">Masci et al. (2011)</ref> trained deep feed forward convolutional auto-encoders, using only max-pooling and saturating tanh non-linearities as a form of regularization, while still showing a modest improvement over randomly initialized CNNs. While tanh was a natural choice at the time,  showed that ReLUs are more suitable for learning given their non-saturating behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>Our method's training framework can be broken up into two phases: (i) unsupervised pre-training and (ii) supervised fine-tuning. We describe those in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UNSUPERVISED PRE-TRAINING</head><p>Our method incorporates aspects of previous unsupervised learning methods in order to learn salient features, yet be efficient to train. Our model is similar to the deconvolutional network in <ref type="bibr" target="#b27">Zeiler et al. (2011)</ref> where the cost we minimize at each layer is the mean square error on the original image. However, unlike the network in Zeiler et al. <ref type="formula" target="#formula_0">(2011)</ref>, our method does not use any form of sparse coding. Our model also is similar to that of <ref type="bibr" target="#b21">Masci et al. (2011)</ref>, however we improve upon it by introducing regularization in the convolutional layers through the use of zero-biases and ReLUs as discussed in <ref type="bibr" target="#b22">Memisevic et al. (2014)</ref>.</p><p>We now describe the model architecture in detail. Like the previous work described above, our model involves several encoding modules followed by several decoding modules. A single encoding module E l (·) consists of a convolution layer F l , a nonlinearity f (·), followed by a pooling layer P s l with switches s l .</p><formula xml:id="formula_1">E l (x) = P s l f (F l x)<label>(3)</label></formula><p>Each encoding module has an associated decoding module D l , which unpools using E l pooling switches s l and deconvolves with E l 's filters, (i.e. F T l ).</p><formula xml:id="formula_2">D l (x) = F T l U s l x<label>(4)</label></formula><p>A two layer network can be written as:</p><formula xml:id="formula_3">r(x) = D 1 (D 2 (E 2 (E 1 (x))))<label>(5)</label></formula><p>We train each encoder/decoder pair in a greedy fashion (i.e. first a 1 layer CAE, then a 2 layer CAE, etc.) while keeping the parameters of previous layers fixed. Like Zeiler et al. <ref type="formula" target="#formula_0">(2011)</ref>, we compute the cost by taking the mean squared error between the original image and the network's reconstruction of the input. Thus, the costs for a one layer network (C 1 (x)) and two layer network (C 2 (x)) would be expressed in the following manner:</p><formula xml:id="formula_4">C 1 (x) = x − D 1 (E 1 (x)) 2 2 (6) C 2 (x) = x − D 1 (D 2 (E 2 (E 1 (x)))) 2 2<label>(7)</label></formula><p>We regularize our learned representation by fixing the biases of our convolutional and deconvolutional layers at zero and using ReLUs as our activation function during encoding. We use linear activations for our decoders. Unlike the work by <ref type="bibr" target="#b22">Memisevic et al. (2014)</ref> which analyzes fullyconnected auto-encoders, our work is the first, to our knowledge, that trains zero-bias CAEs for unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">UNSUPERVISED WEIGHT INITIALIZATION</head><p>Weight initialization is often a key component of successful neural network training. For ReLU's it is important to ensure the input to the ReLU is greater than 0. This can be achieved by setting the bias appropriately. This cannot be done for zero-bias auto-encoders. Instead we use two methods for initializing the weights to achieve this 1) in the first layer, we initialize each of the filters to be a randomly drawn patch from the dataset, 2) on the later layers, we sample weights from a Gaussian distribution and find the nearest orthogonal matrix by taking the singular value decomposition (SVD) of the weight matrix and setting all of the singular values to one. For CNNs we must take into account the additive effect of overlapping patches thus we weight each filter by a 2D hamming window to prevent intensity build-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SUPERVISED FINE-TUNING</head><p>After the weights of the CAE have been trained, we remove all of the decoder modules and leave just the encoding modules. We add an additional fully-connected layer and a softmax layer to the pretrained encoding modules. The weights of these layers are drawn from a Gaussian distribution with zero mean and standard deviation of</p><formula xml:id="formula_5">k/ √ N F AN IN , where k is drawn uniformly from [0.2, 1.2].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING</head><p>For both unsupervised and supervised training we use stochastic gradient descent with a constant momentum of 0.9, and a weight decay parameter of 1e-5. We select the highest learning rate that doesn't explode for the duration of training. For these experiments we do not anneal the learning rate. The only pre-processing we do to each patch is centering (i.e. mean subtraction) and scaling to unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>We run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton <ref type="formula" target="#formula_0">(2009)</ref>) and STL-10 (Coates et al. <ref type="formula" target="#formula_0">(2011)</ref>). CIFAR-10 is a common benchmark for object recognition. Many unsupervised and supervised neural network approaches have been tested on it. It consists of 32x32 pixel color images drawn from 10 object categories. It has 50,000 training images, and 10,000 testing images. STL-10 is also an object recognition benchmark, but was designed to test unsupervised learning algorithms, so it has a relatively small labeled training set of 500 images per class, and an additional unsupervised set which contains 100,000 unlabeled images. The test set contains 800 labeled images per class. All examples are 96x96 pixel color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR-10</head><p>On CIFAR-10, we train a network with structure similar to <ref type="bibr" target="#b21">Masci et al. (2011)</ref>, so that we can directly show the benefits of our modifications. The network consists of three convolutional layers with 96, 144, and 192 filters respectively. The filters in the first two layers are of size 5x5 while the filters in the third layer are of size 3x3. We also add 2x2 max pooling layers after the first two convolutional layers. There is also a full-connected layer with 300 hidden units followed by a softmax layer with 10 output units. All of our nets were trained using our own open source neural network library 1 .</p><p>As stated in the methods section, we first train our unsupervised model on 100% of the training images, do supervised fine-tuning, and report overall accuracy on the test set. We 1) present qualitative results of unsupervised learning, 2) show our zero-bias convolutional auto-encoder performs well compared to previous convolutional auto-encoder work by <ref type="bibr" target="#b21">Masci et al. (2011)</ref> developed before the popularization of rectified linear units, and zero-bias auto-encoders, 3) we show our analysis of various regularization techniques, and vary the ratio of unsupervised to supervised data, 4) for completeness we report our best results when training on the full CIFAR-10 dataset, however this is not the main point of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">QUALITATIVE RESULTS</head><p>One way in which we ensure the quality of our learned representation is by inspecting the first layer filters. We visualize the filters learned by our model in <ref type="figure" target="#fig_0">Figure 1</ref>. So that we can directly compare with the filters presented in <ref type="bibr" target="#b21">Masci et al. (2011)</ref>, we trained an additional zero-bias convolutional auto-encoder with filters of size 7x7x3 (instead of 5x5x3) in the first layer. From <ref type="figure" target="#fig_0">Figure 1</ref>, we can see that, indeed, our model is able to capture interpretable patterns such as Gabor-like oriented edges (both color and intensity) and center-surrounds. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by <ref type="bibr" target="#b21">Masci et al. (2011)</ref> with our zero-bias CAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">UNSUPERVISED PRE-TRAINING FOR TANH CAES AND ZERO-BIAS CAES</head><p>In their paper, <ref type="bibr" target="#b21">Masci et al. (2011)</ref> trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experiment using a zero-bias CNN trained from a random initialization, and compare it to one pretrained using our zero-bias CAE.</p><p>In <ref type="table" target="#tab_0">Table 1</ref> we compare the improvements of our model with that of <ref type="bibr" target="#b21">Masci et al. (2011)</ref>'s, on various subsets of CIFAR-10. As expected, the zero-bias CNN (a ReLU CNN without bias parameters) performs significantly better than the tanh CNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to <ref type="bibr" target="#b21">Masci et al. (2011)</ref> our pre-trained model shows similar or better performance over the randomly initialized CNN. When the ratio of unsupervised to supervised data is high, we experience an 8.44% increase in accuracy as opposed to <ref type="bibr" target="#b21">Masci et al. (2011)</ref>'s 3.22% increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">ANALYSIS OF REGULARIZATION METHODS</head><p>Next, we analyze how different supervised regularization techniques affect our model's performance. Specifically, we consider the effects of dropout, data augmentation (via translations and horizontal flips), unsupervised pre-training (with our zero-bias CAE) and their combinations. We compare each regularization technique to a zero-bias CNN trained from random initialization without any regularization (labeled CNN in <ref type="figure" target="#fig_1">Figure 2</ref>). <ref type="figure" target="#fig_1">Figure 2</ref> shows the classification accuracy improvement over CNN for each type of regularization both individually and together.</p><p>We perform this analysis for subsets of CIFAR-10 with different unsupervised to supervised sample ratios ranging from 50:1 to 1:1, by fixing the unsupervised data size, and varying the number of supervised examples. It is important to note that as this ratio approaches 1:1, the experimental setup favors data augmentation and dropout because the number of virtual supervised samples is larger than number of unsupervised samples.</p><p>In <ref type="figure" target="#fig_1">Figure 2a</ref>, where the ratio of unsupervised to supervised samples is 50:1, there are three notable effects: (i) unsupervised pre-training alone yields a larger improvement (4.09%) than data augmentation (2.67%) or dropout (0.59%), (ii) when unsupervised pre-training is combined with either data augmentation or dropout, the improvement is greater than the sum of the individual contributions, (iii) we experience the largest gains (15.86%) when we combine all three forms of regularization.  We see that effect (ii) is also observed in the case where the ratio of unsupervised to supervised samples is 10:1 <ref type="figure" target="#fig_1">(Figure 2b</ref>), and to a lesser extent when the ratio is 5:1 <ref type="figure" target="#fig_1">(Figure 2c</ref>). Unfortunately, effects (i) and (iii) are not observed when the ratio of unsupervised to supervised samples decreases. We will elaborate on effect (i) below.</p><p>In <ref type="figure">Figure 3</ref>, we observe that the improvement in performance from unsupervised learning decreases rapidly as the ratio of unsupervised to supervised samples decreases. Surprisingly, when the ratio is 1:1, we see that unsupervised learning actually hurts performance (-0.67%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">COMPARISON WITH EXISTING METHODS</head><p>We also compare the performance of our algorithm on the full CIFAR-10 dataset with other techniques in <ref type="table" target="#tab_1">Table 2</ref>, though we show above our method performs worse when the ratio of unsupervised to supervised samples is 1:1. We outperform all methods that use unsupervised pre-training <ref type="bibr" target="#b21">(Masci et al. (2011</ref><ref type="bibr" target="#b5">), Dosovitskiy et al. (2014</ref>, <ref type="bibr" target="#b19">Lin and Kung (2014)</ref>), however we are not competitive with supervised state-of-the-art. We include some representative supervised methods in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">STL-10</head><p>Next, we assess the effects of unsupervised pre-training on STL-10. From the CIFAR-10 experiments, it is clear unsupervised pre-training can be beneficial if the unsupervised dataset is much larger than the supervised dataset. STL-10 was designed with this in mind, and has a ratio of unsupervised to supervised data of 100:1. So we experimentally show this benefit.</p><p>We design our network to have structure similar to <ref type="bibr" target="#b5">Dosovitskiy et al. (2014)</ref>, to ease comparison. The network used consists 3 convolutional layers with 64, 128, and 256 filters in each layer, a fully-connected layer with 512 units, and a softmax layer with 10 output units. We also apply maxpooling layers of size 2x2 after the first two convolutional layers and quadrant pooling after the third convolutional layer.  <ref type="figure">Figure 3</ref>: The benefits of unsupervised learning vs. unsupervised to supervised sample ratio. When the ratio is 50:1, we see a 4.09% increase in performance. But the benefit shrinks as the ratio decreases. When the ratio is 1:1, there is a penalty for using unsupervised pre-training.</p><p>We train the zero-bias CAE on 100,000 unlabeled images. We then fine-tune the network on each of the 10 provided splits of training set, each consisting of 1000 samples (100 samples per class), and evaluate all of them on the test set. The accuracies are subsequently averaged to obtain the final recognition accuracy. Similar to our CIFAR-10 experiments, we also train a zero-bias CNN with the same structure as our zero-bias CAE on each of the splits to further highlight the benefits of unsupervised learning. <ref type="table" target="#tab_2">Table 3</ref> presents our results on the STL-10 dataset and compares them with other methods. As expected, unsupervised pretraining gives a 3.87% increase over the randomly initialized CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">ADDITIONAL DATA AUGMENTATION: COLOR AND CONTRAST</head><p>The current best result on STL-10 <ref type="bibr" target="#b5">(Dosovitskiy et al. (2014)</ref>) makes extensive use of additional data augmentation including: scaling, rotation, color and two forms of contrast. They do not perform these augmentations during supervised training, but during a discriminative unsupervised feature learning period. We test the regularizing effects of these additional augmentations when applied directly to supervised training, and test how these regularization effects hold up when combined with with unsupervised pre-training. To do this, we use some of these additional data-augmentations during our supervised training: color augmentation and contrast augmentation.</p><p>Color augmentation: The images are represented in HSV color space <ref type="bibr">(h, s, v)</ref>. Here we generate a single random number for each image and add it to the hue value for each pixel like so:   <ref type="bibr" target="#b20">Mairal et al. (2014)</ref> 62.32 % Hierarchical Matching Pursuit (HMP) - <ref type="bibr" target="#b2">Bo et al. (2013)</ref> 64.5 % ± 1.0 % NOMP - <ref type="bibr" target="#b19">Lin and Kung (2014)</ref> 67.9 % ± 0.6 % Multi-task Bayesian Optimization - <ref type="bibr" target="#b24">Swersky et al. (2013)</ref> 70.1 % ± 0.6 % Exemplar CNN - <ref type="bibr" target="#b5">Dosovitskiy et al. (2014)</ref> 72 <ref type="formula">.</ref> </p><formula xml:id="formula_6">a ∼ U nif orm(−0.1, 0.1) (8) h = h + a<label>(9)</label></formula><p>And use them to modify the saturation and value for every pixel in the image, like so:</p><formula xml:id="formula_8">s = as b + c (13) v = ds e + f<label>(14)</label></formula><p>We find that a) additional data-augmentation is incredibly helpful, increasing accuracy by 6.5%, b) unsupervised pre-training still maintains an advantage (1.69%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We present a new type of convolutional auto-encoder that has zero-bias and ReLU activations and achieves superior performance to previous methods. We conduct thorough experiments on CIFAR-10 to analyze the effects of unsupervised pre-training as a form of regularization when used in isolation and in combination with supervised forms of regularization such as data augmentation and dropout. We observe that, indeed, unsupervised pre-training can provide a large gain in performance when the ratio of unsupervised to supervised samples is large. Finally, we verify our findings by applying our model to STL-10, a dataset with far more unlabeled samples than labeled samples (100:1). We find that with additional regularization, via color augmentation, our method is able to achieve nearly state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CODE</head><p>All experiments were run using our own open source library Anna, which can be found at: https: //github.com/ifp-uiuc/anna</p><p>Code to reproduce the experiments can be found at: https://github.com/ifp-uiuc/ an-analysis-of-unsupervised-pre-training-iclr-2015</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>First layer filters learned by our zerobias convolutional auto-encoder. Each filter has dimension 7x7x3. (Best viewed in color.) For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>to supervised sample ratio (5000 samples per class), baseline CNN: 80.2% Analysis of the effects of different types of regularization (A: data augmentation, D: dropout, U: unsupervised learning), individually and jointly, on different subsets of CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between Tanh CAE<ref type="bibr" target="#b21">(Masci et al. (2011)</ref>) and our model on various subsets of CIFAR-10.</figDesc><table><row><cell>Unsupervised to supervised ratio</cell><cell>50:1</cell><cell>10:1</cell><cell>5:1</cell><cell>1:1</cell></row><row><cell>(Samples per Class)</cell><cell>(100)</cell><cell>(500)</cell><cell>(1000)</cell><cell>(5000)</cell></row><row><cell>Tanh CNN -Masci et al. (2011)</cell><cell>44.48 %</cell><cell>-</cell><cell cols="2">64.77 % 77.50 %</cell></row><row><cell>Tanh CAE -Masci et al. (2011)</cell><cell>47.70 %</cell><cell>-</cell><cell cols="2">65.65 % 78.20 %</cell></row><row><cell>Zero-bias CNN</cell><cell cols="4">47.01 % 64.76 % 73.30 % 82.73 %</cell></row><row><cell>Zero-bias CAE</cell><cell cols="4">55.45 % 68.42 % 74.06 % 83.64 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Quantitative comparison with other methods on CIFAR-10 (A: Data Augmentation, D:</cell></row><row><cell>Dropout, U: Unsupervised Learning).</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Accuracy</cell></row><row><cell>Convolutional Auto-encoders -Masci et al. (2011)</cell><cell>79.20 %</cell></row><row><cell>Single layer K-means -Coates et al. (2011)</cell><cell>79.60 %</cell></row><row><cell cols="2">Convolutional K-means Networks -Coates and Ng (2011) 82.00 %</cell></row><row><cell>Exemplar CNN -Dosovitskiy et al. (2014)</cell><cell>82.00 %</cell></row><row><cell>Convolutional Kernel Networks -Mairal et al. (2014)</cell><cell>82.18 %</cell></row><row><cell>NOMP -Lin and Kung (2014)</cell><cell>82.90 %</cell></row><row><cell>Max-Out Networks -Goodfellow et al. (2013b)</cell><cell>90.65 %</cell></row><row><cell>Network In Network -Lin et al. (2013)</cell><cell>91.20 %</cell></row><row><cell>Deeply-Supervised Nets -Lee et al. (2014)</cell><cell>91.78 %</cell></row><row><cell>Zero-bias CNN +ADU</cell><cell>86.44 %</cell></row><row><cell>Zero-bias CNN +AD</cell><cell>86.70 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison with other methods on STL-10 (A: Data Augmentation, D: Dropout, C: Color Augmentation, U: Unsupervised Learning).AlgorithmAccuracy Convolutional K-means Networks - 60.1 % ± 1.0 % Convolutional Kernel Networks -</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ifp-uiuc/anna</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This material is based upon work supported by the National Science Foundation under Grant No. 392 NSF IIS13-18971. The two Tesla K40 GPUs used for this research were donated by the NVIDIA Corporation. We would like to acknowledge Theano <ref type="bibr" target="#b1">(Bergstra et al. (2010)</ref>) and Pylearn2 <ref type="bibr" target="#b8">(Goodfellow et al. (2013a)</ref>), on which our code is based. Also, we would like to thank Shiyu Chang for many helpful discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for rgb-d based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.4214</idno>
		<title level="m">Pylearn2: a machine learning research library</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition? In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Computer Science Department</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Deeplysupervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stable and efficient representation learning with nonnegativity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Tsung-Han Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<editor>Tony Jebara and Eric P. Xing</editor>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1323" to="1331" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2627" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked convolutional autoencoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2011</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Zero-bias autoencoders and the benefits of co-adapting features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3337</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contractive autoencoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
