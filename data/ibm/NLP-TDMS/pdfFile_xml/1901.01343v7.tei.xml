<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural Networks with Convolutional ARMA Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bianchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Livi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
						</author>
						<title level="a" type="main">Graph Neural Networks with Convolutional ARMA Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) are a class of models lying at the intersection between deep learning and methods for structured data, which perform inference on discrete objects (nodes) by accounting for arbitrary relationships (edges) among them <ref type="bibr" target="#b3">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b1">Battaglia et al., 2018)</ref>. A GNN combines node features within local neighborhoods on the graph to learn node representations that can be directly mapped into categorical labels or real val-ues <ref type="bibr" target="#b40">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b24">Klicpera et al., 2019)</ref>, or combined to generate graph embeddings for graph classification and regression <ref type="bibr" target="#b36">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b7">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b51">Yang et al., 2016;</ref><ref type="bibr" target="#b17">Hamilton et al., 2017;</ref><ref type="bibr" target="#b0">Bacciu et al., 2018)</ref>.</p><p>The focus of this work is on GNNs that implement a convolution in the spectral domain with a non-linear trainable filter <ref type="bibr" target="#b4">(Bruna et al., 2013;</ref><ref type="bibr" target="#b18">Henaff et al., 2015)</ref>. Such a filter selectively shrinks or amplifies the Fourier coefficients of the graph signal (an instance of the node features) and then maps the node features to a new space. To avoid the expensive spectral decomposition and projection in the frequency domain, state-of-the-art GNNs implement graph filters as low-order polynomials that are learned directly in the node domain <ref type="bibr" target="#b5">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b22">Kipf &amp; Welling, 2016a;</ref>. Polynomial filters have a finite impulse response and perform a weighted moving average filtering of graph signals on local node neighborhoods <ref type="bibr" target="#b44">(Tremblay et al., 2018)</ref>, allowing for fast distributed implementations such as those based on Chebyshev polynomials and Lanczos iterations <ref type="bibr" target="#b42">(Susnjara et al., 2015;</ref><ref type="bibr" target="#b5">Defferrard et al., 2016;</ref><ref type="bibr" target="#b28">Liao et al., 2019)</ref>. Polynomial filters have limited modeling capabilities <ref type="bibr" target="#b21">(Isufi et al., 2016)</ref> and, due to their smoothness, cannot model sharp changes in the frequency response <ref type="bibr" target="#b44">(Tremblay et al., 2018)</ref>. Crucially, polynomials with high degree are necessary to reach high-order neighborhoods, but they tend to be more computationally expensive and, most importantly, overfit the training data making the model sensitive to changes in the graph signal or the underlying graph structure. A more versatile class of filters is the family of Auto-Regressive Moving Average filters (ARMA) <ref type="bibr" target="#b33">(Narang et al., 2013)</ref>, which offer a larger variety of frequency responses and can account for higherorder neighborhoods compared to polynomial filters with the same number of parameters.</p><p>In this paper, we address the limitations of existing graph convolutional layers inspired by polynomial filters and propose a novel GNN convolutional layer based on ARMA filters. Our ARMA layer implements a non-linear and trainable graph filter that generalizes the convolutional layers based on polynomial filters and provides the GNN with enhanced modeling capability, thanks to a flexible design of the filter's frequency response. The ARMA layer captures global graph structures with fewer parameters, overcoming arXiv:1901.01343v7 <ref type="bibr">[cs.</ref>LG] 24 Jan 2021 the limitations of GNNs based on high-order polynomial filters.</p><p>ARMA filters are not localized in node space and require to compute a matrix inversion, which is intractable in the context of GNNs. To address this issue, the proposed ARMA layer relies on a recursive formulation, which leads to a fast and distributed implementation that exploits efficient sparse operations on tensors. The resulting filters are not learned in the Fourier space induced by a given Laplacian, but are localized in the node space and are independent of the underlying graph structure. This allows our GNN to handle graphs with unseen topologies during the test phase of inductive inference tasks.</p><p>The performance of the proposed ARMA layer is evaluated on semi-supervised node classification, graph signal classification, graph classification, and graph regression tasks. Results show that a GNN equipped with ARMA layers outperforms GNNs with polynomial filters in every downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: graph spectral filtering</head><p>We assume a graph with M nodes to be characterized by a symmetric adjacency matrix A ∈ R M ×M and refer to graph signal X ∈ R M ×F as the instance of all features (vectors in R F ) associated with the graph nodes. Let L = I M − D −1/2 AD −1/2 be the symmetrically normalized Laplacian of the graph (where D is the degree matrix), with spectral decomposition L = M m=1 λ m u m u T m . A graph filter is an operator that modifies the components of X on the eigenvectors basis of L, according to a frequency response h acting on each eigenvalue λ m . The filtered graph signal reads</p><formula xml:id="formula_0">X = M m=1 h(λ m )u m u T m X = = U diag[h(λ 1 ), . . . , h(λ M )] U T X.<label>(1)</label></formula><p>This formulation inspired the seminal work of <ref type="bibr" target="#b4">Bruna et al. (Bruna et al., 2013)</ref> that implemented spectral graph convolutions in a neural network. Their GNN learns endto-end the parameters of a filter implemented as h = Bc, where B ∈ R M ×K is a cubic B-spline basis and c ∈ R K is a vector of control parameters. Such filters are not localized, since the full projection on the eigenvectors yields paths of infinite length and the filter accounts for interactions of each node with the whole graph, rather than those limited to the node neighborhood. Since this contrasts with the local design of classic convolutional filters, a follow-up work <ref type="bibr" target="#b18">(Henaff et al., 2015)</ref> introduced a parametrization of the spectral filters with smooth coefficients to achieve spatial localization. However, the main issue with the spectral filtering in Eq. (1) is computational complexity: not only the eigendecomposition of L is computationally expensive, but a double product with U is computed whenever the filter is applied. Notably, U in Eq. (1) is full even when L is sparse. Finally, since these spectral filters depend on a specific Laplacian spectrum, they cannot be transferred to graphs with another structure. For this reason, this spectral GNN cannot be used in downstream tasks such as graph classification or graph regression, where each datum is a graph with a different topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">GNNs based on polynomial filters and limitations</head><p>The desired filter response h(λ) can be approximated by a polynomial of order K,</p><formula xml:id="formula_1">h POLY (λ) = K k=0 w k λ k ,<label>(2)</label></formula><p>which performs a weighted moving average of the graph signal <ref type="bibr" target="#b44">(Tremblay et al., 2018)</ref>. These filters overcome important limitations of the spectral formulation, as they avoid the eigendecomposition and their parameters are independent of the Laplacian spectrum . Polynomial filters are localized in the node space, since the output at each node in the filtered signal is a linear combination of the nodes with their K-hop neighborhoods.</p><p>The order of the polynomial K is assumed to be small and independent of the number M of nodes in the graph.</p><p>To express polynomial filters in the node space, we first recall that the k-th power of any diagonalizable matrix, such as the Laplacian, can be computed by taking the power of its eigenvalues, i.e., L k = U diag[λ k 1 , . . . , λ k M ] U T . It follows that the filtering operation becomes</p><formula xml:id="formula_2">X = w 0 I + w 1 L + w 2 L 2 + · · · + w K L K X = = K k=0 w k L k X.<label>(3)</label></formula><p>Eq.</p><p>(2) and (3) represent a generic polynomial filter. Among the existing classes of polynomials, Chebyshev polynomials are often used in signal processing as they attenuate unwanted oscillations around the cut-off frequencies <ref type="bibr" target="#b41">(Shuman et al., 2011)</ref>, which, in our case, are the eigenvalues of the Laplacian. Fast localized GNN filters can approximate the desired filter response by means of the Chebyshev expansion T k (x) = 2xT k−1 (x) − T k−2 (x) <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref>, resulting in convolutional layers that perform the filtering operation</p><formula xml:id="formula_3">X = σ K−1 k=0 T k (L)XW k ,<label>(4)</label></formula><p>whereL = 2L/λ max − I M , σ(·) is a non-linear activation (e.g., a sigmoid or a ReLU function), and W k ∈ R Fin×Fout are the k trainable weight matrices that map the node features from R Fin to R Fout .</p><p>The output of a k-degree polynomial filter is a linear combination of the input within each vertex's k-hop neighborhood. Since the input beyond the k-hop neighborhood has no impact on the output of the filtering operation, to capture larger structures on the graph it is necessary to adopt high-degree polynomials. However, high-degree polynomials have poor interpolatory and extrapolatory performance since they overfit the known graph frequencies, i.e., the eigenvalues of the Laplacian. This hampers the GNN's generalization capability as it becomes sensitive to noise and small changes in the graph topology. Moreover, evaluating a polynomial with a high degree is computationally expensive both during training and inference <ref type="bibr" target="#b21">(Isufi et al., 2016)</ref>. Finally, since polynomials are very smooth, they cannot model filter responses with sharp changes.</p><p>A particular first-order polynomial filter has been proposed by <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2016a)</ref> for semi-supervised node classification. In their GNN model, called Graph Convolutional Network (GCN), the convolutional layer is a simplified version of a Chebyshev filter, obtained from Eq. (4) by considering K = 1 and by setting</p><formula xml:id="formula_4">W = W 0 = −W 1 X = σ Â XW .<label>(5)</label></formula><p>Additionally,L is replaced byÂ =D −1/2ÃD−1/2 , with A = A + γI M (usually, γ = 1). The modified adjacency matrixÂ contains self-loops that compensate for the removal of the term of order 0 in the polynomial, by ensuring that a node is part of its first-order neighborhood and that its features are preserved (to some extent) after convolution. Higher-order neighborhoods can be reached by stacking multiple GCN layers. On one hand, GCNs reduce overfitting and the heavy computational load of Chebyshev filters with high-order polynomials. On the other hand, since each GCN layer performs a Laplacian smoothing, after few convolutions the node features becomes too smoothed over the graph <ref type="bibr" target="#b27">(Li et al., 2018)</ref> and the initial node features are lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rational filters for graph signals</head><p>An ARMA filter can approximate well any desired filter response h(λ) thanks to a rational design that, compared to polynomial filters, can model a larger variety of filter shapes <ref type="bibr" target="#b44">(Tremblay et al., 2018)</ref>. The filter response of an ARMA filter of order K, denoted in the following as ARMA K , reads</p><formula xml:id="formula_5">h ARMAK (λ) = K−1 k=0 p k λ k 1 + K k=1 q k λ k ,<label>(6)</label></formula><p>which translates to the following filtering relation in the node spacē</p><formula xml:id="formula_6">X = I + K k=1 q k L k −1 K−1 k=0 p k L k X.<label>(7)</label></formula><p>Notice that by setting q k = 0, for every k, one recovers a polynomial filter, which is considered as the MA term of the model. The inclusion of the additional AR term encoded by these coefficients makes the ARMA model robust to noise and allows to capture longer dynamics on the graph sincex depends, in turn, on several steps of propagation of the node features. This is the key to capturing longer dependencies and more global structures on the graph, compared to a polynomial filter with the same degree.</p><p>The matrix inversion in Eq. <ref type="formula" target="#formula_6">(7)</ref> is slow to compute and yields a dense matrix that prevents us from using sparse multiplications to implement the GNN. In this paper, we follow a straightforward approach to avoid computing the inverse, which can be easily extended to a neural network implementation. Specifically, we approximate the effect of an ARMA 1 filter by iterating, until convergence, the firstorder recursionX</p><formula xml:id="formula_7">(t+1) = aMX (t) + bX,<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">M = 1 2 (λ max − λ min )I − L.<label>(9)</label></formula><p>The recursion in Eq. (8) is adopted in graph signal processing to apply a low-pass filter on a graph signal <ref type="bibr" target="#b29">(Loukas et al., 2015;</ref><ref type="bibr" target="#b21">Isufi et al., 2016)</ref>, but it is also equivalent to the recurrent update used in Label Propagation <ref type="bibr" target="#b53">(Zhou et al., 2004)</ref> and Personalized Page Rank <ref type="bibr" target="#b35">(Page et al., 1999)</ref> to propagate information on a graph by means of a random walk with a restart probability.</p><p>Following the derivation in <ref type="bibr" target="#b21">(Isufi et al., 2016)</ref>, we analyze the frequency response of an ARMA 1 filter from the convergence of Eq. (8):</p><formula xml:id="formula_9">X = lim t→∞ (aM) tX(0) + b t i=0 (aM) i X .<label>(10)</label></formula><p>The eigenvectors of M and L are the same, while the eigenvalues are related as follows: µ m = (λ max − λ min )/2 − λ m , where µ m and λ m represent the m-th eigenvalue of M and L, respectively. Since µ m ∈ [−1, 1], for |a| &lt; 1 the first term of Eq. (10), (aM) t , goes to zero when t → ∞, regardless of the initial pointX (0) . The second term, b t i=0 (aM) i , is a geometric series that converges to the matrix b(I − aM) −1 , with eigenvalues b/(1 − aµ m ). It follows that the frequency response of the ARMA 1 filter is</p><formula xml:id="formula_10">h ARMA1 (µ m ) = b 1 − aµ m .<label>(11)</label></formula><p>By summing K ARMA 1 filters, it is possible to recover the analytical form of the ARMA K filter in Eq. <ref type="formula" target="#formula_6">(7)</ref>. The resulting filtering operation is</p><formula xml:id="formula_11">X = K k=1 M m=1 b k 1 − a k µ m u m u T m X,<label>(12)</label></formula><p>with</p><formula xml:id="formula_12">h ARMA K (µ m ) = K k=1 b k 1 − a k µ m .<label>(13)</label></formula><p>Different orders (≤ K) of the numerator and denominator in Eq. (6) are trivially obtained by setting some coefficients to 0. It follows that an ARMA filter generalizes a polynomial filter when all coefficients q k are set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The ARMA neural network layer</head><p>In graph signal processing, the filter coefficients a and b in Eq. <ref type="formula" target="#formula_7">(8)</ref> are optimized with linear regression to reproduce a desired filter response h * (λ), which must be provided a priori by the designer <ref type="bibr" target="#b21">(Isufi et al., 2016)</ref>. Here, we consider a machine learning approach that does not require to specify the target response h * (λ) but in which the parameters are learned end-to-end from the data by optimizing a taskdependent loss function. Importantly, we also introduce non-linearities to enhance the representation capability of the filter response that can be learned.</p><p>The proposed neural network formulation of the ARMA 1 filter implements the recursive update of Eq. (8) with a Graph Convolutional Skip (GCS) layer, defined as</p><formula xml:id="formula_13">X (t+1) = σ LX (t) W + XV ,<label>(14)</label></formula><p>where σ(·) is a non-linearity such as ReLU, sigmoid, or hyperbolic tangent (tanh), X are the initial node features, and W ∈ R Fout×Fout and V ∈ R Fin×Fout are trainable parameters. The modified Laplacian matrixL is defined by setting λ min = 0 and λ max = 2 in Eq. (9) and thenL = M. This is a reasonable simplification since the spectrum of L lies in [0, 2] and the trainable parameters W and V can compensate for the small offset introduced. The unfolded recursion in Eq. (14) corresponds to a stack of GCS layers that share the same parameters.</p><p>Each GCS layer is localized in the node space, as it performs a filtering operation that depends on local exchanges among neighboring nodes and, through the skip connection, also on the initial node features X. The computational complexity of the GCS layer is linear in the number of edges (both in time and space) since the layer can be efficiently implemented as a sparse multiplication betweenL andX <ref type="bibr">(t)</ref> .</p><p>The neural network formulation of an ARMA 1 filter is obtained by iterating Eq. (14) until convergence, i.e., until X (T +1) −X (T ) &lt; , where is a small positive constant and T is the convergence time. The convergence of the update in Eq. <ref type="formula" target="#formula_0">(14)</ref>, which draws a connection to the original recursive formulation of the ARMA 1 filter, is guaranteed by Theorem 1. Theorem 1. It is sufficient that W 2 &lt; 1 and that σ(·) is a non-expansive map for Eq. (14) to converge to a unique fixed point, regardless of the initial stateX (0) . . If the non-linearity σ(·) is a non-expansive map, such as the ReLU function, the following inequality holds:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. LetX</head><formula xml:id="formula_14">X (t+1) a −X (t+1) b 2 = = σ LX (t) a W + XV − σ LX (t) b W + XV 2 ≤ ≤ LX (t) a W + XV −LX (t) b W − XV 2 = = LX (t) a W −LX (t) b W 2 ≤ ≤ L 2 W 2 X (t) a −X (t) b 2 .<label>(15)</label></formula><p>If the non-linearity σ(·) is also a squashing function (e.g., sigmoid or tanh), then the first inequality in (15) is strict.</p><p>Since the largest singular value ofL is ≤ 1 by definition, it follows that L 2 W 2 &lt; 1 and, therefore, (15) implies that Eq. <ref type="formula" target="#formula_0">(14)</ref> is a contraction mapping. The convergence to a unique fixed point and, thus, the inconsequentiality of the initial state, follow by the Banach fixed-point theorem <ref type="bibr" target="#b13">(Goebel &amp; Kirk, 1972)</ref>.</p><p>From Theorem 1 it follows that it is possible to choose an arbitrary &gt; 0 for which</p><formula xml:id="formula_15">∃T &lt; ∞ s.t. X (t+1) −X (t) 2 ≤ , ∀t ≥ T .</formula><p>Therefore, we can easily implement a stopping criterion for the iteration, which is met in finite time.</p><p>Similar to the formulation of the ARMA filter in Eq. (12), the output of the ARMA K convolutional layer is obtained by combining the outputs of K parallel stacks of GCS layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Each GCS stack k may require a different and possibly high number of iterations T k to converge, depending on the value of the node features X and the weight matrices W k and V k . This makes the implementation of the neural network cumbersome, because the computational graph is dynamic and changes every time the weight matrices are updated with gradient descent during training. Moreover, to train the parameters with backpropagation through time the neural network must be unfolded many times if T k is large, introducing a high computational cost and the vanishing gradient issue <ref type="bibr" target="#b2">(Bianchi et al., 2017)</ref>.</p><p>One solution is to follow the approach of Reservoir Computing, where the weight matrices W k and V k in each stack are randomly initialized and left untrained <ref type="bibr" target="#b30">(Lukoševičius &amp; Jaeger, 2009;</ref><ref type="bibr" target="#b10">Gallicchio &amp; Micheli, 2020)</ref>. We notice that the random weights initialization guarantees that the K GCS stacks implement different filtering operations. To compensate for the lack of training, high-dimensional features are exploited to generate rich latent representations that disentangle the factors of variations in the data <ref type="bibr" target="#b43">(Tiňo, 2020)</ref>. However, randomized architectures with high-dimensional feature spaces are memory inefficient and computationally expensive at inference time.</p><p>A second approach, considered in this work, is to drop the requirement of convergence altogether and fix the number of iterations to a constant value T , so that T k = T in each GCS stack k. In this way, we obtain a GNN that is easy to implement, fast to train and evaluate, and not affected by stability issues. Notably, the constraint W 2 &lt; 1 of Theorem 1 can be relaxed by adding to the loss function an L 2 weight decay regularization term.</p><p>Even by stacking a small number T of GCS layers, we expect the GNN to learn a large variety of node representations thanks to the non-linearity and the trainable parameters <ref type="bibr" target="#b38">(Raghu et al., 2017)</ref>. As non-linearity we adopt the ReLU function that, compared to the squashing nonlinearities, improves training efficiency by facilitating the gradient flow <ref type="bibr" target="#b14">(Goodfellow et al., 2016)</ref>.</p><p>Given the limited number of iterations, the initial stateX (0) now influences the final representationX <ref type="bibr">(T )</ref> . A natural choice is to initialize the state withX (0) = 0 ∈ R M ×Fout or with a linear transformation of the node featuresX (0) = XW (0) , where W (0) ∈ R Fin×Fout replaces W in the first layer of the stack. We adopted the latter initialization so that the node features are propagated also by the first GCS layer. We also note that it is possible to set W (0) = V to reduce the number of trainable parameters.</p><p>The output of the ARMA K convolutional layer is computed asX</p><formula xml:id="formula_16">= 1 K K k=1X (T ) k ,<label>(16)</label></formula><formula xml:id="formula_17">whereX (T ) k</formula><p>is the output of the last GCS layer in the kth stack. <ref type="figure" target="#fig_1">Fig. 1</ref> depicts a scheme of the proposed ARMA graph convolutional layer.</p><p>To encourage each GCS stack to learn a filtering operation with a response different from the other stacks, we apply stochastic dropout to the skip connections XV k in each GCS layer. This leads to learning a heterogeneous set of features that, when combined to form the output of the ARMA K layer, yield powerful and expressive node representations. We notice that the parameter sharing in each layer of the GCS stack endows the GNN with a strong regularization that helps to prevent overfitting and greatly reduces the model complexity, in terms of the number of trainable parameters. Finally, since the GCS stacks are independent of each other, the computation of an ARMA layer can be distributed across multiple processing units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Properties and relationship with other approaches</head><p>Contrarily to filters defined directly in the spectral domain <ref type="bibr" target="#b4">(Bruna et al., 2013)</ref>, ARMA filters do not explicitly depend on the eigenvectors and the eigenvalues of L, making them robust to perturbations in the underlying graph structure. For this reason, as formally proven for generic rational filters <ref type="bibr" target="#b25">(Levie et al., 2019a)</ref>, the proposed ARMA filters are transferable, i.e., they can be applied to graphs with different topologies not seen during training.</p><p>The skip connections in our architecture allow stacking many GCS layers without the risk of over-smoothing the node features. Due to the weight sharing, the ARMA architecture has similarities with the recurrent neural networks with residual connections used to process sequential data <ref type="bibr" target="#b49">(Wu et al., 2016)</ref>.</p><p>Similarly to GNNs operating directly in the node domain <ref type="bibr" target="#b40">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b9">Gallicchio &amp; Micheli, 2010)</ref>, each GCS layer computes the filtered signalx (t+1) i at vertex i as a combination of signals x (t) j in its 1-hop neighborhood, j ∈ N (i). Such a commutative aggregation solves the problem of undefined vertex ordering and varying neighborhood sizes, making the proposed operator permutation equivariant.</p><p>The skip connections in ARMA inject in each GCS layer t of the stack the initial node features X. This is different from a skip connection that either takes the output of the previous layer X (t−1) as input <ref type="bibr" target="#b37">(Pham et al., 2017;</ref><ref type="bibr" target="#b17">Hamilton et al., 2017)</ref>, or connects all the layers in a GNN stack directly to the output .</p><p>The ARMA layer can naturally deal with a time-varying topology and graph signals <ref type="bibr" target="#b19">(Holme, 2015;</ref><ref type="bibr" target="#b16">Grattarola et al., 2019)</ref> by replacing the constant term X in Eq. (14) with a time-dependent input X (t) .</p><p>Finally, we discuss the relationship between the proposed ARMA GNN and CayleyNets <ref type="bibr" target="#b26">(Levie et al., 2019b)</ref>, a GNN architecture that also approximates the effect of a rational filter. Specifically, the filtering operation of a Cayley polynomial in the node space is</p><formula xml:id="formula_18">X = w 0 X + 2Re K k=1 w k (L + iI) k (L − iI) −k X.<label>(17)</label></formula><p>To approximate the matrix inversion in Eq. (17) with a sequence of differentiable operations, CayleyNets adopt a fixed number T of Jacobi iterations. In practice, the Jacobi iterations approximate each term (L + iI)(L − iI) −1 as a polynomial of order T with fixed coefficients. Therefore, the resulting filtering operation performed by a CayleyNet assumes the form</p><formula xml:id="formula_19">X ≈ σ   w 0 X + 2Re    K k=1 w k T t=1L t k    X   ,<label>(18)</label></formula><p>whereL is an operator with the same sparsity pattern of L. We note that Eq. For this reason, CayleyNets share strong similarities with the Chebyshev filter in Eq. (4), as it uses a (high-order) polynomial to propagate the node features on the graph for KT hops before applying the non-linearity. On the other hand, each of the K parallel stacks in the proposed ARMA layer propagates the current node representationsX (t) only for 1 hop and combines them with the node features X before applying the non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Spectral analysis of the ARMA layer</head><p>In this section we show how the proposed ARMA layer can implement filtering operations with a large variety of frequency responses. The filter response of the ARMA filter derived in Sec. 3 cannot be exploited to analyze our GNN formulation, due to the presence of non-linearities. Therefore, we first recall that a filter changes the components of a graph signal X on the eigenbase induced by L (which is the same as the one induced byL, according to Sylvester's theorem). By referring to Eq. (1), X is first projected on the eigenspace of L by U T , then the filter h(λ m ) changes the value of the component of X on each eigenvector u m , finally U T maps back to the node space. By left-multiplying U T in Eq. (1) we obtain</p><formula xml:id="formula_20">U TX = diag[h(λ 1 ), . . . , h(λ M )] U T X, M m=1 u T mX = M m=1 h(λ m )u T m X.<label>(19)</label></formula><p>WhenX is the output of the ARMA layer, the term U TX defines how the original components, U T X, are changed by the GNN. Therefore, we can compute numerically the unknown filter response of the ARMA layer as the ratio between U TX and U T X. We define the empirical filter responseh ash</p><formula xml:id="formula_21">m = F in F out Fout f =1 u T mxf Fin f =1 u T m x f ,<label>(20)</label></formula><p>wherex f is column f of the outputX k , x f is column f of the graph signal X, and u m is an eigenvector of L.</p><p>The empirical filter response allows us to analyze the type of filtering implemented by an ARMA layer. We start by comparing the recursion in Eq. (8), which converges to an ARMA 1 filter with response {h ARMA1 (µ m )} M m=1 according to Eq. (11), with the empirical response {h m,k } M m=1 of the k-th GCS stack. To facilitate the interpretation of the results, we set the number of output features of the GCS layer to F out = 1 by letting W = a and V = b1 Fin in Eq. <ref type="formula" target="#formula_0">(14)</ref>. Notice that we are keeping the notation consistent with Eq. (8), where a and b are the parameters of the ARMA 1 filter. In the following we consider the graph and   <ref type="figure">(a, b)</ref>, the empirical filter responses of two GCS stacks for T = 1, 2, 3; the black lines indicate the analytical response of an ARMA1 filter with similar parameters. In (c), the empirical response of a GCN with T = 1, 2, 3 layers. In (d, e), the original components of the input graph signal X (in black), and the components of the graph signalX processed by two GCS stacks for T = 1, 2, 3 (in color). In (f), the components ofX processed by a GCN with T = 1, 2, 3 layers.</p><p>the node features from the Cora citation network. We remark that the examples in this section are not related to the results on the semi-supervised node classification task presented in Sec. 6 and any other dataset could have been used instead of Cora. <ref type="figure" target="#fig_4">Fig. 2(a, b)</ref> show the empirical responsesh 1 andh 2 of two different GCS stacks, when varying the number of layers T . As T increases,h 1 andh 2 become more similar to the analytical responses of the ARMA 1 filters, depicted as a black line in the two figures. This supports our claim that h can estimate the unknown response of the GNN filtering operation. <ref type="figure" target="#fig_4">Fig. 2(d, e)</ref> show how the two GCS stacks modify the components of X on the Fourier basis. In particular, we depict in black the components u T m X, m = 1, . . . , M associated with each graph frequency µ m . In colors, we depict the components u T mX , which show how much the GCS stacks filter the components associated with each frequency. The responses and the signal components in <ref type="figure" target="#fig_4">Fig. 2(a)</ref> and 2(d) are obtained for a = 0.99 and b = 0.1, while in <ref type="figure" target="#fig_4">Fig. 2(b)</ref> and 2(e) for a = 0.7 and b = 0.15.</p><p>In <ref type="figure" target="#fig_4">Fig 2(c)</ref>, we show the empirical response resulting from a stack of GCNs. As also highlighted in recent work <ref type="bibr" target="#b48">(Wu et al., 2019;</ref><ref type="bibr" target="#b31">Maehara, 2019)</ref>, the filtering obtained by stacking one or more GCNs has the undesired effect of symmetrically amplifying the lowest and also the highest frequencies of the spectrum. This is due to the GCN filter response, which is (1 − λ) T in the linear case and can assume negative values when T is odd. The effect is mitigated by summing γI M to the adjacency matrix, which adds self-loops with weight γ and shrinks the spectral domain of the graph filter. For high values of γ, the GCN acts more as a low-pass filter that prevents high-frequency oscillations. This is due to the self-loops that limit the spread of information across the graph and the communication between neighbors. However, even after adding γI M , GCN cuts almost completely the medium frequencies and then amplifies again the higher ones, as shown in <ref type="figure" target="#fig_4">Fig. 2(f)</ref>.</p><p>A stack of GCNs lacks flexibility in implementing different filtering operations, as the only degree of freedom to modify a GCN's response consists of manually tuning the hyperparameter γ to shrink the spectrum. On the other hand, different GCS stacks can generate heterogeneous filter responses, depending on the value of the trainable parameters in each stack. This is what provides powerful modeling capability to the proposed ARMA layer, which can learn a large variety of filter responses that selectively shrink or  amplify the Fourier components of the graph by combining K GCS stacks.</p><p>Similarly to an ARMA 1 filter, each GCS stack behaves as a low-pass filter that gradually dampens the Fourier components as their frequency increases. However, we recall that high-pass and band-pass filters can be obtained as a linear combination of low-pass filters <ref type="bibr" target="#b34">(Oppenheim et al., 2001)</ref>.</p><p>To show this behavior in practice, in <ref type="figure" target="#fig_6">Fig. 3</ref> we report the empirical filter responses and modified Fourier components obtained with two different ARMA K filters, for K = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We consider four downstream tasks: node classification, graph signal classification, graph classification, and graph regression. Our experiments focus on comparing the proposed ARMA layer with GNNs layers based on polynomial filters, namely Chebyshev <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref> and GCN <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2016a)</ref>, and CayleyNets <ref type="bibr" target="#b26">(Levie et al., 2019b</ref>) that, like ARMA, are based on rational spectral filters. As additional baselines, we also include Graph Attention Networks (GAT) <ref type="bibr" target="#b45">(Velickovic et al., 2017)</ref>, GraphSAGE <ref type="bibr" target="#b17">(Hamilton et al., 2017)</ref>, and Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b50">(Xu et al., 2019)</ref>. The comparison with these methods helps to frame the proposed ARMA GNN within the current state of the art. We also mention that other GNNs with graph convolutional filters related to our method have appeared while our work was under review <ref type="bibr" target="#b20">(Ioannidis et al., 2020;</ref><ref type="bibr" target="#b11">Gama et al., 2019;</ref><ref type="bibr" target="#b54">Zou &amp; Lerman, 2020;</ref><ref type="bibr" target="#b12">Gao et al., 2019)</ref>.</p><p>To ensure a fair and meaningful evaluation, we compare the performance obtained with a fixed GNN architecture, where we only change only the graph convolutional layers.</p><p>In particular, we fixed the GNN capacity (number of hidden units), used the same splits in each dataset, and the same training and evaluation procedures. Finally, in all experiments we used the same polynomial order K for polynomial/rational filters, or a stack of K layers for GCN, GAT, GIN, and GraphSAGE layers. The details of every dataset considered in the experiments and the optimal hyperparameters for each model are deferred to Sec. 7.</p><p>Public implementations of the ARMA layer are available in the open-source GNN libraries Spektral <ref type="bibr" target="#b15">(Grattarola &amp; Alippi, 2020</ref>) (TensorFlow/Keras) and PyTorch Geometric <ref type="bibr" target="#b8">(Fey &amp; Lenssen, 2019</ref>) (PyTorch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Node classification</head><p>First, we consider transductive node classification on three citation networks: Cora, Citeseer, and Pubmed. The input is a single graph described by an adjacency matrix A ∈ R M ×M , the node features X ∈ R M ×Fin , and the labels y l ∈ R M l of a subset of nodes M l ⊂ M . The targets are the labels y u ∈ R Mu of the unlabelled nodes. The node features are sparse bag-of-words vectors representing text documents. The binary undirected edges in A indicate citation links between documents. The models are trained using 20 labels per document class (y l ) and the performance is evaluated as classification accuracy on y u .</p><p>Secondly, we perform inductive node classification on the protein-protein interaction (PPI) network dataset. The dataset consists of 20 graphs used for training, 2 for validation, and 2 for testing. Contrarily to the transductive setting, the testing graphs (and the associated node features) are not observed during training. Additionally, each node can belong to more than one class (multi-label classification).</p><p>We use a 2-layers GNN with 16 hidden units for the citation networks and 64 units for PPI. In the citation networks high dropout rates and L 2 -norm regularization are exploited to prevent overfitting. Tab. 1 reports the classification accuracy obtained by a GNN equipped with different graph convolutional layers.</p><p>Transductive node classification is a semi-supervised task that demands using a simple model with strong regularization to avoid overfitting on the few labels available. This is the key of GCN's success when compared to more complex filters, such as Chebyshev. Thanks to its flexible formulation, the proposed ARMA layer can implement the right degree of complexity and performs well on each task. On the other hand, since the PPI dataset is larger and more labels are available during training, less regularization is re- quired and the more complex models are advantaged. This is reflected by the better performance achieved by Chebyshev filters and CayleyNets, compared to GCN. On PPI, ARMA significantly outperforms every other model, due to its powerful modeling capability that allows learning filter responses with different shapes. Since each layer in GAT, GraphSAGE, and GIN combines the features of a node only with those from its 1 st order neighborhood, similarly to a GCN, these architectures need to stack more layers to reach higher-order neighborhoods and suffer from the same oversmoothing issue.</p><p>We notice that the optimal depth T of the ARMA layer reported in Tab. 6 is low in every dataset. We argue that a reason is the small average shortest path in the graphs (see Tab. 5). Indeed, most nodes in the graphs can be reached with only a few propagation steps, which is not surprising since many real networks are small-world <ref type="bibr" target="#b46">(Watts &amp; Strogatz, 1998)</ref>. The ARMA layer exploits sparse operations that are linear in the number of nodes in L and can be trained in a time comparable to a Chebyshev filter. On the other hand, Cay-leyNet is slower than other methods, due to the complex formulation based on the Jacobi iterations that results in a high order polynomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Graph signal classification</head><p>In this task, N different graph signals X n ∈ R M ×Fin , n = 1, . . . , N , defined on the same graph with adjacency ma- trix A ∈ R M ×M , must be mapped to labels y 1 , . . . , y N . We perform these experiments following the same setting of <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref> for the MNIST and 20news datasets.</p><p>MNIST. To emulate a classic CNNs operating on a regular 2D grid, an 8-NN graph is defined on the 784 pixels of the MNIST images. To determine if a vertex v j belongs to the neighborhood N (v i ) of a vertex v i , we compute the Euclidean distance between the 2D coordinates of pixels (vertices) i and j. The elements in A are</p><formula xml:id="formula_22">a ij = 1 if v j ∈ N (v i ); 0 otherwise.<label>(21)</label></formula><p>Each graph signal is a vectorized image x ∈ R 784 . The architecture is a GNN(32)-P(4)-GNN(64)-P(4)-FC(512)-FC Softmax <ref type="formula" target="#formula_0">(10)</ref>, where GNN(n) indicates a GNN layer with n filters, P(s) a pooling operation with stride s, and FC(u) a fully connected layer with u units (the last FC has a Softmax activation). Pooling is implemented by a hierarchical spectral clustering algorithm (GRACLUS) <ref type="bibr" target="#b6">(Dhillon et al., 2007)</ref>, which maps the graph signalx (l) at layer l into a new node feature space</p><formula xml:id="formula_23">x (l+1) ∈ R M l+1 ×F l+1 .</formula><p>Tab. 2 reports the results obtained by using GCN, Chebyshev, CayleyNet, or ARMA. The results are averaged over 10 runs and show that ARMA achieves a slightly higher, and almost perfect, accuracy compared to Chebyshev and CayleyNet, while the performance of GCN is significantly lower. Similarly to the PPI experiment, the larger amount of data allows more powerful architectures to be trained more precisely and to achieve better performance compared to the simpler GCN.</p><p>20news. The dataset consists of 18,846 documents divided into 20 classes. Each graph signal is a document represented by a bag-of-words of the 10 4 most frequent words in the corpus, embedded via Word2vec <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref>. The underlying graph of 10 4 nodes is defined by a 16-NN adjacency matrix built as in Eq. (21), with the difference that the vertex neighborhoods are computed from the Euclidean distance between the embeddings vectors rather than the pixel coordinates. We report results obtained with a single convolutional layer (GCN, Chebyshev, CayleyNet, or ARMA), followed by global average pooling and Softmax. As in <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref>, we use 32 channels for Chebyshev. Instead, for GCN, CayleyNet, and ARMA, better results are obtained with only 16 filters. The classification accuracy reported in Tab. 2 shows that ARMA significantly outperforms every other model also on this dataset.</p><p>For this experiment we used a particular configuration of the ARMA layer with K = 1 and T = 1 (see Tab. 12), which is equivalent to a GCN with a skip connection. The skip connection allows to weight differently the contribution of the original node feature, compared to the features of the neighbors. It is important to notice that, contrary to other downstream tasks, the 20news graph is generated from the similarity of word embeddings. Such an artificial graph always links an embedding vector to its first 16 neighbors. We argue that, for some words, the links might be not very relevant and using a skip connection allows weighting them less.</p><p>Similarly to the node classification datasets, the average shortest path in the 20news graph is low (see Tab. 11). On the other hand, the MNIST graph has a much higher diameter, due to its regular structure with very localized connectivity. This could explain why the optimal depth T of the ARMA layer is larger for MNIST than for any other task (see Tab. 12), as several steps are necessary to mix the node features on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Graph classification</head><p>In this task, the i-th datum is a graph represented by a pair {A i , X i }, i = 1, . . . N , where A i ∈ R Mi×Mi is an adjacency matrix with M i nodes, and X i ∈ R Mi×F are the node features. Each sample must be classified with a label y i . We test the models on five different datasets. We use node degree, clustering coefficients, and node labels as additional node features. For each dataset we adopt a fixed network architecture GNN-GNN-GNN-AvgPool-FC Softmax , where AvgPool indicates a global average pooling layer. We compute the model performance with nested 10-fold cross-validation repeated for 10 runs, using 10% of the training set in each fold for early stopping. Tab. 3 reports the average accuracy and includes the results obtained also by using GAT, GraphSAGE, and GIN as convolutional layers. The GNN equipped with the proposed ARMA layer achieves the highest mean accuracy compared the polynomial filters (Chebyshev and GCN). Compared to Cay-leyNets, which are also based on a rational filter implementation, ARMA achieves not only a higher mean accuracy but also a lower standard deviation. These empirical results indicate that our implementation is robust and confirm the transferability of the proposed ARMA layer, discussed in Sec. 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Graph regression</head><p>This task is similar to graph classification, with the difference that the target output y i is now a real value, rather than a discrete class label. We consider the QM9 chemical database <ref type="bibr" target="#b39">(Ramakrishnan et al., 2014)</ref>, which contains more than 130,000 molecular graphs. The nodes represent heavy atoms and the undirected edges the atomic bonds between them. Nodes have discrete attributes indicating one of four possible elements. The regression task consists of predicting a given chemical property of a molecule given its graph representation. As for graph classification, we evaluate the performance on the 80-10-10 train-validation-test splits of the nested 10-folds. The network architecture adopted to predict each property is GNN(64)-AvgPool-FC(128). We report in Tab. 4 the mean squared error (MSE) averaged over 10 independent runs, relative to the prediction of 9 molecular properties. It can be noticed that each model achieves a very low standard deviation. One reason is the very large amount of training data, which allows the GNN to learn a configuration that generalizes well. Contrarily to the previous tasks, here there is not a clear winner among GCN, Chebyshev, and CayleyNet, since each of them performs better than the others on some tasks. On the other hand, ARMA always achieves the lowest MSE in predicting each molecular property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental details 7.1. Node classification</head><p>Tab. 5 reports for each node classification dataset the number of nodes, number of edges, number of node attributes (size of the node feature vectors), average shortest path of the graph (Avg. SP), and the number of classes that each node can be assigned to. The three citation networks (Cora, Citeseer, and Pubmed) are taken from https://github.com/ tkipf/gcn/raw/master/gcn/data/, while the PPI dataset is taken from http://snap.stanford. edu/graphsage/.  Tab. 6 describes the optimal hyperparameters used in GCN, Chebyshev, CayleyNet, and ARMA for each node classification dataset. For all GNN, we report the L 2 regularization weight, the learning rate (lr) and dropout probability (p drop ). For GCN, we report the number of stacked graph convolutions (L). For Chebyshev, we report the polynomial order (K). For CayleyNet, we report the polynomial order (K) and the number of Jacobi iterations (T ). For ARMA, we report the number of GCS stacks (K) and the stacks' depth (T ). Additionally, we configured the MLP in GIN with 2 hidden layers and trained the parameter , while for Graph-SAGE we used the max aggregator, to differentiate more its behavior from GCN and GIN. Finally, GAT is configured with 8 attention heads and the same number of layers L as GCN.</p><p>Each model is trained for 2000 epochs with early stopping (based on the validation accuracy) at 50 epochs. We used full-batch training, i.e., in each epoch the weights are updated one time, according to a single batch that includes all the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Graph regression</head><p>The QM9 dataset used for graph regression is available at http://quantum-machine.org/datasets/, and its statistics are reported in Tab. 7.</p><p>The hyperparameters are reported in Tab. 8. Only for this task, CayleyNets use only 3 Jacobi iterations, since with more iterations we experienced numerical errors and the loss quickly diverged. All models are trained for 1000 epochs with early stopping at 50 epochs, using the Adam optimizer with learning rate 10 −3 . We used batch size 64 and no L 2 regularization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Graph classification</head><p>The datasets Enzymes, Proteins, D&amp;D, and MUTAG are taken from the Benchmark Data Sets for Graph Kernels https://ls11-www.cs.tu-dortmund. de/staff/morris/graphkerneldatasets, while the dataset Bench-hard is taken from https://github.com/FilippoMB/Benchmark_ dataset_for_graph_classification. The statistics of each graph classification dataset are summarized in Tab. 9.</p><p>For all methods, we use a fixed architecture composed of three GNN layers, each with 32 output units, ReLU activations, and L 2 regularization with a factor of 10 −4 . All models are trained to convergence with Adam, using a learning rate of 10 −3 , batch size of 32, and patience of 50 epochs. We summarize in Tab. 10 the hyperparameters used for ARMA, Chebyshev, and CayleyNets on the different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Graph signal classification</head><p>To generate the datasets we used the code available at github.com/mdeff/cnn_graph. The models are trained for 20 epochs on each dataset. We used batches of size 32 for MNIST and 128 for 20news. In the 20news dataset, the word embeddedings have size 200. MNIST 5e-4 1e-3 0.5 3 25 12 11 5 10 20news 1e-3 1e-3 0.7 1 5 5 10 1 1</p><p>Tab. 11 reports, for each graph signal classification dataset: the number of nodes and edges of the graph and the average shortest path (Avg. SP), the number of classes each graph signal can be assigned to, and the number of graph signals in the training, validation, and test set. Tab. 12 reports the optimal hyperparameters configuration for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>b</head><label></label><figDesc>be two different initial states and W 2 &lt; 1. After applying Eq. (14) for t + 1 steps, we obtain states X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The ARMA convolutional layer. Same color indicates that the weights are shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(17) and (18) slightly simplify the original formulation presented by Levie et al. (Levie et al., 2019b), but allow us to better understand what type of operation is actually performed by the CayleyNet. Specifically, Eq. (18) implements a polynomial filter of order KT , such as the one in Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Comp. of X andX in a GCN stack</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>While each GCS stack behaves as a low-pass filter, an ARMA layer with K = 3 can implement filters of different shapes. The ARMA layer in (a) implements a high-pass filtering operation that dampens low frequencies. The ARMA layer in (b) implements a band-pass filtering operation that mostly allows medium frequencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Training times on the PPI dataset, obtained with an Nvidia Titan Xp GPU. the training times of the GNN model configured with GCN, Chebyshev, CayleyNet, and ARMA layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Node classification accuracy. ±0.6 70.9 ±0.6 78.5 ±0.3 81.3 ±0.1 GraphSAGE 73.7 ±1.8 65.9 ±0.9 78.5 ±0.6 70.0 ±0.0 GIN 75.1 ±1.7 63.1 ±2.0 77.1 ±0.7 78.1 ±2.6 GCN 81.5 ±0.4 70.1 ±0.7 79.0 ±0.5 80.8 ±0.1 Chebyshev 79.5 ±1.2 70.1 ±0.8 74.4 ±1.1 86.4 ±0.1 CayleyNet 81.2 ±1.2 67.1 ±2.4 75.6 ±3.6 84.9 ±1.2 ARMA 83.4 ±0.6 72.5 ±0.4 78.9 ±0.3 90.5 ±0.3</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer Pubmed</cell><cell>PPI</cell></row><row><cell>GAT</cell><cell>83.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Graph signal classification accuracy.</figDesc><table><row><cell>GNN layer</cell><cell>MNIST</cell><cell>20news</cell></row><row><cell>GCN</cell><cell>98.48 ± 0.2</cell><cell>65.45 ± 0.2</cell></row><row><cell>Chebyshev</cell><cell>99.14 ± 0.1</cell><cell>68.24 ± 0.2</cell></row><row><cell>CayleyNet</cell><cell>99.18 ± 0.1</cell><cell>68.84 ± 0.3</cell></row><row><cell>ARMA</cell><cell>99.20 ± 0.1</cell><cell>70.02 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Graph classification accuracy. Method Enzymes Proteins D&amp;D MUTAG BHard GAT 51.7±4.3 72.3±3.1 70.9±4.0 87.3±5.3 30.1±0.7 GraphSAGE 60.3±7.1 70.2±3.9 73.6±4.1 85.7±4.7 71.8±1.0 GIN 45.7±7.7 71.4±4.5 71.2±5.4 86.3±9.1 72.1±1.1 GCN 53.0±5.3 71.0±2.7 74.7±3.8 85.7±6.6 71.9±1.2 Chebyshev 57.9±2.6 72.1±3.5 73.7±3.7 82.6±5.2 71.3±1.2 CayleyNet 43.1±10.7 65.6±5.7 70.3±11.6 87.8±10.0 70.7±2.4 ARMA 60.6±7.2 73.7±3.4 77.6±2.7 91.5±4.2 74.1±0.5 Graph regression mean squared error. 064±0.003 0.126±0.017 1.493±1.414 0.053±0.004 Cv 0.192±0.012 0.215±0.010 0.184±0.009 0.163±0.007</figDesc><table><row><cell>Property</cell><cell>GCN</cell><cell>Chebyshev CayleyNet</cell><cell>ARMA</cell></row><row><cell>mu</cell><cell cols="3">0.445±0.007 0.433±0.003 0.442±0.009 0.394±0.005</cell></row><row><cell>alpha</cell><cell cols="3">0.141±0.016 0.171±0.008 0.118±0.005 0.098±0.005</cell></row><row><cell cols="4">HOMO 0.371±0.030 0.391±0.012 0.336±0.007 0.326±0.010</cell></row><row><cell>LUMO</cell><cell cols="3">0.584±0.051 0.528±0.005 0.679±0.148 0.508±0.011</cell></row><row><cell>gap</cell><cell cols="3">0.650±0.070 0.565±0.015 0.758±0.106 0.552±0.013</cell></row><row><cell>R2</cell><cell cols="3">0.132±0.005 0.294±0.022 0.185±0.043 0.119±0.019</cell></row><row><cell>ZPVE</cell><cell cols="3">0.349±0.022 0.358±0.001 0.555±0.174 0.338±0.001</cell></row><row><cell cols="2">U0 atom 0.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Summary of the node classification datasets.</figDesc><table><row><cell cols="4">Dataset Nodes Edges Node attr. Avg. SP Node classes</cell></row><row><cell>Cora</cell><cell>2708 5429</cell><cell>1433</cell><cell>5.87±1.52 7 (single label)</cell></row><row><cell cols="2">Citeseer 3327 9228</cell><cell>3703</cell><cell>6.31±2.00 6 (single label)</cell></row><row><cell cols="2">Pubmed 19717 88651</cell><cell>500</cell><cell>6.34±1.22 3 (single label)</cell></row><row><cell>PPI</cell><cell>56944 818716</cell><cell>50</cell><cell>2.76±0.56 121 (multi-label)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Hyperparameters for node classification.</figDesc><table><row><cell cols="2">Dataset L2 reg. pdrop lr</cell><cell cols="3">GCN Cheby. Cayley ARMA L K K T K T</cell></row><row><cell>Cora</cell><cell cols="2">5e-4 0.75 0.01 1</cell><cell>2</cell><cell>1 5 2 1</cell></row><row><cell cols="3">Citeseer 5e-4 0.75 0.01 1</cell><cell>3</cell><cell>1 5 3 1</cell></row><row><cell cols="3">Pubmed 5e-4 0.25 0.01 1</cell><cell>3</cell><cell>2 5 1 1</cell></row><row><cell>PPI</cell><cell cols="2">0.0 0.25 0.01 2</cell><cell>3</cell><cell>3 5 3 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Summary of the graph regression dataset. Hyperparameters for graph classification and graph regression.</figDesc><table><row><cell>Samples</cell><cell cols="2">Avg. nodes</cell><cell cols="3">Avg. edges</cell><cell>Node attr.</cell></row><row><cell>133,885</cell><cell>8.79</cell><cell></cell><cell></cell><cell>27.61</cell><cell></cell><cell>1</cell></row><row><cell>Dataset</cell><cell cols="6">GCN Cheby. Cayley L K K T p drop K T ARMA</cell></row><row><cell>QM9</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>0.75</cell><cell>3 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .Table 10 .Table 11 .Table 12 .</head><label>9101112</label><figDesc>Summary of the graph classification datasets.DatasetSamples Classes Avg. nodes Avg. edges Node attr. Node labels Hyperparameters for graph classification and graph regression. Summary of the graph signal classification datasets.Dataset Nodes Edges Avg. SP Class Train Val Test Hyperparameters for graph signal classification.</figDesc><table><row><cell></cell><cell cols="3">Bench-hard</cell><cell>1,800</cell><cell>3</cell><cell>148.32</cell><cell>572.32</cell><cell>-</cell><cell>yes</cell></row><row><cell></cell><cell cols="2">Enzymes</cell><cell></cell><cell>600</cell><cell>6</cell><cell>32.63</cell><cell>62.14</cell><cell>18</cell><cell>no</cell></row><row><cell></cell><cell cols="2">Proteins</cell><cell></cell><cell>1,113</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell><cell>1</cell><cell>no</cell></row><row><cell></cell><cell>D&amp;D</cell><cell></cell><cell></cell><cell>1,178</cell><cell>2</cell><cell>284.32</cell><cell>715.66</cell><cell>-</cell><cell>yes</cell></row><row><cell></cell><cell cols="2">MUTAG</cell><cell></cell><cell>188</cell><cell>2</cell><cell>17.93</cell><cell>19.79</cell><cell>-</cell><cell>yes</cell></row><row><cell>Dataset</cell><cell cols="6">GCN Cheby Cayley L K K T p drop K T ARMA</cell></row><row><cell>Bench-hard</cell><cell>2</cell><cell>2</cell><cell></cell><cell>2 10</cell><cell>0.4</cell><cell>1 2</cell></row><row><cell>Enzymes</cell><cell>2</cell><cell>2</cell><cell></cell><cell>2 10</cell><cell>0.6</cell><cell>2 2</cell></row><row><cell>Proteins</cell><cell>4</cell><cell>4</cell><cell></cell><cell>4 10</cell><cell>0.6</cell><cell>4 4</cell></row><row><cell>D&amp;D</cell><cell>4</cell><cell>4</cell><cell></cell><cell>4 10</cell><cell>0.0</cell><cell>4 4</cell></row><row><cell>MUTAG</cell><cell>4</cell><cell>4</cell><cell></cell><cell>4 10</cell><cell>0.0</cell><cell>4 4</cell></row><row><cell cols="5">MNIST 784 5,928 12.36±5.45 10</cell><cell>55k</cell><cell>5k 10k</cell></row><row><cell cols="7">20news 10k 249,944 4.21±0.94 20 10,168 7,071 7,071</cell></row><row><cell cols="3">Dataset L2 reg. lr pdrop</cell><cell cols="4">GCN Cheby. Cayley ARMA L K K T K T</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduced the ARMA layer, a novel graph convolutional layer based on a rational graph filter. The ARMA layer models more expressive filter responses and can account for larger neighborhoods compared to GNN layers based on polynomial filters of the same order. Our ARMA layer consists of parallel stacks of recurrent operations, which approximate a graph filter with an arbitrary order K by means of efficient sparse tensor multiplications. We reported a spectral analysis of our neural network implementation, which provides valuable insights into the proposed method and shows that our ARMA layer can implement a large variety of filter responses. The experiments showed that the proposed ARMA layer outperforms existing GNN architectures, including those based on polynomial filters and other more complex models, on a large variety of graph machine learning tasks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual graph markov model: A deep and generative approach to graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mateusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for short-term load forecasting: an overview and comparative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maiorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonello</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Cun</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Yann. Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph echo state networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and deep graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3898" to="3905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stability of graph scattering transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8038" to="8048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric scattering for graph data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hirn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2122" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fixed point theorem for asymptotically nonexpansive mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1972" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="171" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12138</idno>
		<title level="m">Graph neural networks in tensorflow and keras with spektral</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Change detection in graph streams by learning graph embeddings on constant-curvature manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zambon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Livi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modern temporal network theory: a colloquium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petter</forename><surname>Holme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">234</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pruned graph scattering transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgios</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJeg7TEYwB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autoregressive moving average graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvin</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Leus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="288" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational graph autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the transferability of spectral graph filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isufi</forename><surname>Elvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kutyniok</forename><surname>Gitta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2018.2879624</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhizhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed autoregressive moving average graph filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Leus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1931" to="1935" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reservoir computing approaches to recurrent neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Lukoševičius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cosrev.2009.03.005</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="149" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Signal processing techniques for interpolation in graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5445" to="5449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Discrete-time signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the expressive power of deep neural networks. In international conference on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chebyshev polynomial approximation for distributed signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Computing in Sensor Systems and Workshops (DCOSS), 2011 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Accelerated filtering on graphs using lanczos method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Susnjara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nathanael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04537</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamical systems as temporal feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Tiňo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Design of graph filters and filterbanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Borgnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cooperative and Graph Signal Processing</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="299" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arantxa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collective dynamics of small-world networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strogatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souza</forename><surname>Tianyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on International Conference on Machine Learning. JMLR. org</title>
		<meeting>the 35th International Conference on International Conference on Machine Learning. JMLR. org</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souza</forename><surname>Tianyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on International Conference on Machine Learning. JMLR. org</title>
		<meeting>the 36th International Conference on International Conference on Machine Learning. JMLR. org</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolfgang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weihua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph convolutional neural networks via scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1046" to="1074" />
		</imprint>
	</monogr>
	<note>Applied and Computational Harmonic Analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
