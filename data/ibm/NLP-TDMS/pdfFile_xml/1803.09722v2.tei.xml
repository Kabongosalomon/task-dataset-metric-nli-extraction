<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DC-NNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a fundamental yet challenging problem in computer vision. The goal is to estimate 2D or 3D locations of body parts given an image or a video, which provides informative knowledge for tasks such as action recognition, robotics vision, human-computer interaction, and autonomous driving. Significant advances have  <ref type="figure">Figure 1</ref>. Given a monocular image and its predicted 3D pose, the human can easily tell whether the prediction is anthropometrically plausible or not (as shown in b) based on the perception of imagepose correspondence and the possible human poses constrained by articulation. We simulate this human perception by proposing an adversarial learning framework, where the discriminator is learned to distinguish ground-truth poses (c) from the predicted poses generated by the pose estimator (a, b), which in turn is enforced to generate plausible poses even on unannotated in-the-wild data.</p><p>been achieved in 2D human pose estimation recently because of the powerful Deep Convolutional Neural Networks (DCNNs) and the availability of large-scale in-the-wild human pose datasets with manual annotations. However, advances in 3D human pose estimation remain limited. The reason is mainly from the difficulty to obtain ground-truth 3D body joint locations in the unconstrained environment. Existing datasets such as Human3.6M <ref type="bibr" target="#b18">[19]</ref> are collected in the constrained lab environment using mocap systems, hence the variations in background, viewpoint, and lighting are very limited. Although DCNNs fit well on these datasets, when being applied on in-the-wild images, where only 2D ground-truth annotations are available (e.g., the MPII human pose dataset <ref type="bibr" target="#b0">[1]</ref>), they may have difficulty in terms of generalization ability due to the large domain shift <ref type="bibr" target="#b43">[44]</ref> between the constrained lab environment images and unconstrained in-the-wild images, as shown in <ref type="figure">Figure 1</ref>.</p><p>On the other hand, given a monocular in-the-wild image and its corresponding predicted 3D pose, it is relatively easy for the human to tell if this estimation is correct or not, as demonstrated in <ref type="figure">Figure 1(b)</ref>. Human makes such decisions mainly based on the human perception of image-pose correspondence and possible human poses constrained by articulation. This human perception can be simulated by a discriminator, which is a neural network that discriminates ground-truth poses from estimations.</p><p>Based on the above observation, we propose an adversarial learning paradigm to distill the 3D human pose structures learned from the fully annotated constrained 3D pose dataset to in-the-wild images without 3D pose annotations. Specifically, we adopt an state-of-the-art 3D pose estimator <ref type="bibr" target="#b56">[57]</ref> as a conditional generator for generating pose estimations conditioned on input images. The discriminator aims at distinguishing ground-truth 3D poses from predicted ones. Through adversarial learning, the generator learns to predict 3D poses that is difficult for the discriminator to distinguish from the ground-truth poses. Since the predicted poses can be also generated from in-the-wild data, the generator must predict indistinguishable poses on both domains to minimize the training error. It provides a way to train the generator, i.e., the 3D pose estimator, with in-thewild data in a weakly supervised manner, and leads a better generalization ability.</p><p>To facilitate the adversarial learning, a multi-source discriminator is designed to take the two key factors into consideration: 1) the description on image-pose correspondence, and 2) the human body articulation constraint. One indispensable information source is the original images. It provides rich visual information for pose-image correspondence. Another information source of the discriminator is the relative offsets and distances between pairs of body parts, which is motivated by traditional approaches based on pictorial structures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>. This information source provides the discriminator with rich domain prior knowledge, which helps the generator to generalize well.</p><p>Our approach improves the state-of-the-art both qualitatively and quantitatively. The main contributions are summarized as follows.</p><p>â€¢ We propose an adversarial learning framework to distill the 3D human pose structures from constrained images to unconstrained domains, where the ground-truth annotations are not available. Our approach allows the pose estimator to generalize well on another domain in a weakly supervised manner instead of hard-coded rules. â€¢ We design a novel multi-source discriminator, which uses visual information as well as relative offsets and distances as the domain prior knowledge, to enhance the generalization ability of the 3D pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2D Human Pose Estimation</head><p>Conventional methods usually solved 2D human poses estimation by tree-structured models, e.g., pictorial structures <ref type="bibr" target="#b31">[32]</ref> and mixtures of body parts <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b4">5]</ref>. These models consist of two terms: a unary term to detect the body joints, and a pairwise term to model the pairwise relationships between two body joints. In <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b4">5]</ref>, a pairwise term was designed as the relative locations and distances between pairs of body joints. The symmetry of appearance between limbs was modeled in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>. Ferrari et al. <ref type="bibr" target="#b12">[13]</ref> designed repulsive edges between opposite-sided arms to tackle the double counting problem. Inspired by aforementioned works, we also use the relative locations and distances between pairs of body joints. But they are used as the geometric descriptor in the adversarial learning paradigm for learning better 3D pose estimation features. The geometric descriptor greatly reduces the difficulty for the discriminator in learning domain prior knowledge such as relative limbs length and symmetry between limbs.</p><p>Recently, impressive advances have been achieved by DCNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56]</ref>. Instead of directly regressing coordinates <ref type="bibr" target="#b41">[42]</ref>, recent state-of-the-art methods used heatmaps, which are generated by a 2D Gaussian centered on the body joint locations, as the target of regression. Our approach uses the state-of-the-art stacked hourglass <ref type="bibr" target="#b28">[29]</ref> as our backbone architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Human Pose Estimation</head><p>Significant progress has been achieved for 3D human pose estimation from monocular images due to the availability of large-scale dataset <ref type="bibr" target="#b1">[2]</ref> and the powerful DCNNs. These methods can be roughly grouped into two categories.</p><p>One-stage approaches directly learn the 3D poses from monocular images. The pioneer work <ref type="bibr" target="#b21">[22]</ref> proposed a multi-task framework that jointly trains pose regression and body part detectors. To model high-dimensional joint dependencies, Tekin et al. <ref type="bibr" target="#b36">[37]</ref> further adopted an autoencoder at the end of the network. Instead of directly regressing the coordinates of the joints, Pavlakoset al. <ref type="bibr" target="#b30">[31]</ref> proposed a voxel representation for each joint as the regression target, and designed a coarse-to-fine learning strategy. These methods heavily depend on fully annotated datasets, and cannot benefit from large-scale 2D pose datasets.</p><p>Two-stage approaches first estimate 2D poses and then lift 2D poses to 3D poses <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30]</ref>. These approaches usually generalize better on images in the wild, since the first stage can benefit from the stateof-the-art 2D pose estimators, which can be trained on images in the wild. The second stage usually regresses the 3D locations from the 2D predictions. For example, Martinez et al. <ref type="bibr" target="#b24">[25]</ref> proposed a simple fully connected residual networks to directly regression 3D coordinates from 2D coordinates. Moreno-Noguer <ref type="bibr" target="#b27">[28]</ref> learned a pairwise distance matrix, which is invariant to image rotation, translation, and reflections, from 2D to 3D space.</p><p>To predict 3D poses for images in the wild, a geometric loss was proposed in <ref type="bibr" target="#b56">[57]</ref> to allow weakly supervised learning of the depth regression module. <ref type="bibr" target="#b25">[26]</ref> adopted transfer learning to generalize to in-the-wild scenes. <ref type="bibr" target="#b26">[27]</ref> built a real-time 3D pose estimation solution with kinematic skeleton fitting. Our framework can use existing 3D pose estimation approaches as the baseline and is complementary to previous works by introducing an adversarial learning framework, in which the predicted 3D poses from in-thewild images are used for learning better 3D pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adversarial Learning Methods</head><p>Adversarial learning for discriminative tasks. Adversarial learning has been proven effective not only for generative tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23]</ref>, but also for discriminative tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b35">36]</ref>. For example, Wang et al. <ref type="bibr" target="#b47">[48]</ref> proposed to learn an adversarial network that generates hard examples with occlusions and deformations for object detection. Wei et al. <ref type="bibr" target="#b49">[50]</ref> designed an adversarial erasing approach for weakly semantic segmentation. An adversarial network was proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> to distinguish the ground-truth poses from the fake ones for human pose estimation. The motivation and problems we are trying to tackle are completely different from these work. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>, the adversarial loss is used to improve pose estimation accuracy with the same domain of the data. In our case, we are trying to use adversarial learning to distill the structures learned from the constrained data (with labels) in lab environments to the unannotated data in the wild. Our approach is also very different. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> only trained the models in one single domain dataset, but ours incorporates the unannotated data into the learning process, which takes a large step in bridging the gap between the following two domains: 1) in-the-wild data without 3D ground-truth annotations and 2) constrained data with 3D ground-truth annotations.</p><p>Adversarial learning for domain adaptation. Recently, adversarial methods have become an increasingly popular incarnation for domain adaptation tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17]</ref>. These methods use adversarial learning to distinguish source domain samples from target domain samples. And adversarial learning aims at obtaining features that are domain uninformative. Different from these methods, our discriminator aims at discriminating ground-truth 3D poses from the estimated ones, which can be generated either from the same domain as the ground-truth, or an unannotated domain (e.g., images in the wild).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head><p>As illustrated in <ref type="figure">Figure 1</ref>, our proposed framework can be formulated as the Generative Adversarial Networks (GANs), which consist of two networks: a generator and a discriminator. The generator is trained to generate samples in a way that confuses the discriminator, which in turn tries to distinguish them from real samples. In our framework, the generator G is a 3D pose estimator, which tries to predict accurate 3D poses to fool the discriminator. The discriminator D distinguishes the ground-truth 3D poses from the predicted ones. Since the predicted poses can be generated from both the images captured from the lab environment (with 3D annotations) and unannotated images in the wild, the human body structures learned from 3D dataset can be adapted to in-the-wild images through adversarial learning.</p><p>During training, we first pretrain the pose estimator G on 3D human pose dataset. Then we alternately optimize the generator G and the discriminator D. For testing, we simply discard the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator: 3D Pose Estimator</head><p>The generator can be viewed as a two-stage pose estimator. We adopt the state-of-the-art architecture <ref type="bibr" target="#b56">[57]</ref> as our backbone network for 3D human pose estimation.</p><p>The first stage is the 2D pose estimation module, which is the stacked hourglass network <ref type="bibr" target="#b28">[29]</ref>. Each stack is in an encoder-decoder structure. It allows for repeated top-down, bottom-up inference across scales with intermediate supervision attached to each stack. We follow the previous practice to use 256 Ã— 256 as input resolution. The outputs are P heatmaps for the 2D body joint locations, where P denotes the number of body joints. Each heatmap has size 64 Ã— 64.</p><p>The second stage is a depth regression module, which consists of several residual modules taking the 2D body joint heatmaps and intermediate image features generated from the first stage as input. The output is a P Ã— 1 vector denoting the estimated depth for each body joint.</p><p>A geometric loss is proposed in <ref type="bibr" target="#b56">[57]</ref> to allow weakly supervised learning of the depth regression module on images in the wild. We discard the geometric loss for a more concise analysis of the proposed adversarial learning, although our method is complementary to theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminator</head><p>The predicted poses by the generator G from both the 3D pose dataset and the in-the-wild images are treated as "fake" examples for training the discriminator D.</p><p>At the adversarial learning stage, the pose estimator (generator G) is learned so that the ground-truth 3D poses and the predicted ones are indistinguishable for the discriminator D. Therefore, this adversarial learning enforces the predictions from in-the-wild images to have similar distri- butions with the ground-truth 3D poses. Although unannotated in-the-wild images are difficult to be directly used for training the pose estimator G, their corresponding 3D poses predictions can be utilized as "fake" examples for learning better discriminator, which in turn is helpful for learning a better pose estimator (generator). Discriminator decides whether the estimated 3D poses are similar to ground-truth or not. The quality of discriminator influences the pose estimator. Therefore, we design a multi-source network architecture and a geometric descriptor for the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-Source Architecture</head><p>In the discriminator, there are three information sources: 1) the original image, 2) the pairwise relative locations and distances, and 3) the heatmaps of 2D locations and the depths of body joints. The information sources take two key factors into consideration: 1) the description on image-pose correspondence; and 2) the human body articulation constraints.</p><p>To model image-pose correspondence, we treat the original image as the first information source, which provides rich visual and contextual information to reduce ambiguities, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a).</p><p>To learn the body articulation constraints, we design a geometric descriptor as the second information source <ref type="figure" target="#fig_1">(Figure 2(b)</ref>), which is motivated by traditional approaches based on pictorial structures. It explicitly encodes the pairwise relative locations and distances between body parts, and reduces the complexity to learn domain prior knowledge, e.g., relative limbs length, limits of joint angles, and symmetry of body parts. Details are given in Section 3.2.2.</p><p>Additionally, we also investigate using heatmaps as another information source, which is effective for 2D adver-sarial pose estimation <ref type="bibr" target="#b6">[7]</ref>. It can be considered as a representation of raw body joint locations, from which the network could extract rich and complex geometric relationships within the human body structure. Originally, heatmaps are generated by a 2D Gaussian centered on the body part locations. In order to incorporate the depth information into this representation, we created P depth maps, which have the same resolution as the 2D heatmaps for body joints. Each map is a matrix denoting the depth of a body joint at the corresponding location. The heatmaps and depth maps are further concatenated as the third information source, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Geometric Descriptor</head><p>Our design of the geometric descriptor is motivated by the quadratic deformation constraints widely used in pictorial structures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5]</ref> for 2D human pose estimation. It encodes the spatial relationships, limbs length and symmetry of body parts. By extending it from 2D to 3D space, we define the 3D geometric descriptor d(Â·, Â·) between pairs of body joints as a 6D vector</p><formula xml:id="formula_0">d(z i , z j ) = [âˆ†x, âˆ†y, âˆ†z, âˆ†x 2 , âˆ†y 2 , âˆ†z 2 ] T ,<label>(1)</label></formula><p>where z i = (x i , y i , z i ) and z j = (x j , y j , z j ) denote the 3D coordinates of the body joint i and j. âˆ†x = x i âˆ’ x j , âˆ†y = y i âˆ’ y j and âˆ†z = z i âˆ’ z j are the relative locations of joint i with respect to joint j. âˆ†x 2 = (x i âˆ’x j ) 2 , âˆ†y 2 = (y i âˆ’y j ) 2 and âˆ†z 2 = (z i âˆ’ z j ) 2 are distances between i and j.</p><p>We compute the 6D geometric descriptor in Eq. (1) for each pair of body joint, which results in a 6 Ã— P Ã— P matrix for P body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning</head><p>GANs are usually trained from scratch by optimizing the generator and the discriminator alternately <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>. For our task, however, we observe that the training will converge faster and get better performance with a pretrained generator (i.e., the 3D pose estimator).</p><p>We first briefly introduce the notation. Let I = {(I n , z n )} N n=1 denote the datasets, where N denote the sample indexes. Specifically, N = {N 2D , N 3D }, where N 2D and N 3D are sample indexes for the 2D and 3D pose datasets. Each sample (I, z) consists of a monocular image I and the ground-truth body joint locations z, where z = {(x j , y j )} P j=1 for 2D pose dataset, and z = {(x j , y j , z j )} P j=1 for 3D pose dataset. Here P denote the number of body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pretraining of the Generator</head><p>We first pretrain the 3D pose estimator (i.e. the generator), which consists of the 2D pose estimation module Human3.6M MPII 60k iters Iniï¿½alizaï¿½on 120k iters <ref type="figure">Figure 3</ref>. The predicted 3D poses become more accurate along with the adversarial learning process. and the depth regression module. We follow the standard pipeline <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref> and formulate the 2D pose estimation as the heatmap regression problem. The ground-truth heatmap S j for body joint j is generated from a Gaussian centered at (x j , y j ) with variance Î£, which is set as an identity matrix empirically. Denote the predicted 2D heatmaps and depth asÅœ j andáº‘ j respectively. The overall loss for training pose estimator is defined as the squared error</p><formula xml:id="formula_1">Lpose = P j=1 ï£« ï£¬ ï£­ nâˆˆN heatmap regression S j n âˆ’Åœ j n 2 2 + nâˆˆN 3D depth regression z j n âˆ’áº‘ j n 2 2 ï£¶ ï£· ï£¸ . (2)</formula><p>As in previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref>, we adopt a pretrained stacked hourglass networks <ref type="bibr" target="#b28">[29]</ref> as the 2D pose estimation module. Then the 2D pose module and the depth regression module are jointly fine-tuned with the loss in Eq.(2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adversarial Learning</head><p>After pretraining the 3D pose estimator G, we alternately optimize G and D. The loss for training discriminator D is,</p><formula xml:id="formula_2">LD = nâˆˆN 3D</formula><p>L cls (D(In, E(Sn, zn)), 1)</p><formula xml:id="formula_3">+ nâˆˆN L cls (D (In, E(G(In))), 0) ,<label>(3)</label></formula><p>where E(S n , z n ) encodes the heatmaps, depth maps and the geometric descriptor as described in Section 3. D(I n , E(S n , z n )) âˆˆ [0, 1] represents the classification score of the discriminator given input image I n and the encoded information E(S n , z n ). G(I n ) is a 3D pose estimator which predicts heatmapsÅœ j n and depth valuesáº‘ j n given an input image I n . L cls is the binary entropy loss defined as L cls (Å·, y) = âˆ’(y log(Å·) + (1 âˆ’ y) log(1 âˆ’Å·)).</p><p>Within each minibatch, half of samples are "real" from the 3D pose dataset, and the rest (I n , E(G(I n ))) are generated by G given an image I n from 3D or 2D pose dataset. Intuitively, L D is optimized to enforce the network D to classify the ground-truth poses as label 1 and the predictions as 0.</p><p>On the contrary, the generator G tries to generalize anthropometrically plausible poses conditioned on an image to fool D via minimizing the following classification loss, L G = nâˆˆN L cls (D (I n , E(G(I n ))), 1) .</p><p>We observe that directly train G and D with the loss proposed in Eq.(3) and Eq.(4) reduces the accuracies of the predicted poses. To regularize the training process, we incorporate the regression loss L pose in Eq.(2) into Eq. <ref type="formula" target="#formula_4">(4)</ref>, which results in the following loss function,</p><formula xml:id="formula_5">L G = Î» nâˆˆN L cls (D (I n , E(G(I n ))), 1) + L pose ,<label>(5)</label></formula><p>where Î» is a hyperparameter to adjust the trade-off between the classification loss and the regression loss. Î» is set as 1e âˆ’ 4 in the experiments. <ref type="figure">Figure 3</ref> demonstrates the improvements of predicted 3D poses with the adversarial learning process. The initial predictions are anthropometrically invalid, and are easily distinguishable by D from the ground-truth poses. A relatively large error L G is thus generated, and G is updated accordingly to fool D better and produce improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets. We conduct experiments on three popular human pose estimation benchmarks: Human3.6M <ref type="bibr" target="#b18">[19]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref> and MPII Human Pose <ref type="bibr" target="#b0">[1]</ref>.</p><p>Human3.6M <ref type="bibr" target="#b18">[19]</ref> dataset is one of the largest datasets for 3D human pose estimation. It consists of 3.6 million images featuring 11 actors performing 15 daily activities, such as eating, sitting, walking and taking a photo, from 4 camera views. The ground-truth 3D poses are captured by the Mocap system, while the 2D poses can be obtained by projection with the known intrinsic and extrinsic camera parameters. We use this dataset for quantitative evaluation.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref> is a recently proposed 3D dataset constructed by the Mocap system with both constrained indoor scenes and complex outdoor scenes. We only use the test split of this dataset, which contains 2929 frames from six subjects performing seven actions, to evaluate the generalization ability quantitatively.</p><p>The MPII Human Pose <ref type="bibr" target="#b0">[1]</ref> is the standard benchmark for 2D human pose estimation. It contains 25K unconstrained images collected from YouTube videos covering a wide range of activities. We adopt this dataset for the 2D pose estimation evaluation and the qualitative evaluation. Evaluation protocols. We follow the standard protocol on Human3.6M to use the subjects 1, 5, 6, 7, 8 for training and the subjects 9 and 11 for evaluation. The evaluation metric is the Mean Per Joint Position Error (MPJPE) in millimeter between the ground-truth and the prediction across all cameras and joints after aligning the depth of the root joints. We refer to this as Protocol #1. In some works, the predictions are further aligned with the ground-truth via a rigid transform <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>, which is referred as Protocol #2. Implementation details. We adopt the network architecture proposed in <ref type="bibr" target="#b56">[57]</ref> as the backbone of our pose estimator. Specifically, for 2D pose module, we adopt a shallower version of stacked hourglass <ref type="bibr" target="#b28">[29]</ref>, i.e. 2 stacks with 1 residual module at each resolution, for fast training in ablation studies ( <ref type="table">Table 2</ref>). The final results in <ref type="table" target="#tab_0">Table 1</ref> are generated with 4 stacks of hourglass with 1 residual module at each resolution (i.e. Ours <ref type="figure" target="#fig_2">(Full-4s)</ref>), which has approximately the same number of parameters but better performance compared with the structure (2 stacks with 2 residual module at each resolution) used in <ref type="bibr" target="#b56">[57]</ref>. The depth regression module consists of three sequential residual and downsampling modules, a global average pooling, and a fully connected layer for regressing the depth. The discriminator consists of three fully connected layers after concatenating the three (or two) branches of features embedded from three information sources, i.e. the image, the heatmaps and depth maps, and the pairwise geometric descriptors. Following the standard training procedure as in <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b24">25]</ref>, we first pretrain the 2D pose estimator on the MPII dataset to match the performance reported in <ref type="bibr" target="#b28">[29]</ref>. Then we train the full pose estimator with the pretrained 2D module on Human3.6M for 200K iterations. To distill the learned 3D poses to the unconstrained dataset, we then alternately train the discriminator and pose estimator for 120k iterations. The batch size is 12 for all the steps. All the experiments were conducted on a single Titan X GPU. The forward time during testing is about 1.1 second for a batch of 24 images. <ref type="table" target="#tab_0">Table 1</ref> reports the comparison with previous methods on Human3.6M. Our method (i.e. Ours (Full-4s)) achieves the state-of-the-art results. For Protocol #1, our method obtains 58.6 of mm of error, which has 9.7% improvements compared to our backbone architecture <ref type="bibr" target="#b56">[57]</ref>, although the geometric loss used in <ref type="bibr" target="#b56">[57]</ref> is not used in our model for clearer analysis. Comparing to the recent best result <ref type="bibr" target="#b11">[12]</ref>, our method still has 3.0% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Human3.6M</head><p>Under Protocol #2 (predictions are aligned with the ground-truth via a rigid transform), our method obtains 37.7mm error, which improves the previous best result <ref type="bibr" target="#b11">[12]</ref>, 45.7mm, on a large margin (17.5% improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Study</head><p>To investigate the efficacy of each component, we conduct ablation analysis on Human3.6M under Protocol #1. For fast training, we adopt a shallower version of the stacked hourglass, i.e. 2 stacks with 1 residual module at each resolution (Ours (Full-2s) in <ref type="table" target="#tab_0">Table 1</ref>), as the backbone architecture for the 2D pose module. Mean errors of all the joints and four limbs (i.e., upper/lower arms and upper/lower legs) are reported in <ref type="table">Table 2</ref>. The notations are as follows:</p><p>â€¢ Baseline refers to the pose estimator without adversarial learning. The mean error of our baseline model is 64.8 mm, which is very close to the 64.9 mm error reported on our backbone architecture in <ref type="bibr" target="#b56">[57]</ref>. â€¢ Map refers to the use of heatmaps and depth maps, as well as the original images for the adversarial training. â€¢ Geo refers to use our proposed geometric descriptors as well as the original images for the adversarial training. â€¢ Full refers to use all the information sources, i.e., original images, heatmaps and depth maps, and geometric descriptors, for adversarial learning. â€¢ Fix 2D refers to training with 2D pose module fixed.</p><p>â€¢ W/o pretrain refers to adversarial learning without pretraining the depth regressor. Geometric features: heatmaps or pairwise geometric descriptor? From <ref type="table">Table 2</ref>, we observe that all the variants with adversarial learning outperform the baseline model. If   <ref type="table">Table 2</ref>. Ablation studies on the Human3.6M dataset under Protocol #1 with 2 stacks of hourglass. The first two rows refer to the baseline pose estimator without adversarial learning. Rest of the rows refer to variants with adversarial learning. Please refer to the text for the detailed descriptor for each variant.  we use the image, the heatmaps and the depth maps as the information source (Map) for the discriminator, the prediction error is reduced by 3.  effectiveness of the proposed geometric features in learning complex constraints in the articulated human body. By combining all the three information sources together (Full), our framework achieves the lowest error.</p><p>Adversarial learning: from scratch or not? The standard practice to train GANs is to learn the generator and the discriminator alternately from scratch <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59]</ref>. The generator is usually conditioned on noise <ref type="bibr" target="#b32">[33]</ref>, text <ref type="bibr" target="#b54">[55]</ref> or images <ref type="bibr" target="#b58">[59]</ref>, and lacks of ground-truth for supervised training. This may not be necessary for our case because our generator is actually the pose estimator and can be pretrained in a supervised manner. To investigate which training strategy is better, we train our full model with or without pretraining the depth regressor. We found that it is easier to learn when the generator is pretrained: It not only obtains lower prediction error (59.7 vs. 63.4 mm), but also converges much faster, as shown by the training and validation curves of mean error vs. epoch in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>Shall we fix the pretrained 2D module? Since the 2D pose estimator is mature enough <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3]</ref>. Is it still necessary to learn our model end-to-end to the 2D pose module with more computational and memory cost? We first investigate this issue with the baseline model. For the baseline model, the top rows of <ref type="table">Table 2</ref> show that end-to-end learning (Baseline) is similar in performance compared to the learning of depth regressor with 2D module fixed (Baseline <ref type="figure">Figure 6</ref>. Qualitative comparison of images in the wild (i.e. the MPII human pose dataset <ref type="bibr" target="#b0">[1]</ref>). anatomically implausible bent of limbs is corrected by the adversarial learning. The last column shows typical failure cases caused by unseen camera views.</p><formula xml:id="formula_6">Ours Baseline Ours Baseline (a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><p>(Fix-2D)). For adversarial learning, on the other hand, the improvement from end-to-end learning is obvious, with 3.4 mm (around 5%) error reduction when compare Full (Fix 2D) with Full in the table. Therefore, end-to-end training is necessary to boost the performance in adversarial learning.</p><p>Adversarial learning for 2D pose estimation. One may wonder the performance of 2D module after the adversarial learning. Therefore, we reported the PCKh@0.5 scores for 2D pose estimation on the MPII validation set in <ref type="table" target="#tab_3">Table 3</ref>. Pretrain refers to our baseline 2D module without adversarial training. Ours refers to the the model after the adversarial learning. We observe that adversarial learning reduces the error rate of 2D pose estimation by 8.1%.</p><p>Qualitative comparison. To understand how adversarial learning works, we compare the poses estimated by the baseline model to those generated with adversarial learning. Specifically, the high-level domain knowledge over human poses, such as symmetry <ref type="figure" target="#fig_2">(Figure 4</ref> (b,c,f,g,i)) and kinematics <ref type="figure" target="#fig_2">(Figure 4</ref> (b,c,g,f,i)), are encoded by the adversarial learning. Hence the generator (i.e. the pose estimator) is able to refine the anatomically implausible poses, which might be caused by left-right switch <ref type="figure" target="#fig_2">(Figure 4 (a, e)</ref>), cluttered background <ref type="figure" target="#fig_2">(Figure 4 (b)</ref>), double counting ( <ref type="figure" target="#fig_2">Figure 4  (c,d,g)</ref>) and severe occlusion <ref type="figure" target="#fig_2">(Figure 4 (f,h,i)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cross-Domain Generalization</head><p>Quantitative results on MPI-INF-3DHP. One way to show that our algorithm is learning to transfer between domains is to test our model on another unseen 3D pose estimation dataset. Thus, we add a cross-dataset experiment on a recently proposed 3D dataset MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref>. For training, only the H36M and MPII are used, while MPI-INF-3DHP is not used. We follow <ref type="bibr" target="#b25">[26]</ref> to use PCK and AUC as the evaluation metrics. Comparisons are reported in <ref type="table" target="#tab_5">Table 4</ref>. Baseline and Adversarial denote the pose estimator without or with the adversarial learning, respectively. We observe that the adversarial learning significantly im- <ref type="bibr">[</ref>  proves the generalization ability of the pose estimator.</p><p>Qualitative results on MPII. Finally, we demonstrate the generalization ability qualitatively on the validation split of the in-the-wild MPII human pose <ref type="bibr" target="#b0">[1]</ref> dataset. Compared with the baseline method without adversarial learning, our discriminator is able to identify the unnaturally bent limbs <ref type="figure">(Figure 6</ref>(a-c,g-i)) and asymmetric limbs ( <ref type="figure">Figure 6(d)</ref>), and to refine the pose estimator through adversarial training. One common failure case is shown in <ref type="figure">Figure 6</ref>(e). The picture is a high-angle shot, which is not covered by the four cameras in the 3D pose dataset. This issue could be probably solved by involving more camera views during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has proposed an adversarial learning framework to transfer the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. A novel multi-source discriminator, as well as a geometric descriptor to encode the pairwise relative locations and distances between body joints, have been introduced to bridge the gap between the predicted pose from both domains and the ground-truth poses. Experimental results validate that the proposed framework improves the pose estimation accuracy on 3D human pose dataset. In the future work, we plan to investigate the augmentation of camera views for better generalization ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The multi-source architecture. It contains three information sources, image, geometric descriptor, as well as the heatmaps and depth maps. The three information sources are separately embedded and then concatenated for deciding if the input is the ground-truth pose or the estimated pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Predicted 3D poses on the Human3.6M validation set. Compared with the baseline pose estimator, the proposed adversarial learning framework (Ours) is able to refine the anatomically implausible poses, which is more similar to the ground-truth poses (GT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Training and validation curves of MPJPE (mm) vs. epoch on the Human3.6M validation set. Better convergence rate and performance have been achieved with the pretrained generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg. LinKDE PAMI'16 [19] 132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6 243.0 162.1 170.7 177.1 96.6 127.9 162.1 Tekin et al., ICCV'16 [38] 102.4 147.2 88.8 125.3 118.0 182.7 112.4 129.2 138.9 224.9 118.4 138.8 126.3 55.1 65.8 125.0 Du et al. ECCV'16 [11] 85.1 112.7 104.9 122.1 139.1 135.9 105.9 166.2 117.5 Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>226.9</cell><cell cols="6">120.0 117.7 137.4 99.3 106.5 126.5</cell></row><row><cell>Chen &amp; Ramanan CVPR'17 [4]</cell><cell>89.9</cell><cell>97.6</cell><cell cols="7">89.9 107.9 107.3 139.2 93.6 136.0 133.1</cell><cell>240.1</cell><cell cols="6">106.6 106.2 87.0 114.0 90.5 114.1</cell></row><row><cell>Pavlakos et al. CVPR'17 [31]</cell><cell>67.4</cell><cell>71.9</cell><cell cols="6">66.7 69.1 72.0 77.0 65.0 68.3</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>Mehta et al. 3DV'17 [26]</cell><cell>52.6</cell><cell>64.1</cell><cell cols="6">55.2 62.2 71.6 79.5 52.8 68.6</cell><cell>91.8</cell><cell>118.4</cell><cell>65.7</cell><cell>63.5</cell><cell>49.4</cell><cell>76.4</cell><cell>53.5</cell><cell>68.6</cell></row><row><cell>Zhou et al. ICCV'17 [57]</cell><cell>54.8</cell><cell>60.7</cell><cell cols="6">58.2 71.4 62.0 65.5 53.8 55.6</cell><cell>75.2</cell><cell>111.6</cell><cell>64.1</cell><cell>66.0</cell><cell>51.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Martinez et al. ICCV'17 [25]</cell><cell>51.8</cell><cell>56.2</cell><cell cols="6">58.1 59.0 69.5 78.4 55.2 58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. AAAI'18 [12]</cell><cell>50.1</cell><cell>54.3</cell><cell cols="6">57.0 57.1 66.6 73.3 53.4 55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Ours (Full-2s)</cell><cell>53.0</cell><cell>60.8</cell><cell cols="6">47.9 57.1 61.5 65.5 50.8 49.9</cell><cell>73.3</cell><cell>98.6</cell><cell>58.8</cell><cell>58.1</cell><cell>42.0</cell><cell>62.3</cell><cell>43.6</cell><cell>59.7</cell></row><row><cell>Ours (Full-4s)</cell><cell>51.5</cell><cell>58.9</cell><cell cols="6">50.4 57.0 62.1 65.4 49.8 52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell cols="10">Protocol #2 Direct. Ramakrishna et al. ECCV'12 [34] 137.4 149.3 141.6 154.3 157.7 158.9 141.8 158.1 168.6</cell><cell>175.6</cell><cell cols="6">160.4 161.7 150.0 174.8 150.2 157.3</cell></row><row><cell>Bogo et al. ECCV'16 [2]</cell><cell>62.0</cell><cell>60.2</cell><cell cols="7">67.8 76.5 92.1 77.0 73.0 75.3 100.3</cell><cell>137.3</cell><cell>83.4</cell><cell>77.3</cell><cell>86.8</cell><cell>79.7</cell><cell>87.7</cell><cell>82.3</cell></row><row><cell>Moreno-Noguer CVPR'17 [28]</cell><cell>66.1</cell><cell>61.7</cell><cell cols="7">84.5 73.7 65.2 67.2 60.9 67.3 103.5</cell><cell>74.6</cell><cell>92.6</cell><cell>69.6</cell><cell>71.5</cell><cell>78.0</cell><cell>73.2</cell><cell>74.0</cell></row><row><cell>Pavlakos et al. CVPR'17 [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.9</cell></row><row><cell>Martinez et al. ICCV'17 [25]</cell><cell>39.5</cell><cell>43.2</cell><cell cols="6">46.4 47.0 51.0 56.0 41.4 40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang et al. AAAI'18 [12]</cell><cell>38.2</cell><cell>41.7</cell><cell cols="6">43.7 44.9 48.5 55.3 40.2 38.2</cell><cell>54.5</cell><cell>64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Ours (Full-4s)</cell><cell>26.9</cell><cell>30.9</cell><cell cols="6">36.3 39.9 43.9 47.4 28.8 29.4</cell><cell>36.9</cell><cell>58.4</cell><cell>41.5</cell><cell>30.5</cell><cell>29.5</cell><cell>42.5</cell><cell>32.2</cell><cell>37.7</cell></row></table><note>Quantitative comparisons of Mean Per Joint Position Error (MPJPE) in millimetre between the estimated pose and the ground-truth on Human3.6M under Protocol #1 and Protocol #2. Some results are borrowed from [12].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>PCKh@0.5 score on the MPII validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>PCK and AUC on the MPI-INF-3DHP dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial learning of structure-aware fully convolutional networks for landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00253</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning knowledge-guided pose grammar machine for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2d human pose estimation in tv shows. Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>MarÃ­n-JimÃ©nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hu-man3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recurrent topic-transition gan for visual paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaption for face recognition in unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast globally optimal 2d human detection with loopy graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05831</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Through-wall human pose estimation using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
