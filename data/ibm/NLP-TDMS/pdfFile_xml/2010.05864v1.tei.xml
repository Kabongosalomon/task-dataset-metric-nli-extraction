<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Webly Supervised Image Classification with Meta-data: Automatic Noisy Label Correction via Visual-Semantic Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
							<email>yangjingkang@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirong</forename><surname>Chen</surname></persName>
							<email>chenweirong@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
							<email>yanxiaopeng@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
							<email>zhenghuabin@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
							<email>fenglitong@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Comp. Sci. &amp; Eng</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">SenseTime Research Qing Yuan Research Institute</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Webly Supervised Image Classification with Meta-data: Automatic Noisy Label Correction via Visual-Semantic Graph</title>
					</analytic>
					<monogr>
						<title level="m">Pro-ceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting> <address><addrLine>Seattle, WA, USA 2020; Seattle, WA, USA; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">10</biblScope>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413952</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision</term>
					<term>Supervised learning by classification</term>
					<term>Neural networks</term>
					<term>Information extraction * Equal Contribution Work done during an internship at SenseTime EIG Research</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Webly supervised learning becomes attractive recently for its efficiency in data expansion without expensive human labeling. However, adopting search queries or hashtags as web labels of images for training brings massive noise that degrades the performance of DNNs. Especially, due to the semantic confusion of query words, the images retrieved by one query may contain tremendous images belonging to other concepts. For example, searching 'tiger cat' on Flickr will return a dominating number of tiger images rather than the cat images. These realistic noisy samples usually have clear visual semantic clusters in the visual space that mislead DNNs from learning accurate semantic labels. To correct real-world noisy labels, expensive human annotations seem indispensable. Fortunately, we find that metadata can provide extra knowledge to discover clean web labels in a labor-free fashion, making it feasible to automatically provide correct semantic guidance among the massive label-noisy web data. In this paper, we propose an automatic label corrector VSGraph-LC based on the visual-semantic graph. VSGraph-LC starts from anchor selection referring to the semantic similarity between metadata and correct label concepts, and then propagates correct labels from anchors on a visual graph using graph neural network (GNN). Experiments on realistic webly supervised learning datasets Webvision-1000 and NUS-81-Web show the effectiveness and robustness of VSGraph-LC. Moreover, VSGraph-LC reveals its advantage on the open-set validation set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep convolutional neural networks (CNNs) are successful by virtue of large-scale datasets with human annotation <ref type="bibr" target="#b22">[23]</ref>. However, human annotation is extremely time-consuming and expensive, which impedes the further expansion of those big datasets <ref type="bibr" target="#b41">[42]</ref>. To overcome this limitation, researchers use web crawlers to collect billions of images and annotate them directly using text queries or hashtags <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. However, due to the ambiguity or polysemy of the query fed into the search engine, label noise is subsequently introduced. Therefore, webly supervised learning, aiming at using huge scalable web-crawled data directly for networks training by suppressing label noise, has attracted great attention recently <ref type="bibr" target="#b0">[1]</ref>.</p><p>Early exploration in this direction relies on human-verified clean subsets. Representative methods using clean subsets include Men-torNet <ref type="bibr" target="#b16">[17]</ref> and CleanNet <ref type="bibr" target="#b23">[24]</ref>. However, with the trend of rapid growth in the size of webly datasets, building clean subsets with manual verification becomes more infeasible, especially when the number of categories exceeds ten thousands <ref type="bibr" target="#b44">[45]</ref>. Therefore, recent works prefer models without clean subset dependencies, making webly supervised learning fully automatic. To this end, some works strengthen networks' endurability against label noise using moving average of model predictions <ref type="bibr" target="#b39">[40]</ref>, loss function modification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> or regularization <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b53">54]</ref> . Co-teaching uses two different networks to mutually detect label noise, doubly ensuring the model's denoising ability <ref type="bibr" target="#b12">[13]</ref>. Other works identify noisy samples based on some hypotheses including data density <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> or model confidence <ref type="bibr" target="#b49">[50]</ref>.  <ref type="figure">Figure 1</ref>: T-SNE visualization <ref type="bibr" target="#b28">[29]</ref> of WebVision-pretrained ResNet50 <ref type="bibr" target="#b14">[15]</ref> features on 10 selected categories. Three observations are highlighted: (1) CNN models that trained from WebVision can distinguish different semantics within a category, even when semantics mismatch category definition.</p><p>(2) Severe semantic label noise is a real-world problem, as majority images of class 'drumsticks' deviate the true concept of percussion mallets. (3) Co-teaching fails to correct the majority semantic label noise, but our VSGraph-LC is able to. Node brightness represents prediction confidence</p><p>Although the aforementioned methods effectively enhance the model against label noise, especially for outliers, suppressing semantic label noise is critical but untouched. To clarify, semantic label noise exists due to query's polysemy or insufficient semantic resolution. Usually, semantic label noise would be severe in some category, which is composed of a large number of samples reflecting another semantic concept. As webly datasets come from real-world, those off-target semantics are usually out-of-distribution (i.e., deviate from all semantic labels or out of interests in the test sets).</p><p>For example, <ref type="figure">Figure 1a</ref> shows that although web label 'drumsticks' has its correct semantic concept of 'percussion mallets' according to the test set, the majority of training samples actually belong to concepts of 'drumstick trees/vegetable' and 'chicken legs' due to polysemy. These out-of-distribution noisy samples clearly cluster themselves in the visual feature space. As a result, density assumption popularly adopted by previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> will regard 'chicken legs' and 'drumstick trees/vegetable' samples falsely as clean data. <ref type="figure">Figure 1b</ref> further shows the ineffectiveness of the representative self-training method Co-teaching, where most samples belong to off-target semantics are still predicted as positive.</p><p>As webly dataset is crawled from the Internet, text metadata associated with web images has great potential to provide valuable Label Descrip-on from WordNet: Drums&amp;ck: a light drums&amp;ck with a rounded head that is used to strike such percussion instruments as chimes, ke8ledrums, marimbas, glockenspiels, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>✓ ✘ ✘</head><p>Similar Dissimilar Dissimilar <ref type="figure">Figure 2</ref>: Exemplar metadata associated with web images labeled as 'Drumstick'. By comparing metadata with label description, images with semantic label noise will be detected information, which, however, has been ignored for a long time. In this paper, we aim to take advantage of extra knowledge provided by metadata to suppress label noise, especially semantic label noise. <ref type="figure">Figure 2</ref> shows that information in metadata and label description can be used to detect semantic label noise. By using off-the-shelf natural language processing (NLP) models <ref type="bibr" target="#b51">[52]</ref>, we convert the comparison between label description and metadata from human cognition level to text feature space, which ensures a fully automated process to precisely locate samples with correct semantic concepts, without the need for expensive manual annotations. Hence, we are motivated to build an automatic pipeline for webly supervised learning with metadata. Specifically, we propose a label corrector named VSGraph-LC, which first selects anchor samples for each category through matching label description from Word-Net <ref type="bibr" target="#b32">[33]</ref> and metadata of every crawled image using a powerful NLP model XLNet <ref type="bibr" target="#b51">[52]</ref>. To help those semantically correct anchors propagate their web labels towards more samples, we leverage a graph neural network (GNN) <ref type="bibr" target="#b17">[18]</ref> training on -NN visual feature graph of the entire training set. The corrected labels substitute the former noisy web labels for finetuning our final model.</p><p>In summary, our contributions are mainly three-fold:</p><p>• We explore two understudied but important factors under webly supervised learning setting: semantic label noise and text metadata. • A human-labor-free label correcting framework that fully exploits the merits of GNN and CNN is proposed as VSGraph-LC, ignited by anchors automatically selected by metadata. • The proposed framework is shown effective on NUS-81-Web and WebVision datasets, and reaches the state-of-the-art result on WebVision-1000. VSGraph-LC produces more appealing results if the test set contains out-of-distribution images 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Webly Supervised Learning</head><p>A formal definition of webly supervised learning is in Section 3.1. Based on the dependency of clean subsets, webly supervised learning methods can be divided into two categories. Methods that use clean subsets to guide the detection of label noise include Men-torNet, which learns a dynamic curriculum as a sample-weighting scheme from a human-labeled subset <ref type="bibr" target="#b16">[17]</ref>. CleanNet transfers knowledge learned from human-verified samples from a fraction of categories to the entire dataset for sample reweighting <ref type="bibr" target="#b23">[24]</ref>. In <ref type="bibr" target="#b47">[48]</ref>, a probabilistic graphical model is learned from a human-verified subset to depict relationships between images, class labels, and label noise. However, with the trend that webly datasets are exceeding billions of training data with more than ten thousand categories <ref type="bibr" target="#b44">[45]</ref>, the exponential increase of human annotations seems infeasible. Therefore, solving webly supervised image classification without any human-verified labels attracts more attention recently. Curricu-lumNet trains an image classifier with a curriculum arranged in an unsupervised manner <ref type="bibr" target="#b11">[12]</ref>. By assuming that samples from highdensity regions in visual feature space have more correct labels, a three-stage training strategy is designed to feed model from clean to noisy samples. A similar assumption is adopted in <ref type="bibr" target="#b13">[14]</ref>, which selects prototypes in high-density regions. All samples get their labels corrected based on the similarity between samples and prototypes. Co-teaching trains two networks simultaneously, letting one be trained on possible clean samples selected by the other. Such cross-update can reduce the self-accumulative error from single model and therefore enhances robustness. However, according to our inspection in Section 1 and <ref type="figure">Figure 1</ref>, previous annotation-free methods are vulnerable to massive semantic label noise. <ref type="bibr" target="#b35">[36]</ref> creatively leverages text from a strong pretrained phrase generator to suppress label confusion, but requires a complicated two-stream design with large network architecture. Thus, we are motivated to propose a fully automatic pipeline to solve the semantic label noise problem, aided by metadata crawled with web images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metadata and Concept Learning</head><p>Online platforms, including search engines and social media, can not only provide abundant web images, but also meaningful metadata. Various computer vision tasks utilize the potential value of metadata, including powerful visual-language models pretraining <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref>, image retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, and visual question answering <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>. However, for the problem of webly supervised learning, metadata is unfortunately neglected.</p><p>Recent work has also proposed several visual concept discovery approaches based on metadata. In <ref type="bibr" target="#b38">[39]</ref>, unreliable concepts extracted from metadata are filtered by cross-validation average precision from a simple classifier. The remaining concepts are then clustered as concept vocabulary for downstream tasks. <ref type="bibr" target="#b1">[2]</ref> selects visual exemplars using a clustering method with metadata, and manually assigns concepts for cluster exemplars to guide the classifier training. Furthermore, ConceptLearner <ref type="bibr" target="#b54">[55]</ref> uses an automatic threshold method for concept allocation to clusters. Still, based on clustering, NEIL <ref type="bibr" target="#b3">[4]</ref> establishes a lifelong training system that progressively identifies concepts and expands the dataset for better classifiers. Concept detectors and extra knowledge of concept relationships are also applied in NEIL. Those labor-free clustering methods focus on efficiently defining reasonable concepts from web data, rather than dealing with massive semantic label noise where web labels and target concepts are mismatched. In this paper, we collect precise concept definitions from WordNet <ref type="bibr" target="#b32">[33]</ref>. Metadata is utilized to pinpoint reliable images with correct concepts for each category. Our work is orthogonal to concept learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Embedding</head><p>Text embedding is a fundamental topic in NLP and has made considerable progress in the past decade. With the emergence of neural probability language models, word2vec becomes one of the most widely used word embedding methods. This method uses self-supervised representation learning, which assumes that words placed in a similar context have close meanings <ref type="bibr" target="#b31">[32]</ref>. Recent works uses powerful Transformer models <ref type="bibr" target="#b40">[41]</ref> with carefully-designed pretraining tasks and large-scale corpora, which can even outperform human performance on multiple NLP benchmarks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52]</ref>. In our work, we use XLNet to encode the metadata and label description into a vector format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph Neural Networks</head><p>Graph neural networks (GNNs) become popular in the last five years when combining deep learning and graph theory to learn from graph structured data <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref>. Representative models Cheb-Net <ref type="bibr" target="#b6">[7]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> use efficient filtering approaches in graph convolution operators to improve scalability and robustness. Plentiful GNN variants emerge sequentially. The simplest GNN model, simple convolutional network (SGC) <ref type="bibr" target="#b45">[46]</ref>, claims that while eliminating unnecessary complexity and redundant computations, its simplification can still maintain accuracy.</p><p>Recent works also attempt to deal with image classification tasks on the graph settings. Some works build a knowledge graph to provide possibility of label coexistence for multi-label classification <ref type="bibr" target="#b4">[5]</ref>. Some other works do not foucs on label space, but use the neighborhood on the visual feature space to enhance the classifier <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51]</ref>. In our work, based on the observation in <ref type="figure">Figure 1</ref> (even if semantic label noise is severe, features extracted from DNNs can still be clustered based on semantic information), we follow the visual feature graph path and train GNN for label correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>To correct the massive semantic label noise, our strategy is to use text metadata to provide correct semantic guidance for label correction, on the graph spanned in the training feature space. Since our label correction method is characterized by semantic guidance on the visual graph, we entitle it as a visual-semantic graph-based label corrector, referred as 'VSGraph-LC'. The flow chart of VSGraph-LC is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The procedures of VSGraph-LC are presented in an algorithm form in the appendix. Details of our approach is explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Notations</head><p>Traditional webly supervised learning problems aim to train the CNN model M ( ) for the optimal parameterˆfrom dataset D = {( 1 , * 1 ), . . . , ( , * )} <ref type="bibr" target="#b24">[25]</ref>. Consider the massive label noise, web  The aim is to provide final labels that are more reliable than web labels for later finetuning. A metadata-based anchor selector is built firstly to provide guidance for GNN training. GNN and CNN labels are the predictions of GNN and the pretrained CNN model, respectively. The final labels take advantages of both CNN and GNN labels label * might not reflect the correct category that belongs to <ref type="bibr" target="#b47">[48]</ref>. Thus, our task is to propose a label corrector VSGraph-LC which provides final labels to correct the former noisy labels * for finetuning on the pretrained CNN model M ( 0 ), with the aid of metadata¯crawled together with and * . For notation, we use non-subscript style to represent the matrix that collects all vectors of the entire dataset. For example, * represents the matrix collecting all * . We also denote the matrix of label names as¯.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Graph Construction</head><p>With a CNN model M ( 0 ) that pretrained on the entire webly training set D, for every sample , we obtain a visual feature that is extracted before fully-connected layer and a CNN label equivalent to ( | , 0 ), the prediction of M ( 0 ). With visual features , we construct a k-nearest neighbor <ref type="bibr" target="#b8">[9]</ref> undirected graph G = {V, E} where node set V contains every sample in the training set attached with corresponding visual and text features for later use. Information of edge set E is in weighted adjacency matrix A.</p><formula xml:id="formula_0">= cos( , ) , if ∈ N ( ) or ∈ N ( ) 0 , otherwise,<label>(1)</label></formula><p>where N ( ) denotes the set of neighbors of node and cos( , ) calculates the cosine similarity between two features and . Till then, the visual graph is built completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text Embedding</head><p>In this section, we prepare embeddings for unstructured text metadata and label description. After obtaining the raw metadata¯from the dataset, we remove all punctuations, digits, and stop words from the raw text, followed by the tokenization, stemming, and lemmatizing. The preprocessed metadata is encoded by an off-the-shelf document embedding model. Denote the joint process of preprocessing and document embedding as function , the metadata embedding can be expressed as</p><formula xml:id="formula_1">= (¯)<label>(2)</label></formula><p>To encode the label concept, however, we cannot apply directly on the label name¯since the label name itself only contains a few words which may be non-descriptive and semantically confusing. This problem can be solved by involving a semantic knowledge base to obtain a detailed label description for a given concept. In our work, we use WordNet <ref type="bibr" target="#b32">[33]</ref>, a lexical database that arranges distinct cognitive synonyms (called 'synsets') in a tree structure, to enhance the descriptive power of the label name. Similar to <ref type="bibr" target="#b43">[44]</ref>, our label description is obtained by extracting the definitions and lemma names of the original web label synset and its adjacent synsets, including hyponyms (subclass of) and member holonyms (part of). The purpose of collecting adjacent synsets is to include potentially related concepts. For instance, the label description of class 'drumstick' is 'drumstick: a stick used for playing a drum' for the original web synset plus 'mallet, hammer: a light drumstick with a rounded head that is used to strike such percussion instruments as chimes, kettledrums, marimbas, glockenspiels, etc. ' for its adjacent synsets.</p><p>For each label name¯∈ {¯1, . . . ,¯}, where is total number of categories, we locate the corresponding synset in WordNet and extract its label description, denoted as (¯). Using still, the label description embedding is obtained by = ( (¯)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Anchor Selection</head><p>From t-SNE visual feature visualization <ref type="bibr" target="#b28">[29]</ref> in <ref type="figure">Figure 1</ref>, we observe that even under the same web label category, the CNN model is still able to cluster features according to concepts, but the discriminative function has a large bias because of semantic label noise. Thus, it is necessary to set anchors for categories to pinpoint the correct concept. We use metadata to achieve this goal. Firstly, we enhance the metadata embeddings by applying a graph smoothing function explained in <ref type="bibr" target="#b17">[18]</ref>. The enhanced metadata embeddingsˆare calculated aŝ</p><formula xml:id="formula_3">= D − 1 2 (A + I)D − 1 2 ,<label>(4)</label></formula><p>which aggregates metadata embeddings of neighbors on the visual graph to alleviate noise from metadata. The degree matrix D is diagonal with its element = . Self-weight ratio is a scalar to decide the proportion of origin in the enhancedˆ. I is an identity matrix.</p><p>For sample , * denotes the label description embedding of its web label name. Cosine similarity betweenˆand * reflects the possibility that it belongs to its web label's correct concept. We select anchors from each class to form anchor set A as Equation 5, igniting the next GNN labeling process. * equals to the -th highest value in web category * , ensuring equal number of anchors are selected in each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A =</head><p>, * cos ˆ, * ≥ * (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Neural Networks Labeling</head><p>We conduct GNN training on the graph G. According to occam's razor <ref type="bibr" target="#b2">[3]</ref>, a basic -layer simple graph convolutional network (SGC) <ref type="bibr" target="#b45">[46]</ref> is implemented. For layer ∈ {1, . . . , }, the input ℎ ( −1) is transformed into ℎ ( ) by </p><formula xml:id="formula_4">ℎ ( ) = D − 1 2 (A + I)D − 1 2 ℎ ( −1) ( ) ,<label>(6)</label></formula><p>After the optimalˆis obtained from training, the labeling process starts by applying the GNN inference for the entire visual feature set. The prediction ( | ,ˆ) is directly utilized as GNN labels .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Correct Label Estimation</head><p>In the experiments, we observe that GNN labels with high confidence are usually reliable samples with clean background. In contrast, samples with low GNN label confidence are either hard samples or open-set noise, which are difficult or impossible to be classified. To fully exploit these samples beyond the reach of GNN, CNN label , the prediction of the pretrained CNN model, is utilized to correct low-confident GNN labels.</p><p>Formally, a method for combining GNN and CNN labels is proposed as</p><formula xml:id="formula_6">= , max( ) ≥ + (1 − ) , otherwise,<label>(8)</label></formula><p>where controls the contribution of CNN labels when GNN labels have lower confidence scores than threshold .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Summary</head><p>In summary, our VSGraph-LC firstly selects anchors based on metadata and label description. Anchors propagate their correct semantic concepts across the visual feature graph. The process of VSGraph-LC finishes when the final label is generated. With the corrected label, we finetune the pretrained CNN model with loss function</p><formula xml:id="formula_7">L = ∑︁ ( , ) ∈D − log ( ( | , )) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate our method VSGraph-LC on WebVision-1000 <ref type="bibr" target="#b24">[25]</ref> and NUS-WIDE <ref type="bibr" target="#b5">[6]</ref>. To accelerate the experiments, we generate a compact Google-500 dataset from massive WebVision-1000 for ablation study and discussion. Our method reaches the state-of-the-art result on WebVision-1000 and proves its robustness and generalization on noisy multi-label dataset NUS-WIDE. Besides, we investigate the progressive training strategy for VSGraph-LC in Section 4.5. We also find our method more powerful on open-set validation set in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets, Metadata and Configurations</head><p>WebVision-1000. WebVision-1000 [25] contains 2.4 web images crawled from Flickr and Google, with keywords from 1000 classlabels in ILSVRC-2012 <ref type="bibr" target="#b7">[8]</ref> (known as ImageNet). The estimated top-1 label accuracy is 48% <ref type="bibr" target="#b11">[12]</ref>. As WebVision shares the same 1000 classes with ImageNet, we also use ImageNet validation set along with WebVision-1000's own validation set for evaluation.</p><p>For the WebVision dataset, every training sample contains extra attributes crawled along with the image, including the rank in searching result, title, description, tags, source website, etc. Considering both cleanness and descriptiveness, we choose 'title' + 'description' attributes for Google images and 'title' + 'tags' attributes for Flickr images as the metadata.</p><p>Google-500. Google-500 only keeps images crawled from google websites in WebVision-1000 for their more completed and cleaner metadata than Flickr. Also, we randomly sample one-half categories to alleviate the large consumption of time and GPU resources without losing generalization with a total of 489755 samples. Validation sets of selected categories remain. We mainly use Google-500 for ablation studies. The metadata details refer to WebVision-1000. <ref type="bibr" target="#b5">[6]</ref> is a real-world web image dataset that contains 269,648 images with the total number of 5018 associated tags crawled from Flickr. Each image contains web tags and human-annotated ground-truth labels for the 81 concepts. Since the dataset does not contain the original web queries, we obtain web labels by extracting labels from images' associated tags among these 81 labels, i.e. we check whether each of the 81 labels appears in its web tags. It is reported in <ref type="bibr" target="#b5">[6]</ref> that on average 50% of the web labels are incorrect and 50% of the ground-truth labels are missing in web labels. All web tags for an image are used as its metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUS-WIDE. NUS-WIDE</head><p>Configuration details. ResNet50 <ref type="bibr" target="#b14">[15]</ref> is selected as our CNN model in all experiments. For all experiments, we set batch size as 256 and mini-batch size as 32 trained on 8 GPUs, except in WebVision-1000 we set batch size as 1024 on 32 GPUs. We use the standard SGD with the momentum of 0.9 and weight decay of 10 −4 . A warm-start linearly reaches the initial learning rate in the first 10 epochs. The remained epochs are ruled by a cosine learning rate scheduler. A simple class reweighting is performed to deal with class imbalance. Google-500/WebVision-1000 requires training from scratch with 120/150 epochs and an initial learning rate of 0.1/0.4. In the finetuning stage, initial learning rate is cut half from origin one without warm-start. Training on NUS-WIDE requires an ImageNet pretrained model with 100 epochs and learning rate 0.002. For the GNN model, we take the 1-layer SGC model with learning rate of 0.1, 5000 epochs and Adam optimizer with weight decay of 10 −6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Google-500</head><p>In this section, we experiment VSGraph-LC on the Google-500 dataset. We first evaluate the anchor selector and scrutinize the quality of selected anchors in Section 4.2.1. We then confirm that the final label calculation policy in Section 3.6 can achieve a better result than only using any one of the component labels. Finally, we evaluate the performance of VSGraph-LC under various hyperparameters, showing its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Anchor Selector.</head><p>With metadata embeddings and label description embeddings well prepared, anchor selector will be built according to Section 3.4. Firstly we choose = 5 for -NN graph G and self weight = 0 for text feature aggregation, which means the enhanced text feature only depends on neighbours' embeddings. The selection of hyperparameter will be explained in Section 4.2.3. <ref type="figure">Figure 4</ref> shows anchors from 3 classes with lowest classification accuracy according to pretrained model M ( 0 ). Top 10 anchors selected by different methods are shown. The visualization shows that M ( 0 ) predicts high confidence on samples with an incorrect semantic concept regarding class 'drumstick' and 'tiger cat', which reflects that M ( 0 ) is misled by massive semantic label noise from these classes. Anchors selected by metadata without graph enhancement can also make mistakes. Fortunately, when the graphenhanced text features are introduced, those mistakes made by isolated metadata are largely mitigated by insurance from samples' neighborhood. In the following experiments, we keep = 10 for the Google-500 anchor selector and expect the selected anchors to provide reliable guidance for the graph neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">GNN Training, Final Labeling and Results.</head><p>With graph G constructed, simple graph convolutional network (SGC) <ref type="bibr" target="#b45">[46]</ref> is utilized for training followed by Section 3.5. <ref type="table" target="#tab_1">Table 1</ref> shows the deficient performance if we only use the GNN label as the final label for finetune. As explained in Section 3.6, with GNN, anchors will only propagate their labels to nearby clean and easy samples on the graph, whereas for those hard samples or open-set noise, GNN prefers to give them very low prediction scores, acting like  <ref type="figure">Figure 5a</ref> shows the effectiveness of using k-NN, where = 5 ensures an optimal result. Figure 5b shows = 0.5 has constant merit comparing to = 0.</p><p>= 0.7 can assist to achieve optimal average accuracy. However, although the advantages of selected hyperparameters, the accuracy difference does not vary much for ∈ [0.5, 0.9], ∈ <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> and any , showing the robustness of proposed VSGraph-LC to hyperparameters. <ref type="table" target="#tab_2">Table 2</ref> reports experimental results on WebVision-1000. VSGraph-LC gains a large improvement on WebVision top-1 by more than 1.2%. The advantage is even larger on ImageNet, especially on Ima-geNet top-5, showing a good generalization ability. In comparison, finetuning by CNN labels only obtains weaker improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WebVision-1000</head><p>Notice that our method also outperforms the state-of-the-art methods on WebVision and ImageNet validation sets. Except Cur-riculumNet, MentorNet and CleanNet both adopt extra humanverified datasets to train a guidance network first, while MentorNet chooses a backbone of InceptionResNetV2 which is stronger than our ResNet50. With these disadvantages, however, our method can still obtain a better performance compared to the above methods. In addition, Multimodal uses ImageNet data for training visual embedding and a query-image pairs dataset for training phrased generation, with stronger InceptionV3 being the backbone. Our VSGraph-LC can still exceed Multimodel in WebVision top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">NUS-WIDE</head><p>NUS-WIDE <ref type="bibr" target="#b5">[6]</ref> is another real-world web image dataset. Different from WebVision, NUS-WIDE is designed for multi-label image classification. Previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b56">57]</ref> only use the dataset with ground-truth labels for standard multi-label learning, while we are interested in the real-world label noise setting in multi-label learning. In this case, we train our model with web labels and evaluate   it on ground-truth labels. To distinguish from previous multi-label classifier learned without noise, we denote their and our setting  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchors by Model Confidence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchors by Metadata w/o Graph Enhancement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchors by Metadata w/ Graph Enhancement (k=5)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion on Progressive Training</head><p>In this section, we discuss the performance of progressive GNN training. The procedure is as follows: After the standard GNN training is completed as Section 3.5, samples with confidence over threshold will be selected as anchors, labeled by GNN labels, for GNN training in the next round. Progressive GNN training utilizes the static graph that built by the pretrained CNN features. We suppose that with increasing iterations, the performance of VSGraph-LC will keep growing, boosted by more anchors. However, <ref type="figure" target="#fig_5">Figure 6a</ref> shows that more iterations cannot guarantee a better performance under Google-500 settings in Section 4.2, even though the number of anchors increases according to top <ref type="figure" target="#fig_5">Figure 6</ref> . We assume that the failure attributes to the quality of graph structure G built by purely Google-500 base model features. Unable to build the connection between easy and hard samples of the identical category, the existing graph G only constrains GNN labels within easy samples, thus disables progressive training. To prove this, we build the visual graph using strong features extracted by an off-theshelf ResNeXt-101 model provided by <ref type="bibr" target="#b48">[49]</ref>, which is trained on more than 940 million images in semi-weakly supervised learning fashion. We evaluate progressive training again by ONLY changing the graph structure, keeping the original 5000 anchors and all node features and scores the same. In this case, initial anchors can reach hard and more informative samples, accumulated as new anchors for progressive training. During the iteration, G remains unchanged as before. The results show a stable improvement iteration by iteration, indicating the importance of visual graph structure for progressive training of VSGraph-LC. If CNN models predict an image with confidence beyond a threshold (0.2 in our paper), we consider it is classified into the predicted category, followed by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref>. Thus, those open-set images might be falsely classified into Google-500's classes. We take per-class precision (C-R), recall (C-R), and F1-measure (C-F1) as metrics in <ref type="table" target="#tab_4">Table 4</ref>, where VSGraph-LC has tremendous improvements than the pretrained model and Co-teaching, showing our model fits the real-world open-set tasks well.</p><p>We also find that finetuning by does not work in OSR as it tends to give every sample high confidence even for open-set images. However, from GNN only gives high confidence to samples near anchors, thus can reject open-set samples and outperforms on OSR. But GNN might miss some hard positive samples, harming performance in the close-set setting. We therefore introduce to combine and , whose OSR ability, in some cases, might be weaker than only using alone because of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we focus on webly supervised learning task and highlight two understudied but critical factors: semantic label noise and text metadata. Based on our extensive exploration on them, we gain insights that CNN model that pretrained from entire webly dataset is able to provide a visual feature space where similar semantic images cluster themselves. With efficient usage of metadata, an effective and automatic label corrector VSGraph-LC is proposed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Colors reflect web labels. Web label 'Drumstick' shows representative images corresponding to 5 regions of interest. Only Region-5 corresponds to the correct concept (b) Prediction by Co-teaching (c) Prediction by VSGraph-LC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Pipeline of VSGraph-LC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where ( ) is the corresponding trainable parameter. Hyperparameter follows Equation 4. Although the GNN operation is applied to the entire visual feature set, the loss is only computed on the selected anchor set A. The first layer input ℎ (0) is assigned by visual features , and the final output ℎ ( ) = ( | , ), where collects all the training parameters ( ) across layers. The loss function is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Selected Anchors for Class 'Drumstick' (b) Selected Anchors for Class 'Spotlight' (c) Selected Anchors for Class 'Tiger Cat'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Exemplar anchors for three most noisy classes in Google-500. Different columns indicate anchors selected by different methods. Using metadata with graph enhancement has perceptible advantages compared to other methods Comparison between different hyperparameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Exploration on progressive training, which only works with graph built by high-quality visual features 4.6 Discussion on Open-Set Recognition (OSR) Due to the complexity of real-world scenarios, DNN is ideally resistant to open-set images, i.e., for test images which do not belong to any category in the validation set, CNN models should produce low confidence for them. To demonstrate our model's superiority on open-set recognition task, we use the final model trained from the Google-500 training set and evaluate it on the entire WebVision/ImageNet validation dataset. To be specific, since the WebVision/ImageNet validation sets share the same 1000 classes, we set these 1000 classes except Google-500's classes as open-set classes (i.e. 500 for classification and other 500 as open set). OSR expects unconfident predictions for open-set images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Littlest Pet Shop yellow tiger stripes Shorthair rare purebred Kitty Cat</head><label></label><figDesc>. . .</figDesc><table><row><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pictures</cell><cell>CNN</cell><cell></cell><cell>Visual Features</cell><cell></cell><cell cols="2">build</cell><cell>input</cell></row><row><cell>Tiger Cat</cell><cell cols="3">Word Embedding WordNet Label Names ̅ Metadata ̅</cell><cell>Word Embedding Metadata Embeddings t input</cell><cell cols="2">Graph</cell><cell>Embeddings l Label Description data Embeddings ! Enhanced Meta-</cell><cell>build build</cell><cell>Selector Anchor</cell><cell>Anchor Set</cell></row><row><cell></cell><cell cols="2">Label Vector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>input</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>train</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Visual Features</cell><cell>build</cell><cell>Graph</cell><cell>GNN</cell><cell>infer</cell><cell cols="2">Graph</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GNN Labels</cell><cell>Final Labels</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN Labels</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on Google-500 dataset</figDesc><table><row><cell>Method</cell><cell>WebVision Top-1 Top-5 Top-1 Top-5 ImageNet</cell></row><row><cell>Pretrained model</cell><cell>66.96 82.68 61.54 78.89</cell></row><row><cell>Co-teaching</cell><cell>67.61 84.04 62.18 80.98</cell></row><row><cell cols="2">Finetune by only 64.79 81.22 60.39 78.76</cell></row><row><cell cols="2">Finetune by only 67.83 83.93 62.68 80.68</cell></row><row><cell>Finetune by</cell><cell>68.14 84.46 63.16 81.45</cell></row><row><cell cols="2">dropping numerous data that harm the data-driven DNNs. Thus,</cell></row><row><cell cols="2">we set = 0.5 for Equation 8, therefore the model finetuned by the</cell></row><row><cell cols="2">final label will have a large improvement than the pretrained model</cell></row><row><cell cols="2">M ( 0 ) and the model finetuned by only CNN or GNN labels. The</cell></row><row><cell cols="2">results also have advantages over Co-teaching.</cell></row><row><cell cols="2">4.2.3 Hyperparameters. In this section, we try several values for</cell></row><row><cell cols="2">critical hyperparameters of , for k-NN graph, and , for final</cell></row><row><cell cols="2">label calculation in Equation 8. Mean values of WebVision/ImageNet</cell></row><row><cell cols="2">top-1/top-5 are used as average accuracy to indicate the overall per-</cell></row><row><cell>formance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The state-of-the-art results on WebVision-1000</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">WebVision ImageNet Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>MentorNet [17]</cell><cell cols="3">InceptionResNetV2 72.60 88.90 64.20 84.80</cell></row><row><cell>CleanNet [24]</cell><cell>ResNet50</cell><cell cols="2">70.31 87.77 63.42 84.59</cell></row><row><cell cols="2">CurriculumNet [12] InceptionV2</cell><cell cols="2">72.10 89.20 64.80 84.90</cell></row><row><cell>Multimodal [36]</cell><cell>InceptionV3</cell><cell>73.15 89.73 -</cell><cell>-</cell></row><row><cell cols="2">Pretrained model ResNet50</cell><cell cols="2">74.25 89.84 68.28 86.23</cell></row><row><cell cols="2">Finetune by only ResNet50</cell><cell cols="2">75.15 89.93 69.07 86.76</cell></row><row><cell>Finetune by</cell><cell>ResNet50</cell><cell cols="2">75.48 90.15 69.42 87.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Results on NUS-81-Web with noisy web labels for</cell></row><row><cell cols="2">training. = 3 is used for calculating C-F1 and O-F1</cell></row><row><cell>Method</cell><cell>C-F1 O-F1 mAP</cell></row><row><cell>Pretrained model</cell><cell>37.51 39.59 43.94</cell></row><row><cell cols="2">Finetune by only 37.62 39.15 43.99</cell></row><row><cell>Finetune by</cell><cell>38.58 40.16 44.83</cell></row><row><cell cols="2">as NUS-81 and NUS-81-Web, respectively. The only difference be-</cell></row><row><cell cols="2">tween NUS-81 and NUS-81-Web is that the NUS-81-Web training</cell></row><row><cell cols="2">set uses web labels rather than annotated ground-truth labels. The</cell></row><row><cell cols="2">train-test split policy adopts the official one. For experiments, we</cell></row><row><cell cols="2">remove samples without any label within the 81 label set. Moreover,</cell></row><row><cell cols="2">the label descriptions are obtained by identifying the most relevant</cell></row><row><cell cols="2">synset on WordNet for each label.</cell></row><row><cell cols="2">4.4.1 Evaluation Metrics. For multi-label classification, we com-</cell></row><row><cell cols="2">pare the overall F1-measure (O-F1), per-class (also known as macro-</cell></row><row><cell cols="2">averaged) F1-measure (C-F1), and mean average precision (mAP) to</cell></row><row><cell cols="2">evaluate the performance. Following [43], we use top = 3 highest</cell></row><row><cell cols="2">confidence labels for each image as the prediction and compare</cell></row><row><cell>with the ground-truth labels.</cell><cell></cell></row></table><note>4.4.2 Anchor Selector and GNN Training. With the high noisy ratio and small dataset size for NUS-81-Web, the model trained directly from scratch using web labels cannot provide strong visual features</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Google-500 open-set problem Pretrained model 40.44/67.23 50.50 37.75/60.36 46.45 Co-teaching 40.38/69.21 51.00 38.27/63.58 47.78 Finetune by 40.34/68.21 50.70 38.54/62.10 47.56 Finetune by 59.64/55.63 57.56 56.74/48.89 52.52 Finetune by 52.47/62.95 57.24 49.66/56.36 52.80 for building -NN graph. Therefore, we finetune the ImageNetpretrained model for our pretrained model M ( 0 ) instead. Regarding hyperparameters, we set = 10 for -NN graph, self-weight = 0 for text enhancement and = 50 for anchor selector. 4.4.3 Results. Experimental results on NUS-81-Web are shown inTable 3. Our proposed method outperforms baseline model and the model finetuned by only CNN labels for all three metrics. For the mAP score, we achieve 0.9% improvement using VSGraph-LC compared to the pretrained model. Our method can also increase both C-F1 and O-F1 by 1.1% and 0.6%, respectively. Unlike the performance on WebVision, finetuning with CNN labels brings no significant improvement on C-F1 and mAP, and even a 0.4% drop on O-F1. This demonstrates that under a high noisy-level setting like NUS-81-Web, labels corrected by our VSGraph-LC method are more reliable than the CNN labels.</figDesc><table><row><cell>Method</cell><cell>WebVision C-P/C-R C-F1</cell><cell>ImageNet C-P/C-R C-F1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Known as open-set recognition task, introduced in<ref type="bibr" target="#b21">[22]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work described in this paper was partially supported by Innovation and Technology Commission of the Hong Kong Special Administrative Region, China (Enterprise Support Scheme under the Innovation and Technology Fund B/E030/18).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A ALGORITHM Algorithm 1 VSGraph-LC (depicted as <ref type="figure">Figure 3</ref>) </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Görkem</forename><surname>Algan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilkay</forename><surname>Ulusoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05170</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Animals on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1463" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="377" to="380" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NUS-WIDE: a real-world web image database from National University of Singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM CIVR</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The distance-weighted k-nearest-neighbor rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sahibsingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dudani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSMC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="325" to="327" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Earth-observation image retrieval based on content, semantics, and metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Espinoza-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="5145" to="5159" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised deep metric learning for community-contributed image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1989" to="1999" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inferring Context from Pixels for Multimodal Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Fuxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic concept discovery from parallel text and visual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2596" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">News Feature: What are the limits of deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Waldrop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="1074" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A semantic approach for text clustering using WordNet and lexical chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyou</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyu</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2264" to="2275" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tencent ml-images: A large-scale multi-label image database for visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="172683" to="172693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Webly Supervised Image Classification with Self-Contained Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to cluster faces on an affinity graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2298" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Classification-reconstruction learning for open-set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4016" to="4025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">Mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conceptlearner: Discovering visual concepts from weakly labeled image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">⊲ Prepare Word Embeddings 1: Obtain word embeddings from raw metadata¯by Eq.2. 2: Obtain word embeddings from label names¯by Eq.3. ⊲ Obtain CNN Labels 3: Obtain pretrained CNN model M ( 0 ) from the entire { , * }. 4: Obtain visual features and CNN labels from M ( 0 )</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Build graph G using , with adjacency matrix A by Eq.1. 6: Obtain graph-enhanced text featuresˆby Eq.4. 7: Generate anchor selector by Eq.5 with , obtain anchor set A. 8: Train GNN on G supervised by A for optimalˆ</title>
	</analytic>
	<monogr>
		<title level="j">⊲ Obtain GNN Labels</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Serve GNN prediction ( | ,ˆ) as GNN labels</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Obtain Final Labels for Finetuning 10: Obtain final pseudo labels by Eq</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
