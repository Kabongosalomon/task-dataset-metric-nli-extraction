<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
							<email>shi.qiu@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
							<email>saeed.anwar@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
							<email>nick.barnes@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the prominence of current 3D sensors, a finegrained analysis on the basic point cloud data is worthy of further investigation. Particularly, real point cloud scenes can intuitively capture complex surroundings in the real world, but due to 3D data's raw nature, it is very challenging for machine perception. In this work, we concentrate on the essential visual task, semantic segmentation, for largescale point cloud data collected in reality. On the one hand, to reduce the ambiguity in nearby points, we augment their local context by fully utilizing both geometric and semantic features in a bilateral structure. On the other hand, we comprehensively interpret the distinctness of the points from multiple resolutions and represent the feature map following an adaptive fusion method at point-level for accurate semantic segmentation. Further, we provide specific ablation studies and intuitive visualizations to validate our key modules. By comparing with state-of-the-art networks on three different benchmarks, we demonstrate the effectiveness of our network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As 3D data acquisition techniques develop rapidly, different types of 3D scanners, e.g. LiDAR scanners <ref type="bibr" target="#b21">[22]</ref> and RGB-D cameras <ref type="bibr" target="#b9">[10]</ref> are becoming popular in our daily life. Basically, 3D scanners can capture data that enables AI-driven machines to better see and recognize the world. As a fundamental data representation, point clouds can be easily collected using 3D scanners, retaining abundant information for further investigation. Therefore, point cloud analysis is playing an essential role in 3D computer vision.</p><p>Research has shown great success in terms of basic classification of small-scale point clouds (i.e., objects containing a few thousand points): for example, face ID <ref type="bibr" target="#b15">[16]</ref> is now a widely used bio-identification for mobile devices. Researchers have recently been investigating a fine-grained analysis of large and complex point clouds <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b49">49]</ref> Figure 1: Examples of semantic segmentation for real point cloud scenes, where the main differences are highlighted and zoomedin. The upper row shows an indoor working environment with ∼0.9 million points: RandLA-Net <ref type="bibr" target="#b18">[19]</ref> falsely classifies the wall around the room corner, while our result is much closer to the ground-truth. The lower row is an outdoor traffic scene containing ∼32 thousand points, where a small bike on the right is correctly identified by our network (in blue), while RandLA-Net mislabels it as vegetation (in green).</p><p>because of the tremendous potential in applications such as autonomous driving, augmented reality, robotics, etc. This paper focuses on the semantic segmentation task to identify each point's semantic label for real point cloud scenes.</p><p>Although there are many notable works <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b56">56]</ref> addressing the semantic segmentation of 2D images which have a simpler structure, point clouds are scattered, irregular, unordered, and unevenly distributed in 3D space, making the corresponding task much more challenging, especially for large scenes made of millions or even billions of points collected from the real world. To deal with the 3D data, many papers try to build data-driven models using deep learning. Specifically, Guo et al. <ref type="bibr" target="#b12">[13]</ref> summarizes the Convolutional Neural Network (CNN) models targeting point clouds into three streams: projectionbased, discretization-based, and point-based methods. As a projection-based example, Lawin et al. <ref type="bibr" target="#b26">[27]</ref> virtually projects 3D point clouds onto images and applies a conventional FCN <ref type="bibr" target="#b35">[35]</ref> to analyze the 2D multi-view representations. Similarly, the discretization-based approaches model point clouds as voxels <ref type="bibr" target="#b19">[20]</ref> or lattices <ref type="bibr" target="#b42">[42]</ref> for CNN processing, and finally interpolate the semantic results back to the original input. However, the mentioned methods are not optimal for real applications due to some common issues: firstly, they require several time-consuming pre/postprocessing steps to make predictions; and secondly, the generated intermediate representations may partially lose the context of the surroundings.</p><p>To avoid the above issues, in this paper, we prefer pointbased networks (details in Sec. 2) that directly process the points for fine-grained analysis. Moreover, for an accurate semantic segmentation on real point cloud scenes, we endeavor to resolve the major drawbacks of existing works:</p><p>Ambiguity in close points. Most current solutions <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">40]</ref> represent a point based on its pre-defined neighbors via a fixed metric like Euclidean distance. However, outliers and overlap between neighborhoods during the neighborhood's construction are difficult to avoid, especially when the points are closely distributed near the boundaries of different semantic classes. To alleviate possible impacts, we attempt to augment the local context by involving a dense region. Moreover, we introduce a robust aggregation process to refine the augmented local context and extract useful neighboring information for the point's representation.</p><p>Redundant features. We notice an increasing number of works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b39">39]</ref> combine similar features multiple times to enhance the perception of the model. In fact, this process causes redundancy and increases the complexity for the model to process large-scale point clouds. To avoid the above problems, we propose to characterize the input information as geometric and semantic clues and then fully utilize them through a bilateral structure. More compactly, our design can explicitly represent complex point clouds.</p><p>Inadequate global representations. Although some approaches <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b29">29]</ref> apply an encoder-decoder <ref type="bibr" target="#b2">[3]</ref> structure to learn the sampled point cloud; the output feature map is inadequate for a fine-grained semantic segmentation analysis since the global perception of the original data would be damaged during the sampling process. In our method, we intend to rebuild such perception by integrating information from different resolutions. Moreover, we adaptively fuse multi-resolutional features for each point to obtain a comprehensive representation, which can be directly applied for semantic prediction.</p><p>To conclude, our contributions are in these aspects:</p><p>• We introduce a bilateral block to augment the local context of the points.</p><p>• We adaptively fuse multi-resolutional features to acquire comprehensive knowledge about point clouds.</p><p>• We present a novel semantic segmentation network using our proposed structures to deal with real point cloud scenes.</p><p>• We evaluate our network on three large-scale benchmarks of real point cloud scenes. The experimental results demonstrate that our approach achieves competitive performances against state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Point-Based Approaches: As mentioned before, pointbased approaches are designed to process unstructured 3D point cloud data directly rather than using its intermediate variants. Particularly, PointNet <ref type="bibr" target="#b37">[37]</ref> applied the multilayer-perceptron (MLP) and symmetric function (e.g., maxpooling) to learn and aggregate point cloud features, respectively. Subsequently, point-wise MLPs were used to extract local features based on neighbor searching methods: e.g., ball-query in PointNet++ <ref type="bibr" target="#b38">[38]</ref>, or k-nearest neighbors (knn) in DGCNN <ref type="bibr" target="#b46">[46]</ref>. Moreover, MLPs were extended to perform point-convolutions: for instance, KPConv <ref type="bibr" target="#b44">[44]</ref> leveraged kernel-points to convolve local point sets, while DPC <ref type="bibr" target="#b10">[11]</ref> defined dilated point groups to increase the receptive fields of the points. Recurrent Neural Network (RNN) and Graph Convolutional Network (GCN) have also been adopted to replace regular CNNs in point-based approaches: for example, Liu et al. <ref type="bibr" target="#b33">[33]</ref> transformed point clouds into sequences and processed the scaled areas using an LSTM structure, and Landrieu et al. <ref type="bibr" target="#b25">[26]</ref> exploited super-point graphs to acquire semantic knowledge.</p><p>Point Clouds Feature Representations: Different from the individual point features in PointNet <ref type="bibr" target="#b37">[37]</ref>, the following methods focus on learning feature representations from local areas. Usually, the point neighbors are defined based on spatial metrics, e.g., 3D Euclidean distances in <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b18">19]</ref> or embedding similarities in <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>. By operating CNN-based modules over the neighborhoods, the local features of point clouds can be collected. However, existing methods have limited capability to capture local details since they have not utilized the given information fully. Some works <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46]</ref> only input the embedded features for each layer and lack the geometric restrictions in deep layers. Although current methods <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b39">39]</ref> employ local descriptors to strengthen the spatial relations, however, the additional computational cost is involved. The latest approaches <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b18">19]</ref> combine the original 3D coordinates in all scales of the network, but the effect is subtle. Differently, we exploit the point features from two properties: the geometric and semantic contexts. By augmenting them in a bilateral fashion, we can synthesize an augmented local context to represent the point.</p><p>Semantic Segmentation Networks: 2D semantic segmentation has been well studied in deep learning research. The basic FCN <ref type="bibr" target="#b35">[35]</ref> applied a fully convolutional architecture to learn the features of each pixel. Further, UNet <ref type="bibr" target="#b41">[41]</ref> designed the symmetric downsampling and upsampling structure for . Firstly, the Feature Extractor (Sec. 4.1) captures the preliminary semantic context F from the input data. Then, the Bilateral Context Module (i.e., a series of the Bilateral Context Blocks) augments the local context of multiple point cloud resolutions. Generally, the Bilateral Context Block requires both semantic and geometric context as bilateral inputs. In particular, the first block inputs the preliminary F and the original 3D coordinates P; while each of the others inputs its previous one's downsampled output and coordinates P, as the semantic and geometric context respectively. Afterward, our Adaptive Fusion Module (Sec. 3.2) upsamples the Bilateral Context Blocks' outputs, then adaptively fuses them as an output feature map. Finally, we predict semantic labels for all points via fully-connected layers.</p><p>image segmentation, while SegNet <ref type="bibr" target="#b2">[3]</ref> proposed the convolutional encoder-decoder structure. More recently, Chen et al. <ref type="bibr" target="#b6">[7]</ref> used a bi-directional gate to leverage multi-modality features, i.e., color and depth, for RGB-D images.</p><p>In terms of 3D point clouds, most approaches are similar to the 2D image frameworks. For small-scale point clouds, the fully convolutional modules in <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b39">39]</ref> are able to manage the complexity of the data. In contrast, for largescale data, some networks <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">51]</ref> apply the convolutional encoder-decoders as SegNet <ref type="bibr" target="#b2">[3]</ref> does, to generate the point-wise representations. However, the performance may be less satisfactory: as lower resolutions are explored, it becomes more difficult to interpret the local context of the unstructured 3D points. Although methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">40]</ref> attempt to tackle this problem by increasing the point's receptive field for a more detailed interpretation, it is expensive to find the optimal settings. Recent RandLA-Net <ref type="bibr" target="#b18">[19]</ref> achieves high efficiency using naive random sampling, while the network's accuracy and stability are sacrificed. Unlike the existing methods, we propose a bilateral augmentation structure to effectively process multi-resolution point clouds, and utilize an adaptive fusion method to represent the comprehensive point-wise features efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>A point cloud containing N points can be described mainly from two aspects: 1) the inherent coordinates in 3D space P ∈ R N ×3 which are explicitly obtained by 3D scanners indicating the geometric context of the points; and 2) the acquired features F ∈ R N ×d in d-dimensional feature space which can be implicitly encoded by CNN-based operations implying latent clues about semantic context. From this point of view, P and F are regarded as two properties of the point cloud features.</p><p>Although P is less informative for semantic analysis, it can enrich the basic perception of geometry for the network. On this front, we aim to fully utilize P and F in a reasonable way, which can support learning a comprehensive feature map for accurate semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bilateral Context Module</head><p>The Bilateral Context Module consists of a number of Bilateral Context Blocks to investigate the point cloud at different resolutions, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. In the Bilateral Context Block, we intend to augment the local context of each point by involving the offsets that are mutually learned from the bilateral input information (i.e., p i ∈ R 3 and f i ∈ R d ), and then aggregate the augmented local context for the point feature representation. Particularly, we propose two novel units and a loss function to fulfill the intention.</p><p>Bilateral Augmentation: For a centroid p i , we find its neighbors ∀p j ∈ N i(p i ) using knn under the metric of 3D-Euclidean distance, while the corresponding neighbor features are denoted as f j . To simultaneously capture both global and local information about the neighborhood, we combine the absolute position of the centroid and the relative positions of its neighbors as the local context G ψ . Ac-</p><formula xml:id="formula_0">cordingly, G ψ (p i ) = [p i ; p j − p i ] represents local geometric context in 3D space, while G ψ (f i ) = [f i ; f j − f i ] shows local semantic context in feature space.</formula><p>However, G ψ (p i ) and G ψ (f i ) may be insufficient to represent the neighborhoods due to two reasons: 1) strict formation under a fixed constraint in 3D space could weaken the generalization capability of G ψ in high-dimensional feature space, and 2) the G ψ neighborhoods may have redundancy in the representations of close regions. To solve these issues and strengthen the generalization capability of the features, we can augment the local context by adding bilateral offsets, which shift the neighbors and densely affiliate them to the neighborhood's centroid.</p><p>To be specific, as the primary concern, we augment the local geometric context G ψ (p i ) based on the rich semantic information of G ψ (f i ). Particularly, we apply an MLP (M) on G ψ (f i ), to estimate the 3-DoF (Degrees of Freedom) bilateral offsets for the neighbors ∀p j ∈ N i(p i ). Therefore, the shifted neighbors are formulated as:</p><formula xml:id="formula_1">p j = M(G ψ (f i )) + p j ,p j ∈ R 3 .<label>(1)</label></formula><p>Afterwards, we gather the auxiliary perception of the shifted neighbors to augment the local geometric context:</p><formula xml:id="formula_2">G ψ (p i ) = [p i ; p j − p i ;p j ]; whereG ψ (p i ) ∈ R k×9</formula><p>and k is the number of neighbors. Subsequently, the d-DoF bilateral offsets for the neighbor features f j can also be collected fromG ψ (p i ) since we expect the augmented local geometric context to further enhance the local semantic context. Similarly, the neighbor features are shifted as:</p><formula xml:id="formula_3">f j = M(G ψ (p i )) + f j ,f j ∈ R d ;<label>(2)</label></formula><p>and the augmented local semantic context is formed as:</p><formula xml:id="formula_4">G ψ (f i ) = [f i ; f j − f i ;f j ], whereG ψ (f i ) ∈ R k×3d .</formula><p>After further projecting theG ψ (p i ) andG ψ (f i ) by MLPs, we concatenate them as an augmented local context G i :</p><formula xml:id="formula_5">G i = concat M G ψ (p i ) , M G ψ (f i ) ∈ R k×d . (3)</formula><p>Augmentation Loss: We also introduce some penalties to regulate the learning process of the bilateral offsets in Eq. 1. Since we should not only provide 3-DoF augmentation for the neighbors but also preserve the geometric integrity of a dense neighborhood, it is preferable to consider the neighbors as a whole instead of taking individual neighbors into account. Intuitively, we encourage the geometric center of the shifted neighbors to approach the local centroid in 3D space by minimizing the 2 distance:</p><formula xml:id="formula_6">L(p i ) = 1 k k j=1p j − p i 2 .</formula><p>(4)</p><p>Mixed Local Aggregation: Point-wise feature representation is crucial for the semantic segmentation task. Although non-parametric symmetric functions can efficiently summarize local information for the points, they cannot explicitly</p><formula xml:id="formula_7">Algorithm 1: Adaptive Fusion Module Pipeline input: M multi-resolution feature maps {S 1 , S 2 , ..., S M }. output: S out for semantic segmentation. 1 for S m ∈ {S 1 , S 2 , ..., S M } do 2 upsample:S m ← S m ; 3 summarize: φ m ←S m ; 4 end for 5 obtain: ∀S m ∈ {S 1 ,S 2 , ...,S M },S m ∈ R N ×c ; and ∀φ m ∈ {φ 1 , φ 2 , ..., φ M }, φ m ∈ R N . 6 regress: {Φ 1 , Φ 2 , ..., Φ M } ← {φ 1 , φ 2 , ..., φ M }, where Φ m ∈ R N . 7 return: S out = M m=1 Φ m ×S m .</formula><p>show the local distinctness, especially for close points sharing similar local context. To handle this problem, we propose a mixed local aggregation method to gather a precise neighborhood representation. Given the augmented local context G i , on the one hand, we directly collect the maximum (prominent) feature from the k neighbors for an overview of the neighborhood. On the other hand, we closely investigate the representations of the neighbors, refining and obtaining more details by learning the high-dimensional barycenter (i.e., weighted mean point) over the neighborhood. In the end, we combine the two types of information, the local max and mean features, to precisely represent the point as:</p><formula xml:id="formula_8">s i = concat max k (G i ), mean k,Θi (G i ) , s i ∈ R 2d ; (5)</formula><p>where Θ i is a set of learnable weights for k neighbors. The implementation details are in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Fusion Module</head><p>To efficiently analyze a real 3D scene consisting of a large number of points, we can gradually explore the point cloud in decreasing resolutions. Although it can be easily realized by applying the cascaded Bilateral Context Blocks for downsampled point cloud subsets, the corresponding output features become implicit and abstract. Therefore, it is essential to restore a feature map providing the original number of points and comprehensively interpret each point's encoded information. Specifically, we choose to fuse fine-grained representations from the multiresolution feature maps adaptively.</p><p>Assume that M lower resolutions of the point cloud are processed by the Bilateral Context Module (i.e., a cascaded set of the Bilateral Context Blocks as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>), we extract a set of multi-resolution feature maps as {S 1 , S 2 , ..., S M } including {N 1 , N 2 , ..., N M } points, respectively. <ref type="bibr" target="#b0">1</ref> As claimed in Alg. 1, for each extracted feature map ∀S m ∈ {S 1 , S 2 , ..., S M }, we conduct progressive upsampling until a full-sized representation for all N points is generated. Following a similar process, we reconstruct the full-sized feature maps {S 1 ,S 2 , ...,S M }.</p><p>Although we manage to interpret the whole point cloud, in terms of each point, the upsampled feature representations that originate from multiple resolutions may result in different scales of information. To integrate the information and refine the useful context for semantic segmentation, we fuse the full-sized feature maps adaptively at point-level.</p><p>To be concrete, we additionally summarize the pointlevel information φ m ∈ R N during the upsampling process of each full-sized feature map's generation, in order to capture basic point-level understanding from different scales. Next, by analyzing those point-level perceptions {φ 1 , φ 2 , ..., φ M } on the whole, we regress the fusion parameters {Φ 1 , Φ 2 , .., Φ M } corresponding to the full-sized feature maps {S 1 ,S 2 , ...,S M }, respectively. In the end, a comprehensive feature map S out for semantic segmentation is adaptively fused throughout multi-resolution features w.r.t. each point. Theoretically, it follows:</p><formula xml:id="formula_9">S out = M m=1 Φ m ×S m , Φ m ∈ R N .<label>(6)</label></formula><p>More details about the Adaptive Fusion Module implementation are presented in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Based on the key structures in Sec. 3, we form an effective network for the semantic segmentation of real point clouds scenes. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, our network has three modules: the Feature Extractor, the Bilateral Context Module, and the Adaptive Fusion Module. We introduce the details of each module in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Extractor</head><p>Besides spatial 3D coordinates, some datasets may include other clues, e.g., RGB colors, light intensity, etc. To create an overall impression of the whole scene, initially, we apply the Feature Extractor to acquire preliminary semantic knowledge from all of the provided information. Given the advantages of an MLP that it can represent the features flexibly in a high-dimensional embedding space, empirically, we apply a single-layer MLP (i.e., a 1-by-1 convolutional layer followed by batch normalization <ref type="bibr" target="#b20">[21]</ref> and an activation function like ReLU) to obtain high-level compact features. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the acquired features F from the Fea-ture Extractor which are forwarded to the Bilateral Context Module, along with the 3D coordinates P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bilateral Context Module Implementation</head><p>As mentioned before, the Bilateral Context Module explores the different resolutions of point cloud data. For the sake of stability, we use CUDA-based Farthest Point Sampling (FPS) to sample the data based on its 3D distribution. Particularly, the Bilateral Context Module deploys cascaded Bilateral Context Blocks to gradually process the lower resolutions of the point cloud: e.g.,</p><formula xml:id="formula_10">N → N 4 → N 16 → N 64 → N 256 .</formula><p>Meanwhile, the dimensions of the outputs are increasing as: 32→128→256→512→1024. In this regard, the behavior of the Bilateral Context Module processing the 3D point clouds is similar to the classical CNNs for 2D images, which extend the channel number while shrinking the image size for a concise description.</p><p>Inside each Bilateral Context Block, an efficient knearest neighbor using the nanoflann <ref type="bibr" target="#b4">[5]</ref> library speeds up neighbor searching in the bilateral augmentation unit. Empirically, we set k=12 for all experiments in this work. For the mixed local aggregation unit, the local max feature is collected by operating a max-pooling function along the neighbors. Following a similar operation in <ref type="bibr" target="#b18">[19]</ref>, we simultaneously refine and re-weight the neighbors through a single-layer MLP and a softmax function, then aggregate the barycenter of local embeddings as the local mean feature. Finally, the local max and mean features are concatenated as the output of the mixed local aggregation unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive Fusion Module Implementation</head><p>As explained in Sec. 3.2, our Adaptive Fusion Module aims to upsample the multi-resolution outputs of the Bilateral Context Module, and then adaptively fuse them as a comprehensive feature map for the whole point cloud scene. To be more specific with the upsampling process, at first, a single-layer MLP integrates the channel-wise information of the output feature maps. Then, we point-wisely interpolate a higher-resolution feature map using nearest neighbor interpolation <ref type="bibr" target="#b22">[23]</ref>, since it is more efficient for large-scale data than Feature Propagation <ref type="bibr" target="#b38">[38]</ref> that requires huge computational cost for neighbors and weights. Moreover, we symmetrically attach the features from the same resolution in order to increase diversity and distinctness for nearby points. Finally, a higher-resolution feature map is synthesized via another single-layer MLP.</p><p>The upsampling process is continuously performed to get full-sized feature maps {S 1 ,S 2 , ...,S M } from the multiresolution outputs of the Bilateral Context Module. During this process, we also use a fully-connected layer to summarize the point-level information φ m once a full-sized feature mapS m is reconstructed. To analyze the summarized information, we concatenate {φ 1 , φ 2 , ..., φ M }, and point-wisely normalize them using softmax. As a result, the fusion parameters {Φ 1 , Φ 2 , ..., Φ M } are adaptively regressed w.r.t. each point. After calculating a weighted sum of the upsampled feature maps (Eq. 6), we eventually combine a feature map containing all points for whole scene semantic segmentation. Besides, a structure chart of this module is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Function</head><p>Using the fused output of the Adaptive Fusion Module, the FC layers predict the confidence scores for all candidate semantic classes. Generally, cross-entropy loss L CE is computed for back-propagation. Further, we include pointlevel augmentation losses L(p i ) that are formulated following Eq. 4. In terms of a Bilateral Context Block processing N m points, the total augmentation loss regarding N m points would be L m = Nm i=1 L(p i ). Hence, for our network containing M Bilateral Context Blocks, the overall loss is:</p><formula xml:id="formula_11">L all = L CE + M m=1 ω m · L m ,<label>(7)</label></formula><p>where ω m is a hyper-parameter of weight for each Bilateral Context Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Datasets: In this work, we are targeting the semantic segmentation of real point cloud scenes. To validate our approach, we conduct experiments on three 3D benchmarks, which present different scenes in the real world.</p><p>• S3DIS: Stanford Large-Scale 3D Indoor Spaces (S3DIS) <ref type="bibr" target="#b1">[2]</ref> dataset is collected from indoor working environments. In general, there are six sub-areas in the dataset, each containing ∼50 different rooms. The number of points in most rooms varies from 0.5 million to 2.5 million, depending on the room's size. All points are provided with both 3D coordinates and color information and labeled as one of 13 semantic categories. We adopt a 6-fold strategy <ref type="bibr" target="#b37">[37]</ref> for evaluation.</p><p>• Semantic3D: The points in Semantic3D <ref type="bibr" target="#b13">[14]</ref> are scanned in natural scenes depicting various rural and urban views. Overall, this dataset contains more than four billion points manually marked in eight semantic classes. In particular, the dataset has two test sets for online evaluation: the full test set (i.e., semantic-8) has 15 scenes with over 2 billion points, while its subset (i.e., reduced-8) has four selected scenes with ∼0.1 billion sampled points. In this work, we use both 3D positions and colors of points for training and then infer the dense scenes of entire semantic-8 test set. • SemanticKITTI: SemanticKITTI <ref type="bibr" target="#b3">[4]</ref> was introduced based on the well-known KITTI Vision <ref type="bibr" target="#b11">[12]</ref> benchmark illustrating complex outdoor traffic scenarios. There are 22 stereo sequences, which are densely recorded as scans (∼0.1 million points in each scan) and precisely annotated in 19 semantic classes. Particularly, 11 of the 22 sequences are provided with labels, while the results of the other ten sequences (over 20k scans) are for online evaluation. As in <ref type="bibr" target="#b3">[4]</ref>, we take sequence 08 as the validation set, while the remaining ten labeled sequences (∼19k scans) are for training.</p><p>Training Settings: We train for 100 epochs on a single GeForce RTX 2080Ti GPU with a batch size between 4 to 6, depending on the amount of input points (about 40 × 2 10 to 64 × 2 10 ) for different datasets. In addition, the Adam <ref type="bibr" target="#b23">[24]</ref> optimizer is employed to minimize the overall loss in Eq. 7; the learning rate starts from 0.01 and decays with a rate of 0.5 after every 10 epochs. We implement the project 2 in Python and Tensorflow <ref type="bibr" target="#b0">[1]</ref> platforms using Linux. Evaluation Metrics: To evaluate our semantic segmentation performance, we largely use the mean Intersectionover-Union (mIoU), the average value of IoUs for all semantic classes upon the whole dataset. Further, we also provide the overall accuracy (OA) regarding all points and the average class accuracy (mAcc) of all semantic classes. As for S3DIS <ref type="bibr" target="#b1">[2]</ref>, we compute the mIoU based on all predicted sub-areas following the 6-fold strategy. Similarly, for both Semantic3D <ref type="bibr" target="#b13">[14]</ref> and SemanticKITTI <ref type="bibr" target="#b3">[4]</ref>, we provide the online submission testing results of general mIoU and   OA, as well as the IoU for each semantic category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Semantic Segmentation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3DIS:</head><p>Tab. 1 quantitatively presents the performance of our network on the S3DIS dataset compared with other state-of-the-art methods. Notably, although recent methods achieve good results regarding overall accuracy, this metric is unable to indicate the semantic segmentation ability due to class imbalance among different categories. In general, we significantly outperform the competitors regarding the metrics of average class accuracy (83.1%) and mIoU (72.2%). Moreover, we visualize the Adaptive Fusion Module's upsampled features maps and adaptive weights in <ref type="figure" target="#fig_1">Fig. 3</ref> (better in a zoom-in and colored view) based on S3DIS, in order to intuitively analyze the module's behavior while fusing the multi-resolution feature maps. Semantic3D: We also perform well on the natural views of the Semantic3D dataset. As Tab. 2 indicates, we surpass other methods in three out of the eight classes; and our method is accurate on three categories, i.e., humanmade and natural terrains, cars, whose IoUs are all higher than 90%. Considering the results of both overall accuracy (94.9%) and mIoU (75.4%) upon two billion testing points, our method accurately classifies the semantic labels of points in real scenes, especially for large-scale data. SemanticKITTI: Although SemanticKITTI is challenging due to the complex scenarios in traffic environments, 2) based on an office scene in S3DIS dataset. By fusing the upsampled feature maps in a simple but adaptive way, we aggregate the advantages from different scales, and generate Sout for semantic segmentation. our network can effectively identify the semantic labels of points. As shown in Tab. 3, we exceed other advanced approaches in 4 of all 19 classes. Particularly, we perform well regarding the small objects in dense scans such as car, truck, other-vehicle, motorcyclist, etc. The outstanding results can be credited to our point-level adaptive fusion method, which thoroughly integrates the different scales. Overall, our network boosts a lot (5.6% mIoU) compared to the latest point and grid-based methods <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b53">53]</ref>, while is slightly behind the state-of-the-art work <ref type="bibr" target="#b52">[52]</ref> using sparse tensor-based framework <ref type="bibr" target="#b7">[8]</ref>. As our main ideas of bilateral augmentation and adaptive fusion are fairly adaptable, more experiments with different frameworks will be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>Bilateral Context Block: In Tab. 4, we study the Bilateral Context Block's structure by investigating the components individually. B 0 is the baseline model which only max-pools the concatenation of the basic local geometric G ψ (p i ) and semantic context G ψ (f i ); while rest models use different components based on the same structure of bilateral augmentation. From model B 1 &amp;B 2 , we observe that the semantic augmentation loss L(f i ) has no effect since augmenting the semantic features in embedding space is implicit. In contrast, the bilateral offsetsp i with the geometric augmentation loss L(p i ) improves a bit (model B 4 &amp;B 5 ). Taking the advantages from both local max and mean features, we conclude that the best form of the Bilateral Context Block is using mixed local aggregation (B 6 ). Adaptive Fusion Module: In Tab. 5, by comparing models A 1 , A 2 &amp;A 3 with the baseline A 0 that only upsamples the final output of the Bilateral Context Module, we notice that utilizing the upsampled features maps that originate from multiple resolutions can benefit the performance. However, the fusion method decides whether the effects are significant or not: regular summation (A 1 ) or multiplication (A 2 ) is not desirable, while concatenation (A 3 ) contributes more to the final prediction. For a general fusion (A 4 ) w.r.t. each feature map, we regress a set of scalars {Ψ m } based on the squeezed information <ref type="bibr" target="#b17">[18]</ref> of the feature maps. Instead, a more flexible fusion operating adaptively at point-level (A 5 ) achieves better results since semantic segmentation relies more on point-wise feature representations. Network Complexity: Network complexity is essential to the practical application of point clouds. In Tab. 6, we use similar metrics as <ref type="bibr" target="#b18">[19]</ref> to study the inference using the trained models. The complexity and capacity (i.e., the number of parameters, and the maximum number of points for prediction) of our model are comparable to <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b18">19]</ref>. Although <ref type="bibr" target="#b18">[19]</ref> is efficient for one-time inference, they require multiple evaluations to minimize the impact of random sampling, while we obtain more effective and stable semantic segmentation results in different real scenes such as the examples shown in <ref type="figure">Fig. 1</ref>. More visualizations and experimental results are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper focuses on fundamental analysis and semantic segmentation for real point clouds scenes. Specifically, we propose a network leveraging the ideas of augmenting the local context bilaterally and fusing multi-resolution features for each point adaptively. Particularly, we achieve outstanding performance on three benchmarks, including S3DIS, Semantic3D, and SemanticKITTI. Further, we analyze the modules' properties by conducting related ablation studies, and intuitively visualize the network's effects. In the future, we expect to optimize the efficiency for real-time applications, exploit the key ideas in different frameworks, and promote the primary structures for more 3D tasks such as object detection, instance segmentation, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This supplementary material provides more network details, experimental results, and visualizations of our semantic segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Details</head><p>In <ref type="figure" target="#fig_0">Fig. 2</ref> of the main paper, we present the general architecture of our semantic segmentation network as well as the structure of the Bilateral Context Block. In this section, we provide more details about the different components of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Key Modules</head><p>Feature Extractor: As stated, we apply a single-layer MLP containing eight 1×1 kernels to extract the semantic context F from the input information I ∈ R N ×Cin , where N is the number of input points. Hence, F is acquired as:</p><formula xml:id="formula_12">F = ReLU BN Conv 8 1×1 (I) , F ∈ R N ×8 ;</formula><p>where Conv denotes a convolution layer whose subscript is the kernel size, and the superscript is the number of kernels. where:</p><formula xml:id="formula_13">S 1 ∈ R N 4 ×32 , S 2 ∈ R N 16 ×128 , S 3 ∈ R N 64 ×256 , S 4 ∈ R N 256 ×512 , S 5 ∈ R N 512 ×1024 .</formula><p>Particularly, the downsampling ratios and feature dimensions are simply adopted from <ref type="bibr" target="#b18">[19]</ref>, since we mainly focus on the structure design rather than fine-tuning the hyperparameters in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Fusion Module:</head><p>In addition to Alg. 1 and Sec. 3.2 in the main paper, we also illustrate the architecture of the Adaptive Fusion Module in <ref type="figure" target="#fig_3">Fig. 4</ref> as a complement. As described in Sec. 4.3 of the main paper, we gradually upsample the extracted feature maps {S 1 , S 2 , S 3 , S 4 , S 5 }, Then, for each upsampled full-sized feature map, we use a fully-connected layer (FC, and its superscript indicates the number of kernels) to summarize the point-level information:</p><formula xml:id="formula_14">φ m = FC 1 (S m ), φ m ∈ R N ;</formula><p>where ∀S m ∈ {S 1 ,S 2 ,S 3 ,S 4 ,S 5 }. Subsequently, we concatenate the {φ 1 , φ 2 , φ 3 , φ 4 , φ 5 }, and point-wisely normalize them using softmax function:</p><formula xml:id="formula_15">Φ = softmax concat(φ 1 , φ 2 , φ 3 , φ 4 , φ 5 ) , Φ ∈ R N ×5 .</formula><p>Next, we separate Φ channel-by-channel, and obtain the fusion parameters:</p><formula xml:id="formula_16">{Φ 1 , Φ 2 , Φ 3 , Φ 4 , Φ 5 },</formula><p>all of which are in R N . Hence, the point-level adaptively fused feature map is calculated as:</p><formula xml:id="formula_17">S out = Φ 1 ×S 1 + Φ 2 ×S 2 + Φ 3 ×S 3 + Φ 4 ×S 4 + Φ 5 ×S 5 ,</formula><p>where S out ∈ R N ×32 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Predictions</head><p>Based on S out , we utilize three fully-connected layers and a drop-out layer (DP, and the drop-out ratio shows at the superscript) to predict the confidence scores for all Q candidate semantic classes:</p><formula xml:id="formula_18">V pred = FC Q DP 0.5 FC 32 FC 64 (S out ) ,</formula><p>where V pred ∈ R N ×Q .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Loss Function</head><p>Eq. 7 of the main paper formulates the overall loss L all of our network based on the cross-entropy loss L CE and the augmentation loss L m for each Bilateral Context Block.</p><p>In practice, our Bilateral Context Module gradually processes a decreasing number of points (N → N 4 → N 16 → N 64 → N 256 ) through five blocks. Empirically, we set the weights {0.1, 0.1, 0.3, 0.5, 0.5} for the corresponding five augmentation losses, since we aim to provide more penalties for lower-resolution processing. Therefore, the overall loss for our network is:</p><formula xml:id="formula_19">L all =L CE + 0.1 · L 1 + 0.1 · L 2 + 0.3 · L 3 + 0.5 · L 4 + 0.5 · L 5 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Areas of S3DIS</head><p>We include more experimental data about our network's semantic segmentation performance. To be specific, Tab. 7 shows our results for each area in the S3DIS dataset, including overall accuracy, average class accuracy, and concrete IoUs for 13 semantic classes. To evaluate each area, we apply the rest five areas as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Reduced-8 Semantic3D</head><p>Further, Tab. 8 presents our online evaluation results on the smaller test set (i.e., reduced-8, which has four scenes including about 0.1 billion points) of the Seman-tic3D dataset. Comparing with Tab. 2 in the main paper (i.e., results of semantic-8, which contains 15 scenes with 2 billion points), we conclude that our semantic segmentation performance regarding large-scale data is relatively better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Ablation Study</head><p>In addition to the specific ablation studies (Sec. 5.3 in the main paper) about our Bilateral Context Block and Adaptive Fusion Module respectively, we also conduct an ablation study to investigate some variants of our network:</p><p>• Baseline model: We replace both our Bilateral Context Block and Adaptive Fusion Module with their baseline forms, which are explained in the ablation studies of the main paper.</p><p>• Efficient model: We apply the random sampling instead of the Farthest Point Sampling (FPS).</p><p>• Dilated model: We use dilated-knn <ref type="bibr" target="#b10">[11]</ref> to search the neighbors of each point, in order to increase the size of point's receptive field. The dilated factor d = 2.  Tab. 9 indicates that such an efficient random sampling (N 1 ) cannot perform as effectively as FPS does since the randomly sampled subsets can hardly retain the integrity of inherent geometry. As there is always a trade-off between the network's efficiency and effectiveness, we look forward to better balancing them in future work. Besides, increasing the size of the point's receptive field (N 2 ) as <ref type="bibr" target="#b10">[11]</ref> may not help in our case. Further, we observe that it is not optimal  to use the equal-weighted Bilateral Context Blocks (N 3 ) for multi-resolution point clouds. Moreover, our network can be flexibly assembled: for an instance of model N 4 that consists of fewer blocks, even though the performance is reduced, it consumes less GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Bilateral Context Block</head><p>In <ref type="figure">Fig. 5</ref>, we present the Bilateral Context Block's output features in a heat-map view. Particularly, we observe that the Bilateral Context Block can clearly raise different responses for close points (in red frames) that are in different semantic classes.</p><p>Besides, we calculate the average neighbor-to-centroid Euclidean-distances and average neighborhood variances in 3D space (Eq. 1 in the main paper) and feature space (Eq. 2), using the S3DIS samples. Tab. 10 shows that shifted neighbors get closer to centroids as expected, in both 3D and feature spaces. Further, the variances inside the neighborhoods also drop. In general, the shifted neighbors tend to form compact neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Visualizations and Failure Cases</head><p>We provide more visualizations of our semantic segmentation network's outputs and some failure cases. Specifically, <ref type="figure" target="#fig_4">Fig. 6</ref> presents our results on six different types of rooms, which are conference, WC, storage, hallway, lobby, office rooms, respectively. Unfortunately, we find that the proposed method is not competent enough for distinguishing the objects that are in similar shapes. The main reason is that the network relies on the local neighborhood of each point, while lacks the geometric information about the specific object that each point belongs to. In the 3rd row of <ref type="figure" target="#fig_4">Fig. 6</ref>, beam is incorrectly classified as door since it looks like the doorframes; while wall is wrongly predicted as board or clutter in the rest of rows.</p><p>In <ref type="figure" target="#fig_5">Fig. 7</ref>, we show the general semantic segmentation performances on some large-scale point clouds of typical urban and rural scenes. Although the ground-truths of Se-mantic3D's test set are unavailable, our semantic predictions of these scenes are visually plausible.</p><p>In addition, we compare our results against the groundtruths on the validation set (i.e., Sequence 08) of Se-manticKITTI dataset in <ref type="figure">Fig. 8</ref>. Particularly, we illustrate some 3D point cloud scenes in the views of 2D panorama, in order to clearly show the failure cases (highlighted in red color). In fact, the proposed network is able to find some small objects that are semantically different from the background, however, the predictions are not accurate enough since we only use the 3D coordinates as input. As Se-manticKITTI is made up of the sequences of scans, in the future, we will take the temporal information into account.   <ref type="bibr" target="#b13">[14]</ref> dataset. The first row is about an urban square, the second one shows a rural farm, the third one illustrates a cathedral scene, and the last is scanned from a street view. <ref type="figure">Figure 8</ref>: Examples of our semantic segmentation predictions of SemanticKITTI <ref type="bibr" target="#b3">[4]</ref> dataset. The first two rows show the general 3D views of the input traffic scenarios ("Input") and our semantic segmentation outputs ("Output"), respectively. The remaining rows compare our predictions ("Ours") and the ground-truths ("GT") in 2D panorama views, where the failure cases are highlighted in red frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The details of our semantic segmentation network and the Bilateral Context Block (the annotations are consistent with the items in Sec. 3.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Behavior analysis of the Adaptive Fusion Module (Sec. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>BN represents a batch normalization layer, while ReLU is a ReLU activation layer. Later on, F is forwarded to the Bilateral Context Module, together with the 3D coordinates P ∈ R N ×3 . Bilateral Context Module: In practice, we apply five Bilateral Context Blocks with Farthest Point Sampling (FPS) to realize the Bilateral Context Module (B). Using the same annotations of the main paper's Sec. 4.2, the extracted multi-resolution feature maps are: {S 1 , S 2 , S 3 , S 4 , S 5 } = B(P, F);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The architecture of the Adaptive Fusion Module. All the annotations are consistent with the items in Sec. 3 of the main paper. respectively. In this case, the upsampled full-sized feature maps are {S 1 ,S 2 ,S 3 ,S 4 ,S 5 }, all of which are in R N ×32 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Examples of our semantic segmentation results of S3DIS<ref type="bibr" target="#b1">[2]</ref> dataset. The first column presents the input point cloud scenes ("Input") of some indoor rooms. The second column shows the semantic segmentation predictions of our network ("Prediction"), while the last column indicates the ground-truths ("Ground-Truth"). The main differences are highlighted in red frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Examples of our semantic segmentation predictions of Semantic3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semantic segmentation (6-fold cross-validation) results (%) on the S3DIS dataset<ref type="bibr" target="#b1">[2]</ref>. (mAcc: average class accuracy, OA: overall accuracy, mIoU: mean Intersection-over-Union. "-" indicates unknown result.)</figDesc><table><row><cell>year</cell><cell>Method</cell><cell>mAcc</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>2017</cell><cell>PointNet [37] PointNet++ [38]</cell><cell>66.2 67.1</cell><cell>78.6 81.0</cell><cell>47.6 54.5</cell></row><row><cell></cell><cell>A-SCN [50]</cell><cell>-</cell><cell>81.6</cell><cell>52.7</cell></row><row><cell>2018</cell><cell>PointCNN [30] SPG [26]</cell><cell>75.6 73.0</cell><cell>88.1 85.5</cell><cell>65.4 62.1</cell></row><row><cell></cell><cell>DGCNN [46]</cell><cell>-</cell><cell>84.1</cell><cell>56.1</cell></row><row><cell></cell><cell>KP-Conv [44]</cell><cell>79.1</cell><cell>-</cell><cell>70.6</cell></row><row><cell>2019</cell><cell>ShellNet [54]</cell><cell>-</cell><cell>87.1</cell><cell>66.8</cell></row><row><cell></cell><cell>PointWeb [55]</cell><cell>76.2</cell><cell>87.3</cell><cell>66.7</cell></row><row><cell></cell><cell>SSP+SPG [25]</cell><cell>78.3</cell><cell>87.9</cell><cell>68.4</cell></row><row><cell></cell><cell>Seg-GCN [28]</cell><cell>77.1</cell><cell>87.8</cell><cell>68.5</cell></row><row><cell></cell><cell>PointASNL [51]</cell><cell>79.0</cell><cell>88.8</cell><cell>68.7</cell></row><row><cell>2020</cell><cell>RandLA-Net [19] MPNet [17]</cell><cell>82.0 -</cell><cell>88.0 86.8</cell><cell>70.0 61.3</cell></row><row><cell></cell><cell>InsSem-SP [32]</cell><cell>74.3</cell><cell>88.5</cell><cell>64.1</cell></row><row><cell></cell><cell>Ours</cell><cell>83.1</cell><cell>88.9</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation (semantic-8) results (%) on the Semantic3D dataset<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">OA mIoU</cell><cell cols="4">man-made natural terrain terrain vegetation vegetation high low</cell><cell>buildings</cell><cell cols="3">hard scanning cars scape artefacts</cell></row><row><cell>TMLC-MS [15]</cell><cell>85.0</cell><cell>49.4</cell><cell>91.1</cell><cell>69.5</cell><cell>32.8</cell><cell>21.6</cell><cell>87.6</cell><cell>25.9</cell><cell>11.3</cell><cell>55.3</cell></row><row><cell cols="2">EdgeConv-PN [9] 89.4</cell><cell>61.0</cell><cell>91.2</cell><cell>69.8</cell><cell>51.4</cell><cell>58.5</cell><cell>90.6</cell><cell>33.0</cell><cell>24.9</cell><cell>68.6</cell></row><row><cell>PointNet++ [38]</cell><cell>85.7</cell><cell>63.1</cell><cell>81.9</cell><cell>78.1</cell><cell>64.3</cell><cell>51.7</cell><cell>75.9</cell><cell>36.4</cell><cell>43.7</cell><cell>72.6</cell></row><row><cell>SnapNet [6]</cell><cell>91.0</cell><cell>67.4</cell><cell>89.6</cell><cell>79.5</cell><cell>74.8</cell><cell>56.1</cell><cell>90.9</cell><cell>36.5</cell><cell>34.3</cell><cell>77.2</cell></row><row><cell>PointConv [48]</cell><cell>91.8</cell><cell>69.2</cell><cell>92.2</cell><cell>79.2</cell><cell>73.1</cell><cell>62.7</cell><cell>92.0</cell><cell>28.7</cell><cell>43.1</cell><cell>82.3</cell></row><row><cell>PointGCR [36]</cell><cell>92.1</cell><cell>69.5</cell><cell>93.8</cell><cell>80.0</cell><cell>64.4</cell><cell>66.4</cell><cell>93.2</cell><cell>39.2</cell><cell>34.3</cell><cell>85.3</cell></row><row><cell cols="2">PointConv-CE [31] 92.3</cell><cell>71.0</cell><cell>92.4</cell><cell>79.6</cell><cell>72.7</cell><cell>62.0</cell><cell>93.7</cell><cell>40.6</cell><cell>44.6</cell><cell>82.5</cell></row><row><cell>RandLA-Net [19]</cell><cell>94.2</cell><cell>71.8</cell><cell>96.0</cell><cell>88.6</cell><cell>65.3</cell><cell>62.0</cell><cell>95.9</cell><cell>49.8</cell><cell>27.8</cell><cell>89.3</cell></row><row><cell>SPG [26]</cell><cell>92.9</cell><cell>76.2</cell><cell>91.5</cell><cell>75.6</cell><cell>78.3</cell><cell>71.7</cell><cell>94.4</cell><cell>56.8</cell><cell>52.9</cell><cell>88.4</cell></row><row><cell>Ours</cell><cell>94.9</cell><cell>75.4</cell><cell>97.9</cell><cell>95.0</cell><cell>70.6</cell><cell>63.1</cell><cell>94.2</cell><cell>41.6</cell><cell>50.2</cell><cell>90.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation (single-scan) results (%) on the SemanticKITTI dataset [4]. 92.5 95.3 41.8 47.5 37.7 34.5 84.5 69.8 68.5 59.5 56.8 11.9 69.4 60.4 66.5 Ours 59.9 90.9 74.4 62.2 23.6 89.8 95.4 48.7 31.8 35.5 46.7 82.7 63.4 67.9 49.5 55.7 53.0 60.8 53.7 52.0</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>PointNet [37]</cell><cell>14.6</cell><cell cols="7">61.6 35.7 15.8 1.4 41.4 46.3 0.1</cell><cell>1.3</cell><cell>0.3</cell><cell cols="5">0.8 31.0 4.6 17.6 0.2</cell><cell>0.2</cell><cell cols="3">0.0 12.9 2.4</cell><cell>3.7</cell></row><row><cell>PointNet++ [38]</cell><cell>20.1</cell><cell cols="7">72.0 41.8 18.7 5.6 62.3 53.7 0.9</cell><cell>1.9</cell><cell>0.2</cell><cell cols="5">0.2 46.5 13.8 30.0 0.9</cell><cell>1.0</cell><cell cols="3">0.0 16.9 6.0</cell><cell>8.9</cell></row><row><cell>SquSegV2 [47]</cell><cell>39.7</cell><cell cols="19">88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 36.3</cell></row><row><cell>TangentConv [43]</cell><cell>40.9</cell><cell cols="19">83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5</cell></row><row><cell>PointASNL [51]</cell><cell>46.8</cell><cell cols="19">87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9</cell></row><row><cell>RandLA-Net [19]</cell><cell>53.9</cell><cell cols="19">90.7 73.7 60.3 20.4 86.9 94.2 40.1 26.0 25.8 38.9 81.4 61.3 66.8 49.2 48.2 7.2 56.3 49.2 47.7</cell></row><row><cell>PolarNet [53]</cell><cell>54.3</cell><cell cols="19">90.8 74.4 61.7 21.7 90.0 93.8 22.9 40.3 30.1 28.5 84.0 65.5 67.8 43.2 40.2 5.6 67.8 51.8 57.5</cell></row><row><cell>MinkNet42 [8]</cell><cell>54.3</cell><cell cols="19">91.1 69.7 63.8 29.3 92.7 94.3 26.1 23.1 26.2 36.7 83.7 68.4 64.7 43.1 36.4 7.9 57.1 57.3 60.1</cell></row><row><cell>FusionNet [52]</cell><cell>61.3</cell><cell cols="4">91.8 77.1 68.8 30.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>k,Θ i</cell><cell cols="2">(Gi) only.)</cell></row><row><cell>Model</cell><cell cols="2">bilateral augmentation offsets loss</cell><cell cols="2">local aggregation</cell><cell>mIoU</cell></row><row><cell>B 0</cell><cell>none</cell><cell>none</cell><cell>max</cell><cell></cell><cell>61.8</cell></row><row><cell cols="2">B 1fi →p i</cell><cell>L(f i )</cell><cell cols="2">mixed</cell><cell>64.2</cell></row><row><cell cols="3">B 2pi →f i L(p i ) + L(f i )</cell><cell cols="2">mixed</cell><cell>64.3</cell></row><row><cell cols="2">B 3pi →f i</cell><cell>none</cell><cell cols="2">mixed</cell><cell>64.2</cell></row><row><cell cols="2">B 4pi →f i</cell><cell>L(p i )</cell><cell>max</cell><cell></cell><cell>64.6</cell></row><row><cell cols="2">B 5pi →f i</cell><cell>L(p i )</cell><cell cols="2">mean</cell><cell>64.8</cell></row><row><cell cols="2">B 6pi →f i</cell><cell>L(p i )</cell><cell cols="2">mixed</cell><cell>65.4</cell></row></table><note>Ablation studies about the Bilateral Context Block testing on Area 5, S3DIS dataset. (pi →fi: learn 3-DoF offsetspi first and d-DoF offsetsfi afterwards as per Sec. 3.1;fi →pi: learnfi, and thenpi; L(·): calculate augmentation loss as per Eq. 4; mixed: mixed local aggregation following Eq. 5; max: local max feature maxk (Gi) only; mean: local mean feature mean</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies about the Adaptive Fusion Module testing on Area 5, S3DIS dataset. ({Sm}: a set of upsampled feature maps,S1,..,SM , as mentioned in Alg. 1; concat, and : the concatenation, element-wise sum and element-wise multiplication for the set {Sm}; {Ψm}: scalars for the set {Sm}; {Φm}: pointlevel fusion parameters as explained in Sec. 3.2 and 4.3.)</figDesc><table><row><cell>Model</cell><cell cols="2">upsampled feature map parameters fusion</cell><cell>S out</cell><cell>mIoU</cell></row><row><cell cols="2">A 0SM</cell><cell cols="2">noneS M</cell><cell>64.1</cell></row><row><cell>A 1</cell><cell>{S m }</cell><cell>none</cell><cell>S m</cell><cell>64.7</cell></row><row><cell>A 2</cell><cell>{S m }</cell><cell>none</cell><cell>S m</cell><cell>64.2</cell></row><row><cell>A 3</cell><cell>{S m }</cell><cell>none</cell><cell>concat({S m })</cell><cell>65.1</cell></row><row><cell>A 4</cell><cell>{S m }</cell><cell>{Ψ m }</cell><cell>Ψ m ×S m</cell><cell>65.1</cell></row><row><cell>A 5</cell><cell>{S m }</cell><cell>{Φ m }</cell><cell>Φ m ×S m</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Complexity analysis of different semantic segmentation networks on SemanticKITTI. ("-" indicates the unknown result.)</figDesc><table><row><cell>Method</cell><cell cols="4">Parameters Max Capacity Inference Speed mIoU (millions) (million points) (scans/second)</cell></row><row><cell>PointNet [37]</cell><cell>0.8</cell><cell>0.49</cell><cell>21.2</cell><cell>14.6</cell></row><row><cell>PointNet++ [38]</cell><cell>0.97</cell><cell>0.98</cell><cell>0.4</cell><cell>20.1</cell></row><row><cell>SPG [26]</cell><cell>0.25</cell><cell>-</cell><cell>0.1</cell><cell>17.4</cell></row><row><cell>RandLA-Net [19]</cell><cell>1.24</cell><cell>1.03</cell><cell>22</cell><cell>53.9</cell></row><row><cell>Ours</cell><cell>1.23</cell><cell>0.9</cell><cell>4.8</cell><cell>59.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Testing Area mAcc OA mIoU ceiling floor wall beam column windowdoor table chair sofa bookcase board clutter</figDesc><table><row><cell>Area 1</cell><cell>87.7</cell><cell>89.5</cell><cell>76.3</cell><cell>96.5</cell><cell cols="2">95.4 80.3 65.4</cell><cell>58.8</cell><cell>78.0</cell><cell>84.3 70.7 82.9 78.0</cell><cell>60.9</cell><cell>73.2</cell><cell>67.9</cell></row><row><cell>Area 2</cell><cell>71.1</cell><cell>86.6</cell><cell>57.8</cell><cell>87.1</cell><cell cols="2">95.1 80.0 19.8</cell><cell>33.3</cell><cell>47.5</cell><cell>69.3 45.6 83.1 52.8</cell><cell>50.7</cell><cell>33.1</cell><cell>54.4</cell></row><row><cell>Area 3</cell><cell>89.7</cell><cell>91.7</cell><cell>80.0</cell><cell>95.8</cell><cell cols="2">98.2 83.3 74.4</cell><cell>40.5</cell><cell>86.0</cell><cell>88.5 74.4 83.7 79.0</cell><cell>73.6</cell><cell>88.9</cell><cell>73.9</cell></row><row><cell>Area 4</cell><cell>77.9</cell><cell>86.1</cell><cell>64.3</cell><cell>94.8</cell><cell cols="2">97.1 78.6 53.0</cell><cell>48.6</cell><cell>30.8</cell><cell>61.0 67.4 77.0 70.1</cell><cell>51.3</cell><cell>44.8</cell><cell>61.6</cell></row><row><cell>Area 5</cell><cell>73.1</cell><cell>88.9</cell><cell>65.4</cell><cell>92.9</cell><cell>97.9 82.3</cell><cell>0.0</cell><cell>23.1</cell><cell>65.5</cell><cell>64.9 78.5 87.5 61.4</cell><cell>70.7</cell><cell>68.7</cell><cell>57.2</cell></row><row><cell>Area 6</cell><cell>92.0</cell><cell>92.5</cell><cell>81.8</cell><cell>96.4</cell><cell cols="2">97.5 86.2 79.9</cell><cell>81.0</cell><cell>78.5</cell><cell>90.1 77.1 88.1 65.1</cell><cell>72.4</cell><cell>79.7</cell><cell>71.2</cell></row><row><cell>6-fold</cell><cell>83.1</cell><cell>88.9</cell><cell>72.2</cell><cell>93.3</cell><cell cols="2">96.8 81.6 61.9</cell><cell>49.5</cell><cell>65.4</cell><cell>73.3 72.0 83.7 67.5</cell><cell>64.3</cell><cell>67.0</cell><cell>62.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="11">: Detailed semantic segmentation results (%) on S3DIS [2] dataset. (mAcc: average class accuracy, OA: overall</cell></row><row><cell cols="8">accuracy, mIoU: mean Intersection-over-Union."6-fold": 6-fold cross-validation result.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">OA mIoU</cell><cell cols="4">man-made natural terrain terrain vegetation vegetation high low</cell><cell>buildings</cell><cell cols="3">hard scanning cars scape artefacts</cell></row><row><cell>SnapNet [6]</cell><cell>88.6</cell><cell>59.1</cell><cell>82.0</cell><cell>77.3</cell><cell>79.7</cell><cell>22.9</cell><cell>91.1</cell><cell>18.4</cell><cell>37.3</cell><cell>64.4</cell></row><row><cell>ShellNet [54]</cell><cell>93.2</cell><cell>69.3</cell><cell>96.3</cell><cell>90.4</cell><cell>83.9</cell><cell>41.0</cell><cell>94.2</cell><cell>34.7</cell><cell>43.9</cell><cell>70.2</cell></row><row><cell>GACNet [45]</cell><cell>91.9</cell><cell>70.8</cell><cell>86.4</cell><cell>77.7</cell><cell>88.5</cell><cell>60.6</cell><cell>94.2</cell><cell>37.3</cell><cell>43.5</cell><cell>77.8</cell></row><row><cell>SPG [26]</cell><cell>94.0</cell><cell>73.2</cell><cell>97.4</cell><cell>92.6</cell><cell>87.9</cell><cell>44.0</cell><cell>83.2</cell><cell>31.0</cell><cell>63.5</cell><cell>76.2</cell></row><row><cell>KPConv [44]</cell><cell>92.9</cell><cell>74.6</cell><cell>90.9</cell><cell>82.2</cell><cell>84.2</cell><cell>47.9</cell><cell>94.9</cell><cell>40.0</cell><cell>77.3</cell><cell>79.7</cell></row><row><cell cols="2">RandLA-Net [19] 94.8</cell><cell>77.4</cell><cell>95.6</cell><cell>91.4</cell><cell>86.6</cell><cell>51.5</cell><cell>95.7</cell><cell>51.5</cell><cell>69.8</cell><cell>76.8</cell></row><row><cell>Ours</cell><cell>94.3</cell><cell>75.3</cell><cell>96.3</cell><cell>93.7</cell><cell>87.7</cell><cell>48.1</cell><cell>94.6</cell><cell>43.8</cell><cell>58.2</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Semantic segmentation (reduced-8) results (%) on Semantic3D [14] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure 5: Visualization of intermediate features and semantic segmentation results for an office scene in S3DIS [2] dataset. P denotes the 3D coordinates of the point cloud, and F presents the semantic information acquired by the Feature Extractor (Sec. 4.1 in the main paper). Further, S means the output of our Bilateral Context Block (Sec. 3.1).</figDesc><table><row><cell cols="2">Model Description</cell><cell>mIoU (%)</cell></row><row><cell>N 0</cell><cell>Baseline model</cell><cell>60.8</cell></row><row><cell>N 1</cell><cell>Efficient model</cell><cell>64.8</cell></row><row><cell>N 2</cell><cell>Dilated model</cell><cell>62.5</cell></row><row><cell>N 3</cell><cell>Equal-weighted model</cell><cell>64.0</cell></row><row><cell>N 4</cell><cell>Simplified model</cell><cell>63.5</cell></row><row><cell>N 5</cell><cell>Proposed model</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation study about different variants of our network, tested on Area 5, S3DIS [2] dataset. We set an equal weight (ω i = 0.3) for all of the augmentation losses in Eq. 7 (i.e., calculating the overall loss L all ) of the main paper.•Simplified model: We only study four resolutions of the point cloud through the Bilateral Context Module. The number of points decreases as: N → N 4 → N 16 → N 64 , while the number of channels goes as: 16 → 64 → 128 → 256.</figDesc><table /><note>• Equal-weighted model:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Variance ↓ 11.9 ↓ 16.3 ↓ 24.7 ↓ 46.2 ↓ 104</figDesc><table><row><cell>Layer</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>#Points</cell><cell></cell><cell cols="2">40960 10240</cell><cell>2560</cell><cell>640</cell><cell>160</cell></row><row><cell>3D Space</cell><cell cols="2">Mean Variance ↓ 0.1 ↓ 12</cell><cell>↓ 24 ↓ 0.2</cell><cell>↓ 47 ↓ 0.5</cell><cell>↓ 85 ↓ 2</cell><cell>↓ 154 ↓ 13</cell></row><row><cell>Feature Space</cell><cell>Mean</cell><cell>↓ 45</cell><cell>↓ 693</cell><cell>↓ 814</cell><cell cols="2">↓ 124 ↓ 317</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>The general changes (×10 −3 ) of neighborhoods by involving bilateral offsets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">N &gt; N 1 &gt; N 2 &gt; ... &gt; N M , N is the original size of a point cloud.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The codes and test results are available at https://github. com/ShiQiu0419/BAAF-Net.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Jurgen Gall</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">nanoflann: a C++ header-only fork of FLANN, a library for nearest neighbor (NN) with kd-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Luis</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjal</forename><surname>Kumar Rai</surname></persName>
		</author>
		<ptr target="https://github.com/jlblancoc/nanoflann,2014.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="644" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Edge-convolution point net for semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhonatan</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5236" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3-d mapping with an rgb-d camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="187" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dilated point convolutions: On the receptive field size of point convolutions on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Semantic3d. net: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS annals of the photogrammetry, remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face identification verification using 3 dimensional modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray J</forename><surname>Bazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cooper</surname></persName>
		</author>
		<idno>097. 1</idno>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning and memorizing representative prototypes for 3d point cloud semantic and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Jaboyedoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Oppikofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Henri</forename><surname>Derron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Loye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pedrazzini</surname></persName>
		</author>
		<title level="m">Use of lidar in landslide investigations: a review. Natural hazards</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="5" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix Järemo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11611" to="11620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pu-gan: a point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7203" to="7212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic context encoding for accurate 3d point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selfprediction for joint instance and semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="187" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Global context reasoning for semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjian</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2931" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Geometric backprojection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12885</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense-resolution network for point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="644" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
