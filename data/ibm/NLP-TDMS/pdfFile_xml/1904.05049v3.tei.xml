<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-18">18 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Shuicheng</roleName><forename type="first">Yan</forename><surname>‡♭</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">♭ Yitu Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-18">18 Aug 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies, and design a novel Octave Convolution (OctConv) operation 1 to store and process feature maps that vary spatially "slower" at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale methods, OctConv is formulated as a single, generic, plug-andplay convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing convolutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The efficiency of Convolutional Neural Networks (CNNs) keeps increasing with recent efforts to reduce the inherent redundancy in dense model parameters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref> and in the channel dimension of feature maps <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>. However, substantial redundancy also exists in the spatial dimension of the feature maps produced by CNNs, where each location stores its own feature descriptor independently, while ignoring common information between adjacent locations that could be stored and processed together.</p><p>As shown in <ref type="figure">Figure 1(a)</ref>, a natural image can be decom-1 https://github.com/facebookresearch/OctConv (a) Separating the low and high spatial frequency signal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.  <ref type="figure">Figure 1</ref>: (a) Motivation. The spatial frequency model for vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> shows that natural image can be decomposed into a low and a high spatial frequency part. (b) The output maps of a convolution layer can also be factorized and grouped by their spatial frequency. (c) The proposed multifrequency feature representation stores the smoothly changing, low-frequency maps in a low-resolution tensor to reduce spatial redundancy. (d) The proposed Octave Convolution operates directly on this representation. It updates the information for each group and further enables information exchange between groups. posed into a low spatial frequency component that describes the smoothly changing structure and a high spatial frequency component that describes the rapidly changing fine details <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Similarly, we argue that the output feature maps of a convolution layer can also be decomposed into features of different spatial frequencies and propose a novel multi-frequency feature representation which stores high-and low-frequency feature maps into different groups as shown in <ref type="figure">Figure 1(b)</ref>. Thus, the spatial resolution of the low-frequency group can be safely reduced by sharing information between neighboring locations to reduce spatial redundancy as shown in <ref type="figure">Figure 1</ref>(c). To accommodate the novel feature representation, we generalize the vanilla convolution, and propose Octave Convolution (OctConv) which takes in feature maps containing tensors of two frequencies one octave apart, and extracts information directly from the low-frequency maps without the need of decoding it back to the high-frequency as shown in <ref type="figure">Figure 1(d)</ref>. As a replacement of vanilla convolution, OctConv consumes substantially less memory and computational resources. In addition, OctConv processes low-frequency information with corresponding (low-frequency) convolutions and effectively enlarges the receptive field in the original pixel space and thus can improve recognition performance. We design the OctConv in a generic way, making it a plug-and-play replacement for the vanilla convolution. Since OctConv mainly focuses on processing feature maps at multiple spatial frequencies and reducing their spatial redundancy, it is orthogonal and complementary to existing methods that focus on building better CNN topology <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>, reducing channel-wise redundancy in convolutional feature maps <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref> and reducing redundancy in dense model parameters <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>. Moreover, different from methods that exploit multi-scale information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12]</ref>, OctConv can be easily deployed as a plug-and-play unit to replace convolution, without the need of changing network architectures or requiring hyper-parameters tuning. Compared to the closely related Multi-grid convolution <ref type="bibr" target="#b24">[25]</ref>, OctConv provides more insights on reducing the spatial redundancy in CNNs based on the frequency model and adopts more efficient interfrequency information exchange strategy with better performance. We further integrate the OctConv into a wide variety of backbone architectures (including the ones featuring group, depth-wise, and 3D convolutions) and demonstrate universality of OctConv.</p><p>Our experiments demonstrate that by simply replacing the vanilla convolution with OctConv, we can consistently improve the performance of popular 2D CNN backbones including ResNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, ResNeXt <ref type="bibr" target="#b46">[47]</ref>, DenseNet <ref type="bibr" target="#b21">[22]</ref>, MobileNet <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref> and SE-Net <ref type="bibr" target="#b18">[19]</ref> on 2D image recognition on ImageNet <ref type="bibr" target="#b10">[11]</ref>, as well as 3D CNN backbones C2D <ref type="bibr" target="#b43">[44]</ref> and I3D <ref type="bibr" target="#b43">[44]</ref> on video action recognition on Kinetics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. The OctConv-equipped Oct-ResNet-152 can match or outperform state-of-the-art manually designed networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref> at lower memory and computational cost. Our contributions can be summarized as follows:</p><p>• We propose to factorize convolutional feature maps into two groups at different spatial frequencies and process them with different convolutions at their corresponding frequency, one octave apart. As the resolution for low frequency maps can be reduced, this saves both storage and computation. This also helps each layer gain a larger receptive field to capture more contextual information. • We design a plug-and-play operation named OctConv to replace the vanilla convolution for operating on the new feature representation directly and reducing spatial re-dundancy. Importantly, OctConv is fast in practice and achieves a speedup close to the theoretical limit. • We extensively study the properties of the proposed Oct-Conv on a variety of backbone CNNs for image and video tasks and achieve significant performance gain even comparable to the best AutoML networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Improving the efficiency of CNNs. Ever since the pioneering work on AlexNet <ref type="bibr" target="#b25">[26]</ref> and VGG <ref type="bibr" target="#b34">[35]</ref>, researchers have made substantial efforts to improve the efficiency of CNNs. ResNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and DenseNet <ref type="bibr" target="#b21">[22]</ref> improve the network topology by adding shortcut connections to early layers. ResNeXt <ref type="bibr" target="#b46">[47]</ref> and ShuffleNet <ref type="bibr" target="#b48">[49]</ref> use sparsely connected group convolutions to reduce redundancy in interchannel connectivity. Xception <ref type="bibr" target="#b8">[9]</ref> and MobileNet <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref> adopt depth-wise convolutions that further reduce the connection density. Meanwhile, NAS <ref type="bibr" target="#b50">[51]</ref>, PNAS <ref type="bibr" target="#b28">[29]</ref> and AmoebaNet <ref type="bibr" target="#b32">[33]</ref> propose to atomically find the best network topology for a given task. Pruning methods, such as DSD <ref type="bibr" target="#b14">[15]</ref> and ThiNet <ref type="bibr" target="#b30">[31]</ref>, focus on reducing the redundancy in the model parameters by eliminating the least significant weight or connections in CNNs. Besides, Het-Conv <ref type="bibr" target="#b35">[36]</ref> propose to replace the vanilla convolution filters with heterogeneous convolution filters that are in different sizes. However, all of these methods ignore the redundancy on the spatial dimension of feature maps, which is addressed by the proposed OctConv, making OctConv orthogonal and complementary to these previous methods.</p><p>Noticeably, OctConv does not change the connectivity between feature maps, making it also different from inceptionalike multi-path designs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Multi-scale Representation Learning. Prior to the success of deep learning, multi-scale representation has long been applied for local feature extraction, such as the SIFT features <ref type="bibr" target="#b29">[30]</ref>. In the deep learning era, multi-scale representation also plays a important role due to its strong robustness and generalization ability. FPN <ref type="bibr" target="#b26">[27]</ref> and PSP <ref type="bibr" target="#b49">[50]</ref> merge convolutional features from different depths at the end of the networks for object detection and segmentation tasks. MSDNet <ref type="bibr" target="#b19">[20]</ref> and HR-Nets <ref type="bibr" target="#b37">[38]</ref>, proposed carefully designed network architectures that contain multiple branches where each branch has it own spatial resolution. The bL-Net <ref type="bibr" target="#b3">[4]</ref> and ELASTIC-Net <ref type="bibr" target="#b42">[43]</ref> adopt similar idea, but are designed as a replacement of residual block for ResNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and thus are more flexible and easier to use. But extra expertise and hyper-parameter tuning are still required when adopt them to architectures beyond ResNet, such as MobileNetV1 <ref type="bibr" target="#b17">[18]</ref>, DenseNet <ref type="bibr" target="#b21">[22]</ref>.</p><p>Multi-grid CNNs <ref type="bibr" target="#b24">[25]</ref> propose a multi-grid pyramid feature representation and define the MG-Conv operator as a replacement of convolution operator, which is conceptually similar to our method but is motivated for exploiting multi-scale features. Compared with MG-Conv, OctConv adopts more efficient design to exchange inter-frequency information with higher performance as can be found in Sec. 3.3 and Sec. 4.3. For video models, the recently proposed Slow-Fast Networks <ref type="bibr" target="#b11">[12]</ref> introduce multi-scale pathways on the temporal dimension. As we show in Section 4.4, this is complementary to OctConv which operates on the spatial dimensions.</p><p>In a nutshell, OctConv focuses on reducing the spatial redundancy in CNNs and is designed to replace vanilla convolution operations without needing to adjust backbone CNN architecture. We extensively compare OctConv to closely related methods in the sections of method and experiment and show that OctConv CNNs give top results on a number of challenging benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first introduce the octave feature representation and then describe Octave Convolution, which operates directly on it. We also discuss implementation details and show how to integrate OctConv into group and depth-wise convolution architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Octave Feature Representation</head><p>For the vanilla convolution, all input and output feature maps have the same spatial resolution, which may not be necessary since some of the feature maps may represent low-frequency information which is spatially redundant and can be further compressed as illustrated in <ref type="figure">Figure 1</ref>.</p><p>To reduce the spatial redundancy, we introduce the octave feature representation that explicitly factorizes the feature map tensors into groups corresponding to low and high frequencies.</p><p>The scale-space theory <ref type="bibr" target="#b27">[28]</ref> provides us with a principled way of creating scale-spaces of spatial resolutions, and defines an octave as a division of the spatial dimensions by a power of 2 (we only explore 2 1 in this work). We follow this fashion and reduce the spatial resolution of the low-frequency feature maps by an octave.</p><p>Formally, let X ∈ R c×h×w denote the input feature tensor of a convolutional layer, where h and w denote the spatial dimensions and c the number of feature maps or channels. We explicitly factorize X along the channel dimension into X = {X H , X L }, where the high-frequency feature maps X H ∈ R (1−α)c×h×w capture fine details and the lowfrequency maps X L ∈ R αc× h 2 × w 2 vary slower in the spatial dimensions (w.r.t. the image locations). Here α ∈ [0, 1] denotes the ratio of channels allocated to the low-frequency part and the low-frequency feature maps are defined an octave lower than the high frequency ones, i.e. at half of the spatial resolution as shown in <ref type="figure">Figure 1</ref>(c).</p><p>In the next subsection, we introduce a convolution operator that operates directly on this multi-frequency feature representation and name it Octave Convolution (OctConv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Octave Convolution</head><p>The octave feature representation presented in Section 3.1 reduces the spatial redundancy and is more compact than the original representation. However, the vanilla convolution cannot directly operate on such a representation, due to differences in spatial resolution in the input features. A naive way of circumventing this is to up-sample the lowfrequency part X L to the original spatial resolution, concatenate it with X H and then convolve, which would lead to extra costs in computation and memory and diminish all the savings from the compression. In order to fully exploit our compact multi-frequency feature representation, we introduce Octave Convolution, which can directly operate on factorized tensors X = {X H , X L } without requiring any extra computational or memory overhead. Vanilla Convolution. Let W ∈ R c×k×k denote a k × k convolution kernel and X, Y ∈ R c×h×w denote the input and output tensors, respectively. Each feature map in Y p,q ∈ R c can be computed by</p><formula xml:id="formula_0">Y p,q = i,j∈N k W i+ k−1 2 ,j+ k−1 2 ⊤ X p+i,q+j ,<label>(1)</label></formula><p>where (p, q) denotes the location coordinate and N k =</p><formula xml:id="formula_1">{(i, j) : i = {− k−1 2 , . . . , k−1 2 }, j = {− k−1 2 , . . . , k−1 2</formula><p>}} defines a local neighborhood. For simplicity, in all equations we omit the padding, we assume k is an odd number and that the input and output data have the same dimensionality, i.e. c in = c out = c. Octave Convolution. The goal of our design is to effectively process the low and high frequency in their corresponding frequency tensor but also enable efficient interfrequency communication. Let X, Y be the factorized input and output tensors. Then the high-and low-frequency feature maps of the output Y = {Y H , Y L } will be given by</p><formula xml:id="formula_2">Y H = Y H→H + Y L→H and Y L = Y L→L + Y H→L , respectively, where Y A→B denotes the convolutional up- date from feature map group A to group B. Specifi- cally, Y H→H , Y L→L denote intra-frequency update, while Y H→L , Y L→H denote inter-frequency communication.</formula><p>To compute these terms, we split the convolutional kernel W into two components W = [W H , W L ] responsible for convolving with X H and X L respectively. Each component can be further divided into intra-and interfrequency part: W H = [W H→H , W L→H ] and W L = [W L→L , W H→L ] with the parameter tensor shape shown in <ref type="figure">Figure 2</ref>(b). Specifically for high-frequency feature map, we compute it at location (p, q) by using a regular convolution for the intra-frequency update, and for the interfrequency communication we can fold the up-sampling over the feature tensor X L into the convolution, removing the need of explicitly computing and storing the up-sampled (a) Detailed design of the Octave Convolution. Green arrows correspond to information updates while red arrows facilitate information exchange between the two frequencies.</p><p>(b) The Octave Convolution kernel. The k × k Octave Convolution kernel W ∈ R c in ×cout×k×k is equivalent to the vanilla convolution kernel in the sense that the two have the exact same number of parameters. <ref type="figure">Figure 2</ref>: Octave Convolution. We set α in = α out = α throughout the network, apart from the first and last OctConv of the network where α in = 0, α out = α and α in = α, α out = 0, respectively. feature maps as follows:</p><formula xml:id="formula_3">Y H p,q =Y H→H p,q + Y L→H p,q = i,j∈N k W H→H i+ k−1 2 ,j+ k−1 2 ⊤ X H p+i,q+j + i,j∈N k W L→H i+ k−1 2 ,j+ k−1 2 ⊤ X L (⌊ p 2 ⌋+i),(⌊ q 2 ⌋+j) ,<label>(2)</label></formula><p>where ⌊·⌋ denotes the floor operation. Similarly, for the low-frequency feature map, we compute the intra-frequency update using a regular convolution. Note that, as the map is in one octave lower, the convolution is also low-frequency w.r.t. the high-frequency coordinate space. For the interfrequency communication we can again fold the downsampling of the feature tensor X H into the convolution as follows:</p><formula xml:id="formula_4">Y L p,q =Y L→L p,q + Y H→L p,q = i,j∈N k W L→L i+ k−1 2 ,j+ k−1 2 ⊤ X L p+i,q+j + i,j∈N k W H→L i+ k−1 2 ,j+ k−1 2 ⊤ X H (2 * p+0.5+i),(2 * q+0.5+j) ,<label>(3)</label></formula><p>where multiplying a factor 2 to the locations (p, q) performs down-sampling, and further shifting the location by half step is to ensure the down-sampled maps well aligned with the input. However, since the index of X H can only be an integer, we could either round the index to (2 * p+i, 2 * q+j) or approximate the value at (2 * p + 0.5 + i, 2 * q + 0.5 + j) by averaging all 4 adjacent locations. The first one is also known as strided convolution and the second one as average pooling. As we discuss in Section 3.3 and <ref type="figure">Fig. 5</ref>, strided convolution leads to misalignment; we therefore use average pooling to approximate this value for the rest of the paper.</p><p>An interesting and useful property of the Octave Convolution is the larger receptive field for the low-frequency feature maps. Convolving the low-frequency part X L with k × k convolution kernels, results in an effective enlargement of the receptive field by a factor of 2 compared to vanilla convolutions. This further helps each OctConv layer capture more contextual information from distant locations and can potentially improve recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>As discussed in the previous subsection, the index {(2 * p + 0.5 + i), (2 * q + 0.5 + j)} has to be an integer for Eq. 3. Instead of rounding it to {(2 * p + i), (2 * q + j)}, i.e. conduct convolution with stride 2 for down-sampling, we adopt average pooling to get more accurate approximation. This helps alleviate misalignments that appear when aggregating information from different scales, as shown in Appendix A. and Appendix C.. We can now rewrite the output Y = {Y H , Y L } of the Octave Convolution using average pooling for down-sampling as:</p><formula xml:id="formula_5">Y H =f (X H ; W H→H ) + upsample(f (X L ; W L→H ), 2) Y L =f (X L ; W L→L ) + f (pool(X H , 2); W H→L )),<label>(4)</label></formula><p>where f (X; W ) denotes a convolution with parameters W , pool(X, k) is an average pooling operation with kernel size k × k and stride k. upsample(X, k) is an up-sampling operation by a factor of k via nearest interpolation.</p><p>The details of the OctConv operator implementation are shown in <ref type="figure">Figure 2</ref>. It consists of four computation paths that correspond to the four terms in Eq. (4): two green paths correspond to information updating for the high-and lowfrequency feature maps, and two red paths facilitate information exchange between the two octaves. Group and Depth-wise convolutions. The Octave Convolution can also be adopted to other popular variants of the vanilla convolution such as group <ref type="bibr" target="#b46">[47]</ref> or depth-wise <ref type="bibr" target="#b17">[18]</ref> convolutions. For the group convolution case, we simply set all four convolution operations that appear inside the design of the OctConv to group convolutions. Similarly, for the depth-wise convolution case, the convolution opera-  <ref type="table">Table 1</ref>: Relative theoretical gains for the proposed multifrequency feature representation over vanilla feature maps for varying choices of the ratio α of channels used by the low-frequency feature. When α = 0, no low-frequency feature is used which is the case of vanilla convolution.</p><p>tions are depth-wise and therefore the information exchange paths are eliminated, leaving only two depth-wise convolution operations. We note that both group OctConv and depth-wise OctConv reduce to their respective vanilla versions if we do not compress the low-frequency part.</p><p>Efficiency analysis. <ref type="table">Table 1</ref> shows the theoretical computational cost and memory consumption of OctConv over the vanilla convolution and vanilla feature map representation. More information on deriving the theoretical gains presented in <ref type="table">Table 1</ref> can be found in the supplementary material. We note the theoretical gains are calculated per convolutional layer. In Section 4 we present the corresponding practical gains on real scenarios and show that our OctConv implementation can sufficiently approximate the theoretical numbers.</p><p>Integrating OctConv into backbone networks. OctConv is backwards compatible with vanilla convolution and can be inserted to regular convolutional networks without special adjustment. To convert a vanilla feature representation to a multi-frequency feature representation, i.e. at the first OctConv layer, we set α in = 0 and α out = α. In this case, OctConv paths related to the low-frequency input is disabled, resulting in a simplified version which only has two paths. To convert the multi-frequency feature representation back to vanilla feature representation, i.e. at the last OctConv layer, we set α out = 0. In this case, Oct-Conv paths related to the low-frequency output is disabled, resulting in a single full resolution output.</p><p>Comparison to Multi-grid Convolution <ref type="bibr" target="#b24">[25]</ref>. The multigrid conv (MG-Conv) <ref type="bibr" target="#b24">[25]</ref> is a bi-directional and crossscale convolution operator. Though being conceptually similar, our OctConv is different from MG-Conv in both the core motivation and design. MG-Conv aims to exploit multi-scale information in existing CNNs, while OctConv is focusing on reducing spatial redundancy among neighborhood pixels. In terms of design, MG-Conv adopts maxpooling for down-sampling. This requires extra memory for storing the index of the maximum value during training and further yields lower accuracy (see Appendix C.). MG-Conv also first up-samples and then convolves with the enlarged feature maps. Differently, OctConv aims for reducing spatial redundancy and is a naive extension of convolution op-erator. It uses average pooling to distill low-frequency features without extra memory cost and its upsampling operation follows the convolution, and is thus more efficient than MG-Conv. The meticulous design of the lateral paths are essential for OctConv to be much more memory and computationally efficient than MG-Conv and improve accuracy without increasing the network complexity. We compare OctConv to MG-Conv in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>In this section, we validate the effectiveness and efficiency of the proposed Octave Convolution for both 2D and 3D networks. We first present ablation studies for image classification on ImageNet <ref type="bibr" target="#b10">[11]</ref> and then compare it with the state-of-the-art. Then, we show the proposed OctConv also works in 3D CNNs using Kinetics-400 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> and Kinetics-600 <ref type="bibr" target="#b1">[2]</ref> datasets. The best results per category/block are highlighted in bold font throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setups</head><p>Image classification. We examine OctConv on a set of most popular CNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref> by replacing the regular convolutions with OctConv (except the first convolutional layer before the max pooling). The resulting networks only have one global hyper-parameter α, which denotes the ratio of low frequency part. We do apple-toapple comparison and reproduce all baseline methods by ourselves under the same training/testing setting for internal ablation studies. All networks are trained with naïve softmax cross entropy loss except that the MobileNetV2 also adopts the label smoothing <ref type="bibr" target="#b39">[40]</ref>, and the best ResNet-152 adopts both label smoothing and mixup <ref type="bibr" target="#b47">[48]</ref> to prevent overfitting. Same as <ref type="bibr" target="#b3">[4]</ref>, all networks are trained from scratch and optimized by SGD with cosine learning rate <ref type="bibr" target="#b12">[13]</ref>. Standard accuracy of single centeral crop <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref> on validation set is reported. Video action recognition. We use both Kinetics-400 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> and Kinetics-600 <ref type="bibr" target="#b1">[2]</ref> for human action recognition. We choose standard baseline backbones from Inflated 3D ConvNet <ref type="bibr" target="#b43">[44]</ref> and compare them with the OctConv counterparts. We follow the setting from <ref type="bibr" target="#b44">[45]</ref> using frame length of 8 as standard input size, training 300k iterations in total, and averaging the predictions over 30 crops during inference time. To make fair comparison, we report the performance of the baseline and OctConv under precisely the same settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study on ImageNet</head><p>We conduct a series of ablation studies aiming to answer the following questions: 1) Does OctConv have better FLOPs-Accuracy trade-off than vanilla convolution? 2) In which situation does the OctConv work the best? Results on ResNet-50. We begin with using the popular ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the baseline CNN and replacing the regular convolution with our proposed OctConv to examine the flops-accuracy trade-off. In particular, we vary the global ratio α ∈ {0.125, 0.25, 0.5, 0.75} to compare the image classification accuracy versus computational cost (i.e. FLOPs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref> with the baseline. The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref> in pink. We make following observations. 1) The flops-accuracy trade-off curve is a concave curve, where the accuracy first rises up and then slowly goes down. 2) We can see two sweet spots: The first at α = 0.5, where the network gets similar or better results even when the FLOPs are reduced by about half; the second at α = 0.125, where the network reaches its best accuracy, 1.2% higher than baseline (black circle). We attribute the increase in accuracy to OctConv's effective design of multi-frequency processing and the corresponding enlarged receptive field which provides more contextual information to the network. While reaching the accuracy peak at 0.125, the accuracy does not suddenly drop but decreases slowly for higher ratios α, indicating reducing the resolution of the low frequency part does not lead to significant information loss. Interestingly, 75% of the feature maps can be compressed to half the resolution with only 0.3% accuracy drop, which demonstrates effectiveness of grouping and compressing the smoothly changed feature maps for reducing the spatial redundancy in CNNs. In <ref type="table" target="#tab_2">Table 2</ref> we demonstrate the theoretical FLOPs saving of Oct-Conv is also reflected in the actual CPU inference time in practice. For ResNet-50, we are close to obtaining theoretical FLOPs speed up. These results indicate OctConv is able to deliver important practical benefits, rather than only saving FLOPs in theory.   Results on more CNNs. To further examine if the proposed OctConv works for other networks with different depth/wide/topology, we select the currently most popular networks as baselines and repeat the same ablation study. These networks are ResNet-(26;50;101;200) <ref type="bibr" target="#b16">[17]</ref>, ResNeXt-(50,32×4d;101,32×4d) <ref type="bibr" target="#b46">[47]</ref>, DenseNet-121 <ref type="bibr" target="#b21">[22]</ref> and SE-ResNet-50 <ref type="bibr" target="#b18">[19]</ref>. The ResNeXt is chosen for assessing the OctConv on group convolution, while the SE-Net <ref type="bibr" target="#b18">[19]</ref> is used to check if the gain of SE block found on vanilla convolution based networks can also be seen on OctConv. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, OctConv equipped networks for different architecture behave similarly to the Oct-ResNet-50, where the FLOPs-Accuracy trade-off is in a concave curve and the performance peak also appears at ratio α = 0.125 or α = 0.25. The consistent performance gain on a variety of backbone CNNs confirms that OctConv is a good replacement of vanilla convolution.</p><p>Frequency Analysis. <ref type="figure" target="#fig_2">Figure 4</ref> shows the frequency analysis results. We conducted the Fourier transform for each group of feature maps and visualized the averaged results.   <ref type="table">Table 4</ref>: ImageNet Classification results for Middle sized models. ‡ refers to method that replaces "Max Pooling" by extra convolution layer(s) <ref type="bibr" target="#b3">[4]</ref>. § refers to method that uses balanced residual block distribution <ref type="bibr" target="#b3">[4]</ref>.</p><p>models increases as the test image resolution grows because OctConv can detect large objects better due to its larger receptive field, see Appendix C. 3) Both the information exchanging paths are important, since removing any of them can lead to accuracy drop, see Appendix C. 4) Shallow networks, e.g. ResNet-26, have a rather limited receptive field, and can especially benefit from OctConv, which greatly enlarges their receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing with SOTAs on ImageNet</head><p>Small models. We adopt the most popular light weight networks as baselines and examine if OctConv works well on these compact networks with depth-wise convolution. In particular, we use the "0.75 MobileNet (v1)" <ref type="bibr" target="#b17">[18]</ref> and "1.0 MobileNet (v2)" <ref type="bibr" target="#b33">[34]</ref> as baseline and replace the regular convolution with our proposed OctConv. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. We find that OctConv can reduce the FLOPs of MobileNetV1 by 34%, and provide better accuracy and faster speed in practice; it is able to reduce the FLOPs of MobileNetV2 by 15%, achieving the same accuracy with faster speed. When the computation budget is fixed, one can adopt wider models to increase the learning capacity because OctConv can compensate the extra computation cost. In particular, our OctConv equipped networks achieve 2% improvement on MobileNetV1 under the same FLOPs and 1% improvement on MobileNetV2. Medium models. In the above experiment, we have compared and shown that OctConv is complementary with a set of state-of-the-art CNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19]</ref>. In this part, we compare OctConv with MG-Conv <ref type="bibr" target="#b24">[25]</ref>, GloRe <ref type="bibr" target="#b7">[8]</ref>, Elastic <ref type="bibr" target="#b42">[43]</ref> and bL-Net <ref type="bibr" target="#b3">[4]</ref> which share a similar idea as our method. Seven groups of results are shown in <ref type="table">Table 4</ref>. In group 1, our Oct-ResNet-26 shows 0.6% better accuracy than R-MG-34 while costing only one third of FLOPs and half of #Params. Also, our Oct-ResNet-50, which costs less than half of FLOPS, achieves 1.9% higher accuracy than R-MG-34. In group 2, adding our OctConv to GloRe network reduces the FLOPs with better accuracy. In group 3, our Oct-ResNeXt-50 achieves better accuracy than the Elastic <ref type="bibr" target="#b42">[43]</ref> based method (78.8% v.s. 78.4%) while reducing the computational cost by 31%. In group 4, the Oct-ResNeXt-101 also achieves higher accuracy than the Elastic based method (79.6% v.s. 79.2%) while costing 38% less computation. When compared to the bL-Net <ref type="bibr" target="#b3">[4]</ref>, OctConv equipped methods achieve better FLOPs-Accuracy trade-off without bells and tricks. When adopting the tricks used in the baseline bL-Net <ref type="bibr" target="#b3">[4]</ref>, our Oct-ResNet-50 achieves 0.9% higher accuracy than bL-ResNet-50 under the same computational budget (group 5), and Oct-ResNeXt-50 (group 6) and Oct-ResNeXt-101 (group 7) get better accuracy under comparable or even lower computational budget. This is because MG-Conv <ref type="bibr" target="#b24">[25]</ref>, Elastic-Net <ref type="bibr" target="#b42">[43]</ref> and bL-Net <ref type="bibr" target="#b3">[4]</ref> are designed following the principle of introducing multi-scale features without considering reducing the spatial redundancy. In contrast, OctConv is born for solving the high spatial redundancy problem in CNNs, uses more efficient strategies to store and process the information throughout the network, and can thus achieve better efficiency and performance.</p><p>Large models. <ref type="table" target="#tab_6">Table 5</ref> shows the results of OctConv in large models. Here, we choose the ResNet-152 as the backbone CNN, replacing the first 7 × 7 convolution by three 3 × 3 convolution layers and removing the max pooling by a lightweight residual block <ref type="bibr" target="#b3">[4]</ref>. We report results for Oct-ResNet-152 with and without the SE-block <ref type="bibr" target="#b18">[19]</ref>.  due to the use of group and depth-wise convolutions. Our proposed method is also complementary to Squeeze-andexcitation <ref type="bibr" target="#b18">[19]</ref>, where the accuracy can be further boosted when the SE-Block is added (last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments of Video Recognition on Kinetics</head><p>In this subsection, we evaluate the effectiveness of Oct-Conv for action recognition in videos and demonstrate that our spatial OctConv is sufficiently generic to be integrated into 3D convolution to decrease #FLOPs and increase accuracy at the same time. As shown in <ref type="table">Table 6</ref>, OctConv consistently decreases FLOPs and meanwhile improves the accuracy when added to C2D and I3D <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, and is also complementary to the Non-local <ref type="bibr" target="#b43">[44]</ref>. This is observed for models pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref> as well as models trained from scratch on Kinetics. The higher accuracy, lower FLOPs and the ability of being complimentary to existing metods, e.g. Non-local method, confirm the effectiveness of the proposed OctConv method. Performance further increases when combining OctConv with the SlowFast Networks <ref type="bibr" target="#b11">[12]</ref>. Specifically, we apply OctConv on the spatial dimensions and SlowFast on the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we address the problem of reducing spatial redundancy that widely exists in vanilla CNN models, and propose a novel Octave Convolution operation to store and process low-and high-frequency features separately to improve the model efficiency. Octave Convolution is suffi- <ref type="bibr" target="#b2">3</ref> The auto-tune is set to off when evaluating the memory cost for more accurate result, and is set to on when measuring speed for fastest speed. <ref type="bibr" target="#b3">4</ref> An extra BatchNorm is added at the beginning of each residual function, otherwise the gradient will easily diverged due to the newly added SE module. This costs more memory and slows down the speed but can provide higher accuracy. <ref type="bibr" target="#b4">5</ref> Note that <ref type="bibr" target="#b11">[12]</ref>   <ref type="table">Table 6</ref>: Action Recognition in videos, ablation study, all models with ResNet50 <ref type="bibr" target="#b15">[16]</ref>.</p><p>ciently generic to replace the regular convolution operation in-place, and can be used in most 2D and 3D CNNs without model architecture adjustment. Beyond saving a substantial amount of computation and memory, Octave Convolution can also improve the recognition performance by effective communication between the low-and high-frequency and by enlarging the receptive field size which contributes to capturing more global information. Our extensive experiments on image classification and video action recognition confirm the superiority of our method for striking a much better trade-off between recognition performance and model efficiency, not only in FLOPs, but also in practice.</p><p>In <ref type="table">Table 1</ref> of the main paper, we reported the relative theoretical gains of the proposed multi-frequency feature representation over regular feature representation with respect to memory footprint and computational cost, as measured in FLOPS (i.e. multiplications and additions). In this section, we show how the gains are estimated in theory.</p><p>Memory cost. The proposed OctConv stores the feature representation in a multi-frequency feature representation as shown in <ref type="figure" target="#fig_3">Figure 6</ref>, where the low frequency tensor is stored in 2× lower spatial resolution and thus cost 75% less space to store the low frequency maps compared with the conventional feature representation. The relative memory cost is conditional on the ratio (α) and is calculated by</p><formula xml:id="formula_6">1 − 3 4 α.<label>(5)</label></formula><p>Computational cost. The computational cost of OctConv is proportional to the number of locations and channels that are needed to be convolved on. Following the design shown in <ref type="figure">Figure 2</ref> in the main paper, we need to compute four paths, namely H → H, H → L, L → H, and L → L.</p><p>We assume the convolution kernel size is k × k, the spatial resolution of the high-frequency feature is h × w, and there are (1 − α)c channels in the high-frequency part and αc channels in the low-frequency part. Then the FLOPS for computing each paths are calculated as below.</p><formula xml:id="formula_7">F LOP S(Y H→H ) = h × w × k 2 × (1 − α) 2 × c 2 F LOP S(Y H→L ) = h 2 × w 2 × k 2 × α × (1 − α) × c 2 F LOP S(Y L→H ) = h 2 × w 2 × k 2 × (1 − α) × α × c 2 F LOP S(Y L→L ) = h 2 × w 2 × k 2 × α 2 × c 2 (6)</formula><p>We omit FLOPS for adding Y H→H and Y L→H together, as well as that of adding Y L→L and Y H→H together, since the FLOPS of such addition is less than h × w × c, and is negligible compared with other computational costs. The computational cost of the pooling operation is also ignorable compared with other computational cost. The nearest neighborhood up-sampling is basically duplicating values which does not involves any computational cost. Therefore, by adding up all FLOPS in Eqn 6, we can estimate the overall FLOPS for compute Y H and Y L in Eqn 7.</p><formula xml:id="formula_8">F LOP S([Y H , Y L ]) = (1 − 3 4 α(2 − α)) × h × w × k 2 × c 2<label>(7)</label></formula><p>For vanilla convolution, the FLOPS for computing output feature map Y of size c × h × w with the kernel size k × k, and input feature map of size c × h × w, can be estimated as below.</p><formula xml:id="formula_9">F LOP S(Y ) = h × w × k 2 × c 2<label>(8)</label></formula><p>three out of four internal convolution operations are conducted on the lower resolution tensors except the first convolution, i.e. f (X H , W H− →H ). Thus, the relative computational cost compared with vanilla convolution using the same kernel size and number of input/out channels is: Therefore, the computational cost ratio between the Oct-Conv and vanilla convolution is (1 − 3 4 α(2 − α)).</p><p>(1 − α) 2 c 2 + 1 2 α(1 − α)c 2 + 1 4 α 2 c 2 c 2 = 1 − 3 4 α(2 − α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(9)</head><p>Note that the computational cost of the pooling operation is ignorable and thus is not considered. The nearest neighborhood up-sampling is basically duplicating values which does not involves any computational cost. <ref type="table" target="#tab_9">Table 7</ref> shows that the gain of OctConv over baseline models increases as the test image resolution grows. Such ability of better detecting large objects can be explained as the larger receptive field of each OctConv. <ref type="table" target="#tab_10">Table 8</ref> shows an ablation study to examine downsampling and inter-octave connectivity on ImageNet. The results confirm the importance of having both interfrequency communication paths. It also shows that pooling methods are better than strided convolution and the average pooling works the best. <ref type="table">Table 9</ref> reports the values that are plotted in <ref type="figure" target="#fig_2">Figure 4</ref> of the main text for clarity of presentation and to allow future work to compare to the precise numbers.    <ref type="table">Table 9</ref>: Ablation study on ImageNet in table form corresponding to the plots in <ref type="figure" target="#fig_2">Figure 4</ref> in the main paper. Note: All networks are trained with naïve softmax loss without label smoothing <ref type="bibr" target="#b39">[40]</ref> or mixup <ref type="bibr" target="#b47">[48]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. ImageNet Ablation Study Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>!</head><label></label><figDesc>"#$%&amp;'()'*+, -./0$%&amp;'()'*+, !"#$%&amp;'()'*+, -./0$%&amp;'()'*+, .*1"&amp;234."*$'5+03*/' .*1"&amp;234."*$)6734' (b) !"#$%&amp;'()'*+, -./0$%&amp;'()'*+, !"#$%&amp;'()'*+, -./0$%&amp;'()'*+, .*1"&amp;234."*$'5+03*/' .*1"&amp;234."*$)6734' (c) !"#$%&amp;'()'*+, -./0$%&amp;'()'*+, !"#$%&amp;'()'*+, -./0$%&amp;'()'*+, .*1"&amp;234."*$'5+03*/' .*1"&amp;234."*$)6734' (d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study results on ImageNet. OctConvequipped models are more efficient and accurate than baseline models. Markers in black in each line denote the corresponding baseline models without OctConv. The colored numbers are the ratio α. Numbers in X axis denote FLOPs in logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Frequency analysis for activation maps in different groups. 'Baseline' refers to vanilla ResNet. 10k activation maps are sampled from ResNet-101(Res3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>(a) The conventional feature representation used by vanilla convolution. (c) The proposed multi-frequency feature representation stores the smoothly changing, lowfrequency maps in a low-resolution tensor to reduce spatial redundancy, used by Octave Convolution. The figure is rotated compared to the one in the main paper for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of ResNet-50. Inference time is measured on Intel Skylake CPU at 2.0 GHz (single thread).</figDesc><table><row><cell>We report</cell></row></table><note>Oct-High-Frequency Group Oct-Low-Frequency Group Baseline Low → High Freq Energy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>From the energy map, the low frequency group does not contain high frequency signal, while the high frequency group contains both low and high frequency signals. This confirms that low-frequency group indeed captures lowfrequency information as expected. Note that OctConv gives the high frequency group the flexibly to store both low and high frequency signals for better learning capacity.</figDesc><table><row><cell>Method</cell><cell cols="5">ratio (α) #Params (M) #FLOPs (M) CPU (ms) Top-1 (%)</cell></row><row><cell>CondenseNet (G = C = 8) [21]</cell><cell>-</cell><cell>2.9</cell><cell>274</cell><cell>-</cell><cell>71.0</cell></row><row><cell>1.5 ShuffleNet (v1) [49]</cell><cell>-</cell><cell>3.4</cell><cell>292</cell><cell>-</cell><cell>71.5</cell></row><row><cell>1.5 ShuffleNet (v2) [32]</cell><cell>-</cell><cell>3.5</cell><cell>299</cell><cell>-</cell><cell>72.6</cell></row><row><cell>0.75 MobileNet (v1) [18]</cell><cell>-</cell><cell>2.6</cell><cell>325</cell><cell>13.4</cell><cell>70.3  *</cell></row><row><cell>0.75 Oct-MobileNet (v1) (ours)</cell><cell>.375</cell><cell>2.6</cell><cell>213</cell><cell>11.9</cell><cell>70.5</cell></row><row><cell>1.0 Oct-MobileNet (v1) (ours)</cell><cell>.5</cell><cell>4.2</cell><cell>321</cell><cell>18.4</cell><cell>72.5</cell></row><row><cell>1.0 MobileNet (v2) [34]</cell><cell>-</cell><cell>3.5</cell><cell>300</cell><cell>24.5</cell><cell>72.0</cell></row><row><cell>1.0 Oct-MobileNet (v2) (ours)</cell><cell>.375</cell><cell>3.5</cell><cell>256</cell><cell>17.1</cell><cell>72.0</cell></row><row><cell>1.125 Oct-MobileNet (v2) (ours)</cell><cell>.5</cell><cell>4.2</cell><cell>295</cell><cell>26.3</cell><cell>73.0</cell></row></table><note>Summary. 1) OctConv can help CNNs improve the ac- curacy while decreasing the FLOPs, deviating from other methods that reduce the FLOPs with a cost of lower ac- curacy. 2) At test time, the gain of OctConv over baseline</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ImageNet classification results for Small models.</figDesc><table><row><cell cols="6">indicates it is better than original reproduced by MXNet</cell></row><row><cell cols="6">GluonCV v0.4 [14]. The inference speed is tested using</cell></row><row><cell cols="6">TVM on Intel Skylake processor (2.0GHz, single thread) 2 .</cell></row><row><cell>Method</cell><cell>ratio (α)</cell><cell>Depth</cell><cell cols="3">#Params (M) #FLOPs (G) Top-1 (%)</cell></row><row><cell>R-MG-34 [25]</cell><cell>-</cell><cell>34</cell><cell>32.9</cell><cell>5.8</cell><cell>75.5</cell></row><row><cell>Oct-ResNet-26 (ours)</cell><cell>.25</cell><cell>26</cell><cell>16.0</cell><cell>1.9</cell><cell>76.1</cell></row><row><cell>Oct-ResNet-50 (ours)</cell><cell>.5</cell><cell>50</cell><cell>25.6</cell><cell>2.4</cell><cell>77.4</cell></row><row><cell>ResNet-50 + GloRe [8] (+3 blocks Res4)</cell><cell>-</cell><cell>50</cell><cell>30.5</cell><cell>5.2</cell><cell>78.4</cell></row><row><cell>Oct-ResNet-50 (ours) + GloRe [8] (+3 blocks Res4)</cell><cell>.5</cell><cell>50</cell><cell>30.5</cell><cell>3.1</cell><cell>78.8</cell></row><row><cell>ResNeXt-50 + Elastic [43]</cell><cell>-</cell><cell>50</cell><cell>25.2</cell><cell>4.2</cell><cell>78.4</cell></row><row><cell>Oct-ResNeXt-50 (32×4d) (ours)</cell><cell>.25</cell><cell>50</cell><cell>25.0</cell><cell>3.2</cell><cell>78.8</cell></row><row><cell>ResNeXt-101 + Elastic [43]</cell><cell>-</cell><cell>101</cell><cell>44.3</cell><cell>7.9</cell><cell>79.2</cell></row><row><cell>Oct-ResNeXt-101 (32×4d) (ours)</cell><cell>.25</cell><cell>101</cell><cell>44.2</cell><cell>5.7</cell><cell>79.6</cell></row><row><cell>bL-ResNet-50  ‡ (α = 4, β = 4) [4]</cell><cell>-</cell><cell>50 (+3)</cell><cell>26.2</cell><cell>2.5</cell><cell>76.9</cell></row><row><cell>Oct-ResNet-50  ‡ (ours)</cell><cell>.5</cell><cell>50 (+3)</cell><cell>25.6</cell><cell>2.5</cell><cell>77.8</cell></row><row><cell>Oct-ResNet-50 (ours)</cell><cell>.5</cell><cell>50</cell><cell>25.6</cell><cell>2.4</cell><cell>77.4</cell></row><row><cell>bL-ResNeXt-50  ‡ (32×4d) [4]</cell><cell>-</cell><cell>50 (+3)</cell><cell>26.2</cell><cell>3.0</cell><cell>78.4</cell></row><row><cell>Oct-ResNeXt-50  ‡ (32×4d) (ours)</cell><cell>.5</cell><cell>50 (+3)</cell><cell>25.1</cell><cell>2.7</cell><cell>78.6</cell></row><row><cell>Oct-ResNeXt-50 (32×4d) (ours)</cell><cell>.5</cell><cell>50</cell><cell>25.0</cell><cell>2.4</cell><cell>78.4</cell></row><row><cell>bL-ResNeXt-101  ‡  § (32×4d) [4]</cell><cell>-</cell><cell>101 (+1)</cell><cell>43.4</cell><cell>4.1</cell><cell>78.9</cell></row><row><cell>Oct-ResNeXt-101  ‡  § (32×4d) (ours)</cell><cell>.5</cell><cell>101 (+1)</cell><cell>40.1</cell><cell>4.2</cell><cell>79.4</cell></row><row><cell>Oct-ResNeXt-101  ‡ (32×4d) (ours)</cell><cell>.5</cell><cell>101 (+1)</cell><cell>44.2</cell><cell>4.2</cell><cell>79.1</cell></row><row><cell>Oct-ResNeXt-101 (32×4d) (ours)</cell><cell>.5</cell><cell>101</cell><cell>44.2</cell><cell>4.0</cell><cell>78.9</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>ImageNet Classification results for Large models. The names of OctConv-equiped models are in bold font and performance numbers for related works are copied from the corresponding papers. Networks are evaluated using CuDNN v10.0 4 in flop16 on a single Nvidia Titan V100 (32GB) for their training memory cost and speed. Works that employ neural architecture search are denoted by ( ). We set batch size to 128 in most cases, but had to adjust it to 64 (noted by † ), 32 (noted by ‡ ) or 8 (noted by § ) for networks that are too large to fit into GPU memory.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>reports 36.1 GFLOPs at a spatial size of 256 2 , while we report (training) GFLOPs at 224 2 for all methods.</figDesc><table><row><cell>Method</cell><cell cols="2">ImageNet Pretrain #FLOPs (G) Top-1 (%)</cell></row><row><cell cols="2">(a) Kinetics-400 [3]</cell><cell></cell></row><row><cell>I3D</cell><cell>28.1</cell><cell>72.6</cell></row><row><cell>Oct-I3D, α=0.1, (ours)</cell><cell>25.6</cell><cell>73.6 (+1.0)</cell></row><row><cell>Oct-I3D, α=0.2, (ours)</cell><cell>22.1</cell><cell>73.1 (+0.5)</cell></row><row><cell>Oct-I3D, α=0.5, (ours)</cell><cell>15.3</cell><cell>72.1 (-0.5)</cell></row><row><cell>C2D</cell><cell>19.3</cell><cell>71.9</cell></row><row><cell>Oct-C2D, α=0.1, (ours)</cell><cell>17.4</cell><cell>73.8 (+1.9)</cell></row><row><cell>I3D</cell><cell>28.1</cell><cell>73.3</cell></row><row><cell>Oct-I3D, α=0.1, (ours)</cell><cell>25.6</cell><cell>74.6 (+1.3)</cell></row><row><cell>I3D + Non-local</cell><cell>33.3</cell><cell>74.7</cell></row><row><cell>Oct-I3D + Non-local, α=0.1, (ours)</cell><cell>28.9</cell><cell>75.7 (+1.0)</cell></row><row><cell>SlowFast-R50 [12]</cell><cell>27.6 5</cell><cell>75.6</cell></row><row><cell>Oct-SlowFast-R50, α=0.1, (ours)</cell><cell>24.5</cell><cell>76.2 (+0.6)</cell></row><row><cell>Oct-SlowFast-R50, α=0.2, (ours)</cell><cell>22.9</cell><cell>75.8 (+0.2)</cell></row><row><cell cols="2">(b) Kinetics-600 [2]</cell><cell></cell></row><row><cell>I3D</cell><cell>28.1</cell><cell>74.3</cell></row><row><cell>Oct-I3D, α=0.1, (ours)</cell><cell>25.6</cell><cell>76.0 (+1.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>ImageNet classification accuracy. The short length of input images are resized to the target crop size while keeping the aspect ratio unchanged. A centre crop is adopted if the input image size is not square. ResNet-50 backbone trained with crops size of 256 × 256 pixels.</figDesc><table><row><cell>Method</cell><cell cols="2">Down-sampling Low − → High High − → Low Top-1 (%)</cell></row><row><cell></cell><cell>avg. pooling</cell><cell>76.0</cell></row><row><cell>Oct-ResNet-50 ratio: 0.5</cell><cell>avg. pooling avg. pooling</cell><cell>76.4 76.4</cell></row><row><cell></cell><cell>strided conv.</cell><cell>76.3</cell></row><row><cell></cell><cell>max. pooling</cell><cell>77.0</cell></row><row><cell></cell><cell>avg. pooling</cell><cell>77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Ablation on down-sampling and inter-octave connectivity on ImageNet. Note that MG-Conv<ref type="bibr" target="#b24">[25]</ref> uses max pooling for down-sampling.</figDesc><table><row><cell>Backbone</cell><cell></cell><cell cols="5">baseline α = 0.125 α = 0.25 α = 0.5 α = 0.75</cell></row><row><cell>ResNet-26</cell><cell>GFLOPs Top-1 acc.</cell><cell>2.353 73.2</cell><cell>2.102 75.8</cell><cell>1.871 76.1</cell><cell>1.491 75.5</cell><cell>1.216 74.6</cell></row><row><cell>DenseNet-121</cell><cell>GFLOPs Top-1 acc.</cell><cell>2.852 75.4</cell><cell>2.428 76.1</cell><cell>2.044 75.9</cell><cell>--</cell><cell>--</cell></row><row><cell>ResNet-50</cell><cell>GFLOPs Top-1 acc.</cell><cell>4.105 77.0</cell><cell>3.587 78.2</cell><cell>3.123 78.0</cell><cell>2.383 77.4</cell><cell>1.891 76.7</cell></row><row><cell>SE-ResNet-50</cell><cell>GFLOPs Top-1 acc.</cell><cell>4.113 77.6</cell><cell>3.594 78.7</cell><cell>3.130 78.4</cell><cell>2.389 77.9</cell><cell>1.896 77.4</cell></row><row><cell>ResNeXt-50</cell><cell>GFLOPs Top-1 acc.</cell><cell>4.250 78.4</cell><cell>--</cell><cell>3.196 78.8</cell><cell>2.406 78.4</cell><cell>1.891 77.5</cell></row><row><cell>ResNet-101</cell><cell>GFLOPs Top-1 acc.</cell><cell>7.822 78.5</cell><cell>6.656 79.2</cell><cell>5.625 79.2</cell><cell>4.012 78.7</cell><cell>--</cell></row><row><cell>ResNeXt-101</cell><cell>GFLOPs Top-1 acc.</cell><cell>7.993 79.4</cell><cell>--</cell><cell>5.719 79.6</cell><cell>4.050 78.9</cell><cell>--</cell></row><row><cell>ResNet-200</cell><cell>GFLOPs Top-1 acc.</cell><cell>15.044 79.6</cell><cell>12.623 80.0</cell><cell>10.497 79.8</cell><cell>7.183 79.5</cell><cell>--</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For small models, we should notice according to arithmetic intensity<ref type="bibr" target="#b45">[46]</ref>, real execution time is not only bounded by FLOPS.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank Min Lin and Xin Zhao for helpful discussions of the code development.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. The Misalignment Problem</head><p>As shown in <ref type="figure">Figure 5</ref>, up-sampling after the strided convolution with odd convolutional filter, e.g. 3 × 3, will cause the entire feature map to move to the lower right, which is problematic when we add the up-sampled shifted map with the unshifted map. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Application of fourier analysis to the visibility of gratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="566" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Learning Representations</title>
		<meeting>the Seventh International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Oxford psychology series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">De</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">K De</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>Spatial vision</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04433</idno>
		<title level="m">Zhongyue Zhang, and Shuai Zheng. Gluoncv and gluonnlp: Deep learning in computer vision and natural language processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04381</idno>
		<title level="m">Dense-sparse-dense training for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2752" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Math kernel library for deep neural networks (mkldnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multigrid neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6665" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
		<title level="m">Scale-space theory in computer vision</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thinet: pruning cnn filters for a thinner net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04120</idno>
		<title level="m">Hetconv: Heterogeneous kernelbased convolutions for deep cnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mallat</forename><surname>Stephane</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The lifting scheme: A construction of second generation wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="546" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clip-q: Deep network compression learning by in-parallel pruning-quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7873" to="7882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Elastic: Improving cnns with instance specific scaling policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05262</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/video-nonlocal-net" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Auto-tuning performance on multicore computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename><surname>Samuel Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
