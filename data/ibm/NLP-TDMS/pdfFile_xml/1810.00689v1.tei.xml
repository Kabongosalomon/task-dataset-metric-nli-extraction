<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-Level Convolutional Neural Networks for Pedestrian Detection Using Saliency and Boundary Box Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyong</forename><surname>Yun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Part-Level Convolutional Neural Networks for Pedestrian Detection Using Saliency and Boundary Box Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrians in videos have a wide range of appearances such as body poses, occlusions, and complex backgrounds, and there exists the proposal shift problem in pedestrian detection that causes the loss of body parts such as head and legs. To address it, we propose part-level convolutional neural networks (CNN) for pedestrian detection using saliency and boundary box alignment in this paper. The proposed network consists of two sub-networks: detection and alignment. We use saliency in the detection sub-network to remove false positives such as lamp posts and trees. We adopt bounding box alignment on detection proposals in the alignment sub-network to address the proposal shift problem. First, we combine FCN and CAM to extract deep features for pedestrian detection. Then, we perform part-level CNN to recall the lost body parts. Experimental results on various datasets demonstrate that the proposed method remarkably improves accuracy in pedestrian detection and outperforms existing state-of-the-arts in terms of log average miss rate at false position per image (FPPI).</p><p>Index Terms-Convolutional neural network, pedestrian detection, proposal shift problem, boundary box alignment, saliency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection is a classical task in computer vision which is an operation capturing target objects in images (or video) and feeding back the category and localization of the object. Latest solutions on object detection achieve high computing speed and accuracy. For example, YOLO <ref type="bibr" target="#b1">[2]</ref> produces very high performance in object detection: more than 40 frames per second (FPS) and 78 mean average precision (MAP) on PASCAL Visual Object Classes challenge 2007 (VOC2007). As a sub-field of object detection, pedestrian detection is often applied to video surveillance, automotive safety, and robotics applications. Pedestrian, a special instance in object detection, has a unique trait in videos. Pedestrians in videos have a wide variety of appearances such as body</p><p>The earlier version of this paper has been presented at the 42nd IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP), New Orleans, LA, USA, March 5-9, 2017 <ref type="bibr" target="#b0">[1]</ref>. This work was supported by the National Natural Science Foundation of <ref type="bibr">China</ref>   pose, clothing, lighting and occlusion, while the background might be changed in a limited range. The wide range of intraclass variety against relatively small background change has a negative effect on detectors. Above all, many detectors which work well on detecting common objects heavily suffer from occlusion in pedestrian detection, which leads to the decrease of the location quality represented by bounding boxes. Thus, occlusion handling is required to help the detectors recall test samples in different level of occlusions.</p><p>Felzenszwalb et al. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> proposed a star model to search the whole image for body parts by a multi-scale sliding window technique. This work has inspired researchers to consider part detection in deep learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Ouyang and Wang <ref type="bibr" target="#b6">[7]</ref> designed a unique part detection layer with 20 convolutional filters of different sizes to detect body parts of the corresponding size ratio. These deep learningbased methods assume that the detection proposals are given by conventional detectors such as SquaresChnFtrs <ref type="bibr" target="#b9">[10]</ref>. Thus, recent CNN-based pedestrian detectors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have transformed pedestrian detection to classification of the detection proposals. Thus, detectors avoid redundant exhaustive search over whole images. JointDeep <ref type="bibr" target="#b6">[7]</ref> and SDN <ref type="bibr" target="#b8">[9]</ref> used "HOG+CSS" as features and a Linear SVM as a classifier to generate detection proposals (HOG: Histogram of oriented gradient, CSS: Color-self-similarity). The "HOG+CSS+SVM" proposer recalled most pedestrian candidates from images. Also, the performance of the CNN detector was improved by hard negatives generated by the "HOG+CSS+SVM" proposer. Other detection proposals were generated by ACF <ref type="bibr" target="#b14">[15]</ref>, LDCF <ref type="bibr" target="#b15">[16]</ref>, SquaresChnFtrs <ref type="bibr" target="#b9">[10]</ref>, and checkerboards <ref type="bibr" target="#b16">[17]</ref>. For the 2-stage detectors which combine detection proposal and classification are influenced significantly by the performance of detection proposers, especially for intersection over union (IoU) of bounding boxes.</p><p>In this paper, we propose part-level CNN for pedestrian detection using fully convolutional networks (FCN) and class activation map (CAM). The proposed network consists of two sub-networks of detection and alignment. In the detection sub-network, we use saliency to assign different weights to pedestrians and background. Based on saliency, we remove false positives such as lamp posts and trees from pedestrians. We adopt the alignment sub-network to recall the lost body parts caused by the detection sub-network. In the alignment sub-network, we utilize localization features of CNN such as FCN abd CAM to produce confidence maps and infer accurate accurate pedestrian location, i.e. bounding box alignment. Although FCN-based feature maps in our previous work <ref type="bibr" target="#b0">[1]</ref> preserved the localization capability of CNN well, its output resolution was relatively low for bounding box alignment. Therefore, it was hard to obtain accurate feature maps even with upsampling used. To address the resolution problem, we add CAM into the alignment sub-network. With the help of CAM, we produce high resolution feature maps for bounding box alignment. In this work, we divide the proposed CNN detector for training into three body parts considering efficiency: head, torso and legs. In our previous work <ref type="bibr" target="#b0">[1]</ref>, we divided it into five parts of head, left torso, right torso, left leg and right leg. Moreover, we utilize the detection subnetwork to obtain pedestrian proposals, while our previous work <ref type="bibr" target="#b0">[1]</ref> used SquaresChnFtrs <ref type="bibr" target="#b9">[10]</ref> based on a combination of conventional hand-crafted features. Experimental results show that the proposed method effectively removes false positives by saliency as well as successfully recall the lost body parts by boundary box alignment. The proposed method achieves 10% performance improvement in pedestrian detection over our previous work <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the whole framework of the proposed method.</p><p>Compared with the existing methods, main contributions of this paper are as follows:</p><p>• We use saliency in the detection sub-network to remove background areas such as lamp posts and trees from pedestrians. • We combine FCN and CAM into the alignment subnetwork to enhance the resolution of confidence maps and successfully recall the lost body parts. The rest of this paper is organized as follows. Section II relevant research trends.In Section III, the proposed method are described in detail. Section IV experimentally compares the proposed method with existing methods. Section V draws conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Up to the present, researchers have proposed many outstanding works for pedestrian detection, and in this section we mainly focus on deep learning models. The first deep model was an unsupervised deep model proposed by Sermanet et al. <ref type="bibr" target="#b17">[18]</ref> to consider limited training data. This model used a few tricks: 1) Multi-stage features, 2) connections to skip layers and integrate global shape information with local distinctive motif information, 3) unsupervised method based on convolutional sparse coding to pre-train the filters at each stage. A series of methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> combined part detection and deep models to improve the detection accuracy in body part occlusion. DBN-Isol <ref type="bibr" target="#b4">[5]</ref> proposed the deformable part model (DPM) <ref type="bibr" target="#b3">[4]</ref> based on a deep belief network to estimate the visibility of pedestrians. JointDeep <ref type="bibr" target="#b6">[7]</ref> was a deep model that was composed of feature extraction, occlusion handling, deformation and classification in a single network. MultiSDP <ref type="bibr" target="#b18">[19]</ref> built a multi-stage classifier to deal with complex distributed samples in pedestrian datasets. SDN <ref type="bibr" target="#b8">[9]</ref> used switchable Restricted Boltzmann Machines (RBMs) to extract high-level features for body parts. They divided human body into three parts: head-shoulder, upper-body and lowerbody. Tian et al. <ref type="bibr" target="#b10">[11]</ref> introduced datasets for scene labeling which contained city street scenes to aid the detector for distinguishing background from the proposals. The idea was that the scene labeling datasets contained information similar to the background in pedestrian datasets. Considering part detection, Tian et al. <ref type="bibr" target="#b5">[6]</ref> also proposed DeepParts to handle occlusion with an extensive body part pool. In this method, SVM detector was not used directly for the CNN output due to its small improvement. Moreover, general object detectors <ref type="bibr" target="#b19">[20]</ref> have been applied to pedestrian detection. Hosang et al. <ref type="bibr" target="#b13">[14]</ref> analyzed the feasibility of the region-based CNN <ref type="bibr" target="#b19">[20]</ref> (R-CNN) framework for the pedestrian detection task. They adopted SquaresChnFtrs <ref type="bibr" target="#b9">[10]</ref>, i.e. a stand-alone pedestrian detector, as the detection proposer and a R-CNN model for classification. Following R-CNN, region proposal network (RPN) built in faster R-CNN <ref type="bibr" target="#b20">[21]</ref> produced detection proposals by the network itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The proposed pedestrian detection framework consists of two sub-networks: detection and alignment. We use a proposal-and-classification approach to detect pedestrians with multi-scales. To get detection prosals, we perform fast pedestrian detection in the detection sub-network based on region proposal network (RPN). To remove false positives, we use saliency in the detection sub-network. Then, we align bounding boxes in the alignment sub-network to recall the lost body parts caused by the detection sub-network. We combine FCN and CAM into the alignment sub-network for accurate pedestrian localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection Framework</head><p>Network Architecture: The first stage is to generate detection proposals. As shown in <ref type="figure">Fig. 3</ref>, the detection sub-network consists of five convolutional units, one fully-connected (FC) layer, and one global max pooling (GMP) layer for classification and localization. The five convolutional units are configured similar to the VGG-16 network <ref type="bibr" target="#b21">[22]</ref>. Each convolutional unit consists of two or three 3 × 3 convolutional layers and one max-pooling layer. The fifth convolutional unit is connected by a global max pooling layer instead of a max pooling layer. These convolutional layer produces a feature map of size 1 × 1 × 512. The feature map is connected to the FC layer, which is separated by two output layers. The first output layer is the classification layer, while the second output layer is the bounding box regression layer. This output layer architecture is taken from Faster R-CNN <ref type="bibr" target="#b20">[21]</ref>. For the network training, the loss (L d ) is defined as follows:</p><formula xml:id="formula_0">L d = L cls d + L bbox d<label>(1)</label></formula><p>where L cls d is the classification loss, i.e. softmax-log loss, and L bbox d is the bounding box regression loss, i.e. smooth L1 loss.</p><p>Also, we add three convolutional layers and five deconvolutional blocks in the saliency network since the last pooling </p><formula xml:id="formula_1">Layer Filter Size (w × h) Output etc. pool 5 2 × 2 18 × 25 512 ← conv 5-3, ↓ conv 6-1 3 × 3 18 × 25 512→1024 conv 6-2 3 × 3 18 × 25 1024 conv 6-3 3 × 3 18 × 25 1024 upsample 1 - 35 × 49 1024 size ↑ conv 7-1 3 × 3 35 × 49 1024→512 conv 7-2 3 × 3 35 × 49 512 upsample 2 - 69 × 97 512 size ↑ conv 8-1 3 × 3 69 × 97 512 → 256 conv 8-2 3 × 3 69 × 97 256 upsample 3 - 137 × 193 256 size ↑ conv 9-1 3 × 3 137 × 193 256→128 upsample 4 - 273 × 385 128 size ↑ conv 10-1 3 × 3 273 × 385 128→64 upsample 5 - 545 × 769 64 size ↑ conv 11-1 3 × 3 545 × 769 64 → 32 conv 11-2 3 × 3 545 × 769 32 → 1</formula><p>layer in the detection sub-network to get saliency maps for pedestrians. The deconvolutional block consists of one bilinear upsampling layer, one or three convolutional units. The layer configuration of the deconvolution block for the saliency network is described in <ref type="table" target="#tab_1">Table I</ref>. In the last deconvolution block, the output value is limited to 0 to 1 using sigmoid function. For the network training, we calculate the saliency loss L s by simple Euclidean distance from the ground truth.</p><p>For detection proposals, we train the detection sub-network jointly with the saliency network by optimizing the following combined loss function:</p><formula xml:id="formula_2">L = L d + L s<label>(2)</label></formula><p>where L d and L s are losses of the detection network and of the saliency network, respectively.</p><p>Detection Proposal: We use Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> to extract detection proposals for pedestrians. However, the detection results include some false positives such as vehicle parts, trees, and post lamps. To remove them, we apply different weights to the background and foreground so that the detector focuses on the pedestrian area. To determine the weight, we obtain pedestrian saliency maps using the saliency network from the input image. We update the class probability (score) using saliency map as follows:</p><formula xml:id="formula_3">f w (b) = f (b) * w f<label>(3)</label></formula><p>The weight w f is defined as follows:</p><formula xml:id="formula_4">w f = { 1 if f (b) &gt; th b 1 N x,y∈b s(x, y) otherwise,<label>(4)</label></formula><p>where b is bounding boxes of proposals, s(x, y) is saliency scores in the position (x, y), and f (b) is class scores of the selected bounding box. th b is the threshold value for distinguishing between foreground and background. The new . Then, we use nonmax suppression (NMS) <ref type="bibr" target="#b20">[21]</ref> to determine the final detection proposal samples. <ref type="figure">Fig. 4</ref> shows some examples of the detection proposal samples generated by the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alignment Framework</head><p>Network Architecture: The second stage is to align the bounding box using part-level detector. Our part-level detector is a combination of one root detector which detects root position of pedestrians and three part-level detectors which detect human body parts of head, torso, and legs. The root/part detector networks are configured similar to VGG-16 network. As shown in <ref type="figure" target="#fig_3">Fig 5,</ref> the alignment sub-network has two output layers: One is the output layer to obtain FCN and the other is the output layer to obtain CAM with global average pooling.</p><p>Our root-detector produces confidence score and root position for detection proposals. Bounding box alignment is performed on the root detector, and we treat this updated position of the aligned bounding box as an anchor position, i.e. the final position. Similarly, part confidence score and part position are produced by each part-level detector. Note that the part detection stage is implemented based on the updated position. Theoretically, bounding box alignment helps the proposed detector by better detection proposals as well as recall the lost body parts which is out of the ground truth. We compute a weighted sum of the confidence scores with a spatial distance penalty term as the final confidence score of a detection proposal.</p><p>Converting CNN into FCN/CAM: In general, detectors suffer from low detection IoU such as R-CNN, which causes poor localization quality of the detection proposals. In this work, we name it as the proposal shift problem. Hosang et al. <ref type="bibr" target="#b13">[14]</ref> reported that the best detection proposal method Spa-tialPooling+ <ref type="bibr" target="#b22">[23]</ref> recalled 93% samples with 0.5 IoU threshold while only recalling 10% samples with 0.9 IoU threshold. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> clustered all false positives in 3 categories, and localization quality is one of the main source of false positives. Detection proposals shift the position of samples by various direction and distance. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, body parts frequently appear out of the region of the detection proposal, which leads to bad detection response: low confidence score and/or IoU. Thus, we introduce a novel technique based on FCN and CAM to align the bounding boxes. According to the response of FCN and CAM, we generate much larger heat maps. Then, we predict the new position of pedestrians.</p><p>To perform bounding box alignment, a larger detection region is needed as the input of the detector. In this larger detection region, our root detector outputs a coarse position of a pedestrian. We simply convert root/part networks into FCN version and generate root/part CAM to get coarse position information, named as root/part-net. In root/part-net, the last pooling layer is fully connected with FC1 by an inner product weight matrix. Thus, the size of the input image is supposed to be fixed. With the trained root/part-net, we change the shape and dimension of the parameters between the last pooling layer and FC1 to make these weight matrix convolute on the large feature map. By expanding 25% from the size of bounding box and changing the size of the input image to 160 × 96, we obtain a confidence score heat map (C f cn ) of the size 5×3. According to the study on visualizing deep learning <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, the deeper the layers, the more abstract the information extracted. That is, the object neurons respond to transform simple edges to advanced information. We use the advanced information to identify categories in input images <ref type="bibr" target="#b26">[27]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the global average pooling (GAP) produces the average space value of the attribute map of each unit in the 4th convolutional layer, and uses the weighted sum of the attribute values to output the final object position. The weighted sum of confidence class activation map (C cam ) is as follows:</p><formula xml:id="formula_5">C cam = x,y k w c k f k (x, y)<label>(5)</label></formula><p>where f k (x, y) denotes the activation of the unit k in the 4th convolutional layer for the input images, and w c k is the weighted value corresponding to the class position in the unit k. Based on the previous research <ref type="bibr" target="#b26">[27]</ref>, it is expected that each unit in the convolutional layer is activated by the visual pattern within the receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift-and-Stitch for a Larger Confidence Map:</head><p>To predict a coarse position of a pedestrian in the large detection region, a higher resolution of C f cn and C cam are needed. We use a simple trick to obtain it as follows. Since there are total s = 32 pixels between every step, we shift the proposal windows by f steps on the horizontal and vertical axis uniformly and make total distance no more than 32 pixels. This means that the shift distance of every stride is s/f . Also, we take root-FCN as an example, and root-FCN generates a 5 × 3 heat map by every step interlacing all f 2 outputs according to the relative direction of every shift-and-stitch. As a result, a (5·f )×(3·f ) heat map is generated.</p><p>Once got a larger C f cn and C cam , we apply a simple upsampling method to produce a nice aspect ratio score heat map which equals to the aspect ratio of the input region. In this way, shift direction for the target position is calculated without a stretch operation. A coarse body position is estimated by selecting a region having the largest average value in the up-sampled C f cn and C cam . We use an enlarging ratio parameter L to determine the size of the target bounding box.</p><p>Width/height of the rectangle w/h is obtained by multiplying L with the width/height of the input region W /H.</p><formula xml:id="formula_6">w/h = L · W/H<label>(6)</label></formula><p>Define the coarse position in the input large region as (x p , y p ), the original position as (x o , y o ). Then, we update x by</p><formula xml:id="formula_7">∆x f cn = 2 × n i=1 (C t f cn,i − C o f cn,i ) 2 n i=1 C t f cn,i 2 + n i=1 C o f cn,i 2 * (x p − x o ) (7)</formula><p>where C t f cn,i is the value of the i-th element in the target rectangle in the confidence score heat map, C o f cn,i is the value of the i-th element in the original rectangle, and n is the total number of elements in the rectangles. ∆x cam , ∆y f cn , ∆y cam is obtained in the same way. The position of the detection proposal is updated by</p><formula xml:id="formula_8">x a = x o + ∆x f cn + ∆x cam 2 (8)</formula><p>y a is also updated in the same way. The updated position of the detection proposal (x a , y a ) is named as anchor position. Based on the anchor position (x a , y a ), our part-level detector is operated to yield part scores and part positions.</p><p>Part Merging: Part detection is considered in the alignment sub-network. The part detector has a different receptive size filter for the aligned BB generated by the root detector. Part score score p and part position (x p , y p ) that indicate the possibility and area the part appearance, respectively, are produced by each of the part detectors. The final detection score is defined as:</p><formula xml:id="formula_9">score = score root + i={parts} w i * (score i + P i ) (9)</formula><p>where score root is the output score of the body detector; score i is the output score of three body parts; w i is the weight that indicates the importance of part scores, and we set i={parts} w i = 1 in this work. P i is the penalty term of the spatial distance between anchor position and part position: where a and b are weights of the penalty term that balance the orientation and geometrical shifting distance; (x a , y a ) is the anchor position which is the position of an aligned detection proposal. For position of the detection, we simply use the anchor position as the final position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Target Labels for Training data: Currently, the datasets such as Caltech <ref type="bibr" target="#b27">[28]</ref>, INRIA <ref type="bibr" target="#b28">[29]</ref> and ETH <ref type="bibr" target="#b29">[30]</ref> do not provide part-level and saliency annotations. Inspired by <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, we have cropped all ground truth into three parts uniformly and assign their corresponding part labels automatically to generate training data for our part detectors. We have trained part detectors for three body parts of head, torso and legs. In Caltech pedestrian dataset, every frame in which a given sample is visible has two bounding boxes. One bounding box indicates the full extent of the entire body (BB-full), while the other is for visible region (BB-vis). For part detectors, we only select BB-vis for part division to avoid collecting background regions into positives. To generate training data for saliency, we draw a white rectangles in the black background using ground truth bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization and Settings for Training:</head><p>We have implemented the entire learning network using TensorFlow <ref type="bibr" target="#b30">[31]</ref>. We have performed the learning of the proposed network on a PC with NVIDIA GTX 1080ti of 11GB memory. We have initialized the parameters of convolutional units from VGG-16 <ref type="bibr" target="#b21">[22]</ref>, which is pre-trained on ImageNet dataset. If not belong to VGG-16 network, Xivier initialization method <ref type="bibr" target="#b31">[32]</ref> is used for the weight initialization of the proposed network. For optimization, we have used ADAM optimizer <ref type="bibr" target="#b32">[33]</ref> for learning with the learning rate 0.001 and the iteration epoch 15. Also, we avoid overfitting, and apply a dropout technique <ref type="bibr" target="#b33">[34]</ref> to the final fully-connected layer with the probability 0.5 for normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Benchmark</head><p>As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, we evaluate performance of the proposed method on three datsets: Caltech <ref type="bibr" target="#b27">[28]</ref>, INRIA <ref type="bibr" target="#b28">[29]</ref> and ETH <ref type="bibr" target="#b29">[30]</ref>.</p><p>Caltech-USA: This dataset <ref type="bibr" target="#b27">[28]</ref> consists of approximately 10 hours of 640 × 480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total 350,000 bounding boxes and 2,300 unique pedestrians have been annotated. We use every 3rd frame to extract training data followed by <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b15">[16]</ref>  annotations of pedestrian bounding boxes.</p><p>To evaluate the proposed pedestrian detection method, we mainly use a reasonable subset <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref> which contains pedestrians that have over 50 pixels height and over 65% visibility. We perform evaluations on the final output: List of detected bounding boxes with category scores. We use the standard parameter setting on Caltech dataset. We use log-average miss rate to evaluate the detector's performance computed by average miss rate at false positive per image (FPPI) rates evenly spaced in log-space in the range 10 −2 to 10 0 . The area that overlap with the ground truth exceeds 50% is set to the true as follows:</p><formula xml:id="formula_10">overlap = area(BB dt BB gt ) area(BB dt BB gt ) &gt; 0.5<label>(11)</label></formula><p>where BB dt and BB gt are detection bounding box and ground truth bounding box, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of Part-Level Detectors</head><p>We conduct a set of experiments on Caltech dataset to investigate the detection accuracy of the proposed method. We provide the performance of the pedestrian detection on saliency weights, shift handling, and part merging. When saliency weights are applied to the detection proposals, FPPI is 18.82% ('Proposed I' in <ref type="table" target="#tab_1">Table II</ref>). In comparison with the previous results, the saliency weights help to ensure the correct detection proposal as shown in <ref type="figure">Fig. 4</ref>). We also confirm that FPPI decreases 12.40% by solving the proposal shift problem when the bounding box alignment is applied ('Proposed II' in <ref type="table" target="#tab_1">Table II</ref>). We apply part-level detection to the larger detection region. Part-level detectors are able to recall the lost body parts beyond detection proposals. With the aligned anchor positions, part positions are more accurate by localizing the largest area with average scores. The spatial distance penalty term between anchor and part positions is very effective to consider the proposal shift problem.</p><p>We provide some successful detection results by adding saliency <ref type="figure" target="#fig_0">(Figs. 8 and 10</ref>), shift handling <ref type="figure" target="#fig_0">(Fig. 10)</ref>, and partlevel detector <ref type="figure">(Fig. 9</ref>). The saliency helps to distinguish background components similar to pedestrians. Without saliency, it is easy to falsely detect car parts (Figs. 8a and 8b) or trees (Figs. 8c and 8d) as pedestrians because cars or trees have similar shapes to pedestrians. The proposed method improves the detection performance by separating one box with two pedestrians <ref type="figure" target="#fig_0">(Fig. 10e</ref>) and detecting pedestrians blurred by motion <ref type="figure" target="#fig_0">(Fig. 10g)</ref>. Moreover, the proposed method recalls the lost body parts by bounding box alignment as shown in <ref type="figure" target="#fig_0">Figs. 10a-10d)</ref>. The part-level detector is able to detect partially-occluded or low-resolution pedestrians that the upper body is visible <ref type="figure">(Fig. 9a</ref>) and the body parts are occluded <ref type="figure">(Fig. 9b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with Other Deep Models</head><p>Caltech: We compare the performance of the proposed method with those of other deep models: JoinDeep <ref type="bibr" target="#b6">[7]</ref>, SDN <ref type="bibr" target="#b8">[9]</ref>, LDCF <ref type="bibr" target="#b15">[16]</ref>, TA-CNN <ref type="bibr" target="#b10">[11]</ref>, Checkerboards+ <ref type="bibr" target="#b16">[17]</ref>, and SA-FasterRCNN <ref type="bibr" target="#b34">[35]</ref>. <ref type="table" target="#tab_1">Table III</ref> shows performance comparison between different methods on Caltech dataset. The proposed method performs the second by 12.4% based on saliency and bounding box alignmen and achieves a slightly higher miss rate than SA-FasterRCNN <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INRIA:</head><p>We also conduct performance comparison on INRIA dataset with InformedHaee <ref type="bibr" target="#b35">[36]</ref>, LCDF <ref type="bibr" target="#b15">[16]</ref>, Franken <ref type="bibr" target="#b36">[37]</ref>, Roerei <ref type="bibr" target="#b9">[10]</ref>, and SA-FasterRCNN <ref type="bibr" target="#b34">[35]</ref>. <ref type="table" target="#tab_1">Table IV</ref> shows their performance on INRIA dataset. The  Method MR(%) JointDeep <ref type="bibr" target="#b6">[7]</ref> 39.3 SDN <ref type="bibr" target="#b8">[9]</ref> 37.9 CifarNet <ref type="bibr" target="#b13">[14]</ref> 28.4 LDCF <ref type="bibr" target="#b15">[16]</ref> 24.8 AlexNet <ref type="bibr" target="#b13">[14]</ref> 23.3 TA-CNN <ref type="bibr" target="#b10">[11]</ref> 20.9 Checkerboards+ <ref type="bibr" target="#b16">[17]</ref> 17.1 SA-FasterRCNN <ref type="bibr" target="#b34">[35]</ref> 9.7 Proposed 12.4 Method MR(%) InformedHarr <ref type="bibr" target="#b35">[36]</ref> 14.43 LDCF <ref type="bibr" target="#b15">[16]</ref> 13.79 Franken <ref type="bibr" target="#b36">[37]</ref> 13.70 Roerei <ref type="bibr" target="#b9">[10]</ref> 13.53 SA-FasterRCNN <ref type="bibr" target="#b34">[35]</ref> 8.04 RPN+BF <ref type="bibr" target="#b37">[38]</ref> 6.88 Proposed <ref type="bibr">10.34</ref> INRIA dataset is a group of people-centric data rather than on real roads in a complex environment, which is much different from ETH or Caltech. It includes various types of data covering body parts, and is suitable for performance evaluation of body part detection and pedestrian detection from complex backgrounds. We evaluate the performance of the proposed method with part-level detection. As shown in <ref type="table">Table.</ref> IV, the proposed method achieves comparable performance of 10.34% to state-of-the-arts in a partiallyocclusion dataset. Method MR(%) JointDeep <ref type="bibr" target="#b6">[7]</ref> 45 LDCF <ref type="bibr" target="#b15">[16]</ref> 45 Franken <ref type="bibr" target="#b36">[37]</ref> 40 Roerei <ref type="bibr" target="#b9">[10]</ref> 43 TA-CNN <ref type="bibr" target="#b10">[11]</ref> 35 RPN+BF <ref type="bibr" target="#b37">[38]</ref> 30 <ref type="bibr">Proposed 31.12</ref> ETH: ETH dataset is not a road environment, but it is worth assessing pedestrian detection performance by containing a large number of pedestrians. The proposed method shows a relatively low miss rate of 32.12%. We compare our detector with JointDeep <ref type="bibr" target="#b6">[7]</ref>, LCDF <ref type="bibr" target="#b15">[16]</ref>, Franken <ref type="bibr" target="#b36">[37]</ref>, Roerei <ref type="bibr" target="#b9">[10]</ref>, TA-CNN <ref type="bibr" target="#b10">[11]</ref> and RPN+BF <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_6">Table V</ref> shows performance comparison between them on ETH dataset. As shown in the table, the proposed method performs the second in MR (RPN+BF is the best) and achieves comparable performance to state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have proposed part-level CNN for pedestrian detection using saliency and boundary box alignment. We have used saliency in the detection sub-network to remove false positives such as lamp posts and trees. We have utilized boundary box alignment in the alignment sub-network to recall the lost body parts. We have generated confidence maps using FCN and CAM, and estimated accurate position of pedestrians based on them. Experimental results demonstrate that the proposed method achieves competitive performance on Caltech, INRIA, and ETH datasets with state-of-the-art deep models for pedestrian detection in terms of MR.</p><p>In our future work, we will investigate pedestrian detection in low light condition such as night time with the help of near infrared (NIR) data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Proposal shift problem in pedestrian detection. The colored boxes are the detection proposals, while the black boundaries are their ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Whole framework of the proposed method. The proposed pedestrian network consists of two sub-networks: detection and alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Architecture of the proposed detection sub-network. Examples of detection proposal with saliency weight. (a) Input image, (b) detection proposal (w/o w f ), (c) NMS result of (b), (d) Saliency map, (e) detection proposal (with w f ), (f) NMS result of (e) class score f w (b) is calculated by the product of the weight value w f and the bounding box score f (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Network architecture of the proposed part-level detector based on VGG-16 network with class activation map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Pipeline for bounding box alignment. Origin: Original bounding box. The pedestrian is localized at the top left corner of a bounding box. Extend: Enlarged bounding box. Confidence map: Output of FCN and CAM. Better: Aligned bounding box. The lost head part is recalled and thus the pedestrian is accurately localized. P = a * (|x p −x a |+|y p −y a |)+b * (|x p −x a | 2 −|y p −y a | 2 ) (10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Three datasets for experiments. (a) Caltech-USA. (b) INRIA. (c) ETH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Some successful detection results. The left and right images show the detection results of 'basic (without saliency)' and 'proposed (with saliency)', respectively. Blue box: False positive. Best viewed in color. Some successful detection results. The left and right images show the detection results of Basic (without saliency) and Proposed (with "Saliency + Shift Handling + Part Detectors"). Green box: True positive. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Successful detection results by the proposed method. The left and right images show the detection results of Basic (without saliency) and Proposed (with "Saliency + Shift Handling"), respectively. Blue box: Basic detection result. Green box: Proposed detection result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(No. 61872280) and the International S&amp;T Cooperation Program of China (No. 2014DFG12780). I. Yun and J. Kim are with the College of Information and Communication Engineering, Sungkyunkwan University, Suwon, Gyeonggi 16419, Korea email: jkkim@skku.edu. A. O. Hero is with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109-2122, USA e-mail: hero@umich.edu.</figDesc><table /><note>X. Wang and C. Jung are with the School of Electronic Engineering, Xidian University, Xian, Shaanxi 710071, China e-mail: zhengzk@xidian.edu.cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Layer configuration of the deconvolutional block for the saliency network. Input size: 600 × 800. Change of in/out channels: →. Change of layer size: ↓, ↑. Data flow: ←.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This dataset [29] consists of 1,382 training images and 288 testing images taken from a personal digital image collections or the web using Google images. Only upright person (with person height &gt; 100 pixels ) have been annotated. The original positive images are of very high resolution (approximately 2592 × 1944 pixels), and thus we have cropped these images to highlight persons. Our model is trained with all training images and evaluated on the 288 testing images. ETH: This dataset [30] consists of 1,450 training images and 354 testing images with a resolution of 640 × 480 (bayered). The dataset provides the camera calibration and</figDesc><table><row><cell>. The</cell></row><row><cell>4,024 standard testing dataset (sampling every 30th frame</cell></row><row><cell>from test videos) are evaluated.</cell></row><row><cell>INRIA:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Performance evaluation on Caltech dataset (Unit=%). Proposed I: "Detection Proposal + Saliency". Proposed II: "Proposed I + Shift Handling + Part Detectors".</figDesc><table><row><cell>Subset</cell><cell>[1]</cell><cell>Proposed I</cell><cell>Proposed II</cell></row><row><cell>Reasonable</cell><cell>22.52</cell><cell>18.82</cell><cell>12.40</cell></row><row><cell>Scale=Large</cell><cell>8.87</cell><cell>8.70</cell><cell>4.50</cell></row><row><cell>Scale=Near</cell><cell>11.96</cell><cell>10.98</cell><cell>6.03</cell></row><row><cell>Scale=Medium</cell><cell>65.54</cell><cell>53.71</cell><cell>53.71</cell></row><row><cell>Occ=None</cell><cell>19.69</cell><cell>16.03</cell><cell>11.43</cell></row><row><cell>Occ=Partial</cell><cell>43.74</cell><cell>36.32</cell><cell>16.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison between different methods on Caltech dataset (MR: Miss rate).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparison between different methods on INRIA dataset (MR: Miss rate).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Performance comparison between different methods on ETH dataset (MR: Miss rate).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Part-level fully convolutional networks for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>the IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2267" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3258" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling mutual visibility relationship in pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3222" to="3229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3666" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5079" to="5087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC 2015</title>
		<meeting>BMVC 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08160</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4073" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1243" to="1257" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;08)</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1505" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
