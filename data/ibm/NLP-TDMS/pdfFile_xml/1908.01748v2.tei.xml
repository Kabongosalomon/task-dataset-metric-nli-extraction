<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SqueezeNAS: Fast neural architecture search for faster semantic segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Shaw</surname></persName>
							<email>albert@deepscale.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepScale Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hunter</surname></persName>
							<email>daniel@deepscale.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepScale Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
							<email>forrest@deepscale.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepScale Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Sidhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepScale Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SqueezeNAS: Fast neural architecture search for faster semantic segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For real time applications utilizing Deep Neural Networks (DNNs), it is critical that the models achieve high-accuracy on the target task and low-latency inference on the target computing platform. While Neural Architecture Search (NAS) has been effectively used to develop low-latency networks for image classification, there has been relatively little effort to use NAS to optimize DNN architectures for other vision tasks. In this work, we present what we believe to be the first proxyless hardware-aware search targeted for dense semantic segmentation. With this approach, we advance the state-of-the-art accuracy for latency-optimized networks on the Cityscapes semantic segmentation dataset. Our latency-optimized small SqueezeNAS network achieves 68.02% validation class mIOU with less than 35 ms inference times on the NVIDIA Xavier. Our latency-optimized large SqueezeNAS network achieves 73.62% class mIOU with less than 100 ms inference times. We demonstrate that significant performance gains are possible by utilizing NAS to find networks optimized for both the specific task and inference hardware. We also present detailed analysis comparing our networks to recent state-of-the-art architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Motivation</head><p>In recent years, Deep Neural Networks (DNNs) have become a dominant approach for solving numerous problems in computer vision. Image classification tasks such as Ima-geNet <ref type="bibr" target="#b0">[1]</ref> and CIFAR10 <ref type="bibr" target="#b1">[2]</ref> are the de facto "playground" for designing DNN model architectures. When developing DNNs for a target task other than image classification (e.g. semantic segmentation or object detection), a popular approach is to use architecture-transfer: start with an image classification network and append a few task-specific layers to the end of the network. <ref type="bibr" target="#b0">1</ref> We believe architecture-transfer has become mainstream because of a number of conventional-wisdom assumptions that have permeated the computer vision community. In the following, we enumerate these assumptions and present evidence for why these assumptions are becoming outdated.</p><p>• Assumption 1: The most accurate neural network for ImageNet image classification will also be the most accurate backbone for the target task. Reality: ImageNet accuracy is only loosely correlated with accuracy on a target task. For example, SqueezeNet is a small neural network that achieves significantly lower ImageNet classification accuracy than VGG <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>. However, SqueezeNet is more accurate than VGG when used for the task of identifying similar patches in a set of images <ref type="bibr" target="#b4">[5]</ref>. Thus, the right DNN design varies depending on the target task. • Assumption 2: Neural Architecture Search (NAS) is prohibitively expensive. Reality: It is true that some NAS methods based on genetic algorithms (e.g. <ref type="bibr" target="#b5">[6]</ref>) or reinforcement learning (e.g. <ref type="bibr" target="#b6">[7]</ref>) often require thousands of GPU days to converge on a good DNN design because they train hundreds or thousands of different DNNs before converging. However, recent "supernetwork" approaches such as DARTS <ref type="bibr" target="#b7">[8]</ref> and FBNet <ref type="bibr" target="#b8">[9]</ref> have turned the problem inside out. They can train one supernetwork that contains millions of DNN designs, but it still converges on an optimal DNN design within 10 GPU days. So, the "right" DNN design depends on the target task, and modern NAS methods can quickly converge on the right DNN for a task. A similar issue arises when we look at choosing the right DNN for a target computing platform (e.g. a specific version of a CPU, GPU, or TPU):</p><p>• Assumption 3: Fewer multiply-accumulate (MAC) operations will yield lower latency on a target computing platform.</p><p>Reality: In a recent study, Almeida et al. showed that two DNNs with the same number of MACs can have a 10x difference in latency on the same computing platform <ref type="bibr" target="#b9">[10]</ref>. Further, when the FBNet authors optimized networks for different smartphones, they found a DNN that ran fast on the iPhone X, but slow on the Samsung Galaxy S8; as well as a DNN ran fast on the iPhone, but slow on the Samsung <ref type="bibr" target="#b8">[9]</ref>. Depending on the processor and the kernel implementations, different convolution dimensions run faster or slower, even when the number of MACs is held constant.</p><p>To make use of these new realities, we propose a playbook for producing the lowest-latency, highest-accuracy DNNs on a target task and a target computing platform:</p><p>1. Run Neural Architecture Search directly on the target task (e.g. object detection or semantic segmentation), and not on a proxy task (e.g. image classification). 2 2. Use modern supernetwork-based NAS, and enjoy the fact the search converges quickly. 3. Configure the NAS to optimize for both accuracy (on the target task) and latency (on the target platform).</p><p>In the rest of this paper, we investigate the effectiveness of this playbook by doing a proxyless search using the Cityscapes semantic segmentation dataset <ref type="bibr" target="#b10">[11]</ref>, targeting low-latency inference on the NVIDIA Xavier embedded GPU computing platform <ref type="bibr">[12]</ref>, and producing fast and accurate DNNs. We refer to the optimized DNNs generated in this study as SqueezeNAS networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Semantic Segmentation</head><p>Semantic segmentation is the computer vision task of assigning a class for each pixel in a given image. It is a workhorse in many computer vision applications areas, from automotive (segmenting the road and lane lines) to aerial imagery analysis. To train and evaluate semantic segmentation models, a number of datasets have been developed such as Cityscapes <ref type="bibr" target="#b10">[11]</ref>, ADE20k <ref type="bibr" target="#b12">[13]</ref>, NYUDv2 <ref type="bibr" target="#b13">[14]</ref>, and PAS-CAL VOC <ref type="bibr" target="#b14">[15]</ref> which have made the research in semantic segmentation algorithms much more accessible.</p><p>DNNs initially found success with image classification tasks; AlexNet <ref type="bibr" target="#b15">[16]</ref> and its successors dramatically increased the state-of-the-art accuracies on the ImageNet and CI-FAR10 classification tasks. Following this success, Long et al. developed Fully Convolutional Networks for Semantic Segmentation <ref type="bibr" target="#b16">[17]</ref> (FCN) by utilizing an Imagenet backbone -achieving then state-of-the-art performance on VOC PAS-CAL and NYUDv2. DeepLab <ref type="bibr" target="#b17">[18]</ref> later leveraged dilated convolutions to further increase the accuracy on segmentation benchmarks. The typical workflow of these approaches is to start with an image classification DNN and then adapt it for higher resolution, increasing the compute proportionally to the number of pixels. This part is usually called the encoder or backbone. The semantic segmentation network's decoder uses the low resolution feature maps from the encoder to perform more computation and generates an output prediction for each pixel that is the same size as the input resolution. This decoder or "head" can be a series of deconvolutions like in FCN, or something much more complex like the dilated Spatial Pyramid Pooling (ASPP) module seen in the DeepLab <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> Family.</p><p>Semantic segmentation, however, is a very different task from image classification. One way semantic segmentation networks differs from image classification networks is that they usually requires much higher resolution inputs to get good results. Image classification networks commonly use an input at a 224x224 resolution, while segmentation networks often use more than 40 times the number of pixels. Segmentation networks also typically have exotic architectures due to the fact that they have a dense high resolution output. Large input resolutions also means that segmentation networks often use trillions of Multiply-Accumulates (MACs) for a single image prediction, whereas accurate image classification networks are usually in the tens of billions. Many early deep learning approaches focused on maximizing accuracy, without a regard to the number of operations or latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Efficient Network Design</head><p>From 2012 to 2016, a substantial portion of the computer vision research community focused on designing DNNs that achieved the highest possible accuracy on image classification. These networks were then modified and finetuned to perform other tasks such as object detection and semantic segmentation. This led to significant year-over-year improvements in accuracy on image classification (from AlexNet <ref type="bibr" target="#b15">[16]</ref>, to ZFNet <ref type="bibr" target="#b20">[21]</ref>, to VGGNet <ref type="bibr" target="#b3">[4]</ref>, to ResNet <ref type="bibr" target="#b21">[22]</ref>), which further led into improved accuracy on the other computer vision tasks. This also led to an upward trend in computation time as well as parameter count. To mitigate this, starting in 2016 with SqueezeNet <ref type="bibr" target="#b2">[3]</ref>, Iandola et al. were successfully able to design networks that were 50 times smaller in parameters compared to AlexNet <ref type="bibr" target="#b15">[16]</ref>. MobileNets <ref type="bibr" target="#b22">[23]</ref> and ShuffleNet <ref type="bibr" target="#b23">[24]</ref> came soon after, optimizing their networks to have fewer computational operations, with the goal of reducing latency. The problem of reducing the size, the number of operations, and ultimately the latency of DNN inference became a widely-studied problem in computer vision research. One thing to note is that this research typically requires expertise in both computer vision as well as computer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural Architecture Search (NAS)</head><p>Since classification networks have commonly been used as the encoder for other computer vision tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, they are often a target of NAS searches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> in efforts to exceed the performance of expert designed networks. However, many prior NAS works such as some that use Reinforcement Learning or Evolutionary search algorithms can often require thousands of GPU days per search <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6]</ref>. The compute time of these searches would further increase if they were run directly on these high resolution vision tasks. Howard et al. in MobileNetV3 <ref type="bibr" target="#b35">[36]</ref> Image Low Level Features ....</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Segmentation Predictions</head><p>Searched Encoder <ref type="figure">Figure 1</ref>: General Encoder-Decoder Structure of our Segmentation Networks. We search the architecture space of the "Searched Encoder". We use either an ASPP <ref type="bibr" target="#b18">[19]</ref> inspired decoder or the LR-ASPP Decoder depending on the search space. created networks for semantic segmentation by modifying classification networks that were produced by NAS. The NAS in that work had the objective of minimizing latency of the low resolution image classification network for mobile phones, and not for our ultimate goal of semantic segmentation at high resolution.</p><formula xml:id="formula_0">× × ( ⋅ ) i n 1x1 (group) Conv, Relu ( / ) × ( / ) × ( ⋅ ) i n KxK DWConv (Dilated), Relu ( / ) × ( / ) × ( ⋅ ) o u t 1x1 (group) Conv × × i n +</formula><p>Many works have developed methods to greatly reduce the search time of NAS <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Recently, supernetworkbased NAS approaches have been proposed which have led to search times that are orders of magnitude faster by searching over millions of potential DNN designs while training just one supernetwork <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. While there has been some work searching directly on other vision tasks, most of these do not also directly optimize for hardware latency <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>. In our work described later in this paper, a gradient-based NAS method optimizes a supernetwork for both high semantic segmentation accuracy as well as low latency on our target hardware. Our particular NAS algorithm utilizes the Gumbel-Softmax <ref type="bibr" target="#b38">[39]</ref> approximation of the categorical choice distribution which is also used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture Search Space</head><p>In this work, we explore the space of encoders for semantic segmentation networks consisting of sequential Inverted Residual Blocks <ref type="bibr" target="#b39">[40]</ref>. The blocks are parameterized as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In each architecture search, we constrain the macro-architecture and find optimal parameters for each block. This search space was chosen to be similar to the FBNet <ref type="bibr" target="#b8">[9]</ref>, MobileNetV2 <ref type="bibr" target="#b39">[40]</ref>, and MobileNetV3 <ref type="bibr" target="#b35">[36]</ref> network families which allows us to directly compare our segmentation optimized networks to their classification optimized networks.</p><p>The general structure of all our networks is shown in <ref type="figure">Figure</ref> 1. We follow a common structure of some segmentation networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref> where the decoder uses both the final output features from the encoder as well as a low level feature map from an earlier layer in the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Constrained Macro-Architecture</head><p>In our experiments we searched 3 search spaces: Small, Large, and XLarge. To define each of these architecture spaces, we first constrain the macro-architecture of the encoder networks. The macro-architectures describe the total number of blocks N in the encoder, which decoder is used, and which layer our lower level features come from. For each block, we fix the input and output channels(C in and C out ) and whether each block uses a stride of s = 1 or s = 2 in the depthwise convolution layer. It should be noted that since we allow each block to choose a no-op skip connection, the final layer count can be less than N .</p><p>The specifics of each of the three search spaces are shown in Appendix C. They were chosen to be comparable to the MobileNetV2 <ref type="bibr" target="#b39">[40]</ref> and MobileNetV3 <ref type="bibr" target="#b35">[36]</ref> segmentation networks. In the Small and Large search spaces, we use the LR-ASPP <ref type="bibr" target="#b35">[36]</ref> decoder. In the XLarge search space, we use the variation of the ASPP decoder with fully depthwise convolutions proposed in Chen et al. <ref type="bibr" target="#b19">[20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Block Search Space</head><p>Within each macro-architecture space, our NAS picks the optimal hyperparameters for each block or replaces it with a no-op skip connection. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, these hyperparameters define whether the 1x1 convolutions are grouped, whether the depthwise convolution is dilated with a rate 2, the size of the kernel k for the depthwise convolution, and the expansion ratio e. We choose from 12 possible configurations as shown in <ref type="figure">Figure 7</ref> and Appendix B as well as the skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Neural Architecture Search Algorithm</head><p>The particular approach and search space we use is similar to those used in <ref type="bibr" target="#b8">[9]</ref>. We consider architecture search as a path-selection problem within a stochastic supernetwork such that any particular architecture in our search space is represented by some path through our supernetwork. As illustrated in <ref type="figure">Figure 3</ref>, we define our supernetwork to be a sequence of superblocks that each contain the candidate block choices. Running inference for a sampled architecture of the stochastic supernetwork is shown in <ref type="figure">Figure 4</ref>.</p><p>We simultaneously co-optimize the convolutional weights (w) and architecture parameters (θ) of the stochastic supernetwork to minimize our loss function which is defined as</p><formula xml:id="formula_1">L(θ, w) = L P (θ, w) + α * L E (θ)<label>(1)</label></formula><p>where L P represents the problem-specific loss, L E is resource aware-loss term, and the hyperparameter α controls the tradeoff made between the two. As this work focuses on semantic segmentation, L P is a pixel-level cross-entropy loss. For L E we experiment with both the estimated total inference latency on our target-platform as well as the estimated number of Multiply-Accumulates for the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Gumbel-Softmax</head><p>In order to make computation and optimization of the stochastic supernetwork tractable, each superblock picks a candidate block independent of the choices of other superblocks. Thus, we can model the choice of a candidate block as sampling from an independent categorical distribution where the probability of choosing candidate block j for superblock i in the network is p(i, j). We define this probability using the softmax function on our architecture parameters (θ) for each superblock.</p><formula xml:id="formula_2">p(i, j|θ) = e θi,j 13 j e θi,j<label>(2)</label></formula><p>The categorical distribution is difficult to directly optimize efficiently, so we use the Gumbel-Softmax relaxation of the categorical distribution proposed in Jang et al. <ref type="bibr" target="#b38">[39]</ref>. Sampling from the Gumbel-Softmax distribution allows us to efficiently optimize the architecture distribution by using gradient descent on the stochastic supernetwork. The Gumbel-Softmax distribution is controlled by a temperature parameter t. As t approaches zero, the Gumbel-Softmax distribution becomes equivalent to the categorical distribution. The temperature parameter is annealed from 5.0 to 1.0 during our search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Early Stopping</head><p>A caveat of our supernetwork approach is that the optimization requires computation through every single candidate block for every iteration regardless of the learned architecture distribution. As optimal network architectures converge, the probability that a low performing candidate block is chosen decreases, but it still continues to use compute. So we use a compute optimization when the estimated probability of a candidate block being chosen is less than 0.5%. We simply remove it from the supernetwork. While there is some low probability that a removed candidate block could be optimal later in the search process, we have not seen this in practice. This compute optimization can cut search time in half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Resource-Aware Architecture Search</head><p>We define our resource aware loss as follows:</p><formula xml:id="formula_3">L E (θ) = N j 13 i p(i, j|θ i )C(i, j)<label>(3)</label></formula><p>C(i, j) represents the network resource cost of choosing candidate j in block i of the network. We model the resource cost of each block to be independent of others. C can also be implemented as a lookup table similar to FBNet <ref type="bibr" target="#b8">[9]</ref> so the resource costs only needs to be calculated once. Depending on how we build the lookup table, we can optimize for many different objectives ranging from hardware-agnostic metrics such as MACs or parameter size to hardware-aware costs like inference-time, memory accesses, or energy usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>We demonstrate two key ideas: first, Neural Architecture Search (NAS) is a powerful tool that can yield high-accuracy, low-latency networks. The second idea is that optimizing for hardware-agnostic metrics such as Multiply-Accumulates (MACs) is not an ideal proxy and can lead to sub-optimal latency results.</p><p>To demonstrate this, we use search spaces similar to prior work: the Small, Large, XLarge search spaces, which we define in Section 3.1. We first use our NAS method along with a hardware-agnostic objective (MACs) to generate a semantic segmentation network in each of our search spaces. These networks are comparable with current state-of-theart networks on the MACs/Accuracy trade-off curve. We then measure the latency of these low-MAC networks on an embedded platform (NVIDIA Xaiver) as a baseline. Finally, we use our NAS method again on the same search spaces, but optimize with a hardware-aware objective (latency) to find 3 new networks targeted at similar latencies of the networks generated in the previous search.</p><p>All search experiments are done on the Cityscapes[11] semantic segmentation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hardware-Agnostic Search</head><p>For our hardware-agnostic architecture searches, we apply our NAS method with a Multiply-Accumulates (MACs) minimization objective to create networks that are on the pareto-optimal tradeoff curve of MACs vs mIOU. To implement this, for each block i in the network, we compute the number of Multiply-Accumulates for each candidate block j and store the results in the lookup table C such that C(i, j) = M ACS i,j .</p><p>We then perform an independent search in each of the three search spaces and obtain three MAC-optimized SqueezeNAS-MAC networks. As shown in <ref type="table">Table 1</ref>, we achieve results that exceed the performance of prior work without NAS. We also achieve comparable results with MobileNetV3 <ref type="bibr" target="#b35">[36]</ref> w.r.t the number of MACs. We finally measure the inference time of the 3 networks on a NVIDIA Xavier using cuDNN 7.3.1. As in many applications requiring real-time inference, we use batch size = 1 for all of our latency tests throughout the paper. The results can be seen in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Hardware-Aware Search</head><p>Our hardware-aware searches use the same NAS algorithm and architectural search space as the hardwareagnostic approach, but now we use a latency minimization objective for the resource-aware loss; formulated as C(i, j) = Latency i,j . To compute the latency of every candidate j in each block i, we measure the inference time of all candidates on our target platform. We conduct 3 new independent hardware-aware searches that target the latencies measured from the hardware-agnostic networks. The results of these searches yield the three SqueezeNAS-LAT networks. Our hardware-aware searches find networks that have significantly higher accuracies at the same or lower latency compared to the hardware-agnostic networks seen in <ref type="table">Table 1</ref>. The latency-optimized networks have a higher number of MACs, but they still run faster on our target device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Architecture Search</head><p>In our supernetwork-based architecture search, we train directly on the Cityscapes training set, without using any proxy task. After we finish optimizing the supernetwork, we sample 200 discrete architectures from the optimal architecture distribution. We estimate the performance of each architecture by running inference on the Cityscapes fine validation dataset using the architecture path within the supernetwork as shown in <ref type="figure">Figure 4</ref>. After validating the 200 architectures, we choose one from this estimated pareto-optimal frontier and retrain the singular architecture. The MACoptimized networks are chosen to have comparable MACs to the MobileNetV3 segmentation networks, and the Latencyoptimized networks are chosen to have inference latencies comparable with our MAC-optimized baseline networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Training Details</head><p>For comparability with other results, we follow a similar pretraining scheme to that used in <ref type="bibr" target="#b19">[20]</ref>. After the architecture search is complete, we pretrain our sampled networks on ImageNet classification using the training regime used in ResNet <ref type="bibr" target="#b21">[22]</ref>. We then do a stage of training on COCO <ref type="bibr" target="#b42">[43]</ref> segmentation masks using the scheme used in  DeepLabV3+ <ref type="bibr" target="#b19">[20]</ref>. Then, we train on the Cityscapes coarse training set annotations for 40 epochs, and finally we train on the Cityscapes fine training set annotations for 100 epochs, cutting the learning rate by 10 at 50 and 75 epochs. All segmentation training uses patch sizes of 768x768 pixels and are optimized with SGD with momentum, using a base learning rate of 0.05 and a weight-decay of 1e-5. We use servers with 8 Nvidia Turing GPUs with 24GB of VRAM and train in mixed precision, allowing us to both leverage the tensor cores on the GPUs and fit a larger batch in VRAM. When we search larger supernetworks, we employ Synchronized BatchNorm <ref type="bibr" target="#b43">[44]</ref> to keep our BatchNorm <ref type="bibr" target="#b44">[45]</ref> batch sizes large enough for training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results</head><p>First, our hardware-agnostic NAS method is able to produce networks that are competitive with the state-of-theart with respect to both MACs and latency. Compared to expert designed networks found without NAS such as EDANet <ref type="bibr" target="#b41">[42]</ref> and MobileNetV2 <ref type="bibr" target="#b39">[40]</ref>, our MAC-optimized networks achieve higher accuracy at a fraction of the MACs, as shown in <ref type="table">Table 1</ref>. Our SqueezeNAS-MAC-Small network achieves more than 3% higher absolute mIOU compared to the EDANet <ref type="bibr" target="#b41">[42]</ref> segmentation network, which has three times more MACs than ours. Our SqueezeNAS-MAC-Large network achieves more than 2.5% higher absolute mIOU compared to the MobileNetV2 <ref type="bibr" target="#b39">[40]</ref> segmentation network, which has more than double the MACs of our network.</p><p>Our hardware-aware networks all have higher accuracy while having less latency compared to their hardwareagnostic counterparts. The SqueezeNAS-LAT-Small network is 1.3% more accurate, 35% faster, and has 50% more MACs compared to SqueezeNAS-MAC-Small. The SqueezeNAS-LAT-Large network is 1.2% more accurate, 4% faster, and has more than double the number of MACs compared to SqueezeNAS-MAC-Large. This means that we're able to achieve double the number of operations in the same inference time window, as seen in <ref type="figure" target="#fig_4">Figure 10</ref>. This allows us to have much more expressive models that yield better accuracy while running at the same framerate.</p><p>We also compare our networks to the efficient segmentation networks proposed in MobileNetV3 <ref type="bibr" target="#b35">[36]</ref>. These networks were optimized for image classification using NAS and were then modified for the semantic segmentation task. The SqueezeNAS-MAC-Large network is able to match the accuracy of the MobileNetV3-Large network while using   less MACs as seen in <ref type="table">Table 1</ref>. It should be noted that the SqueezeNAS-MAC-Small network does perform worse than MobileNetV3-Small. However, the MobileNetV3 networks do use Squeeze-Excitation <ref type="bibr" target="#b45">[46]</ref> and Hard Swish <ref type="bibr" target="#b46">[47]</ref> activations which our networks do not. SqueezeNAS-LAT-Small runs 20% faster than MobileNetV3-Small while achieving an mIOU that is only 0.26% lower. SqueezeNAS-LAT-Large achieves over 1.2% higher accuracy with less than 6% higher latency.</p><p>We have noticed a small gap in our validation and test accuracies. This may be due to the small size of the Cityscapes dataset or the lack of our use of test-time augmentations.</p><p>The full validation set results are shown in <ref type="table">Table 1</ref>. Test set results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Each network was found in less than 15 GPU-days, which is more than 100 times less than some reinforcement learning and genetic search methods as shown in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Network Analysis</head><p>We now compare the block choices of the hardwareagnostic, hardware-aware, and MobileNetV3 segmentation networks. Since the three families all use the same Inverted Residual blocks, we can place MobileNetV3's building blocks into our 13 candidate blocks which can be seen in <ref type="figure">Figure 7</ref>. One caveat to note is that we are not accounting <ref type="bibr" target="#b2">3</ref> Approximated from TPUv2 Hours 4 Starts with a MnasNet network (search time is approximated from TPUv2 Hours) and adapts it with the NetAdapt NAS algorithm. The Ne-tAdapt search time is not included since it is not reported in the paper <ref type="bibr" target="#b35">[36]</ref>. <ref type="figure">Figure 7</ref>: Visualization of the search space. Each of these blocks represent a MobileNetV2 <ref type="bibr" target="#b39">[40]</ref> Inverted Residual block as seen in <ref type="figure" target="#fig_0">Figure 2</ref>. k represents the kernel size of the middle depthwise convolution layer. e represents the expansion multiple for the depthwise convolution. d represents the dilation rate of the depthwise convolution. g represents the number of groups(1 if not listed) in the 1x1 convolutions. Finally we have a no-op skip connection that can be chosen. for the Squeeze-Excitation <ref type="bibr" target="#b45">[46]</ref> blocks that are in some Mo-bileNetV3 blocks for visualization, and the expansion ratios are approximated to be either 1, 3, or 6.</p><p>We visualize the small networks in <ref type="figure" target="#fig_2">Figure 8</ref>. We first examine our SqueezeNAS-MAC-Small network and see that it uses a mix of low and high expansion blocks. It also uses the highest compute candidate block possible for its second and third downsampling blocks. The last thing to note is that our NAS method chose to use dilated 3x3 blocks for the last stage of the network. This is a very common trend that we see in expert designed, high resolution semantic segmentation networks such as DeepLabV3 <ref type="bibr" target="#b18">[19]</ref> and PSPNet <ref type="bibr" target="#b47">[48]</ref>.</p><p>The next small network we examine is our SqueezeNAS-LAT-Small, which is more accurate and lower latency than the previous network. A radical difference that we immediately see is that the network uses many more skip connections instead of low expansion blocks. This makes the macroarchitecture look very similar to that of MobileNetV3-Small, also visualized in <ref type="figure" target="#fig_2">Figure 8</ref>. Both networks do aggressive down-sampling and push their compute (via higher expansion ratios) later in the network, where the resolution is lower and the base channel count is higher. This yields a higher arithmetic-intensity. <ref type="bibr" target="#b4">5</ref> On devices like GPUs, which are typically memory bandwidth bound, higher arithmetic-intensity allows for more operations for the same memory bandwidth. It is interesting to see how both of the latency optimizing NAS methods produce similar networks that follow intuition from a computer architecture perspective. The networks differ in that our network uses more blocks but with a smaller kernel sizes near the end of the network. (3x3 dilated vs 5x5). Which is consistent with our hardware-agnostic network and other related segmentation work.</p><p>We now visually compare the large networks in <ref type="figure" target="#fig_3">Figure 9</ref>. Both SqueezeNAS-MAC-Large and SqueezeNAS-LAT-Large, follow the a trend similar to our smaller networks where they all have high compute down-sampling blocks, as well as heavy use of dilated convolutions in the second half of the networks. If we compare the MAC and latency networks, we see that the MAC network has the majority of its compute in the middle, whereas the latency network pushes its com- pute towards the end where it would yield a higher overall arithmetic-intensity for the network. This also has the sideeffect of more than doubling the total number of MACs but still decreasing latency. We can conclude with saying that our NAS method is effective at producing high-throughput networks while maintaining low latency as seen in <ref type="figure" target="#fig_4">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In Section 1, we presented a playbook for replacing architecture-transfer with neural architecture search to develop DNNs that are optimized for specific tasks and for specific computing platforms. After following this playbook throughout this paper, we have learned the following.</p><p>First, by doing a proxyless search on a semantic segmentation dataset, our NAS produced the SqueezeNAS family of models, which achieve superior latency-accuracy tradeoffs relative to MobileNetV3 on the semantic segmentation validation set. We attribute our superior results, at least in part, to the fact that the backbone of the MobileNetV3 semantic segmentation network was designed by NAS for the proxy task of image classification on mobile phones (that is to say, it was not designed in a proxyless manner for semantic segmentation on embedded GPU devices).</p><p>Second, while the MobileNetV3 authors searched for thousands of GPU days, our approach produced these results in 7 to 15 GPU days per search. In other words, modern supernetwork-based NAS can now produce state-of-the-art results in less than a weekend of search time on an 8 GPU server.</p><p>Third, recall that we did two sets of NAS experiments: one in which we searched for low-MAC models, and one where we searched for low-latency models on a target computing platform. We achieved substantially faster and more accurate models when searching for latency on the target platform. Finally, given the growing diversity of chips and computing platforms designed for deep neural networks, we believe that using NAS to optimize for low latency on a target computing platform will continue to grow in importance. The decoder used was the LR-ASPP proposed in Howard et al. <ref type="bibr" target="#b35">[36]</ref> with 128 channels in its layers. It uses the lower level feature from the last block with output stride 8. The "MAC Network Layer" and "Latency Network Layer" indicate the layers chosen by our MAC and Latency optimized Neural Architecture Searches Respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameters of Macro Search Spaces</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Diagram showing the architecture of the Inverted Residual blocks we use in our search space. They are parameterized so that the number of groups (g ∈ {1, 2}) in the 1x1 convolutions, the dilation rate of the depthwise convolution (d ∈ {1, 2}), the kernel size (k ∈ {3, 5}), and the expansion ratio (e ∈ 1, 3, 6) may vary for different candidate blocks. The 12 possible configurations are shown in shown in Figure 7 and Appendix B. C in , C out , and stride (s ∈ {1, 2}) are defined by the macro level parameters shown in Appendix C. A residual connection is used if C in = C out and s = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>MACs vs mIOU on Cityscapes validation set. SqueezeNAS MAC-optimized and latencyoptimized models compared to MobileNetV3<ref type="bibr" target="#b35">[36]</ref> segmentation models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Small Networks. Networks are lined up at their down-sampling block represented by the color red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Large Networks. Networks are lined up at their down-sampling block represented by the color red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of Throughput (GigaMACs per second) vs Latency, of SqueezeNAS networks and MobileNetV3[36] segmentation networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Diagram of a supernetwork with N superblocks, which each contain 13 possible candidate block choices. Diagram of an architecture path of a sampled architecture from a supernetwork. In this example, the 1st superblock uses candidate block 1, the 2nd superblock uses candidate block 3, and the N th superblock uses candidate block 2.</figDesc><table><row><cell></cell><cell cols="2">Super Block 1</cell><cell></cell><cell></cell><cell>Super Block 2</cell><cell></cell><cell></cell><cell>Super Blocks 3 to N</cell><cell cols="2">Super Block N</cell><cell></cell></row><row><cell></cell><cell cols="2">Choice 1</cell><cell></cell><cell></cell><cell>Choice 1</cell><cell></cell><cell></cell><cell></cell><cell>Choice 1</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell cols="2">Choice 2 Choice 3</cell><cell>Softmax</cell><cell>Gumbel</cell><cell>Choice 2 Choice 3</cell><cell>Softmax</cell><cell>Gumbel</cell><cell>.....</cell><cell>Choice 2 Choice 3</cell><cell>Softmax</cell><cell>Gumbel</cell><cell>Output</cell></row><row><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Choice 13</cell><cell></cell><cell></cell><cell>Choice 13</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Choice 13</cell><cell></cell></row><row><cell cols="2">Figure 3: Input</cell><cell cols="3">Choice 1 Choice 2 Choice 3 Super Block 1</cell><cell cols="2">Choice 1 Choice 2 Choice 3 Super Block 2</cell><cell cols="2">..... Super Blocks 3 to N</cell><cell>Choice 1 Choice 2 Choice 3 Super Block N</cell><cell>Output</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">...</cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Choice 13</cell><cell>Choice 13</cell><cell></cell><cell></cell><cell></cell><cell>Choice 13</cell><cell></cell><cell></cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test mIOU of Different Architectures on Cityscapes. The latency values were benchmarked on the NVIDIA Xavier on the 30 Watt power setting.</figDesc><table><row><cell>Architecture</cell><cell>Search Time (GPU Days)</cell></row><row><cell>NAS with RL[7]</cell><cell>22,400</cell></row><row><cell>NASNet[34]</cell><cell>2,000</cell></row><row><cell>MnasNet[35]</cell><cell>2,000 3</cell></row><row><cell>MobileNetV3[36]</cell><cell>&gt; 2,000 4</cell></row><row><cell>AmoebaNet[6]</cell><cell>3,150</cell></row><row><cell>FBNet[9]</cell><cell>9</cell></row><row><cell>DARTS[8]</cell><cell>4</cell></row><row><cell>SqueezeNAS MAC Small</cell><cell>7.0</cell></row><row><cell>SqueezeNAS MAC Large</cell><cell>9.7</cell></row><row><cell cols="2">SqueezeNAS MAC XLarge 14.6</cell></row><row><cell>SqueezeNAS LAT Small</cell><cell>8.7</cell></row><row><cell>SqueezeNAS LAT Large</cell><cell>9.4</cell></row><row><cell>SqueezeNAS LAT XLarge</cell><cell>11.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Search times of SqueezeNAS Networks compared to other NAS methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>C.1. Small Network Search SpaceEncoder Macro Architecture and found MAC/Latency optimized networks</figDesc><table><row><cell>Operator</cell><cell>C in</cell><cell>Cout</cell><cell>s</cell><cell cols="3">Output Stride MAC Network Layer Latency Network Layer</cell></row><row><cell>conv2d, 3x3</cell><cell>3</cell><cell>16</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Searched Inverted Residual</cell><cell>16</cell><cell>16</cell><cell>2</cell><cell>4</cell><cell>k3_d1_e1_g2</cell><cell>k3_d1_e1_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>16</cell><cell>16</cell><cell>1</cell><cell>4</cell><cell>k3_d1_e1_g1</cell><cell>skip</cell></row><row><cell>Searched Inverted Residual</cell><cell>16</cell><cell>16</cell><cell>1</cell><cell>4</cell><cell>k3_d1_e1_g1</cell><cell>skip</cell></row><row><cell>Searched Inverted Residual</cell><cell>16</cell><cell>16</cell><cell>1</cell><cell>4</cell><cell>k3_d1_e1_g2</cell><cell>skip</cell></row><row><cell>Searched Inverted Residual</cell><cell>16</cell><cell>24</cell><cell>2</cell><cell>8</cell><cell>k5_d1_e6_g1</cell><cell>k3_d1_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>24</cell><cell>24</cell><cell>1</cell><cell>8</cell><cell>k3_d1_e1_g1</cell><cell>k3_d1_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>24</cell><cell>24</cell><cell>1</cell><cell>8</cell><cell>k3_d1_e1_g1</cell><cell>skip</cell></row><row><cell>Searched Inverted Residual</cell><cell>24</cell><cell>24</cell><cell>1</cell><cell>8</cell><cell>k3_d2_e1_g2</cell><cell>skip</cell></row><row><cell>Searched Inverted Residual</cell><cell>24</cell><cell>40</cell><cell>2</cell><cell>16</cell><cell>k5_d1_e6_g1</cell><cell>k5_d1_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>40</cell><cell>40</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e6_g1</cell><cell>k3_d2_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>40</cell><cell>40</cell><cell>1</cell><cell>16</cell><cell>skip</cell><cell>k5_d1_e1_g2</cell></row><row><cell>Searched Inverted Residual</cell><cell>40</cell><cell>40</cell><cell>1</cell><cell>16</cell><cell>k3_d1_e1_g2</cell><cell>k3_d1_e1_g2</cell></row><row><cell>Searched Inverted Residual</cell><cell>40</cell><cell>48</cell><cell>1</cell><cell>16</cell><cell>k5_d1_e6_g1</cell><cell>k3_d2_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>48</cell><cell>48</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e3_g1</cell><cell>k3_d2_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>48</cell><cell>48</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e1_g1</cell><cell>k3_d2_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>48</cell><cell>48</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e1_g1</cell><cell>k3_d2_e1_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>48</cell><cell>96</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e6_g1</cell><cell>k3_d2_e6_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>96</cell><cell>96</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e1_g2</cell><cell>k3_d2_e3_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>96</cell><cell>96</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e1_g2</cell><cell>k3_d2_e3_g1</cell></row><row><cell>Searched Inverted Residual</cell><cell>96</cell><cell>96</cell><cell>1</cell><cell>16</cell><cell>k3_d2_e1_g2</cell><cell>k3_d2_e3_g1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our terminology, we refer to the task-specific end of the network as the head, and we refer to the portion of the network that was originally designed for image classification as the backbone.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If you wish to use outside data from an other task for pretraining, first perform a proxyless search to produce the DNN architecture, then reset the weights and do pretraining on outside data, and finally finetune on the target task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Arithmetic Intensity is the ratio of MACs to memory traffic<ref type="bibr" target="#b48">[49]</ref>. When arithmetic intensity drops below a certain threshold, the latency is dominated by the time to access data from memory.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Group   k3_d1_e1_g2  3  1  1  2  k3_d1_e1_g1  3  1  1  1  k3_d1_e3_g1  3  1  3  1  k3_d1_e6_g1  3  1  6  1  k3_d2_e1_g2  3  2  1  2  k3_d2_e1_g1  3  2  1  1  k3_d2_e3_g1  3  2  3  1  k3_d2_e6_g1  3  2  6  1  k5_d1_e1_g2  5  1  1  2  k5_d1_e1_g1  5  1  1  1  k5_d1_e3_g1  5  1  3  1  k5_d1_e6_g1  5  1  6  1</ref> <p>The decoder used was the LR-ASPP proposed in Howard et al. <ref type="bibr" target="#b35">[36]</ref> with 128 channels in its layers. For the lower level feature we used the output of the last block with output stride 8. The "MAC Network Layer" and "Latency Network Layer" indicate the layers chosen by our MAC and Latency optimized Neural Architecture Searches Respectively. The decoder used is the ASPP with fully depthwise convolutions proposed in Chen et al. <ref type="bibr" target="#b18">[19]</ref>. It uses the lower level feature from the last block with output stride 4. The "MAC Network Layer" and "Latency Network Layer" indicate the layers chosen by our MAC and Latency optimized Neural Architecture Searches Respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. XLarge Network Search Space</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNetlevel accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<ptr target="http://arxiv.org/abs/1602.07360" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>742. 1, 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EmBench: Quantifying performance variations of deep neural networks across modern commodity devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laskaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leontiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Venieris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Embedded and Mobile Deep Learning (EMDL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Jetson AGX Xavier developer kit</title>
		<idno>12] NVIDIA</idno>
		<ptr target="https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit2" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.25726832" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<ptr target="http://arxiv.org/abs/1606.009152" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<ptr target="http://arxiv.org/abs/1706.055872,3" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<ptr target="http://arxiv.org/abs/1704.048612" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rylqooRqK7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HylVB3AqYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian meta-network architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/1812.09584</idno>
		<ptr target="http://arxiv.org/abs/1812.09584" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/pham18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Concentrated-comprehensive convolutions for lightweight semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno>abs/1812.04920</idno>
		<ptr target="http://arxiv.org/abs/1812.049206" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient dense modules of asymmetric convolution for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1809.06323</idno>
		<ptr target="http://arxiv.org/abs/1809.063236" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1710.05941</idno>
		<ptr target="http://arxiv.org/abs/1710.059417" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Roofline: An insightful visual performance model for floatingpoint programs and multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lawrence Berkeley National Lab.(LBNL)</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
