<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Estimation using Deep Consensus Voting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ita</forename><surname>Lifshitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Estimation using Deep Consensus Voting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, with the resurgence of deep learning techniques, the accuracy of human pose estimation from a single image has improved dramatically. Yet despite this recent progress, it is still a challenging computer vision task and state-of-the-art results are far from human performance.</p><p>The general approach in previous works, such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">19]</ref>, is to train a deep neural net as a keypoint detector for all keypoints. Given an image I, the net is fed a patch of the image I y ⊂ I centered around pixel y and predicts if y is one of the M keypoints of the model. This process is repeated in a sliding window approach, using a fully convolutional implementation, to produce M heat maps, one for each keypoint. Structure prediction, usually by a graphical model, is then used to combine these heat maps into a single pose prediction. This approach has several drawbacks. First, most pixels belonging to the person are not themselves any of the keypoints and therefore contribute only limited information to the pose estimation process. Information from the entire person can be used to get more reliable predictions, particularly in the face of partial occlusion where the keypoint itself is not visible. Another drawback is that while the individual keypoint predictors use state-of-the-art classification methods to produce high quality results, the binary terms in the graphical model, enforcing global pose consistency, are based only on relative keypoint location statistics gathered from the training data and are independent of the input image.  <ref type="bibr" target="#b0">[1]</ref>. Each pose is represented as a stick figure, inferred from predicted joints. Different limbs in the same image are colored differently, same limb across different images has the same color.</p><p>To overcome these limitations, we propose a novel approach in which for every patch center y we predict the location of all keypoints relative to y, instead of classifying y as one of the keypoints. This enables us to use 'wisdom of the crowd' by aggregating many votes to produce accurate keypoint detections. In addition, by looking at agreements between votes, we infer informative imagedependent binary terms between keypoints. Our binary terms are generated by consensus voting -we look at a set of keypoints pairs, and for each possible pair of locations, we aggregate all votes for this combination. The total vote will be high if both keypoint locations get strong votes from the same voters. We show that this approach produces competitive results on the challenging MPII human-pose <ref type="bibr" target="#b13">[14]</ref> and the Leeds sports pose <ref type="bibr" target="#b8">[9]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human body pose estimation in still images is a challenging task. The need to cope with a variety of poses in addition to a large range of appearances due to different clothing, scales and light conditions gave rise to many approaches to dealing with the various aspects of this task. The most common approach is to use a keypoint detector combined with a pictorial structure for capturing relations between parts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">21]</ref>. In addition to the standard pictorial structure <ref type="bibr" target="#b6">[7]</ref>, poselet-based features were used to incorporate higher-order part dependencies <ref type="bibr" target="#b13">[14]</ref>. Alternative methods, like the chains-model <ref type="bibr" target="#b9">[10]</ref> replace the pictorial structure with a set of voting chains, starting at the head keypoint and ending at the hand.</p><p>With the reappearance of convolutional neural nets, part detectors became more reliable, leading to a significant improvement in accuracy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">20]</ref>. The works of <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b18">18]</ref> focus on multi-scale representation of body parts in order to infer the location of keypoints. In <ref type="bibr" target="#b14">[15]</ref>, the authors deal with simultaneous detection and pose estimation of multiple people. Recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">20]</ref> use an iterative scheme to refine pose estimation. As in our approach, in <ref type="bibr" target="#b3">[4]</ref> an image dependent binary term is learned. They, however, learn the binary terms explicitly while in our model it arises naturally from the voting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of the Method</head><p>We now describe the main parts of our algorithm, which will be explained in detail in next sections.</p><p>At inference, we first use a deep neural net, described in section 4.2, to predict for each image patch I y centered around pixel y, and for each keypoint j, the location of the keypoint relative to y. From this we can compute the probability of keypoint location K j being equal to a possible location x as seen from I y , P y (K j = x). We aggregate these votes over all image patches to get the probability distribution for each keypoint location {P (K j = x)} M j=1 . Examples of P y (K j ) and P (K j ) are shown in figures 3(a)-3(d) sec. 4.</p><p>Next we compute our consensus voting binary term. The voting net above was trained using a separate loss per keypoint, which is equivalent to an independence assumption, i.e. for each y,</p><formula xml:id="formula_0">P y (K i = x i , K j = x j ) = P y (K i = x i ) · P y (K j = x j ).<label>(1)</label></formula><p>If we now average over all locations y we get a joint distribution</p><formula xml:id="formula_1">P (K i = x i , K j = x j ) ∝ y P y (K i = x i ) · P y (K j = x j )<label>(2)</label></formula><p>in which the keypoints are no longer independent. Because of the multiplication, the joint distribution is high when both locations get strong votes from the same voters. More details on the consensus voting can be found in sec. 5.</p><p>Finally, we estimate the pose by minimizing an energy function over the unary and binary terms generated by the voting scheme. We do this sequentially, focusing at each step on a subset of keypoints. We start with the most reliable keypoints until making the full pose estimation. This process is presented in more details in sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Keypoint Voting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voting Representation</head><p>The first stage in our pose-estimation method is a keypoint detector learned by a deep neural net, which we apply to the image patches in a fully convolutional fashion <ref type="bibr" target="#b12">[13]</ref>. This is an accelerated way to run the net in a sliding window manner with the stride determined by pooling layers of the network. At each patch center y, the net predicts the location of each keypoint K 1 , ..., K M relative to y. This differs from previous methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b18">18]</ref> where the net only needed to classify if the center y is any of the M keypoints.</p><p>The problem of predicting the relative displacement vectors {K 1 −y, ..., K M − y} is a regression problem which is usually solved by minimizing an L 2 loss function. However, for the current task the L 2 loss has shortcomings, as the net produces only a single prediction as output. Consequently, in cases of ambiguity, e.g. difficulty to distinguish between left and right hand, the optimal L 2 loss would be the mean location instead of "splitting" the vote between both locations. Indeed, when trained using this approach, we found that the net performance is degraded by this problem. To better address this issue, we modify the prediction task into a classification problem by discretizing the image into log-polar bins centered around y, as seen in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. Using log-polar binning allows for more precise predictions for keypoints near y and a rough estimate for far away keypoints. We classify into 50 classes, one for the central location, one for background i.e. non-person, and the rest are four log-polar rings around the center with each ring divided into 12 angular bins. Since not all people in the training set are labeled, we are unable to use background locations for training the non-person label. For this reason, we ignore image locations far from the person of interest, as seen in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, and use non-person images from the PASCAL dataset for non-person label training.  We augmented the 16 humanly annotated keypoints supplied in the dataset with additional 12 keypoints generated from the original ones, by taking the middle point between two neighboring skeleton keypoints, e.g. the middle point between the shoulder and elbow. We also obtained estimated location of the hands by extrapolating the elbow-wrist vector by 30%. This produces a total of 30 keypoints and allows us to have a more dense coverage of the body. All keypoints can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Net Architecture</head><p>The net architecture we use is based on the VGG-16 network <ref type="bibr" target="#b17">[17]</ref>. We use the first 13 convolutional layers parameters of the VGG-16 net, pre-trained on the imagenet dataset <ref type="bibr" target="#b15">[16]</ref>. On top of these layers we employ a max pooling with stride 1 and use convolution with holes <ref type="bibr" target="#b2">[3]</ref> in order to increase resolution and generate a descriptor of size 2048 every 8 pixels. To further increase resolution we learn an upsampling deconvolution layer <ref type="bibr" target="#b12">[13]</ref> and get a probability distribution over 50 classes, indicating the relative keypoint location, every 4 pixels. The last two layers are distinct per keypoint resulting in 30 distributions on 50 bins every 4 pixels. More details about the structure of the net can be found in <ref type="table" target="#tab_2">Table  1</ref>. The net training was done over images from the MPII Human pose dataset <ref type="bibr" target="#b0">[1]</ref> in a cascaded fashion. First, training the added layers, denoted by layers 11-14, in table 1(b), while keeping the first 13 convolutional layers with learning rate 0. Then training layer 9 in table 1(b) and finally fine tuning the entire network. We start with learning rate of 0.001 and no weight decay and continue with learning rate of 0.0001 and 0.0002 weight decay. Since the size of each classification bin is considerably different, we minimize a weighted softmax loss (a) V GG16: 244x244x3 input image; 1x1000 output labels 1 2 3 4 5 6 7 8 9 10 11 12 13 layer 2 x conv max 2 x conv max 3 x conv max 3 x conv max 3 x conv max fc fc fc filter-stride <ref type="table" target="#tab_2">channels  64  64  128  128  256  256  512  512  512  512  4096  4096  1000  activation  relu  idn  relu  idn  relu  idn  relu  idn  relu  idn  relu  relu  soft  size  224  112  112  56  56  28  28  14  14  7  1  1  1  (b) HP E-W IS: 504x504x3 input image; 102x102x50x30 output label maps  1  2  3  4  5  6  7  8  9  10  11  12  13</ref> 14  function, where each class is weighted inversely proportional to the size of its bin.</p><formula xml:id="formula_2">3-1 2-2 3-1 2-2 3-1 2-2 3-1 2-2 3-1 2-2 - - -</formula><formula xml:id="formula_3">layer 2 × conv max 2 × conv max 3 × conv max 3 × conv max 3 × hconv max hconv conv convK i deconvK i filter-stride 3-1 2-2 3-1 2-2 3-1 2-2 3-1 2-1 3-1 3-1 7-1 1-1 1-1 6-2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Voting Scheme</head><p>At each patch center y and for each keypoint j the network returns a softmax probability distribution over the log-polar bins s j y ∈ R 1×1×C . At inference we use deconvolution, with a predefined fixed kernel w ∈ R H k ×W k ×C , to transform the log-polar representation s j y back to the image space and get the probability distribution of keypoint location over pixels P y (K j = x). The deconvolution kernel maps each channel, representing a log-polar bin, to the corresponding image locations. We use a deconvolution kernel of size (H k × W k × C) = (65 × 65 × 50). Most of the kernel weights are zeros, shown as black in <ref type="figure" target="#fig_3">fig. 3(e)</ref>. At the top of the figure we show an illustration of weights for a specific bin. Since this bin corresponds to the upper left log-polar segment it is zero at all locations except for the pixels of that segment which are set to 1 |bin| .</p><formula xml:id="formula_4">P y (K j = x) = deconv(s j y , w)<label>(3)</label></formula><p>Then P y (K j ) is simplyP y (K j ) translated by y. Examples for P y (K j ) are shown in <ref type="figure" target="#fig_3">fig. 3</ref>(a) and 3(b). We aggregate these votes over all patch centers to get the final probability of each keypoint at each location.   <ref type="figure" target="#fig_3">fig. 3</ref>(c) and 3(d). Note that the size of the aggregated distribution is larger than H 4 × W 4 , the image size at the output resolution. This enables the algorithm to generate votes for locations outside the visible image.</p><formula xml:id="formula_5">P (K j = x) = y∈Y P y (K j = x) = deconv(s j , w)<label>(4)</label></formula><p>During inference, we make use of the Caffe <ref type="bibr" target="#b7">[8]</ref> implementation for deconvolution by adding an additional deconvolution layer with fixed weights w on top of the net softmax output. This layer generates the aggregated probability distribution of each keypoint in a fast and efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Consensus Voting</head><p>The voting scheme described in section 4 produces a heat map per keypoint, which represents the probability of a keypoint being at each location. The next challenge is to combine all the estimated distributions into a single pose prediction. What makes this task especially challenging is that beyond just having to handle false keypoint detections, we need to handle true detections but of other individuals in the image as well. Other challenges include self-occlusion, and confusion between left and right body parts.</p><p>The standard approach to solve this problem is to minimize an objective of the following form:</p><formula xml:id="formula_6">N i=1 φ i (x i ) + (i,j)∈E φ (i,j) (x i , x j )<label>(5)</label></formula><p>The φ i (x i ) unary term is the score of having keypoint i at location x i . The φ (i,j) (x i , x j ) binary term measures compatibility and is the score of having keypoint i at location x i and keypoint j at location x j . The edge set E is some preselected subset of pairs of keypoints.</p><p>Usually in pose estimation, the binary term is image independent and comes from a prior on relative keypoint location. For example, given the location of the shoulder, it produces a set of possible elbow displacements based on training samples pose statistics. While this can be helpful in excluding anatomically implausible configurations, an image independent binary term has limitations. For example, the location of the left shoulder relative to the head is strongly dependent on whether the person is facing forwards or backwards and this information is not incorporated into the standard binary term. One main advantage of our keypoint voting scheme is that we can compute from it an image-based "consensus voting" binary term, which has more expressive power. This is especially important for less common poses, where the image-independent prior gives a low probability to the right pose.</p><p>At each image location y, we compute from the net output the distribution P y (K i = x) for each keypoint. These probabilities were trained under a (naive) independence assumption, i.e. a separate softmax loss for each keypoint, more formally</p><formula xml:id="formula_7">P y (K i = x i , K j = x j ) = P y (K i = x i ) · P y (K j = x j ).<label>(6)</label></formula><p>If we now average over all locations y, we get a joint distribution</p><formula xml:id="formula_8">P (K i = x i , K j = x j ) ∝ y P y (K i = x i ) · P y (K j = x j )<label>(7)</label></formula><p>which is no longer independent. By having each center y vote for a combination of keypoints, the probabilities become dependent through the voters. For  P (k i = x i , k j = x j ) to be high, it is not enough for x i and x j to receive strong votes separately, the combination needs to get strong votes from many common voters. For this reason we call this the consensus voting term.</p><p>In <ref type="figure" target="#fig_5">Fig. 4</ref> we show an example of the conditional probability P (K i = x i |K j = x j ) calculated from the previously described joint probability. As can be seen in <ref type="figure" target="#fig_5">Fig. 4(b)</ref>, the left elbow of the person to the right has a weak response. In addition, there is a misleading weak response to the elbow of another individual nearby. After conditioning on the left shoulder location, we see a strong response in <ref type="figure" target="#fig_5">Fig 4(d)</ref>, but only in the correct location.</p><p>In our algorithm, we use the unary term of the form φ i (x i ) = − log(P (K i = x i )). The binary term φ (i,j) (x i , x j ) we use is a weighted combination of the joint distribution − log(P (k i = x i , k j = x j )) just described and the commonly used binary term based on relative keypoint location statistics. The hyperparameters where tuned using the TRW-S algorithm <ref type="bibr" target="#b11">[12]</ref> on a validation set.</p><p>Computing the consensus voting is challenging, since calculating equation 7 in a naive way is computationally expensive. Each keypoint has N 2 possible locations, where the image is of size N ×N . Considering all possible combinations yields N 4 pairs of locations. For each pair of possible locations, we sum over all N 2 voters, resulting in O(N 6 ) running time. In order to reduce running time, we use the observation that eq. 7 is in fact a convolution and use the highly optimized Caffe GPU implementation <ref type="bibr" target="#b7">[8]</ref> to calculate the binary tables. In addition we calculate the binary term over a coarse scale of 1/12 of the image scale, using only the first two log-polar rings. This reduces the running time of a single keypoint pair to ∼ 100ms. The restriction to the first two rings limits the maximal distance between keypoints, which we overcome by using the augmented keypoints shown in <ref type="figure" target="#fig_1">Fig 2(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Pose Prediction</head><p>In the previous sections, we described the unary and binary terms which are the basic building blocks of our algorithm. We now present additional steps that we employ to improve performance on top of these basic building blocks. First, we add geometrical constraints on our augmented keypoints. Second, we perform the inference in parts, starting from the reliable parts proceeding to the less certain ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Local Geometric Constraints</head><p>We generate additional keypoints, as seen in <ref type="figure" target="#fig_1">Fig. 2(c)</ref> by taking the mid-point between keypoints, e.g. shoulder and elbow. While we could simply minimize eq. 5 with these added variables as well, this fails to take into account the fact that these new points are determined by other points. We can enforce these constraints by removing the new synthetic variables and rewriting our binary constraints. Assume our two original keypoints had indexes i and j and the middle point had index . Focusing on the relevant terms, instead of solving</p><formula xml:id="formula_9">min xi,xj ,x φ i (x i ) + φ j (x j ) + φ (x ) + φ (i, ) (x i , x ) + φ ( ,j) (x , x j )<label>(8)</label></formula><p>we add the geometric constraint by solving min xi,xj</p><formula xml:id="formula_10">φ i (x i ) + φ j (x j ) +φ (i,j) (x i , x j ).<label>(9)</label></formula><p>Where we definẽ</p><formula xml:id="formula_11">φ (i,j) (x i , x j ) = φ (f (x i , x j )) + φ (i, ) (x i , f (x i , x j )) + φ ( ,j) (f (x i , x j ), x j )</formula><p>and f (x, y) = 1 2 (x + y).</p><p>This is equivalent to adding the constraint that x is the mid-point between x i and x j , but faster to optimize. By adding this mid-point constraint, and using it as a linking feature <ref type="bibr" target="#b10">[11]</ref>, we get a more reliable binary term which also looks at the appearance of the space between the two respective keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sequential Prediction</head><p>An issue that arises when optimizing eq. 5 over all keypoint is that not all keypoints are detected with equal accuracy. Some, like the head, are detected with high accurately while others, like the wrist, are more difficult to locate. In some cases, occluded or unclear keypoints can distort predictions of more visible keypoints. In order to have the more certain keypoints influence the prediction of less certain ones, but not vice versa, we predict the keypoints in stages. We start with the most reliable keypoints, and at each stage use the previously predicted keypoints as an "anchor" to predict the other parts. We have three stages. First, we locate the head, neck, thorax and pelvis. After that we locate the shoulders and hips. Last, we locate all remaining keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">MPII</head><p>We tested our method on the MPII human pose dataset <ref type="bibr" target="#b0">[1]</ref>, which consists of 19,185 training and 7,247 testing images of various activities containing over 40K annotated people. The dataset is highly challenging and has people in a wide array of poses. At test time we are given an image with a rough location and scale of the person of interest and need to return the location of 16 keypoints: head-top, upper-neck, thorax, pelvis, shoulders, elbows, wrist, hips, knees and ankles.</p><p>The standard evaluation is made on a subset of test images, named "single person", where the person of interest is well separated from other people. We note that several images in the "single person" dataset still have another person nearby. In order to restrict our algorithm to the person in question, we crop a window of size 504 × 504 (using zero padding if needed) around the person using the given position and scale. In addition, to insure that we return the pose of the correct person, we multiply our mid-body keypoint heatmap (the synthetic point between thorax and pelvis) with a mask centered around the given person position. We trained the net described in section 4 using Caffe <ref type="bibr" target="#b7">[8]</ref> and get the final keypoint prediction by sequential optimization, as described in section 6.2, using the TRW-S algorithm <ref type="bibr" target="#b11">[12]</ref>. Various hyperparameters where tuned using a validation set containing 3300 annotated poses. Examples of our model's predictions are shown in <ref type="figure" target="#fig_6">fig. 5</ref>.</p><p>Performance is measured by the PCKh metric <ref type="bibr" target="#b0">[1]</ref>, where a keypoint location is considered correct if its distance to the ground truth location is no more than   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Leeds Sports</head><p>The Leeds sports dataset <ref type="bibr" target="#b8">[9]</ref> (LSP) contains 2, 000 images of people in various sport activities, 1, 000 for training and 1, 000 for testing. The task is to return 14 keypoints, the same keypoints as in the MPII dataset except for the thorax and pelvis.</p><p>The LSP dataset has two evaluation settings, person-centric (PC) and observercentric (OC). We use the person-centric settings where right/left labels are according to body parts (same as the MPII annotation) and not according to relative image location. The standard performance measure for the LSP dataset is strict percentage of correct parts (PCP) metric. The PCP score measures limb detection: a limb is considered to be correctly detected if the distances between the detected limb endpoints and groundtruth limb endpoints are within half of   <ref type="figure" target="#fig_8">fig.  7</ref>. At test time we run our algorithm twice, once with the input image and once with it flipped up-side-down, and pick the pose with the optimal score. This is done in order to handle up-side-down people which are more common in the LSP dataset than the MPII dataset, and are therefore under-represented at training time.</p><p>We compare the performance of our approach to that of other leading pose estimation methods in table 3. Our performance is comparable to that of Pishchulin et al. <ref type="bibr" target="#b14">[15]</ref>, and superior to other methods. We note that in <ref type="bibr" target="#b14">[15]</ref> the authors use the LSP-Extended dataset, containing additional 10,000 anno-  <ref type="table">Table 3</ref>: PCP results on the LSP dataset (PC).</p><p>tated poses, not used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>In this work, we proposed a method for dealing with the challenging task of human pose estimation in still images. We presented a novel approach of using a deep convolutional neural net for keypoint voting rather than keypoint detection. The keypoint voting scheme has several useful properties compared to keypoint detection. First, all image regions of the evaluated person participate in the voting, utilizing the 'wisdom of the crowd' to produce reliable keypoint predictions. Second, any informative location can contribute to multiple keypoints, not just to a single one. This allows us to use consensus voting in order to compute expressive image-dependent joint keypoint probabilities. Empirical results on the diverse MPII human pose and Leeds sports pose datasets show competitive results, improving the state-of-the-art on a subset of evaluated keypoints. We showed that our model generalized well from the MPII dataset to the LSP dataset, using only 1000 samples for fine tuning. Models and code will be made publicly available. Additional contributions of the current scheme are the use of log-polar bins for location prediction rather than estimating L2 translations, and the use of convolutions for fast aggregation of votes from multiple locations.</p><p>The voting scheme is a natural tool for estimating the locations of unobserved body parts. In future work, we plan to harness this property for dealing with occlusions, resulting from closely interacting people, which are difficult to handle by existing schemes. In addition, we plan to combine our voting scheme with iterative methods, refining predictions by feeding the output of previous iterations as input to the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our model's predicted pose estimation on the MPII-human-pose database test-set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Log-polar bins, used for keypoints locations voting, centered around the left upper arm; (b) Patch centers outside the person of interest, marked in blue, are not used for training; (c) Our model makes use of 30 keypoints: 16 annotated body joints, supplied in the dataset, 12 synthetically generated midsection keypoints and estimated hands locations. Original keypoints marked in blue, synthetically generated keypoints in green</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Voting map from location y (yellow rectangle) for Right Wrist P y (K RightW rist ); (b) Voting map from location y (yellow rectangle) for Left Shoulder P y (K Lef tShoulder ); (c) Right wrist probability P (K RightW rist ), generated by aggregating voting maps for right wrist; (d) Left shoulder probability P (K Lef tShoulder ), generated by aggregating voting maps for left shoulder; (e) The deconvolution kernel w. The weights of a specific channel are illustrated at the top The term P (K j ) ∈ R ( H 4 +H k −1)×( W 4 +W k −1) is the aggregated votes of keypoint j, and s j ∈ R H 4 × W 4 ×C is the softmax distribution output of keypoint j. Examples for P (K j ) are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) Left shoulder probability P (K Lef tShoulder ); (b)Left elbow probability P (K Lef tElbow ); (c) Conditional probability of left elbow given left shoulder of person on the left (yellow rectangle); (d) Conditional probability of left elbow given left shoulder of person on the right (yellow rectangle)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Additional results of our model's predicted joint positions on the MPII-human-pose database test-set<ref type="bibr" target="#b0">[1]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Pose estimation results on the MPII dataset for varying PCKh thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Additional results of our model's predicted joint positions on the LSP database test-set<ref type="bibr" target="#b8">[9]</ref> the limb length.We use the model trained on the MPII human pose dataset and fine-tune it on the LSP training set. Examples of our model's predictions are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparisons between the network architectures of V GG<ref type="bibr" target="#b15">16</ref> and HP E-W IS, as shown in (a) and (b). Each table contains five rows, representing the 'name of layer', 'receptive field of filter -stride', 'number of output feature maps', 'activation function' and 'size of output feature maps', respectively. The terms 'conv', 'max', 'fc', 'hconv' and 'deconv' represent the convolution, max pool-</figDesc><table /><note>ing, fully connection, convolution with holes [3] and deconvolution upsampling [13], respectively. The terms 'relu', 'idn', 'soft' and 'w-soft' represent the acti- vation functions, rectified linear unit, identity, softmax and weighted-softmax, respectively. The last two layers are distinct per keypoint.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>PCKh results on the MPII single person dataset. Works marked with are arXIV preprints, not yet peer-reviewed.half the head segment length. In table 2 we compare our results to the leading methods on the MPII human pose leaderboard. We show competitive results with mean PCKh score of 85.0% and state-of-the-art performance on the head keypoints.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<title level="m">Human pose estimation with iterative error feedback</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuewei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The chains model for detecting parts by their context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dinerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Harari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using linking features in learning non-parametric part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="326" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convergent tree-reweighted message passing for energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06645</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Join training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
