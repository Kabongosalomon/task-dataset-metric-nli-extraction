<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Premise Selection for Theorem Proving by Deep Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Premise Selection for Theorem Proving by Deep Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated reasoning over mathematical proofs is a core question of artificial intelligence that dates back to the early days of computer science <ref type="bibr" target="#b0">[1]</ref>. It not only constitutes a key aspect of general intelligence, but also underpins a broad set of applications ranging from circuit design to compilers, where it is critical to verify the correctness of a computer system <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>.</p><p>A key challenge of theorem proving is premise selection <ref type="bibr" target="#b4">[5]</ref>: selecting relevant statements that are useful for proving a given conjecture. Theorem proving is essentially a search problem with the goal of finding a sequence of deductions leading from presumed facts to the given conjecture. The space of this search is combinatorial-with today's large mathematical knowledge bases <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, the search can quickly explode beyond the capability of modern automated theorem provers, despite the fact that often only a small fraction of facts in the knowledge base are relevant for proving a given conjecture. Premise selection thus plays a critical role in narrowing down the search space and making it tractable.</p><p>Premise selection has been traditionally tackled as hand-designed heuristics based on comparing and analyzing symbols <ref type="bibr" target="#b7">[8]</ref>. Recently, machine learning methods have emerged as a promising alternative for premise selection, which can naturally be cast as a classification or ranking problem. Alama et al. <ref type="bibr" target="#b8">[9]</ref> trained a kernel-based classifier using essentially bag-of-words features, and demonstrated large improvement over the state of the art system. Alemi et al. <ref type="bibr" target="#b4">[5]</ref> were the first to apply deep learning approaches to premise selection and demonstrated competitive results without manual feature engineering. Kaliszyk et al. <ref type="bibr" target="#b9">[10]</ref> introduced HolStep, a large dataset of higher-order logic proofs, and provided baselines based on logistic regression and deep networks.</p><p>In this paper we propose a new deep learning approach to premise selection. The key idea of our approach is to represent mathematical formulas as graphs and embed them into vector space. This is different from prior work on premise selection that directly applies deep networks to sequences of characters or tokens <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. Our approach is motivated by the observation that a mathematical formula can be represented as a graph that encodes the syntactic and semantic structure of the formula. For example, the formula ∀x∃y(P (x) ∧ Q(x, y)) can be expressed as the graph shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, where edges link terms to their constituents and connect quantifiers to their variables. Our hypothesis is that such graph representations are better than sequential forms because a graph makes explicit key syntactic and semantic structures such as composition, variable binding, and co-reference. Such an explicit representation helps the learning of invariant feature representations. For example, P (x, T (f (z) + g(z), v)) ∧ Q(y) and P (y) ∧ Q(x) share the same top level structure P ∧ Q, but such similarity would be less apparent and harder to detect from a sequence of tokens because syntactically close terms can be far apart in the sequence.</p><p>Another benefit of a graph representation is that we can make it invariant to variable renaming while preserving the semantics. For example, the graph for ∀x∃y(P (x) ∧ Q(x, y) ( <ref type="figure" target="#fig_0">Fig. 1)</ref> is the same regardless of how the variables are named in the formula, but the semantics of quantifiers and co-reference is completely preserved-the quantifier ∀ binds a variable that is the first argument of both P and Q, and the quantifier ∃ binds a variable that is the second argument of Q.</p><p>It is worth noting that although a sequential form encodes the same information, and a neural network may well be able to learn to convert a sequence of tokens into a graph, such a neural conversion is unnecessary-unlike parsing natural language sentences, constructing a graph out of a formula is straightforward and unambiguous. Thus there is no obvious benefit to be gained through an end-to-end approach that starts from the textual representation of formulas.</p><p>To perform premise selection, we convert a formula into a graph, embed the graph into a vector, and then classify the relevance of the formula. To embed a graph into a vector, we assign an initial embedding vector for each node of the graph, and then iteratively update the embedding of each node using the embeddings of its neighbors. We then pool the embeddings of all nodes to form the embedding of the entire graph. The parameters of each update are learned end to end through backpropagation. In other words, we learn a deep network that embeds a graph into a vector; the topology of the unrolled network is determined by the input graph.</p><p>We perform experiments using the HolStep dataset <ref type="bibr" target="#b9">[10]</ref>, which consists of over two million conjecturestatement pairs that can be used to evaluate premise selection. The results show that our graphembedding approach achieves large improvement over sequence-based models. In particular, our approach improves the state-of-the-art accuracy on HolStep by 7.3%.</p><p>Our main contributions of this work are twofold. First, we propose a novel approach to premise selection that represents formulas as graphs and embeds them into vectors. To the best our knowledge, this is the first time premise selection is approached using deep graph embedding. Second, we improve the state-of-the-art classification accuracy on the HolStep dataset from 83% to 90.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Research on automated theorem proving has a long history <ref type="bibr" target="#b10">[11]</ref>. Decades of research has resulted in a variety of well-developed automated theorem provers such as Coq <ref type="bibr" target="#b11">[12]</ref>, Isabelle <ref type="bibr" target="#b12">[13]</ref>, and E <ref type="bibr" target="#b13">[14]</ref>. However, no existing automated provers can scale to large mathematical libraries due to combinatorial explosion of the search space. This limitation gave rise to the development of interactive theorem proving <ref type="bibr" target="#b10">[11]</ref>, which combines humans and machines in theorem proving and has led to impressive achievements such as the proof of the Kepler conjecture <ref type="bibr" target="#b14">[15]</ref> and the formal proof of the Feit-Thompson problem <ref type="bibr" target="#b15">[16]</ref>.</p><p>Premise selection as a machine learning problem was introduced by Alama et al. <ref type="bibr" target="#b8">[9]</ref>, who constructed a corpus of proofs to train a kernelized classifier using bag-of-word features that represent the occurrences of terms in a vocabulary. Deep learning techniques were first applied to premise selection in the DeepMath work by Alemi et al. <ref type="bibr" target="#b4">[5]</ref>, who applied recurrent networks and convolutional to formulas represented as textual sequences, and showed that deep learning approaches can achieve competitive results against baselines using hand-engineered features. Serving the needs for large datasets for training deep models, Kaliszyk et al. <ref type="bibr" target="#b9">[10]</ref> introduced the HolStep dataset that consists of 2M statements and 10K conjectures, an order of magnitude larger than the DeepMath dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>A related task to premise selection is proof guidance <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, the selection of the next clause to process inside an automated theorem prover. Proof guidance differs from premise selection in that proof guidance depends on the logical representation, inference algorithm, and current state inside a theorem prover, whereas premise selection is only about picking relevant statements as the initial input to a theorem prover that is treated as a black box. Because proof guidance is tightly integrated with proof search and is invoked repeatedly, efficiency is as important as accuracy, whereas for premise selection efficiency is not as critical.</p><p>Loos et al. <ref type="bibr" target="#b22">[23]</ref> were the first to apply deep networks to proof guidance. They experimented with both sequential representations and tree representations (recursive neural networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>). Note that their tree representations are simply the parse trees, which, unlike our graphs, are not invariant to variable renaming and do not capture how quantifiers bind variables. Whalen et al. <ref type="bibr" target="#b21">[22]</ref> used GRU networks to guide the exploration of partial proof trees, with formulas represented as sequences of tokens.</p><p>In addition to premise selection and proof guidance, other aspects of theorem proving have also benefited from machine learning. For example, Kühlwein et al. <ref type="bibr" target="#b25">[26]</ref> applied kernel methods to strategy finding, the problem of searching for good parameter configurations for an automated prover. Similarly, Bridge et al. <ref type="bibr" target="#b26">[27]</ref> applied SVM and Gaussian Processes to select good heuristics, which are collections of standard settings for parameters and other decisions.</p><p>Our graph embedding method is related to a large body of prior work on embeddings and graphs. Deepwalk <ref type="bibr" target="#b27">[28]</ref>, LINE <ref type="bibr" target="#b28">[29]</ref> and Node2Vec <ref type="bibr" target="#b29">[30]</ref> focus on learning node embeddings. Similar to Word2Vec <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, they optimize the embedding of a node to predict nodes in a neighborhood. Recursive neural networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25]</ref> and Tree LSTMs <ref type="bibr" target="#b33">[34]</ref> consider embeddings of trees, a special type of graphs. Neural networks on general graphs were first introduced by Gori et al <ref type="bibr" target="#b34">[35]</ref> and Scarselli et al <ref type="bibr" target="#b35">[36]</ref>. Many follow-up works <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref> proposed specific architectures to handle graphbased input by extending recurrent neural network to graph data <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> or making use of graph convolutions based on spectral graph theories <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. Our approach is most similar to the work of <ref type="bibr" target="#b36">[37]</ref>, where they encode molecular fragments as neural fingerprints with graph-based convolutions for chemical applications. But to the best of our knowledge, no previous deep learning approaches on general graphs preserve the order of edges. In contrast, we propose a novel way of graph embedding that can preserve the information of edge ordering, and demonstrate its effectiveness for premise selection.</p><p>3 FormulaNet: Formulas to Graphs to Embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulas to Graphs</head><p>We consider formulas in higher-order logic <ref type="bibr" target="#b43">[44]</ref>. A higher-order formula can be defined recursively based on a vocabulary of constants, variables, and quantifiers. A variable or a constant can act as a value or a function. For example, ∀f ∃x(f (x, c) ∧ P (f )) is a higher-order formula where ∀ and ∃ are quantifiers, c is a constant value, P, ∧ are constant functions, x is a variable value, and f is both a variable function and a variable value.</p><p>To construct a graph from a formula, we first parse the formula into a tree, where each internal node represents a constant function, a variable function, or a quantifier, and each leaf node represents a variable value or a constant value. We then add edges that connect a quantifier node to all instances of its quantified variables, after which we merge (leaf) nodes that represent the same constant or variable. Finally, for each occurrence of a variable, we replace its original name with VAR, or VARFUNC if it acts as a function. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates these steps.</p><p>Formally, let S be the set of all formulas, C v be the set of constant values, C f the set of constant functions, V v the set of variable values, V f the set of variable functions, and Q the set of quantifiers. Let s be a higher-order logic formula with no free variables-any free variables can be bounded by adding quantifiers ∀ to the front of the formula. The graph G s = (V s , E s ) of formula s can be recursively constructed as follows:</p><formula xml:id="formula_0">• if s = α, where α ∈ C v ∪ V v , then G s ← ({α}, ∅),</formula><p>i.e. the graph contains a single node α. •</p><formula xml:id="formula_1">if s = f (s 1 , s 2 , . . . , s n ), where f ∈ C f ∪ V f and s 1 , . . . , s n ∈ S, then we perform G s ← ( n i V si ∪ {f }, n i E si ∪ {(f, ν(s i ))} i ) followed by G s ← MERGE_C(G s ),</formula><p>where ν(s i ) is the "head node" of s i and MERGE_C is an operation that merges the same constant (leaf) nodes in the graph.</p><formula xml:id="formula_2">• if s = φ x t, where φ ∈ Q, t ∈ S, x ∈ V v ∪ V f , then we perform G s ← V t ∪ {f }, E t ∪ {(φ, ν(t)) v∈Vt[x] {(φ, v)} , followed by G s ← MERGE x (G s ) if x ∈ V v ∪ V f and G s ← RENAME x (G s ), where V t [x]</formula><p>is the nodes that represent the variable x in the graph of t, MERGE x is an operation that merges all nodes representing the variable x into a single node, and RENAME x is an operation that renames x to VAR (or VARFUNC if x acts as a function).</p><p>By construction, our graph is invariant to variable renaming, yet no syntactic or semantic information is lost. This is because for a variable node (either as a function or value), its original name in the formula is irrelevant in the graph-the graph structure already encodes where it is syntactically and which quantifier binds it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graphs to Embeddings</head><p>To embed a graph to a vector, we take an approach similar to performing convolution or message passing on graphs <ref type="bibr" target="#b36">[37]</ref>. The overall idea is to associate each node with an initial embedding and iteratively update them. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, suppose v and each node around v has an initial embedding. We update the embedding of v by the node embeddings in its neighborhood. After multi-step updates, the embedding of v will contain information from its local strcuture. Then we max-pool the node embeddings across all of nodes in the graph to form an embedding for the graph.</p><p>To initialize the embedding for each node, we use the one-hot vector that represents the name of the node. Note that in our graph all variables have the same name VAR (or VARFUNC if the variable acts as a function), so their initial embeddings are the same. All other nodes (constants and quantifiers) each have their names and thus their own one-hot vectors.</p><p>We then repeatedly update the embedding of each node using the embeddings of its neighbors. Given a graph G = (V, E), at step t + 1 we update the embedding x t+1 v of node v as follows:</p><formula xml:id="formula_3">x t+1 v = F t P x t v + 1 d v (u,v)∈E F t I (x t u , x t v ) + (v,u)∈E F t O (x t v , x t u ) ,<label>(1)</label></formula><p>where d v is the degree of node v, F t I and F t O are update functions using incoming edges and outgoing edges, and F t P is an update function to conbine the old embeddings with the new update from neighbor nodes. We parametrize these update functions as neural networks; the detailed configurations will be given in Sec. 4.2.</p><p>It is worth noting that all node embeddings are updated in parallel using the same update functions, but the update functions can be different across steps to allow more flexibility. Repeated updates allow each embedding to incorporate information from a bigger neighborhood and thus capture more global structures. Interestingly, with zero updates, our model reduces to a bag-of-words representation, that is, a max pooling of individual node embeddings. To predict the usefulness of a statement for a conjecture, we send the concatenation of their embeddings to a classifier. The classification can also be done in the unconditional setting where only the statement is given; in this case we directly send the embedding of the statement to a classifier. The parameters of the update functions and the classifiers are learned end to end through backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Order-Preserving Embeddings</head><p>For functions in a formula, the order of its arguments matters. That is, f (x, y) cannot generally be presumed to mean the same as f (y, x). But our current embedding update as defined in Eqn. 1 is invariant to the ordering of arguments. Given that it is possible that the ordering of arguments can be a useful feature for premise selection, we now consider a variant of our basic approach to make our graph embeddings sensitive to the ordering of arguments. In this variant, we update each node considering the ordering of its incoming edges and outgoing edges. In other words, a treelet is a subgraph that consists of a head node v, a left child u and a right child w. We use T G to denote all treelets of graph G, that is,</p><formula xml:id="formula_4">T G = {(u, v, w) : (v, u) ∈ E, (v, w) ∈ E, r v (u) &lt; r v (w)}.</formula><p>Now, when we update a node embedding, we consider not only its direct neighbors, but also its roles in all the treelets it belongs to:</p><formula xml:id="formula_5">x t+1 v = F t P x t v + 1 d v (u,v)∈E F t I (x t u , x t v ) + (v,u)∈E F t O (x t v , x t u ) + 1 e v (v,u,w)∈T G F t L (x t v , x t u , x t w ) + (u,v,w)∈T G F t H (x t u , x t v , x t w ) + (u,w,v)∈T G F t R (x t u , x t w , x t v ) (2) where e v = |{(u, v, w) : (u, v, w) ∈ T G ∨ (v, u, w) ∈ T G ∨ (u, w, v) ∈ T G }|</formula><p>is the number of total treelets containing v. In this new update equation, F L is an update function that considers a treelet where node v is the left child. Similarly, F H considers a treelet where node v is the head and F R considers a treelet where node v is the right child. As in Sec. 3.2, the same update functions are applied to all nodes at each step, but across steps the update functions can be different. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the update equation of a concrete example.</p><p>Our design of Eqn. 2 now allows a node to be embedded differently dependent on the ordering of its own arguments and dependent on which argument slot it takes in a parent function. For example, the function node f can now be embedded differently for f (a, b) and f (b, a) because of the output of F H can be different. As another example, in the formula g(f (a), f (a)), there are two function nodes with the same name f , same parent g, and same child a, but they can be embedded differently because only F L will be applied to the f as the first argument of g and only F R will be applied to the f as the second argument of g.</p><p>To distinguish the two variants of our approach, we call the method with the treelet update terms FormulaNet, as opposed to the basic FormulaNet-basic without considering edge ordering. </p><formula xml:id="formula_6">F H / F L / F R FC dim=256 BN ReLU FC dim=256 BN ReLU concat FC dim=256 BN ReLU FC dim=128 BN ReLU FC dim=2 FC dim=2 FC 256 BN ReLU (a) (b) (c) (d) F I / F O concat concat F P x v x u x v x u x w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation</head><p>We evaluate our approach on the HolStep dataset <ref type="bibr" target="#b9">[10]</ref>, a recently introduced benchmark for evaluating machine learning approaches for theorem proving. It was constructed from the proof trace files of the HOL Light theorem prover <ref type="bibr" target="#b6">[7]</ref> on its multivariate analysis library <ref type="bibr" target="#b44">[45]</ref> and the formal proof of the Kepler conjecture. The dataset contains 11,410 conjectures, including 9,999 in the training set and 1,411 in the test set. Each conjecture is associated with a set of statements, each with a ground truth label on whether the statement is useful for proving the conjecture. There are 2,209,076 conjecture-statement pairs in total. We hold out 700 conjectures from the training set as the validation set to tune hyperparameters and perform ablation analysis.</p><p>Following the evaluation setup proposed in <ref type="bibr" target="#b9">[10]</ref>, we treat premise selection as a binary classification task and evaluate classification accuracy. Also following <ref type="bibr" target="#b9">[10]</ref>, we evaluate two settings, the conditional setting where both the conjecture and the statement are given, and the unconditional setting where the conjecture is ignored. In HolStep, each conjecture is associated with an equal number of positive statements and negative statements, so the accuracy of random prediction is 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Configurations</head><p>The initial one-hot vector for each node has 1909 dimensions, representing 1909 unique tokens. These 1909 tokens include 1906 unique constants from the training set and three special tokens, "VAR", "VARFUNC", and "UNKNOWN" (representing all novel tokens during testing).</p><p>The update functions in Eqn. 1 and Eqn. 2 are parametrized as neural networks. <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>, (b) shows their configurations. All update functions are configured the same: concatenation of inputs followed by two fully connected layers with ReLUs, Batch Normalizations <ref type="bibr" target="#b45">[46]</ref>.</p><p>The classifier for the conditional setting takes in the embeddings from the conjecture and the statement. Its configuration is shown in <ref type="figure" target="#fig_4">Fig. 4 (c)</ref>. The classifier for the unconditional setting uses only the embedding of the statement; its configuration is shown in <ref type="figure" target="#fig_4">Fig. 4 (d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>We train our networks using RMSProp <ref type="bibr" target="#b46">[47]</ref> with 0.001 learning rate and 1 × 10 −4 weight decay. We lower the learning rate by 3X after each epoch. We train all models for five epochs and all networks converge after about three or four epochs.</p><p>It is worth noting that there are two levels of batching in our approach: intra-graph batching and inter-graph batching. Intra-graph batching arises from the fact that to embed a graph, each update function (F P , F I , F O , F L , F H , F R in Eqn. 2) is applied to all nodes in parallel. This is the same as training each update function as a standalone network with a batch of input examples. Thus batch normalization can be applied to the inputs of each update function within a single graph.</p><p>Furthermore, this batch normalization within a graph can be run in the training mode even when we are only performing inference to embed a graph, because there are multiple input examples to each update function within a graph. Another level of batching is the regular batching of multiple graphs in training, as is necessary for training the classifier. As usual, batch normalization across graphs is done in the evaluation mode in test time. We also apply intermediate supervision after each step of embedding update using a separate classifier. For training, our loss function is the sum of cross-entropy losses for each step. We use the prediction from the last step as our final predictions. <ref type="table" target="#tab_0">Table 1</ref> compares the accuracy of our approach versus the best existing results <ref type="bibr" target="#b9">[10]</ref>. Our approach improves the best existing result by a large margin from 83% to 90.3% in the conditional setting and from 83% to 90.0% in the unconditional setting. We also see that FormulaNet gives a 1% improvement over the FormulaNet-basic, validating our hypothesis that the order of function arguments provides useful cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Consistent with prior work <ref type="bibr" target="#b9">[10]</ref>, conditional and unconditional selection have similar performances. This is likely due to the data distribution in HolStep. In the training set, only 0.8% of the statements appear in both a positive statement-conjecture pair and a negative statement-conjecture pair, and the upper performance bound of unconditional selection is 97%. In addition, HolStep contains 9,999 unique conjectures but 1,304,888 unique statements for training, so it is likely easier for the network to learn useful patterns from statements than from conjectures.</p><p>We also apply Deepwalk <ref type="bibr" target="#b27">[28]</ref>, an unsupervised approach for generating node embeddings that is purely based on graph topology without considering the token associated with each node. For each formula graph, we max-pool its node embeddings and train a classifier. The accuracy is 61.8% (conditional) and 61.7% (unconditional). This result suggests that for embedding formulas it is important to use token information and end-to-end supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Experiments</head><p>Invariance to Variable Renaming One motivation for our graph representation is that the meaning of formulas should be invariant to the renaming of variable values and variable functions. To achieve such invariance, we perform two main transformations of a parse tree to generate a graph: (1) we convert the tree to a graph by linking quantifiers and variables, and (2) we discard the variable names.</p><p>We now study the effect of these steps on the premise selection task. We compare FormulaNet-basic with the following three variants whose only difference is the format of the input graph:</p><p>• Tree-old-names: Use the parse tree as the graph and keep all original names for the nodes. An example is the tree in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. • Tree-renamed: Use the parse tree as the graph but rename all variable values to VAR and variable functions to VARFUNC. • Graph-old-names: Use the same graph as FormulaNet-basic but keep all original names for the nodes, thus making the graph embedding dependent on the original variable names. An example is the graph in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>.</p><p>We train these variants on the same training set as FormulaNet-basic. To compare with FormulaNetbasic, we evaluate them on the same held-out validation set. In addition, we generate a new validation set (Renamed Validation) by randomly permutating the variable names in the formulas-the textual representation is different but the semantics remains the same. We also compare all models on this renamed validation set to evaluate their robustness to variable renaming. <ref type="table" target="#tab_1">Table 2</ref> reports the results. If we use a tree with the original names, there is a slight drop when evaluate on the original validation set, but there is a very large drop when evaluated on the renamed validation set. This shows that there are features exploitable in the original variable names and the model is exploiting it, but the model is essentially overfitting to the bias in the original names and cannot generalize to renamed formulas. The same applies to the model trained on graphs with the original names, whose performance also drops drastically on renamed formulas.  It is also interesting to note that the model trained on renamed trees performs poorly, although it is invariant to variable renaming. This shows that the syntactic and semantic information encoded in the graph on variables-particularly their quantifiers and coreferences-is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Update</head><p>Steps An important hyperparameter of our approach is the number of steps to update the embeddings. Zero steps can only embed a bag of unstructured tokens, while more steps can embed information from larger graph structures. <ref type="table" target="#tab_2">Table 3</ref> compares the accuracy of models with different numbers of update steps. Perhaps surprisingly, models with zero steps can already achieve an accuracy of 81.5%, showing that much of the performance comes from just the names of constant functions and values. More steps lead to notable increases of accuracy, showing that the structures in the graph are important. There is a diminishing return after 3 steps, but this can be reasonably expected because a radius of 3 in a graph is a fairly sizable neighborhood and can encompass reasonably complex expressions-a node can influence its grand-grandchildren and grand-grandparents. In addition, it would naturally be more difficult to learn generalizable features from long-range patterns because they are more varied and each of them occurs much less frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization of Embeddings</head><p>To qualitatively examine the learned embeddings, we find out a set of nodes with similar embeddings and visualize their local structures in <ref type="figure">Fig. 5</ref>. In each row, we use a node as the query and find the nearest neighbors across all nodes from different graphs. We can see that the nearest neighbors have  <ref type="figure">Figure 5</ref>: Nearest neighbors of node embeddings after step 1 with FormulaNet. Query nodes are in the first column. The color of each node is coded by a t-SNE <ref type="bibr" target="#b47">[48]</ref> projection of its step-0 embedding into 2D. The closer the colors, the nearer two nodes are in the step-0 embedding space. similar structures in terms of topology and naming. This demonstrates that our graph embeddings can capture syntactic and semantic structures of a formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have proposed a deep learning-based approach to premise selection. We represent a higher-order logic formula as a graph that is invariant to variable renaming but fully preserves syntactic and semantic information. We then embed the graph into a continuous vector through a novel embedding method that preserves the information of edge ordering. Our approach has achieved state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The formula ∀x∃y(P (x) ∧ Q(x, y)) can be represented as a graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>From a formula to a graph: (a) the input formula; (b) parsing the formula into a tree; (c) merging leaves and connecting quantifiers to variables; (d) renaming variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example of applying the order-preserving updates in Eqn. 2. To update node v, we consider its neighbors and its position in all treelets (see Sec. 3.3) it belongs to.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Before we define our new update equation, we need to introduce the notion of a treelet. Given a node v in graph G = (V, E), let (v, w) ∈ E be an outgoing edge of v, and let r v (w) ∈ {1, 2, . . .} be the rank of edge (v, w) among all outgoing edges of v. We define a treelet of graph G = (V, E) as a tuple of nodes (u, v, w) ∈ V × V × V such that (1) both (v, u) and (v, w) are edges in the graph and (2) (v, u) is ranked before (v, w) among all outgoing edges of v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Configurations of the update functions and classifiers: (a) F P in Eqn. 1 and 2; (b) F I , F O in Eqn. 1 and 2, and F L , F H , F R in Eqn. 2; (c) conditional classifier; (d) unconditional classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy on the test set of our approach versus baseline methods on HolStep in the unconditional setting (conjecture unknown) and the conditional setting (conjecture given).</figDesc><table><row><cell></cell><cell cols="4">CNN [10] CNN-LSTM [10] FormulaNet-basic FormulaNet</cell></row><row><cell>Unconditional</cell><cell>83</cell><cell>83</cell><cell>89.0</cell><cell>90.0</cell></row><row><cell>Conditional</cell><cell>82</cell><cell>83</cell><cell>89.1</cell><cell>90.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The accuracy of FormulaNet-basic and its ablated versions on original and renamed validation set.Tree-old-names Tree-renamed Graph-old-names Our Graph</figDesc><table><row><cell>Original Validation</cell><cell>89.7</cell><cell>84.7</cell><cell>89.8</cell><cell>89.9</cell></row><row><cell>Renamed Validation</cell><cell>82.3</cell><cell>84.7</cell><cell>83.5</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Validation accuracy of proposed models with different numbers of update steps on conditional premise selection.</figDesc><table><row><cell>Number of steps</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="6">FormulaNet-basic 81.5 89.3 89.8 89.9 90.0</cell></row><row><cell>FormulaNet</cell><cell cols="5">81.5 90.4 91.0 91.1 90.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of automated reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voronkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Formal verification in hardware design: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greenstreet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Design Automation of Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="193" />
			<date type="published" when="1999" />
			<publisher>TODAES</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Formal verification of an os kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Elphinstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Andronick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Derrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhammika</forename><surname>Elkaduwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Kolanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Norrish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Formal verification of a realistic compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepmath -deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Een</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2235" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Brief Overview of Mizar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Naumowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Korniłowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="67" to="72" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Light</surname></persName>
		</author>
		<title level="m">An Overview</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sine qua non for large theory reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kryštof</forename><surname>Hoder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Voronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="299" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Premise selection for mathematics by corpus analysis and kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Alama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Holstep: A machine learning dataset for higher-order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">History of interactive theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freek</forename><surname>Wiedijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Logic</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="135" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The COQ Proof Assistant: User&apos;s Guide: Version 5.6. INRIA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Dowek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Felty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Herbelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherin</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Paulin-Mohring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Werner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The isabelle framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarius</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nipkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Theorem Proving in Higher Order Logics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">E-a brainiac theorem prover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2, 3</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrud</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat Tat</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Magron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mclaughlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02155</idno>
		<title level="m">Thang Tat Nguyen</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. A formal proof of the kepler conjecture</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Machine-Checked Proof of the Odd Order Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Asperti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Garillot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assia</forename><surname>Mahboubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidi</forename><forename type="middle">Ould</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Biha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Rideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Théry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="163" to="179" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic acquisition of search guiding heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Suttner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Ertel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="470" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning from previous proof experience: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Denzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning search control knowledge for equational deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>IOS Press</publisher>
			<biblScope unit="volume">230</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Internal guidance for satallax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Färber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Automated Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="349" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Femalecop: fairly efficient machine learning connection prover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logic for Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Holophrasm: a neural automated theorem prover for higher-order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Whalen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep network guided proof search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06972</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Males: A framework for automatic tuning of automated theorem provers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="116" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Machine learning for first-order theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">B</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="172" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A formulation of the simple theory of types. The journal of symbolic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonzo</forename><surname>Church</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1940" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The hol light theory of euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<ptr target="https://class.coursera.org/neuralnets-2012-001/lecture" />
	</analytic>
	<monogr>
		<title level="j">Coursera Lecture slides</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
