<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Petrich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Gamal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
						</author>
						<title level="a" type="main">Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance "teacher" network provides pseudo-labels to adapt an appearance "student" network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Our proposed adaptation method outperforms the state-of-theart on DAVIS and FBMS with 6.8% and 1.2% in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1% and 25.9% in mIoU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The robotics and vision communities greatly improved video object segmentation over the recent years. The main approaches in video object segmentation could be categorized into semi-supervised and unsupervised approaches. In semi-supervised video object segmentation approaches (e.g., <ref type="bibr" target="#b33">[34]</ref>[2] <ref type="bibr" target="#b11">[12]</ref>, the method is initialized manually by a segmentation mask in the first few frames, then the segmented object is tracked throughout the video sequence. On the other hand, unsupervised methods <ref type="bibr" target="#b13">[14]</ref>[32] <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b30">[31]</ref> attempt to discover the primary object automatically and segment it through the video sequence. Motion is one of the fundamental cues that can help improve unsupervised video object segmentation. While there has been recent success in deep learning approaches for segmenting motion (e.g., <ref type="bibr" target="#b31">[32]</ref>[9] <ref type="bibr" target="#b30">[31]</ref>), current approaches depend mainly on prior large-scale training data.</p><p>Video semantic segmentation for robotics is widely used in different applications such as autonomous driving <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b25">[26]</ref>, and robot manipulation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b9">[10]</ref>. Object segmentation can aid in grasping, manipulating objects, and learning object affordances <ref type="bibr" target="#b4">[5]</ref>. In robot manipulation, learning to segment new <ref type="bibr" target="#b0">1</ref> Mennatullah Siam, Chen Jian, Steven Lu, Laura Petrich and Martin Jagersand are with the University of Alberta, Canada. e-mail: men-natul@ualberta.ca. <ref type="bibr" target="#b1">2</ref> Mahmoud Gamal is with Cairo University, Egypt. <ref type="bibr" target="#b2">3</ref> Mohamed El-Hoseiny is with Facebook AI Research. e-mail: elho-seiny@fb.com. objects incrementally, has significant importance. Real world environments have far more objects and more appearance variation than can be feasibly trained a-priori. Current largescale datasets such as Image-Net <ref type="bibr" target="#b14">[15]</ref> do not cover this. A recent trend in robotics is toward human-centered artificial intelligence. Human-centered AI involves learning by instruction using a human teacher. Such human-robot interaction (HRI) mimics children being taught novel concepts from few examples <ref type="bibr" target="#b17">[18]</ref>. In the robotic setting, a human teacher demonstrates an object by moving it and showing different poses, while verbally or textually teaching its label. The robot is then required to segment the objects in other settings where it is either static or manipulated by the human or the robot itself. We demonstrated this HRI setting in our team submission to the KUKA Innovation Challenge at the Hannover Fair <ref type="bibr" target="#b24">[25]</ref>. This HRI setting has few differences to conventional video object segmentation: (1) Abundance of the different poses of the object. <ref type="bibr" target="#b1">(2)</ref> The existence of different instances/classes within the same category. (3) Different challenges introduced by cluttered backgrounds, different rigid and non-rigid transformations, occlusions and illumination changes. In this paper, we focus on these robotics challenges and provide a new dataset and a new method to study such a scenario.</p><p>We collected a new dataset to benchmark (I)nteractive (V)ideo (O)bject (S)egmentation in the HRI scenario. The dataset contains two types of videos: (1) A human teacher showing different household objects in varying poses for interactive learning. (2) Videos of the same objects used in a kitchen setting while serving and eating food. The objects occur both as static objects and active objects being manipulated. Manipulation was performed by both humans and robots. The aim of this dataset is to facilitate incremental learning and immediate use in a collaborative humanrobot environments, such as assistive robot manipulation. Datasets that have a similar setting such as ICUBWorld transformations dataset <ref type="bibr" target="#b21">[22]</ref>, and the Core50 dataset <ref type="bibr" target="#b16">[17]</ref> were proposed. These datasets include different instances within the same category. They benchmark solutions to object recognition in a similar HRI setting but do not provide segmentation annotations unlike our dataset. Other datasets were concerned with the activities of daily living such as the ADL dataset <ref type="bibr" target="#b23">[24]</ref>. The dataset was comprised of egocentric videos for activities. However, such ADL datasets do not contain the required teaching videos to match the HRI setting we are focusing on. <ref type="table" target="#tab_0">Table I</ref> summarizes the most relevant datasets suited to the HRI setting.</p><p>The main contribution of our collected IVOS dataset is providing the manipulation tasks setting with objects being manipulated by humans or a robot. In addition to providing segmentation annotation for both teaching videos and manipulation tasks. It enables researchers to analyze the effect of different transformations such as translation, scale, and rotation on the incremental learning of video object segmentation. It acts as a benchmark for interactive video object segmentation in the HRI setting. It also provides videos of both human and similarly robot manipulation tasks with the segmentation annotations along with the corresponding robot trajectories. Thus, it enables further research in learning robot trajectories from visual cues with semantics.</p><p>We propose a novel teacher-student adaptation method based on motion cues for video object segmentation. Our method enables a human teacher to demonstrate objects moving with different transformations and associates them with labels. During inference, our approach can learn to segment the object without manual segmentation annotation. The teacher model is a fully convolutional network that combines motion and appearance, denoted as "Motion+Appearance". The adapted student model is a one-stream appearanceonly fully convolutional network denoted as "Appearance". Combining motion and appearance in the teacher network allows the creation of pseudo-labels for adapting the student network. Our work is inspired from the semi-supervised on-line method <ref type="bibr" target="#b33">[34]</ref>. This work uses manual segmentation masks for initialization. Instead, our approach tackles a more challenging problem and does not require manual segmentation; it relies on the pseudo-labels provided by the teacher model. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the proposed method. The two main reasons behind using the adaptation targets from the teacher model is: (1) The student model is more computationally efficient. The inference and adaptation time for the teacher model is 1.5x of the student model's. The adaptation occurs only once on the first frame, then the more efficient student model can be used for inference. (2) The • Providing a Dataset for Interactive Video Object Segmentation (IVOS) in a Human-Robot Interaction setting, and including manipulation tasks unlike previous datasets. • A teacher-student adaptation method is proposed to learn new objects from a human teacher without providing manual segmentation labels. We propose a novel pseudo-label adaptation based on a teacher model that is dependant on motion. Adaptation with discrete and continuous pseudo-labels are evaluated to demonstrate different adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. IVOS DATASET</head><p>We collected IVOS for the purpose of benchmarking (I)nteractive (V)ideo (O)bject (S)egmentation in the HRI setting. We collect the dataset in two different settings: (1) Human teaching objects. (2) Manipulation tasks setting. Unlike previous datasets in human robot interaction IVOS dataset provides video sequences for manipulation tasks. In addition to providing segmentation annotation for both teaching videos and manipulation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Teaching Objects</head><p>For teaching, videos are collected while a human moves an object with her hand. The unstructured human hand motion naturally provides different views of the object and samples  <ref type="figure">Figure 2</ref> shows a sample for the objects being captured under different transformations with the segmentation masks. In each session a video for the object held by a human with relatively cluttered scene background is recorded. A GRAS-20S4C-C fire-wire camera is used to record the data along with a Kinect sensor <ref type="bibr" target="#b28">[29]</ref>. The collected data is annotated manually with polygonal masks using the VGG Image Annotator tool <ref type="bibr" target="#b5">[6]</ref>. The final teaching videos contains 12 object categories, with a total of 36 instances under these categories. The detection crops are provided for all the frames, while the segmentation masks are provided for 20 instances with ∼ 18,000 annotated masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Manipulation Tasks Setting</head><p>The manipulation task benchmark includes two video categories: one with human manipulation, and the other with robot manipulation. Activities of Daily Living (ADL) such as food preparation are the focus for the recorded tasks. The aim of this benchmark is to further improve perception systems in robotics for assisted living. Robot trajectories are created through kinesthetic teaching, and the robot pose way-points are provided in the dataset. In order to create typical robot velocity and acceleration, profiles trajectories were generated from these way-points using splines as is standard in robotics.</p><p>The collected sequences are further annotated with segmentation masks similar to the teaching objects setting. <ref type="figure">Figure 3</ref> shows some of the recorded frames with groundtruth annotations. It covers 4 main manipulation tasks: cutting, pouring, stirring, and drinking for both robot and human manipulation covering a total of 56 tasks. The dataset contains ∼ 8,900 frames with segmentation masks, along with the recorded robot trajectories to enable further research on how to learn these trajectories from visual cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD A. Baseline Network Architecture</head><p>The student model in this work is built on the wide ResNet architecture presented in <ref type="bibr" target="#b35">[36]</ref>. The network is comprised of 16 residual blocks. Dilated convolution <ref type="bibr" target="#b36">[37]</ref> is used to increase the receptive field without decreasing the resolution. The output from the network is bilinearly upsampled to the initial image resolution. The loss function used is bootstrapped cross entropy <ref type="bibr" target="#b34">[35]</ref>, which helps with class imbalance. It computes the cross entropy loss from a fraction of the hardest pixels. Pre-trained weights on PASCAL dataset for objectness is used from <ref type="bibr" target="#b33">[34]</ref>, to help the network generalize to different objects in the scene. Then it is trained on DAVIS training set, the student model without adaptation is denoted as the baseline model throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher Model Appearance+Motion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student Model Appearance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Adaptation</head><p>Optical Flow  The teacher network incorporates motion from optical flow, where a two-stream wide ResNet for motion and appearance is used. Each stream contains 11 residual blocks for memory efficiency reasons. The output feature maps are combined by multiplying the output activation maps from both motion and appearance streams. After combining features another 5 residual blocks are used with dilated convolution. The input to the motion stream is the optical flow computed using <ref type="bibr" target="#b15">[16]</ref>, and converted into RGB representation using the Sintel color wheel representation <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Teacher-Student Adaptation using Pseudo-labels</head><p>There is an analogy between this work and the work in <ref type="bibr" target="#b32">[33]</ref>, where a student method is learning to mimic a teacher method. In our work the teacher method is a motion dependent one, and the student method tries to mimic the teacher during inference through motion adaptation. The teacher-student training helps the network understands the primary object in the scene in an unsupervised manner. Unlike the work in <ref type="bibr" target="#b33">[34]</ref> that first fine-tunes the network based on the manual segmentation mask then adapts it online with the most confident pixels. Our method provides a natural human robot interaction that does not require manual labelling for initialization.</p><p>Our approach provides two different adaptation methods, adapting based on discrete or continuous labels. The teacher network pseudo-labels are initially filtered to remove parts representing the human moving using the output human segmentation from Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>. When discrete labels are used it is based on pseudo-labels from the confident pixels in the teacher network output. Such a method provides superior accuracy, but on the expense of tuning the parameters that determine these confident pixels. Another method that utilizes continuous labels adaptation from the teacher network is also introduced. This method alleviates the need for any hyper-parameter tuning but on the cost of degraded accuracy. <ref type="figure" target="#fig_2">Figure 4</ref> summarizes the adaptation scheme, and shows the output pseudo-labels, the output segmentation before and after adaptation.</p><p>In the case of discrete pseudo-labels, the output probability maps from the teacher network is further processed in a similar fashion to the semi-supervised method <ref type="bibr" target="#b33">[34]</ref>. Initially the confident positive pixels are labeled, then a geometric distance transform is computed to label the most confident negative pixels as shown in Algorithm 1. In the case of continuous labels, the output probability maps are used without further processing. This has the advantage of not using any hyper-parameters or discrete label segmentation. It generalizes better to different scenarios on the expense of degraded accuracy. Inspiring from the relation between cross entropy and KL-divergence as in equations 1. The cross entropy loss can be viewed as a mean to decrease the divergence between the true distribution p and the predicted one q, in addition to the uncertainty implicit in H(p). In our case the true distribution is the probability maps from the teacher network, while the predicted is the student network output. <ref type="figure" target="#fig_4">Figure 5</ref> shows the difference between the pseudo-labels for both discrete and continuous variants. Conditional random fields is used as a post-processing step on DAVIS and FBMS. </p><formula xml:id="formula_0">D KL (p|q) = i p i log p i q i (1a) D KL (p|q) = i p i log 1 q i − H(p) (1b) H(p, q) = H(p) + D KL (p|q)<label>(1c)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>For all experiments the DAVIS training data is used to train our Appearance model and the Appearance+Motion model. The optimization method used is Adam <ref type="bibr" target="#b12">[13]</ref> with learning rate 10 −6 during training, and 10 −5 during on-line adaptation. In on-line adaptation 15 iterations are used in the scale/rotation experiments and 50 in the tasks experiments. Adaptation is only conducted once at the initialization of the video object segmentation. The positive threshold used to identify highly confident positive samples is 0.8, and the negative threshold distance to the foreground mask is 220 in case of DAVIS benchmark. Since IVOS is recorded in an indoor setup, a negative distance threshold of 20 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generic Video Object Segmentation</head><p>In order to evaluate the performance of our proposed motion adaptation (MotAdapt) method with respect to the state-of-the-art, we experiment on generic video object segmentation datasets. <ref type="table" target="#tab_0">Table II</ref> shows quantitative analysis on DAVIS benchmark compared to the state-of-the-art unsupervised methods. One of the variants of MotAdapt based on discrete labels outperforms the state of the art with 6.8% in F-measure, and 1% in mIoU. <ref type="table" target="#tab_0">Table III</ref> shows quantitative results on FBMS dataset, where our MotAdapt outperforms the state of the art with 1.2% in F-measure and 10% in recall. <ref type="figure">Figure 6</ref> shows qualitative results on FBMS highlighting the improvement gain from motion adaptation compared to LVO <ref type="bibr" target="#b31">[32]</ref>. <ref type="figure">Figure 7</ref> shows qualitative evaluation on DAVIS, where it demonstrates the benefit from motion adaptation compared to the baseline (top row), and compared to LVO <ref type="bibr" target="#b31">[32]</ref> and ARP <ref type="bibr" target="#b13">[14]</ref> (bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Object Segmentation in HRI Setting</head><p>Our method is evaluated in the HRI scenario on our dataset IVOS. The teaching is performed on the translation sequences, with only the first two frames used to generate pseudo-labels for adaptation. An initial evaluation is conducted on both scale and rotation sequences, in order to assess the adaptation capability to generalize to different poses and transformations. <ref type="table" target="#tab_0">Table IV</ref> shows the comparison between the baseline method without adaptation, and the two variants of motion adaptation on the scale, rotation and tasks sequences. The discrete and continuous variants for our motion adaptation outperform the baseline with 54.5% and 49.3% respectively on the scale sequences. Similarly on the rotation sequences it outperforms the baseline with 37.7% and 35.7% respectively. The main reason for this large gap, is that general segmentation methods will segment all objects in the scene as foreground, while our teaching method adaptively learns the object of interest that was demonstrated by the human teacher.</p><p>All manipulation tasks sequences where the category bottle existed is evaluated and cropped to include the working area. Our method outperforms the baseline on the tasks with 25.9%. The first variant of our adaptation method generally outperforms the second variant with continuous labels adaptation. However the second variant has the advantage that it can work on any setting such as DAVIS and IVOS without tuning any hyper-parameters. <ref type="figure">Figure 8</ref> shows the output from our adaptation method when it is recognized by the robot, and while the robot has successfully manipulated that object.</p><p>V. CONCLUSIONS In this paper we proposed a novel approach for visual learning by instruction. Our proposed motion adaptation (MotAdapt) method provides a natural interface to teaching robots to segment novel object instances. This enables robots to manipulate and grasp these objects. Two variants of the adaptation scheme is experimented with. Our results show that Mot-Adapt outperforms the state of the art on DAVIS and FBMS and outperforms the baseline on IVOS dataset.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed Teacher-Student adaptation method for video object segmentation. The teacher model based on motion cues is able to provide pseudo-labels to adapt the student model. Blue: confident positive pixels. Cyan: ignored region in the adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Samples of collected Dataset IVOS, Teaching Objects Setting. (a) Translation split. (b) Scale split. (c) Planar Rotation split. (d) Out-of-plane Rotation. (e) Non rigid transformations. Samples of collected IVOS dataset, Robot manipulation Tasks Setting with segmentation annotation. Manipulation Tasks: (a) Drinking. (b) Stirring. (c) Pouring Water. (d) Pouring Cereal. different geometric transformations. We provide transformations such as translation, scale, planar rotation, out-of-plane rotation, and other transformations such as opening the lid of a bottle. Two illumination conditions are provided: daylight and indoor lighting, which sums up to 10 sessions of recording for both illumination and transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Motion Adaptation of fully convolutional residual networks pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 2 :</head><label>12</label><figDesc>Motion Adaptation Algorithm. Input: X: images used for teaching. N: number of samples used. M teacher : Teacher Model. M student : Student Model. Output:M student : Adapted Student Model. 1: function TEACH(N , X, M teacher , M student ) for i in N do 3: P i = M teacher (X i ) 4:M student = Adapt(P i , M student ) 5: end for 6: end function Discrete Labels Adaptation Method 7: function ADAPT(A t , M student ) indices ← (A t &gt; POS TH ) 10: dt ← DITANCE TRANSFORM(Mask) 11: neg indices ← (dt &gt; NEG DT TH ) 12: Mask[pos indices] ← 1, Mask[neg indices] ← 0 return finetune(M student ,Mask) 13: end function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>(a,b) Discrete adaptation targets (pseudo-labels), cyan is the unknown region, blue is the confident positive pixels. (c, d) Continuous adaptation targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Qualitative evaluation on DAVIS16. (a) LVO [32]. (b) ARP [14]. (c) Baseline. (d) MotAdapt. Qualitative evaluation on IVOS Manipulation Tasks Setting. (a) Teaching Phase, Discrete Labels. (b) Teaching Phase, Continuous Labels. (c) Inference Phase before manipulation. (d) Inference Phase, during manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of different datasets. T:Turntable, H:handheld be used to generate pseudo-labels for the potential object of interest. It does not Urequire the human to provide manual segmentation mask during the teaching phase which provides a natural interface to the human. Consequently, the adapted student model can segment the object of the interest whether it is static or moving. If the adapted model was still dependant on optical flow it will only be able to recognize the object in motion.Our proposed method outperforms the state-of-the-art on the popular DAVIS<ref type="bibr" target="#b22">[23]</ref> and FBMS<ref type="bibr" target="#b18">[19]</ref> benchmarks with 6.8% and 1.2% in F-measure respectively. On our new IVOS dataset results show the motion adapted network outperforms the baseline with 46.1% and 25.9% in mIoU on Scale/Rotation and Manipulation Tasks respectively. Our code 1 and IVOS dataset 2 are publicly available. A video description and demonstration is available at 3 . Our main contributions are :</figDesc><table><row><cell>Dataset</cell><cell>Sess.</cell><cell>Cat.</cell><cell>Obj.</cell><cell>Acq.</cell><cell>Tasks</cell><cell>Seg.</cell></row><row><cell>RGB-D [27]</cell><cell>-</cell><cell>51</cell><cell>300</cell><cell>T</cell><cell></cell><cell></cell></row><row><cell>BIG BIRD [28]</cell><cell>-</cell><cell>-</cell><cell>100</cell><cell>T</cell><cell></cell><cell></cell></row><row><cell>ICUB 28 [21]</cell><cell>4</cell><cell>7</cell><cell>28</cell><cell>H</cell><cell></cell><cell></cell></row><row><cell>ICUB World [22]</cell><cell>6</cell><cell>20</cell><cell>200</cell><cell>H</cell><cell></cell><cell></cell></row><row><cell>Core50 [17]</cell><cell>11</cell><cell>10</cell><cell>50</cell><cell>H</cell><cell></cell><cell></cell></row><row><cell>IVOS</cell><cell>12</cell><cell>12</cell><cell>36</cell><cell>H</cell><cell></cell><cell></cell></row><row><cell>teacher model can</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Adaptation Target Output After Adaptation with Discrete targets Image Output Prediction Before Adaptation Image Discrete Continuous Output After Adaptation with Continuous targets</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Quantitative comparison on DAVIS benchmark. MotAdapt-1: Continuous Labels, MotAdapt-2: Discrete Labels.</figDesc><table><row><cell cols="2">Measure</cell><cell>NLC[7]</cell><cell>SFL[3]</cell><cell>LMP [31]</cell><cell>FSeg [9]</cell><cell>LVO [32]</cell><cell>ARP [14]</cell><cell>Baseline</cell><cell>MOTAdapt-1</cell><cell>MOTAdapt-2</cell></row><row><cell></cell><cell>Mean</cell><cell>55.1</cell><cell>67.4</cell><cell>70.0</cell><cell>70.7</cell><cell>75.9</cell><cell>76.2</cell><cell>74.0</cell><cell>75.3</cell><cell>77.2</cell></row><row><cell>J</cell><cell>Recall</cell><cell>55.8</cell><cell>81.4</cell><cell>85.0</cell><cell>83.5</cell><cell>89.1</cell><cell>91.1</cell><cell>85.7</cell><cell>87.1</cell><cell>87.8</cell></row><row><cell></cell><cell>Decay</cell><cell>12.6</cell><cell>6.2</cell><cell>1.3</cell><cell>1.5</cell><cell>0.0</cell><cell>0.0</cell><cell>7.0</cell><cell>5.0</cell><cell>5.0</cell></row><row><cell></cell><cell>Mean</cell><cell>52.3</cell><cell>66.7</cell><cell>65.9</cell><cell>65.3</cell><cell>72.1</cell><cell>70.6</cell><cell>74.4</cell><cell>75.3</cell><cell>77.4</cell></row><row><cell>F</cell><cell>Recall</cell><cell>51.9</cell><cell>77.1</cell><cell>79.2</cell><cell>73.8</cell><cell>83.4</cell><cell>83.5</cell><cell>81.6</cell><cell>83.8</cell><cell>84.4</cell></row><row><cell></cell><cell>Decay</cell><cell>11.4</cell><cell>5.1</cell><cell>2.5</cell><cell>1.8</cell><cell>1.3</cell><cell>7.9</cell><cell>0.0</cell><cell>3.3</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Quantitative results on FBMS dataset (test set).</figDesc><table><row><cell>Measure</cell><cell>FST [20]</cell><cell>CVOS [30]</cell><cell>CUT [11]</cell><cell>MPNet-V[31]</cell><cell>LVO[32]</cell><cell>Base</cell><cell>ours</cell></row><row><cell>P</cell><cell>76.3</cell><cell>83.4</cell><cell>83.1</cell><cell>81.4</cell><cell>92.1</cell><cell>80.8</cell><cell>80.7</cell></row><row><cell>R</cell><cell>63.3</cell><cell>67.9</cell><cell>71.5</cell><cell>73.9</cell><cell>67.4</cell><cell>76.1</cell><cell>77.4</cell></row><row><cell>F</cell><cell>69.2</cell><cell>74.9</cell><cell>76.8</cell><cell>77.5</cell><cell>77.8</cell><cell>78.4</cell><cell>79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>mIoU on IVOS over the different transformations and tasks. IVOS dataset teaching is conducted on few samples from the translation, then evaluating on scale, rotation and manipulation tasks. MotAdapt-1: Continuous Labels. MotAdapt-2: Discrete Labels. Qualitative Evaluation on the FBMS dataset. Top: LVO<ref type="bibr" target="#b31">[32]</ref>. Bottom: ours.</figDesc><table><row><cell>Model</cell><cell>Scale</cell><cell>Rotation</cell><cell>Manipulation Tasks</cell></row><row><cell>Baseline</cell><cell>14.5</cell><cell>13.8</cell><cell>14.7</cell></row><row><cell>MotAdapt-1</cell><cell>63.8</cell><cell>49.5</cell><cell>30.2</cell></row><row><cell>Mot-Adapt-2</cell><cell>69.0</cell><cell>51.5</cell><cell>40.6</cell></row><row><cell>LVO</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MotAdapt</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/MSiam/motion_adaptation 2 https://msiam.github.io/ivos/ 3 https://youtu.be/36hMbAs8e0c</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05198</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06750</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Affordancenet: An end-to-end deep learning approach for object affordance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zissermann</surname></persName>
		</author>
		<ptr target="http://www.robots.ox.ac.uk/vgg/software/via/" />
		<title level="m">VGG image annotator (VIA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive segmentation for manipulation in unstructured environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1377" to="1382" />
		</imprint>
	</monogr>
	<note>ICRA&apos;09</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02646</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03550</idno>
		<title level="m">Core50: a new dataset and benchmark for continuous object recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Categorization and naming in children: Problems of induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Markman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Teaching icub to recognize objects using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Interactive Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object identification from few examples by improving the invariance of a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ciliberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4904" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">KUKA Innovation Award Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Robotics</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=aLcw73dtOo" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and pose estimation based on pre-trained convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bigbird: A large-scale 3d database of object instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Achim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance assessment and calibration of the kinect 2.0 time-of-flight range camera for use in motion capture applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lichti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ferber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fig Working Week</title>
		<meeting>the Fig Working Week</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07217</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<idno type="arXiv">arXiv:1704.05737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05691</idno>
		<title level="m">Do deep convolutional nets really need to be deep and convolutional?</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
