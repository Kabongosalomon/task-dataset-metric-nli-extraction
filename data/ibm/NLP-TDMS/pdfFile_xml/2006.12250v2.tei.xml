<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
							<email>hzxie@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<email>h.yao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
							<email>s.zhang@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
							<email>shangchenzhou@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
							<email>sunwenxiu@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images</title>
					</analytic>
					<monogr>
						<title level="j" type="main">International Journal of Computer Vision</title>
					</monogr>
					<idno type="DOI">10.1007/s11263-020-01347-6</idno>
					<note type="submission">Received: 24 December 2019 / Revised: 28 March 2020 / Accepted: 12 June 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering the 3D shape of an object from single or multiple images with deep neural networks has been attracting increasing attention in the past few years. Mainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to sequentially fuse feature maps of input images. However, RNN-based approaches are unable to produce consistent reconstruction results when given the same input images with different orders. Moreover, RNNs may forget important features from early input images due to long-term memory loss. To address these issues, we propose a novel framework for single-view and multi-view 3D object reconstruction, named Pix2Vox++. By using a well-designed encoderdecoder, it generates a coarse 3D volume from each input image. A multi-scale context-aware fusion module is then introduced to adaptively select high-quality reconstructions for different parts from all coarse 3D volumes to obtain a fused 3D volume. To further correct the wrongly recovered parts in the fused 3D volume, a Haozhe Xie</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inferring the complete and precise 3D shape of an object is essential in robotics, 3D modeling and animation, object recognition, and medical diagnosis. Traditional methods, such as Structure from Motion (SfM) <ref type="bibr">(Özyeil et al., 2017)</ref> and Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b7">(Fuentes-Pacheco et al., 2015)</ref>, match features across images captured from slightly different views, and then use the triangulation principle to recover 3D coordinates of the image pixels. Although these methods can produce 3D reconstruction with satisfactory quality, they typically capture multiple images of the same object using well-calibrated cameras, which is not practical or feasible in some situations .</p><p>Recently, several deep learning-based approaches, including 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref>, LSM <ref type="bibr" target="#b16">(Kar et al., 2017)</ref>, DeepMVS <ref type="bibr" target="#b14">(Huang et al., 2018)</ref>, RayNet <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref>, and AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref>, recover the 3D shape of an object from one or more RGB images without complex camera calibration and show promising results. Both 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref> and LSM <ref type="bibr" target="#b16">(Kar et al., 2017)</ref> formulate multi-view 3D reconstruction as a sequence learning problem and fuse multiple feature maps extracted by a shared encoder from input images using recurrent neural net-v " <ref type="figure">Fig. 1</ref> Overview of the proposed Pix2Vox++. The network recovers the 3D shape of an object from arbitrary (uncalibrated) single or multiple images. The reconstruction result can be refined when more input images are available. Note that the weights of the encoder and decoder are shared among all views.</p><formula xml:id="formula_0"># v $ # v % # v &amp; v '</formula><p>works (RNNs). The feature maps are incrementally refined when more views of the object are available. However, RNN-based methods suffer from three limitations. First, RNNs are unable to consistently estimate the 3D shape of an object due to permutation variance <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> when given the same images with different orders. Second, important features of early input images may be forgotten <ref type="bibr" target="#b27">(Pascanu et al., 2013)</ref> due to long-term memory loss in RNNs. Third, RNN-based methods are time-consuming since input images are processed sequentially without parallelization <ref type="bibr" target="#b15">(Hwang and Sung, 2015)</ref>.</p><p>To overcome the shortcomings of the RNN-based methods, DeepMVS <ref type="bibr" target="#b14">(Huang et al., 2018)</ref> employs max pooling to aggregate deep features across a set of unordered images for multi-view stereo reconstruction. Ray-Net <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref> applies average pooling to aggregate the deep features extracted from the same voxel to recover the 3D structure. Both max and average pooling eliminate the above limitations of RNNs, but they capture only max or mean values without learning to attentively preserve useful information. Very recent AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref> uses an attentional aggregation module to automatically predict a weight matrix as attention scores for input features. However, aggregating features before the decoder is challenging for images with complex backgrounds and may cause problems in reconstructing objects in real-world scenarios.</p><p>To address the issues mentioned above, we propose Pix2Vox++, a novel framework for single-view and multiview 3D reconstruction. It contains four modules: encoder, multi-scale context-aware fusion, decoder, and refiner. As shown in <ref type="figure">Figure 1</ref>, the encoder and decoder generate coarse 3D volumes from multiple input images in parallel, which eliminate the effect of orders of input images and accelerate computation. The multi-scale context-aware fusion module then selects high-quality reconstructions from all coarse 3D volumes and generates a fused 3D volume, which exploits information from all input images without long-term memory loss. Finally, the refiner further corrects the wrongly recovered parts of the fused 3D volumes to obtain a refined reconstruction.</p><p>The contributions can be summarized as follows:</p><p>-We present a unified framework for both single-view and multi-view 3D object reconstruction, namely Pix2Vox++. It is composed of a well-designed encoder, decoder, and refiner, which shows strong abilities to handle 3D reconstruction in both synthetic and real-world images. -We propose a multi-scale context-aware fusion module to adaptively select high-quality reconstructions for each part from different coarse 3D volumes in parallel to produce a fused reconstruction of the whole object. -We construct a large-scale dataset, named Things3D, containing 1.68M images of 280K objects collected from over 39K indoor scenarios. To the best of our knowledge, it is the first large-scale dataset for multiview 3D object reconstruction from naturalistic images. -Experimental results on the ShapeNet, Pix3D, and Things3D datasets demonstrate that the proposed approaches outperform state-of-the-art methods in terms of both accuracy and efficiency.</p><p>A preliminary version of this work has been published in ICCV 2019 <ref type="bibr" target="#b51">(Xie et al., 2019)</ref>. We make several extensions in this work compared to the preliminary version. First, we replace VGG <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2015)</ref> with ResNet <ref type="bibr" target="#b12">(He et al., 2016)</ref> as the new backbone network. The improved method contains 25% fewer parameters and is 5% faster during inference compared to the one in the preliminary version. In addition, there is a 1.5% increase in Intersection-over-Union (IoU) on ShapeNet. Second, we propose the multi-scale context-aware fusion to aggregate multi-scale features from multiple coarse 3D volumes. Compared to the context-aware fusion in the preliminary version, it brings about a 1% increase in IoU for multi-view reconstruction at 128 3 resolution. Third, we add several layers in the decoder to generate 3D volumes with higher resolutions of 64 3 and 128 3 which preserve better details of 3D objects. Finally, we propose a large-scale naturalistic dataset for multi-view 3D reconstruction, which provides 708 times more 3D models than Pix3D <ref type="bibr" target="#b38">(Sun et al., 2018)</ref>. Codes and pretrained models are publicly available at https://gitlab.com/hzxie/Pix2Vox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review 3D object reconstruction methods closely related to this work. Comprehensive reviews of 3D object reconstruction approaches can be found in <ref type="bibr" target="#b10">Han et al. (2019)</ref>. Single-view 3D Reconstruction. Predicting the complete 3D shape of an object from a single image is a long-standing and extremely challenging task. Many attempts have been made to address this issue, such as Shape from X <ref type="bibr" target="#b0">(Barron and Malik, 2015)</ref>, where X may represent silhouettes <ref type="bibr" target="#b5">(Dibra et al., 2017)</ref>, shading <ref type="bibr" target="#b31">(Richter and Roth, 2015)</ref>, or texture <ref type="bibr" target="#b44">(Witkin, 1981)</ref>. However, these methods are rarely applied to real-world scenarios, as they all require strong presumptions and abundant expertise in natural images . With the success of generative adversarial networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b19">(Kingma and Welling, 2014)</ref>, 3D-VAE-GAN <ref type="bibr" target="#b45">(Wu et al., 2016)</ref> uses GAN and VAE to generate 3D reconstructions by taking a single-view image as input. MarrNet <ref type="bibr" target="#b46">(Wu et al., 2017)</ref> reconstructs 3D objects by estimating depth, surface normals, and silhouettes of 2D images. ShapeHD  extends MarrNet by incorporating a shape naturalness network to improve reconstruction results. <ref type="bibr" target="#b17">Kato and Harada (Kato and Harada, 2019)</ref> adopt a discriminator to ensure that reconstructed shapes are reasonable from any viewpoint. OGN <ref type="bibr" target="#b39">(Tatarchenko et al., 2017)</ref> uses octree to represent high-resolution 3D volumes with a limited memory budget. Matryoshka Networks <ref type="bibr" target="#b32">(Richter and Roth, 2018)</ref> recursively decomposes a 3D shape into nested shape layers, which outperforms octree-based reconstruction methods. Recently, several representations for 3D models, including point cloud <ref type="bibr" target="#b6">(Fan et al., 2017)</ref>, mesh <ref type="bibr" target="#b42">(Wang et al., 2018)</ref>, and signed distance field <ref type="bibr" target="#b52">(Xu et al., 2019)</ref>, have been adopted in 3D object reconstruction to reduce memory requirements for highresolution 3D volumetric grids. PSG <ref type="bibr" target="#b6">(Fan et al., 2017)</ref> firstly recovers a point cloud from a single image with deep neural networks. Pixel2Mesh <ref type="bibr" target="#b42">(Wang et al., 2018)</ref> is the first to reconstruct the 3D shape in a triangular mesh from a single image. DISN <ref type="bibr" target="#b52">(Xu et al., 2019)</ref> predicts the underlying signed distance field given a single input image. With available fine-grained 3D part annotations <ref type="bibr" target="#b25">(Mo et al., 2019b)</ref>, several methods <ref type="bibr" target="#b29">(Paschalidou et al., 2020;</ref><ref type="bibr" target="#b24">Mo et al., 2019a;</ref><ref type="bibr" target="#b56">Zhu et al., 2018)</ref> perform 3D reconstruction by compositing 3D parts in a hierarchical manner. However, single-view 3D object reconstruction is an ill-posed and inherently ambiguous problem since partial observation of an object can theoretically be associated with an infinite number of possible 3D models.</p><p>Multi-view 3D Reconstruction. Traditionally, 3D dense reconstruction in SfM and SLAM requires a collection of RGB images <ref type="bibr" target="#b11">(Harltey and Zisserman, 2006)</ref>. The 3D structure of an object is recovered by dense feature extraction and matching, or by minimizing reprojection errors <ref type="bibr" target="#b1">(Cadena et al., 2016)</ref>. However, the matching process becomes extremely difficult when multiple viewpoints are separated by a large margin. Furthermore, scanning all surfaces of an object before reconstruction is sometimes impossible, leading to incomplete 3D shapes with occluded or hollowed-out areas . Recently, deep neural networks have been designed to learn the 3D shape from multiple RGB images. Both 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref> and LSM <ref type="bibr" target="#b16">(Kar et al., 2017)</ref> are RNN-based, resulting in the networks being permutation variant and inefficient for aggregating features from long sequence images. Deep-MVS <ref type="bibr" target="#b14">(Huang et al., 2018)</ref> and RayNet <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref> employ max and average pooling to aggregate deep features. Recent AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref> uses an attentional aggregation module to effectively aggregate deep features. However, these methods capture only partial information, ignoring many deep features, which may lead to low-quality reconstruction. In addition to volumetric representations, recent works also reconstruct 3D objects in the form of point clouds and meshes. <ref type="bibr" target="#b20">Lin et al. (2018)</ref> use 2D convolutional operations to predict a dense point cloud from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. Pixel2Mesh++  recovers 3D mesh by leveraging cross-view information with a graph convolutional network. <ref type="bibr" target="#b21">Lin et al. (2019)</ref> reconstruct 3D objects from aligned videos by optimizing object meshes for multi-view photometric consistency while constraining mesh deformations with a shape prior. These methods require extrinsic camera parameters or aligned images as input. However, it is . To reduce the model size, the refiner is removed in Pix2Vox++/F. not always feasible to obtain extrinsic camera parameters, especially from those scenarios that viewpoints are separated by a large margin.</p><p>3 The Proposed Method: Pix2Vox++</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The proposed Pix2Vox++ aims to reconstruct the 3D shape of an object from either single or multiple RGB images. The 3D shape of an object is represented by a 3D voxel grid, where 0 and 1 denote an empty cell and an occupied cell, respectively. The key components of Pix2Vox++ are illustrated in <ref type="figure">Figure 1</ref>. First, the encoder produces feature maps from input images. Second, the decoder takes each feature map as input and correspondingly generates a coarse 3D volume. Third, single or multiple 3D volumes are forwarded to the multi-scale context-aware fusion module that adaptively selects high-quality reconstructions for different parts from all coarse 3D volumes in parallel and generates a fused 3D volume. Finally, the refiner further corrects the wrongly recovered parts of the fused 3D volume to produce the final reconstruction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>To achieve a good balance between accuracy and model size, we implement two versions of the proposed framework: Pix2Vox++/F and Pix2Vox++/A, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The former involves much fewer parameters and lower computational complexity. The latter has more parameters, which can reconstruct more accurate 3D shapes but has higher computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Encoder</head><p>The encoder aims to compute a set of features for the decoder to recover the 3D shape of the object. The first three convolutional blocks of ResNet <ref type="bibr" target="#b12">(He et al., 2016)</ref> are used to obtain a 512 × 28 2 feature map from a 224×224×3 image. We adopt ResNet-18 and ResNet-50 for Pix2Vox++/F and Pix2Vox++/A, respectively. ResNet is followed by three sets of 2D convolutional layers, batch normalization layers, and ReLU layers to embed semantic information into feature maps. The kernel sizes of the three convolutional layers are 3 2 , with padding of 1. There is a max pooling layer with a kernel size of 2 2 after the second and third ReLU layers. In Pix2Vox++/F, the output channels in the convolutional layer are numbered 128, 64 and 64, respectively. In Pix2Vox++/A, the output channels of the three convolutional layers are 512, 256 and 256, respectively. The feature maps produced by Pix2Vox++/F and Pix2Vox++/A are of sizes 64 × 7 2 and 256 × 7 2 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Decoder</head><p>The decoder is responsible for transforming information of 2D feature maps into 3D volumes. Low-resolution Reconstruction. There are five 3D transposed convolutional layers in both Pix2Vox++/F and Pix2Vox++/A. Specifically, the first four transposed convolutional layers are of kernel sizes 4 3 , with strides of 2 and paddings of 1. There is an additional transposed convolutional layer with a bank of 1 3 filter. Each transposed convolutional layer is followed by a batch normalization layer and a ReLU activation except for the last layer followed by a sigmoid function. In Pix2Vox++/F, the output channels of transposed convolutional layers are numbered 128, 64, 32, 8 and 1, respectively. In Pix2Vox++/A, the output channel numbers of the five transposed convolutional layers are 512, 128, 32, 8 and 1, respectively. The decoder outputs a 32 3 voxelized shape in the object's canonical view. High-resolution Reconstruction. To generate 3D volumes at 64 3 resolution, there are six transposed convolutional layers in the decoders of Pix2Vox++/F and Pix2Vox++/A. Each convolutional layer is with a batch normalization and a ReLU activation except for the last layer followed by a sigmoid function. In Pix2Vox++/F, the output channels of the six transposed convolutional layers are numbered 128, 64, 32, 16, 8 and 1, respectively. In Pix2Vox++/A, the output channel numbers of transposed convolutional layers are 512, 128, 32, 16, 8 and 1, respectively. To generate 3D volumes at 128 3 resolution, there are seven transposed convolutional layers in the decoders of Pix2Vox++/F and Pix2Vox++/A. For Pix2Vox++/F, the output channels of the seven transposed convolutional layers are numbered 128, 64, 32, 32, 32, 8 and 1, respectively. For Pix2Vox++/A, the output channel numbers of the seven transposed convolutional layers are 512, 128, 32, 32, 32, 8 and 1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Multi-scale Context-aware Fusion</head><p>Different parts of an object can be seen from different viewpoints. The reconstruction qualities of visible parts are much higher than those of invisible parts. Inspired by this observation, we propose a multi-scale contextaware fusion module to adaptively select high-quality reconstruction for each part from different coarse 3D volumes. The selected reconstructions are fused to produce a 3D volume of the entire object. As shown in <ref type="figure">Figure 3</ref>, the multi-scale context-aware fusion generates higher scores for high-quality reconstructions, which can eliminate the effect of the missing of the wrongly recovered parts.</p><p>As shown in <ref type="figure">Figure 4</ref>, given coarse 3D volumes and the corresponding context, the multi-scale context-aware fusion module generates a score map for each coarse volume and then fuses them into one volume by weighted summation of all coarse volumes according to their score maps. The spatial information of voxels is preserved in the multi-scale context-aware fusion module, and thus Pix2Vox++ can use multi-view information to recover the structure of an object better.</p><p>Deeper convolutional layers have larger receptive fields, which help to explore contextual information in 3D volumes. However, the features in deeper convolutional layers may lose details of the object. To address this problem, we concatenate multiple feature maps with different scales to preserve details in shallower convolutional layers, which is important for recovering details in high-resolution 3D volumes. Specifically, the multi-scale context-aware fusion generates the context c r of the r-th coarse volume v c r by concatenating the output of the last two layers in the decoder. The context scoring network then generates a score m r for the context of the r-th coarse voxel. The context scoring network consists of five sets of 3D convolutional layers, each with a kernel size of 3 3 and padding of 1, followed by a batch normalization and a leaky ReLU activation. The numbers of output channels of the convolutional layers are 9 except for the last layer whose number of output channels is 1. The feature maps generated by the first four convolutional layers are concatenated and forwarded to the fifth convolutional layer. The learned score m r for context c r is normalized across all learned scores. We choose softmax as the normalization func-tion. Therefore, the score s (i,j,k) r at the position (i, j, k) for the r-th voxel can be calculated as</p><formula xml:id="formula_1">s (i,j,k) r = exp m (i,j,k) r n p=1 exp m (i,j,k) p (1)</formula><p>where n represents the number of views. Finally, the fused voxel v f is produced by summing up the product of coarse voxels and the corresponding scores altogether.</p><formula xml:id="formula_2">v f = n r=1 s r v c r (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Refiner</head><p>The refiner can be seen as a residual network, which aims to correct the wrongly recovered parts of a 3D volume. It follows the concept of a 3D encoder-decoder with U-net connections <ref type="bibr" target="#b33">(Ronneberger et al., 2015)</ref> that preserves the local structure in the fused volume. Low-resolution Reconstruction. To generate 3D volumes at 32 3 resolution, the encoder has three 3D convolutional layers, each with a bank of 4 3 filters with padding of 2, followed by a batch normalization layer, a leaky ReLU activation and a max pooling layer with kernel size of 2 3 . The output channels of convolutional layers are numbered 32, 64 and 128, respectively. The encoder is finally followed by two fully connected layers with dimensions of 2048 and 8192. The decoder consists of three transposed convolutional layers, each with a bank of 4 3 filters with padding of 2 and stride of 1. Except for the last transposed convolutional layer which is followed by a sigmoid function, other layers are followed by a batch normalization layer, and a ReLU activation. The numbers of output channels of the transposed convolutional layers are 64, 32 and 1, respectively. High-resolution Reconstruction. To generate 3D volumes at 64 3 resolution, we add a 3D convolutional layer before the first convolutional layer and a 3D transposed convolutional layer before the last layer. Both layers are with output channels of 16, kernel sizes of 4 3 , paddings of 2, followed by batch normalization layers and leaky ReLU activations. There is a max pooling layer with a kernel size of 2 3 after the added 3D convolutional layer. To generate 3D volumes at 128 3 resolution, there are five 3D convolutional layers in the encoder, whose output channels are 8, 16, 32, 64 and 128, respectively. In the decoder, there are five 3D transposed convolutional layers with kernel sizes of 4 3 , paddings of 2, and stride of 1. The output channels of the five layers are 64, 32, 16, 8 and 1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>The loss function of the network is defined as the mean value of the voxel-wise binary cross entropies between the reconstructed object and the ground truth. More formally, it can be defined as</p><formula xml:id="formula_3">= 1 N N i=1 [gt i log(p i ) + (1 − gt i ) log(1 − p i )]<label>(3)</label></formula><p>where N denotes the number of voxels in the ground truth. p i and gt i represent the predicted occupancy and the corresponding ground truth. The smaller the value is, the closer the prediction is to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Dataset: Things3D</head><p>There are several datasets available for 3D object reconstruction, including ShapeNet <ref type="bibr" target="#b48">(Wu et al., 2015)</ref> and Pix3D <ref type="bibr" target="#b38">(Sun et al., 2018)</ref>. ShapeNet is a large dataset for 3D models, but does not contain naturalistic background images. Pix3D has real images, but it only contains 395 3D models and 10,069 images, which is not enough to train networks <ref type="bibr" target="#b40">(Tatarchenko et al., 2019)</ref>.</p><p>To generate a large-scale dataset for 3D object reconstruction with naturalistic backgrounds, <ref type="bibr" target="#b37">Su et al. (2015)</ref> and <ref type="bibr" target="#b21">Lin et al. (2019)</ref> randomly warp and crop spherical images from the SUN database <ref type="bibr" target="#b49">(Xiao et al., 2010)</ref> and SUN360 database <ref type="bibr" target="#b50">(Xiao et al., 2012)</ref> to sample background images, respectively. Consequently, multiview images are obtained by compositing foreground and background images together at the corresponding camera poses. In contrast, we generate more realistic images from diverse virtual scenes with Blender, an open-source 3D creation suite, where we can easily control camera pose and location, as well as lighting conditions.</p><p>We present a large-scale naturalistic dataset for 3D object reconstruction, named Things3D, which contains 1.68M images of 280K objects (with 21K unique objects) collected from over 39K SUNCG <ref type="bibr" target="#b36">(Song et al., 2017)</ref> indoor scenarios. Sample images and corresponding CAD models are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. To increase the diversity of 3D objects, we use 3D models in the ShapeNet dataset instead of the ones in the SUNCG dataset. In particular, each 3D model in SUNCG scenes is replaced with one that is randomly selected from the same category in the ShapeNet dataset. In addition, the replaced 3D model is of equal or smaller size than the original. We randomly sample 24 viewing spheres for each object with yaw ∈ [0, 360), pitch = 30, roll = 0 degrees. The distance between the camera and the object is set to 10 unit length. The camera has a focal length of 96 mm. The power of light is uniformly sampled from <ref type="bibr">[500,</ref><ref type="bibr">2000]</ref>, and the specular of light is randomly sampled from [0.75, 3]. Both the camera and the light track to the rendered object. Specially, we ignore rendered images if more than 12.5% of the object is occluded. The images are of resolution 256 × 256. In addition to rendered images, information about the canonical orientation and ground truth for objects is provided as well. The data generation process lasts 32 days and runs on 15 servers with 4 Intel Xeon E5-2682 v4@2.50GHz CPUs and 256 GB RAM. The dataset is available at https://gateway.haozhexie.com/ ?fileName=Things3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present extensive experimental evaluations of Pix2Vox++ on the ShapeNet <ref type="bibr" target="#b48">(Wu et al., 2015)</ref>, Pix3D <ref type="bibr" target="#b38">(Sun et al., 2018)</ref>, and Things3D datasets. We first describe the datasets and evaluation protocols. Next, we demonstrate the implementation details of the proposed methods. Finally, we report experimental evaluations of the proposed methods against state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>ShapeNet. The ShapeNet dataset <ref type="bibr" target="#b48">(Wu et al., 2015)</ref> is a collection of 3D CAD models organized according to the WordNet taxonomy. We use a subset of the ShapeNet dataset consisting of 44K models and 13 major categories following <ref type="bibr" target="#b4">Choy et al. (2016)</ref>. More specifically, we use renderings provided by 3D-R2N2 which contains 24 random views of size 137 × 137 for each 3D model. We also apply a uniform colored background to the image during training and testing.</p><p>Pix3D. The Pix3D dataset <ref type="bibr" target="#b38">(Sun et al., 2018)</ref> provides perfectly aligned real-world images and CAD models. The dataset contains 395 3D models of nine object classes. Each model is associated with a set of real images, capturing the exact object in diverse environments. Following <ref type="bibr" target="#b38">Sun et al. (2018)</ref>, we use 2,894 untruncated and unoccluded images from the chair category for testing in the following experiments. Things3D. The Things3D dataset proposed in this paper contains 1.68M images of 280K objects collected from over 39K SUNCG <ref type="bibr" target="#b36">(Song et al., 2017)</ref> indoor scenarios. See Section 4 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>To evaluate the reconstruction quality of the proposed methods, we binarize probabilities at a fixed threshold of 0.3 and use intersection over union (IoU) as a similarity measure between prediction and ground truth. More formally,</p><formula xml:id="formula_4">IoU = i,j,k I(p (i,j,k) &gt; t)I(p (i,j,k) ) i,j,k I I(p (i,j,k) &gt; t) + I(p (i,j,k) )<label>(4)</label></formula><p>wherep <ref type="bibr">(i,j,k)</ref> and p (i,j,k) represent the predicted occupancy probability and ground truth at (i, j, k), respectively. I(·) is an indicator function and t denotes a voxelization threshold. Higher IoU values indicate better reconstruction results. Following <ref type="bibr" target="#b40">Tatarchenko et al. (2019)</ref>, we also take F-Score as an extra metric to evaluate the performance of 3D reconstruction results, which can be defined as</p><formula xml:id="formula_5">F-Score(d) = 2P (d)R(d) P (d) + R(d)<label>(5)</label></formula><p>where P (d) and R(d) denote the precision and recall for a distance threshold d, respectively. They can be computed as</p><formula xml:id="formula_6">P (d) = 1 n R r∈R min g∈G ||g − r|| &lt; d (6) R(d) = 1 n G g∈G min r∈R ||g − r|| &lt; d<label>(7)</label></formula><p>where R and G denote the reconstructed and ground truth point clouds, respectively. n R and n G are the numbers of points in R and G, respectively. For voxel reconstruction methods, we first apply the marching cubes algorithm <ref type="bibr" target="#b22">(Lorensen and Cline, 1987)</ref> to generate the object surface. We then sample 8,192 points from the object surface to compute F-Score between prediction and ground truth. For mesh and signed distance field reconstruction methods, we also sample 8,192 points from the object surface to compute F-Score. Higher F-Score values indicate better reconstructions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>We train the proposed methods with batch size 64 using 224 × 224 RGB images as input. The output voxelized reconstruction is 32 3 in size. We implement our networks in PyTorch <ref type="bibr" target="#b30">(Paszke et al., 2019)</ref> and train both Pix2Vox++/F and Pix2Vox++/A using an Adam optimizer <ref type="bibr" target="#b18">(Kingma and Ba, 2015)</ref> with a β 1 of 0.9 and a β 2 of 0.999. The initial learning rate is set to 0.001 and decayed by 2 after 150 epochs. First, we train networks except for the multi-scale context-aware fusion using single-view images for 250 epochs. We then train the networks using multi-view images for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on the ShapeNet Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Single-view 3D Object Reconstruction</head><p>To evaluate the performance of the proposed methods in handling clean background images, we compare our methods against several state-of-the-art methods, including 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref>, OGN <ref type="bibr" target="#b39">(Tatarchenko et al., 2017)</ref>, Matryoshaka <ref type="bibr" target="#b32">(Richter and Roth, 2018)</ref>, AtlasNet <ref type="bibr" target="#b9">(Groueix et al., 2018)</ref>, Pixel2Mesh <ref type="bibr" target="#b42">(Wang et al., 2018)</ref>, OccNet <ref type="bibr" target="#b23">(Mescheder et al., 2019)</ref>, IM-Net <ref type="bibr" target="#b3">(Chen and Zhang, 2019)</ref>, and AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref>. Tables 1 and 2 show the IoU and F-Score@1% of all methods on the ShapeNet test set. Both Pix2Vox++/F and Pix2Vox++/A outperform all competitive methods in terms of both IoU and F-Score@1%. <ref type="figure" target="#fig_3">Figure 6</ref> provides qualitative results for single-view reconstruction results showing that Pix2Vox++/F and Pix2Vox++/A generate more visually compelling 3D shapes than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Multi-view 3D Object Reconstruction</head><p>To evaluate the performance of reconstructing 3D objects from multi-view images, we compare the proposed methods with 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref> and AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref>. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the proposed Pix2Vox++/F and Pix2Vox++/A consistently outperform 3D-R2N2 and AttSets in all numbers of views. <ref type="figure">Figure 7</ref> shows several examples reconstructed from three  input images. Both Pix2Vox++/F and Pix2Vox++/A are able to recover better details than other methods. Pix2Vox++/A performs better in 3D object reconstruction by comparing with Pix2Vox++/F. To provide a detailed analysis of the multi-scale context-aware fusion, we visualize score maps of corresponding coarse volumes when reconstructing the 3D shape of a table and a chair, as shown in <ref type="figure">Figure 3</ref>. The chair seat on the right is of low quality, and the score of the corresponding part is lower than that in the other coarse volumes. The fused 3D volume is obtained by combining selected high-quality reconstruction parts, where bad reconstructions can be effectively eliminated by our scoring scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Higher-Resolution 3D Object Reconstruction</head><p>Low-resolution 3D volumes are naturally limited to the low level of details they can represent. To evaluate the performance of Pix2Vox++ in high-resolution 3D reconstruction, we compare it to Matryoshka Networks <ref type="bibr" target="#b32">(Richter and Roth, 2018)</ref> and OGN <ref type="bibr" target="#b39">(Tatarchenko et al., 2017)</ref>. We follow the experimental setup of OGN and predict 3D volumes of ShapeNet-Cars at 64 3 and 128 3 resolutions given a single RGB image. We then upsample the predicted 3D volumes to 256 3 resolution and compute IoU with ground truth shapes. To make a fair comparison, we use the same dataset split and ground truth shapes generated by OGN. We report quantita-  tive results in <ref type="table" target="#tab_3">Table 4</ref> and qualitative results at 128 3 resolution in <ref type="figure" target="#fig_4">Figure 8</ref>. Experimental results on singleview reconstruction show that both Pix2Vox++/F and Pix2Vox++/A outperform Matryoshka Networks and OGN at 64 3 resolution. At 128 3 resolution, Pix2Vox++/A outforms Matryoshka Networks and OGN. Pix2Vox++/F is comparable to both competitive methods. As shown in <ref type="figure" target="#fig_4">Figure 8</ref>, our methods recover better details than compared methods. Furthermore, we provide results for high-resolution reconstruction from multi-view images.</p><p>Experimental results show that Pix2Vox++/A outperforms Pix2Vox++/F in all numbers of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation on the Pix3D Dataset</head><p>To evaluate the performance of the proposed methods on real-world images, we evaluate our methods for single-view reconstruction on the Pix3D dataset. We train Pix2Vox++/F and Pix2Vox++/A on ShapeNet-Chairs and Things3D-Chairs and test both networks on the chair category of the Pix3D dataset. As shown in <ref type="table" target="#tab_4">Table 5</ref>, our networks trained on Things3D-Chairs have better results than those trained on ShapeNet-Chairs. Following Pix3D <ref type="bibr" target="#b38">(Sun et al., 2018)</ref>, we use Render for CNN  to generate 60 images for each chair in the ShapeNet dataset by adding random backgrounds sampled from the SUN database <ref type="bibr" target="#b49">(Xiao et al., 2010)</ref>, i.e. ShapeNet-Chairs-RfC. We also render a new dataset, called Things-3D-Chairs-RfC, by rendering chairs in the naturalist scenes where the camera poses are sampled from a distribution estimated on the Pix3D dataset. <ref type="table" target="#tab_4">Table 5</ref> illustrates that our networks trained on Things3D-Chairs-RfC have better results in reconstructing 3D objects in Pix3D than those trained on ShapeNet-Chairs-RfC. The two above experiments show that the networks trained on datasets generated by rendering naturalist scenes archive better re- sults than those trained on datasets with random backgrounds. Furthermore, the proposed Pix2Vox++/A trained on Things3D-Chairs-RfC archives the best results on the Pix3D dataset, suggesting that the Thing3D dataset helps the networks generalize better to realworld datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluation on the Things3D Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Single-view 3D Object Reconstruction</head><p>To evaluate the performance of the proposed methods in dealing with naturalistic images, we compare our methods to several state-of-the-art methods on the Things3D test set. We fine-tune all competitive methods on Things3D training and crop the input images as required by each method. To make a fair comparison, all methods are fed with the same input images during testing. The IoU and F-Score@1% are reported in <ref type="table" target="#tab_5">Tables 6 and 7</ref>, respectively. Both Pix2Vox++/F and Pix2Vox++/A outperform all competitive methods. <ref type="figure">Figure 11</ref> shows the qualitative results, which indicate that Pix2Vox++/A has the best ability to recover the 3D shapes from a single natural scene image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Multi-view 3D Object Reconstruction</head><p>We also compare the proposed methods with 3D-R2N2 <ref type="bibr" target="#b4">(Choy et al., 2016)</ref> and AttSets <ref type="bibr" target="#b54">(Yang et al., 2020)</ref> in reconstructing objects in natural scenes from multi-view images. As mentioned in Section 4, different objects have different occlusions in different scenes, which leads to different numbers of views for these objects. To use the same test set in different numbers of views, we only use test samples with no less than eight rendering images in this experiment. To make a fair comparison, we feed all methods with the same images and crop the images as required by each method during testing. <ref type="table" target="#tab_7">Table  8</ref> shows the multi-view reconstruction results on the Things3D test set. The proposed Pix2Vox++/F and Pix2Vox++/A consistently outperform 3D-R2N2 and AttSets in all numbers of views. As shown in <ref type="figure">Figure  9</ref>, Pix2Vox++/F and Pix2Vox++/A perform better at reconstructing the 3D shape of an object from multiple natural images. <ref type="figure">Fig. 11</ref> Example of single-view 3D object reconstruction on Things3D. For voxel reconstruction methods, the output 3D volumes are at 32 3 resolution.  6 Analysis and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effectiveness of Different Backbone Models</head><p>To provide a detailed analysis of different backbone models, we replace ResNet50 in Pix2Vox++/A with other backbone models, including VGG <ref type="bibr" target="#b35">(Simonyan and Zisserman, 2015)</ref> and DenseNet <ref type="bibr" target="#b13">(Huang et al., 2017)</ref>. We only use partial convolutional layers in backbone models that produce feature maps of size 512 × 28 × 28 to guarantee that the rest of the network architecture of the encoder is the same. We report the IoU on ShapeNet in <ref type="table">Table 9</ref>. Encoders with the pretrained models perform slightly better than those without pretrained models. Compared to VGG and DenseNet, the encoder with ResNet50 has the best performance in terms of both accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effectiveness of the Refiner</head><p>Pix2Vox++/A uses a refiner to further correct the wrongly recovered parts in the fused 3D volume, which has an IoU of 0.670 for single-view reconstruction on ShapeNet. In contrast, IoU decreases to 0.658 without  <ref type="figure" target="#fig_0">Fig. 12</ref> Effectiveness of the refiner and the number of views on the evaluation IoU. a refiner. As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, removing the refiner causes considerable degeneration in reconstruction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effectiveness of the Camera Parameters</head><p>Pix2Vox++ recovers the 3D shape of an object without knowing the camera parameters. It aligns multiview features with the supervision of ground truth 3D volumes with canonical orientation. In contrast, LSM <ref type="table" target="#tab_0">Table 10</ref> Comparison of multi-view 3D object reconstruction on ShapeNet at 32 3 resolution. We report the mean IoU for all categories. Note that both LSM and LSM-Ctx-Fusion take camera parameters as an additional input. The ShapeNet dataset is provided by LSM <ref type="bibr" target="#b16">(Kar et al., 2017</ref>  <ref type="bibr" target="#b16">(Kar et al., 2017)</ref> aligns multi-view features with the unprojection operation, which requires camera parameters as input. <ref type="table" target="#tab_0">Table 10</ref> shows multi-view reconstruction results on ShapeNet compared to LSM. Experimental results show that LSM significantly outperforms Pix2Vox++/F and Pix2Vox++/A with more than one view, indicating that precise camera parameters help align features of multi-view images better.</p><p>To further demonstrate the superior ability of the multi-scale context-aware fusion in multi-view stereo (MVS) systems, we replace the recurrent fusion in LSM with the multi-scale context-aware fusion to fuse features extracted from multiple input images, denoted by LSM-Ctx-Fusion. As shown in <ref type="table" target="#tab_0">Table 10</ref>, LSM-Ctx-Fusion outperforms LSM in all numbers of views.  <ref type="figure">. 13</ref> Example of multi-view 3D object reconstruction on ShapeNet-Cars at 128 3 resolution. "Ctx Fusion" and "MCtx Fusion" denote the context-aware fusion and the multi-scale context-aware fusion, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison with Other Fusion Methods</head><p>To quantitatively evaluate multi-scale context-aware fusion, we replace the multi-scale context-aware fusion in Pix2Vox++/A with the average fusion, context-aware fusion, 3D convolutional LSTM, and attentional aggregation, respectively. Average Pooling Fusion. In the average pooling fusion, the voxel at (i, j, k) among different coarse volumes are averaged. Specifically, the value of the fused voxel v f can be calculated as</p><formula xml:id="formula_7">v f (i,j,k) = 1 n n r=1 v r (i,j,k)<label>(8)</label></formula><p>As shown in <ref type="table" target="#tab_0">Table 13</ref>, replacing the multi-scale contextaware fusion with the average fusion in Pix2Vox++/F and Pix2Vox++/A causes degeneration in reconstruction results.</p><p>Context-aware Fusion. We also compare the multiscale context-aware fusion with the context-aware fusion in the preliminary version <ref type="bibr" target="#b51">(Xie et al., 2019)</ref>, denoted by Pix2Vox++/F ‡ and Pix2Vox++/A ‡ , respectively. As shown in <ref type="table" target="#tab_0">Table 13</ref>, IoU has an approximately 1% increase in the multi-scale context-aware fusion compared to the context-aware fusion. Accurately recon- structing a 3D volume at high resolution is challenging due to increasing voxels reflecting object details. Both the average fusion and context-aware fusion may not fully exploit the features of multi-view images. In contrast, the proposed multi-scale context-aware fusion preserves object details by concatenating feature maps with different scales. As shown in <ref type="table" target="#tab_0">Table 11</ref>, the multiscale context-aware fusion outperforms the average fusion and the context-aware fusion in reconstructing highresolution 3D volumes. As illustrated in <ref type="figure">Figure 13</ref>, the multi-scale context-aware fusion recovers better details than the context-aware fusion. 3D Convolutional LSTM. To further compare with RNN-based fusion, we remove the multi-scale contextaware fusion from Pix2Vox++/A and add a 3D convolutional LSTM <ref type="bibr" target="#b4">(Choy et al., 2016)</ref> after the encoder. To fit the 3D convolutional LSTM input, we add an additional fully connected layer with a dimension of 1024 before it. The resulting method is named Pix2Vox++/A-R2N2. As shown in <ref type="table" target="#tab_0">Table 13</ref>, both Pix2Vox++/A and Pix2Vox++/A † consistently outperform Pix2Vox++/A-R2N2 in all numbers of views. Attentional Aggregation. To demonstrate the superior reconstruction ability over the attentional aggregation <ref type="bibr" target="#b54">(Yang et al., 2020)</ref>, we remove the multiscale context-aware fusion from Pix2Vox++/A and add an attentional aggregation module after the encoder, denoted by Pix2Vox++/A-AttSets. Experimental results in <ref type="table" target="#tab_0">Table 13</ref> show that Pix2Vox++/A outperforms Pix2Vox++/A-AttSets in all numbers of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Viewer-centered vs. Object-centered Coordinates</head><p>As mentioned in Section 6.3, Pix2Vox++ relies on the object-centered coordinates to align multi-view features. However, object-centered coordinates encourage the network to memorize observed meshes, which may lead to poor generalization abilities <ref type="bibr" target="#b34">(Shin et al., 2018)</ref>.</p><p>To evaluate the generalization capability of the proposed methods, we compare the performance of recon- structing 3D objects from "seen" and "unseen" categories in both viewer-centered and object-centered coordinates. For object-centered prediction, different views of the same object should produce the same 3D shape.</p><p>In viewer-centered coordinates, the reconstructed 3D object should be oriented according to the input viewpoint, so different views of the same object correspond to different 3D shapes.</p><p>In this experiment, we use Blender to render objects in 57 categories of ShapeNetCore <ref type="bibr" target="#b2">(Chang et al., 2015)</ref> from 24 random views for each object. When reconstructing 3D objects from "unseen" categories, all pretrained models have never "seen" either the objects in these categories or the labels of the objects before. More specifically, all methods are pretrained on the 13 major categories of ShapeNet and tested on the remaining 44 categories of ShapeNetCore with the same input images. As shown in <ref type="table" target="#tab_0">Table 12</ref>, 3D shapes produced by object-centered models outperform those produced by viewer-centered models for objects from "seen" categories, suggesting that reconstructing viewer-centered 3D shapes of objects is more challenging. For "unseen" categories, viewer-centered models perform better than object-centered models, suggesting that viewer-centered reconstruction improves the generalization ability to reconstruct 3D shapes from "unseen" categories. Compared to OGN <ref type="bibr" target="#b39">(Tatarchenko et al., 2017)</ref> and Matryo-shka Networks <ref type="bibr" target="#b32">(Richter and Roth, 2018)</ref>, Pix2Vox++/F and Pix2Vox++/A perform better at reconstructing 3D shapes in both object-centered and viewer-centered coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Space and Time Complexity</head><p>To test the space and time complexity of our methods, we compare them with several state-of-the-art methods in terms of number of parameters, memory usage, and inference time. <ref type="table" target="#tab_0">Table 14</ref> presents the comparison results in single-view and multi-view reconstruction, where the voxel reconstruction methods are at 32 3 resolution. Table 15 provides a comparison of the single-view reconstruction results at 128 3 resolution.</p><p>Running times are obtained on the same PC with a single NVIDIA GTX 1080 Ti GPU. For more precise timing, we exclude reading and writing time when evaluating inference time. For multi-view reconstruction, both Pix2Vox++/F and Pix2Vox++/A outperform 3D-R2N2 and AttSets in inference time and training time. Both Pix2Vox++/F and Pix2Vox++/A are approximately seven times faster in forward inference than 3D-R2N2 for single-view reconstruction at 32 3 resolution. Although the proposed methods outperform OGN <ref type="bibr" target="#b39">(Tatarchenko et al., 2017)</ref> and Matryoshka Net- <ref type="table" target="#tab_0">Table 15</ref> The numbers of parameters, memory footprint, and inference time at 128 3 resolution on the ShapeNet dataset. P2V/F and P2V/A denote Pix2Vox++/F and Pix2Vox++/A for 128 3 resolution. "Inf. Time" stands for "Inference Time" for single-view reconstruction with a batch size of 1. Note that the memory is measured in backward computation of single-view reconstruction with a batch size of 1. works <ref type="bibr" target="#b32">(Richter and Roth, 2018)</ref> in reconstructing highresolution 3D volumes, memory requirements scale dramatically with the resolution of 3D volumes because our methods do not use efficient data representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a unified framework for both single-view and multi-view 3D reconstruction, named Pix2Vox++. Compared with existing methods that directly fuse the features from multi-view images, the proposed framework fuses the 3D volumes reconstructed from input images, which better preserves multi-view spatial constraints. In addition, we construct the first large-scale naturalistic dataset for multi-view 3D object reconstruction, named Things3D, containing 1.68M images of 280K objects collected from over 39K indoor scenes. Quantitative and qualitative evaluation for both single-view and multi-view reconstruction on the ShapeNet, Pix3D, and Things3D benchmarks shows that the proposed methods perform favorably against state-of-the-art methods. The proposed methods are also computationally efficient, about seven times faster than 3D-R2N2 in terms of inference time in single-view reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>The network architectures of Pix2Vox++/F (top) and Pix2Vox++/A (bottom) for low-resolution reconstruction. EDLoss and RLoss are defined in Equation 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Visualization of the score maps in the multi-scale context-aware fusion module. An overview of the multi-scale context-aware fusion module. It aims to select high-quality reconstructions for each part to construct the final results. The objects in the bounding box describe the procedure score calculation for a coarse volume v c 1 . The other scores are calculated according to the same procedure. Note that the weights of the context scoring network are shared among different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Sample images and the corresponding CAD models in Things3D, where each 3D model is rendered from a diverse set of naturalistic scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Example of single-view 3D object reconstruction on ShapeNet. For voxel reconstruction methods, the output 3D volumes are at 32 3 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Example of single-view 3D object reconstruction on ShapeNet-Cars at 128 3 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10</head><label>10</label><figDesc>Example of single-view 3D object reconstruction on Pix3D at 32 3 resolution trained on ShapeNet-Chairs-RfC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison of single-view 3D object reconstruction on ShapeNet at 32 3 resolution. We report the mean IoU per category. The best number for each category is highlighted in bold.</figDesc><table><row><cell>Category</cell><cell cols="10">3D-R2N2 OGN Matryoshka AtlasNet Pixel2Mesh OccNet IM-Net AttSets Pix2Vox++/F Pix2Vox++/A</cell></row><row><cell>airplane</cell><cell>0.513</cell><cell>0.587</cell><cell>0.647</cell><cell>0.493</cell><cell>0.508</cell><cell>0.532</cell><cell>0.702</cell><cell>0.594</cell><cell>0.607</cell><cell>0.674</cell></row><row><cell>bench</cell><cell>0.421</cell><cell>0.481</cell><cell>0.577</cell><cell>0.431</cell><cell>0.379</cell><cell>0.597</cell><cell>0.564</cell><cell>0.552</cell><cell>0.544</cell><cell>0.608</cell></row><row><cell>cabinet</cell><cell>0.716</cell><cell>0.729</cell><cell>0.776</cell><cell>0.257</cell><cell>0.732</cell><cell>0.674</cell><cell>0.680</cell><cell>0.783</cell><cell>0.782</cell><cell>0.799</cell></row><row><cell>car</cell><cell>0.798</cell><cell>0.828</cell><cell>0.850</cell><cell>0.282</cell><cell>0.670</cell><cell>0.671</cell><cell>0.756</cell><cell>0.844</cell><cell>0.841</cell><cell>0.858</cell></row><row><cell>chair</cell><cell>0.466</cell><cell>0.483</cell><cell>0.547</cell><cell>0.328</cell><cell>0.484</cell><cell>0.583</cell><cell>0.644</cell><cell>0.559</cell><cell>0.548</cell><cell>0.581</cell></row><row><cell>display</cell><cell>0.468</cell><cell>0.502</cell><cell>0.532</cell><cell>0.457</cell><cell>0.582</cell><cell>0.651</cell><cell>0.585</cell><cell>0.565</cell><cell>0.529</cell><cell>0.548</cell></row><row><cell>lamp</cell><cell>0.381</cell><cell>0.398</cell><cell>0.408</cell><cell>0.261</cell><cell>0.399</cell><cell>0.474</cell><cell>0.433</cell><cell>0.445</cell><cell>0.448</cell><cell>0.457</cell></row><row><cell>speaker</cell><cell>0.662</cell><cell>0.637</cell><cell>0.701</cell><cell>0.296</cell><cell>0.672</cell><cell>0.655</cell><cell>0.683</cell><cell>0.721</cell><cell>0.721</cell><cell>0.721</cell></row><row><cell>rifle</cell><cell>0.544</cell><cell>0.593</cell><cell>0.616</cell><cell>0.573</cell><cell>0.468</cell><cell>0.656</cell><cell>0.723</cell><cell>0.601</cell><cell>0.594</cell><cell>0.617</cell></row><row><cell>sofa</cell><cell>0.628</cell><cell>0.646</cell><cell>0.681</cell><cell>0.354</cell><cell>0.622</cell><cell>0.669</cell><cell>0.694</cell><cell>0.703</cell><cell>0.696</cell><cell>0.725</cell></row><row><cell>table</cell><cell>0.513</cell><cell>0.536</cell><cell>0.573</cell><cell>0.301</cell><cell>0.536</cell><cell>0.659</cell><cell>0.621</cell><cell>0.590</cell><cell>0.609</cell><cell>0.620</cell></row><row><cell>telephone</cell><cell>0.661</cell><cell>0.702</cell><cell>0.756</cell><cell>0.543</cell><cell>0.762</cell><cell>0.794</cell><cell>0.762</cell><cell>0.743</cell><cell>0.782</cell><cell>0.809</cell></row><row><cell>watercraft</cell><cell>0.513</cell><cell>0.632</cell><cell>0.591</cell><cell>0.355</cell><cell>0.471</cell><cell>0.579</cell><cell>0.607</cell><cell>0.601</cell><cell>0.583</cell><cell>0.603</cell></row><row><cell>Overall</cell><cell>0.560</cell><cell>0.596</cell><cell>0.635</cell><cell>0.352</cell><cell>0.552</cell><cell>0.626</cell><cell>0.659</cell><cell>0.642</cell><cell>0.645</cell><cell>0.670</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparison of single-view 3D object reconstruction on ShapeNet. We report the mean F-Score@1% per category. For voxel reconstruction methods, the points are sampled from triangular meshes generated by the marching cube algorithm. The best number for each category is highlighted in bold.</figDesc><table><row><cell>Category</cell><cell cols="10">3D-R2N2 OGN Matryoshka AtlasNet Pixel2Mesh OccNet IM-Net AttSets Pix2Vox++/F Pix2Vox++/A</cell></row><row><cell>airplane</cell><cell>0.412</cell><cell>0.487</cell><cell>0.446</cell><cell>0.415</cell><cell>0.376</cell><cell>0.494</cell><cell>0.598</cell><cell>0.489</cell><cell>0.493</cell><cell>0.583</cell></row><row><cell>bench</cell><cell>0.345</cell><cell>0.364</cell><cell>0.424</cell><cell>0.439</cell><cell>0.313</cell><cell>0.318</cell><cell>0.361</cell><cell>0.406</cell><cell>0.399</cell><cell>0.478</cell></row><row><cell>cabinet</cell><cell>0.327</cell><cell>0.316</cell><cell>0.381</cell><cell>0.350</cell><cell>0.450</cell><cell>0.449</cell><cell>0.345</cell><cell>0.367</cell><cell>0.363</cell><cell>0.408</cell></row><row><cell>car</cell><cell>0.481</cell><cell>0.514</cell><cell>0.481</cell><cell>0.319</cell><cell>0.486</cell><cell>0.315</cell><cell>0.304</cell><cell>0.497</cell><cell>0.523</cell><cell>0.564</cell></row><row><cell>chair</cell><cell>0.238</cell><cell>0.226</cell><cell>0.302</cell><cell>0.406</cell><cell>0.386</cell><cell>0.365</cell><cell>0.442</cell><cell>0.334</cell><cell>0.262</cell><cell>0.309</cell></row><row><cell>display</cell><cell>0.227</cell><cell>0.215</cell><cell>0.400</cell><cell>0.451</cell><cell>0.319</cell><cell>0.468</cell><cell>0.466</cell><cell>0.310</cell><cell>0.253</cell><cell>0.296</cell></row><row><cell>lamp</cell><cell>0.267</cell><cell>0.249</cell><cell>0.276</cell><cell>0.217</cell><cell>0.219</cell><cell>0.361</cell><cell>0.371</cell><cell>0.315</cell><cell>0.287</cell><cell>0.315</cell></row><row><cell>speaker</cell><cell>0.231</cell><cell>0.225</cell><cell>0.279</cell><cell>0.199</cell><cell>0.190</cell><cell>0.249</cell><cell>0.200</cell><cell>0.211</cell><cell>0.256</cell><cell>0.152</cell></row><row><cell>rifle</cell><cell>0.521</cell><cell>0.541</cell><cell>0.514</cell><cell>0.405</cell><cell>0.340</cell><cell>0.219</cell><cell>0.407</cell><cell>0.524</cell><cell>0.553</cell><cell>0.574</cell></row><row><cell>sofa</cell><cell>0.274</cell><cell>0.290</cell><cell>0.326</cell><cell>0.337</cell><cell>0.343</cell><cell>0.324</cell><cell>0.354</cell><cell>0.334</cell><cell>0.320</cell><cell>0.377</cell></row><row><cell>table</cell><cell>0.340</cell><cell>0.352</cell><cell>0.374</cell><cell>0.373</cell><cell>0.502</cell><cell>0.549</cell><cell>0.461</cell><cell>0.419</cell><cell>0.385</cell><cell>0.406</cell></row><row><cell>telephone</cell><cell>0.504</cell><cell>0.528</cell><cell>0.598</cell><cell>0.545</cell><cell>0.485</cell><cell>0.273</cell><cell>0.423</cell><cell>0.469</cell><cell>0.588</cell><cell>0.633</cell></row><row><cell>watercraft</cell><cell>0.305</cell><cell>0.328</cell><cell>0.360</cell><cell>0.296</cell><cell>0.266</cell><cell>0.347</cell><cell>0.369</cell><cell>0.315</cell><cell>0.346</cell><cell>0.390</cell></row><row><cell>Overall</cell><cell>0.351</cell><cell>0.368</cell><cell>0.391</cell><cell>0.362</cell><cell>0.398</cell><cell>0.393</cell><cell>0.405</cell><cell>0.395</cell><cell>0.394</cell><cell>0.436</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of multi-view 3D object reconstruction on ShapeNet at 32 3 resolution. We report the mean IoU and F-Score@1% for all categories.</figDesc><table><row><cell>Methods (IoU)</cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell><cell>5 views</cell><cell>8 views</cell><cell cols="3">12 views 16 views 20 views</cell></row><row><cell>Metric: IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-R2N2</cell><cell>0.560</cell><cell>0.603</cell><cell>0.617</cell><cell>0.625</cell><cell>0.634</cell><cell>0.635</cell><cell>0.636</cell><cell>0.636</cell><cell>0.636</cell></row><row><cell>AttSets</cell><cell>0.642</cell><cell>0.662</cell><cell>0.670</cell><cell>0.675</cell><cell>0.677</cell><cell>0.685</cell><cell>0.688</cell><cell>0.692</cell><cell>0.693</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.645</cell><cell>0.669</cell><cell>0.678</cell><cell>0.682</cell><cell>0.685</cell><cell>0.690</cell><cell>0.692</cell><cell>0.693</cell><cell>0.694</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.670</cell><cell>0.695</cell><cell>0.704</cell><cell>0.708</cell><cell>0.711</cell><cell>0.715</cell><cell>0.717</cell><cell>0.718</cell><cell>0.719</cell></row><row><cell cols="2">Metric: F-Score@1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-R2N2</cell><cell>0.351</cell><cell>0.368</cell><cell>0.372</cell><cell>0.378</cell><cell>0.382</cell><cell>0.383</cell><cell>0.382</cell><cell>0.382</cell><cell>0.383</cell></row><row><cell>AttSets</cell><cell>0.395</cell><cell>0.418</cell><cell>0.426</cell><cell>0.430</cell><cell>0.432</cell><cell>0.444</cell><cell>0.445</cell><cell>0.447</cell><cell>0.448</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.394</cell><cell>0.422</cell><cell>0.432</cell><cell>0.437</cell><cell>0.440</cell><cell>0.446</cell><cell>0.449</cell><cell>0.450</cell><cell>0.451</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.436</cell><cell>0.452</cell><cell>0.455</cell><cell>0.457</cell><cell>0.458</cell><cell>0.459</cell><cell>0.460</cell><cell>0.461</cell><cell>0.462</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparison of single-view and multi-view 3D object reconstruction on ShapeNet-Cars at 64 3 and 128 3 resolutions. We report the mean IoU and F-Score@1% of all models.</figDesc><table><row><cell>Methods</cell><cell>1 view</cell><cell>2 views</cell><cell>IoU</cell><cell>4 views</cell><cell>8 views</cell><cell>1 view</cell><cell cols="2">F-Score@1% 2 views 4 views</cell><cell>8 views</cell></row><row><cell>Resolution: 64 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGN</cell><cell>0.771</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.361</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Matryoshka</cell><cell>0.784</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.380</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.793</cell><cell>0.807</cell><cell></cell><cell>0.811</cell><cell>0.815</cell><cell>0.401</cell><cell>0.429</cell><cell>0.439</cell><cell>0.453</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.803</cell><cell>0.813</cell><cell></cell><cell>0.815</cell><cell>0.819</cell><cell>0.418</cell><cell>0.448</cell><cell>0.450</cell><cell>0.457</cell></row><row><cell>Resolution: 128 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGN</cell><cell>0.782</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.390</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Matryoshka</cell><cell>0.794</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>0.426</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.817</cell><cell>0.832</cell><cell></cell><cell>0.838</cell><cell>0.840</cell><cell>0.459</cell><cell>0.502</cell><cell>0.520</cell><cell>0.528</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.826</cell><cell>0.837</cell><cell></cell><cell>0.841</cell><cell>0.843</cell><cell>0.475</cell><cell>0.509</cell><cell>0.521</cell><cell>0.539</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Comparison of single-view 3D object reconstruction on Pix3D at 32 3 resolution. We report the mean IoU and F-Score@1% of the chair category. The best number is highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>IoU</cell><cell>F-Score@1%</cell></row><row><cell cols="2">Training on ShapeNet-Chairs</cell><cell></cell></row><row><cell>Pix2Vox++/F</cell><cell>0.179</cell><cell>0.012</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.204</cell><cell>0.018</cell></row><row><cell cols="2">Training on Things3D-Chairs</cell><cell></cell></row><row><cell>Pix2Vox++/F</cell><cell>0.256</cell><cell>0.028</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.269</cell><cell>0.036</cell></row><row><cell cols="2">Training on ShapeNet-Chairs-RfC</cell><cell></cell></row><row><cell>Pix3D</cell><cell>0.282</cell><cell>0.041</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.276</cell><cell>0.042</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.292</cell><cell>0.068</cell></row><row><cell cols="2">Training on Things3D-Chairs-RfC</cell><cell></cell></row><row><cell>Pix2Vox++/F</cell><cell>0.297</cell><cell>0.072</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.324</cell><cell>0.084</cell></row><row><cell cols="3">Fig. 9 Example of multi-view 3D object reconstruction on</cell></row><row><cell cols="2">Things3D at 32 3 resolution.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Comparison of single-view 3D object reconstruction on Things3D at 32 3 resolution. We report the mean IoU per category. The best number for each category is highlighted in bold.</figDesc><table><row><cell cols="11">Category 3D-R2N2 OGN Matryoshka AtlasNet Pixel2Mesh OccNet IM-Net AttSets Pix2Vox++/F Pix2Vox++/A</cell></row><row><cell>chair</cell><cell>0.327</cell><cell>0.212</cell><cell>0.399</cell><cell>0.251</cell><cell>0.372</cell><cell>0.432</cell><cell>0.462</cell><cell>0.403</cell><cell>0.435</cell><cell>0.442</cell></row><row><cell>display</cell><cell>0.240</cell><cell>0.153</cell><cell>0.332</cell><cell>0.259</cell><cell>0.311</cell><cell>0.328</cell><cell>0.324</cell><cell>0.301</cell><cell>0.324</cell><cell>0.349</cell></row><row><cell>lamp</cell><cell>0.257</cell><cell>0.189</cell><cell>0.323</cell><cell>0.196</cell><cell>0.306</cell><cell>0.361</cell><cell>0.328</cell><cell>0.334</cell><cell>0.350</cell><cell>0.362</cell></row><row><cell>piano</cell><cell>0.072</cell><cell>0.060</cell><cell>0.234</cell><cell>0.064</cell><cell>0.087</cell><cell>0.168</cell><cell>0.156</cell><cell>0.194</cell><cell>0.190</cell><cell>0.244</cell></row><row><cell>sofa</cell><cell>0.457</cell><cell>0.450</cell><cell>0.548</cell><cell>0.284</cell><cell>0.490</cell><cell>0.525</cell><cell>0.550</cell><cell>0.554</cell><cell>0.560</cell><cell>0.569</cell></row><row><cell>table</cell><cell>0.159</cell><cell>0.116</cell><cell>0.305</cell><cell>0.137</cell><cell>0.247</cell><cell>0.317</cell><cell>0.297</cell><cell>0.306</cell><cell>0.305</cell><cell>0.320</cell></row><row><cell>Overall</cell><cell>0.313</cell><cell>0.244</cell><cell>0.395</cell><cell>0.228</cell><cell>0.360</cell><cell>0.414</cell><cell>0.419</cell><cell>0.400</cell><cell>0.419</cell><cell>0.430</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Comparison of single-view 3D object reconstruction on Things3D. We report the mean F-Score@1% per category and the average F-Score@1% for all categories. The best number for each category is highlighted in bold.</figDesc><table><row><cell cols="11">Category 3D-R2N2 OGN Matryoshka AtlasNet Pixel2Mesh OccNet IM-Net AttSets Pix2Vox++/F Pix2Vox++/A</cell></row><row><cell>chair</cell><cell>0.166</cell><cell>0.096</cell><cell>0.231</cell><cell>0.268</cell><cell>0.248</cell><cell>0.272</cell><cell>0.253</cell><cell>0.244</cell><cell>0.240</cell><cell>0.273</cell></row><row><cell>display</cell><cell>0.136</cell><cell>0.126</cell><cell>0.164</cell><cell>0.124</cell><cell>0.128</cell><cell>0.266</cell><cell>0.277</cell><cell>0.172</cell><cell>0.150</cell><cell>0.163</cell></row><row><cell>lamp</cell><cell>0.177</cell><cell>0.098</cell><cell>0.208</cell><cell>0.166</cell><cell>0.170</cell><cell>0.272</cell><cell>0.248</cell><cell>0.229</cell><cell>0.249</cell><cell>0.275</cell></row><row><cell>piano</cell><cell>0.012</cell><cell>0.006</cell><cell>0.127</cell><cell>0.069</cell><cell>0.057</cell><cell>0.036</cell><cell>0.095</cell><cell>0.099</cell><cell>0.108</cell><cell>0.136</cell></row><row><cell>sofa</cell><cell>0.189</cell><cell>0.214</cell><cell>0.257</cell><cell>0.268</cell><cell>0.261</cell><cell>0.264</cell><cell>0.259</cell><cell>0.253</cell><cell>0.252</cell><cell>0.270</cell></row><row><cell>table</cell><cell>0.108</cell><cell>0.086</cell><cell>0.182</cell><cell>0.182</cell><cell>0.177</cell><cell>0.201</cell><cell>0.185</cell><cell>0.172</cell><cell>0.198</cell><cell>0.200</cell></row><row><cell>Overall</cell><cell>0.165</cell><cell>0.118</cell><cell>0.223</cell><cell>0.226</cell><cell>0.217</cell><cell>0.259</cell><cell>0.244</cell><cell>0.231</cell><cell>0.238</cell><cell>0.263</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Comparison of multi-view 3D object reconstruction on Things3D at 32 3 resolution. We report the mean IoU and F-Score@1% for all categories.</figDesc><table><row><cell cols="2">Methods</cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell><cell>5 views</cell><cell>6 views</cell><cell>7 views</cell><cell>8 views</cell></row><row><cell cols="2">Metric: IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3D-R2N2</cell><cell>0.307</cell><cell>0.316</cell><cell>0.322</cell><cell>0.325</cell><cell>0.329</cell><cell>0.331</cell><cell>0.332</cell><cell>0.334</cell></row><row><cell>AttSets</cell><cell></cell><cell>0.402</cell><cell>0.415</cell><cell>0.422</cell><cell>0.427</cell><cell>0.429</cell><cell>0.431</cell><cell>0.433</cell><cell>0.434</cell></row><row><cell cols="2">Pix2Vox++/F</cell><cell>0.417</cell><cell>0.433</cell><cell>0.442</cell><cell>0.447</cell><cell>0.451</cell><cell>0.454</cell><cell>0.456</cell><cell>0.458</cell></row><row><cell cols="2">Pix2Vox++/A</cell><cell>0.428</cell><cell>0.444</cell><cell>0.452</cell><cell>0.456</cell><cell>0.460</cell><cell>0.462</cell><cell>0.465</cell><cell>0.467</cell></row><row><cell cols="3">Metric: F-Score@1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3D-R2N2</cell><cell>0.142</cell><cell>0.148</cell><cell>0.151</cell><cell>0.153</cell><cell>0.156</cell><cell>0.157</cell><cell>0.158</cell><cell>0.159</cell></row><row><cell>AttSets</cell><cell></cell><cell>0.228</cell><cell>0.240</cell><cell>0.245</cell><cell>0.247</cell><cell>0.248</cell><cell>0.249</cell><cell>0.250</cell><cell>0.250</cell></row><row><cell cols="2">Pix2Vox++/F</cell><cell>0.230</cell><cell>0.240</cell><cell>0.243</cell><cell>0.246</cell><cell>0.248</cell><cell>0.249</cell><cell>0.250</cell><cell>0.251</cell></row><row><cell cols="2">Pix2Vox++/A</cell><cell>0.260</cell><cell>0.271</cell><cell>0.274</cell><cell>0.275</cell><cell>0.276</cell><cell>0.277</cell><cell>0.278</cell><cell>0.279</cell></row><row><cell cols="10">Table 9 The numbers of parameters, inference time, and the corresponding IoUs of Pix2Vox++/A with different backbone</cell></row><row><cell cols="2">models on ShapeNet.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pretrained Models</cell><cell></cell><cell># Parameters (M)</cell><cell cols="2">Inference Time (ms)</cell><cell></cell><cell></cell><cell>IoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w/o Pretrained</cell><cell cols="2">w/ Pretrained</cell></row><row><cell>VGG16</cell><cell></cell><cell></cell><cell>97.78</cell><cell></cell><cell>11.14</cell><cell></cell><cell>0.659</cell><cell></cell><cell>0.661</cell></row><row><cell>VGG19</cell><cell></cell><cell></cell><cell>98.37</cell><cell></cell><cell>11.19</cell><cell></cell><cell>0.658</cell><cell></cell><cell>0.660</cell></row><row><cell cols="2">ResNet50</cell><cell></cell><cell>96.31</cell><cell></cell><cell>10.64</cell><cell></cell><cell>0.669</cell><cell></cell><cell>0.670</cell></row><row><cell cols="2">DenseNet101</cell><cell></cell><cell>102.85</cell><cell></cell><cell>16.78</cell><cell></cell><cell>0.668</cell><cell></cell><cell>0.669</cell></row><row><cell cols="2">DenseNet169</cell><cell></cell><cell>109.02</cell><cell></cell><cell>18.26</cell><cell></cell><cell>0.668</cell><cell></cell><cell>0.669</cell></row><row><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Intersection over Union (IoU)</cell><cell>0.67 0.68 0.69 0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.66</cell><cell></cell><cell>Pix2Vox++/A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Pix2Vox++/A without Refiner</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">The Number of Views</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc>Comparison of multi-view 3D object reconstruction on ShapeNet-Cars at 128 3 resolution. We report the mean IoU for all categories. The marker † and ‡ denote the multi-scale context-aware fusion is replaced with the average pooling fusion and context-aware fusion, respectively.</figDesc><table><row><cell>Methods</cell><cell cols="4">1 view 2 views 4 views 8 views</cell></row><row><cell>Pix2Vox++/F  †</cell><cell>0.803</cell><cell>0.804</cell><cell>0.805</cell><cell>0.806</cell></row><row><cell>Pix2Vox++/F  ‡</cell><cell>0.803</cell><cell>0.784</cell><cell>0.778</cell><cell>0.768</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.803</cell><cell>0.813</cell><cell>0.815</cell><cell>0.819</cell></row><row><cell>Pix2Vox++/A  †</cell><cell>0.826</cell><cell>0.828</cell><cell>0.829</cell><cell>0.829</cell></row><row><cell>Pix2Vox++/A  ‡</cell><cell>0.826</cell><cell>0.813</cell><cell>0.808</cell><cell>0.801</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.826</cell><cell>0.837</cell><cell>0.841</cell><cell>0.843</cell></row><row><cell>Fig</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12</head><label>12</label><figDesc>Comparison of single-view 3D object reconstruction on the ShapeNetCore dataset. We report the mean IoU for all categories in both object-centered and viewer-centered coordinates. Note that "Unseen" denotes no instances from the categories are seen during training.</figDesc><table><row><cell>Methods</cell><cell cols="2">Object-centered Seen Unseen</cell><cell cols="2">Viewer-centered Seen Unseen</cell></row><row><cell>OGN</cell><cell>0.593</cell><cell>0.154</cell><cell>0.404</cell><cell>0.267</cell></row><row><cell>Matryoshka</cell><cell>0.634</cell><cell>0.187</cell><cell>0.427</cell><cell>0.299</cell></row><row><cell>Pix2Vox++/F</cell><cell>0.632</cell><cell>0.216</cell><cell>0.449</cell><cell>0.324</cell></row><row><cell>Pix2Vox++/A</cell><cell>0.688</cell><cell>0.241</cell><cell>0.485</cell><cell>0.386</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13</head><label>13</label><figDesc>Comparison of multi-view 3D object reconstruction on ShapeNet at 32 3 resolution. We report the mean IoU per category and the average IoU for all categories. The marker † and ‡ denote the multi-scale context-aware fusion is replaced with the average pooling fusion and context-aware fusion, respectively.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell><cell>5 views</cell><cell>8 views</cell><cell cols="3">12 views 16 views 20 views</cell></row><row><cell>Pix2Vox++/F  †</cell><cell></cell><cell>0.645</cell><cell>0.655</cell><cell>0.664</cell><cell>0.668</cell><cell>0.670</cell><cell>0.672</cell><cell>0.673</cell><cell>0.674</cell><cell>0.675</cell></row><row><cell>Pix2Vox++/F  ‡</cell><cell></cell><cell>0.645</cell><cell>0.663</cell><cell>0.673</cell><cell>0.676</cell><cell>0.680</cell><cell>0.683</cell><cell>0.686</cell><cell>0.687</cell><cell>0.688</cell></row><row><cell>Pix2Vox++/F</cell><cell></cell><cell>0.645</cell><cell>0.669</cell><cell>0.678</cell><cell>0.682</cell><cell>0.685</cell><cell>0.690</cell><cell>0.692</cell><cell>0.693</cell><cell>0.694</cell></row><row><cell>Pix2Vox++/A  †</cell><cell></cell><cell>0.670</cell><cell>0.680</cell><cell>0.690</cell><cell>0.695</cell><cell>0.699</cell><cell>0.703</cell><cell>0.704</cell><cell>0.705</cell><cell>0.706</cell></row><row><cell>Pix2Vox++/A  ‡</cell><cell></cell><cell>0.670</cell><cell>0.690</cell><cell>0.699</cell><cell>0.702</cell><cell>0.706</cell><cell>0.710</cell><cell>0.712</cell><cell>0.713</cell><cell>0.714</cell></row><row><cell cols="2">Pix2Vox++/A-R2N2</cell><cell>0.663</cell><cell>0.672</cell><cell>0.680</cell><cell>0.684</cell><cell>0.686</cell><cell>0.688</cell><cell>0.689</cell><cell>0.689</cell><cell>0.690</cell></row><row><cell cols="2">Pix2Vox++/A-AttSets</cell><cell>0.638</cell><cell>0.675</cell><cell>0.689</cell><cell>0.696</cell><cell>0.701</cell><cell>0.707</cell><cell>0.710</cell><cell>0.713</cell><cell>0.713</cell></row><row><cell>Pix2Vox++/A</cell><cell></cell><cell>0.670</cell><cell>0.695</cell><cell>0.704</cell><cell>0.708</cell><cell>0.711</cell><cell>0.715</cell><cell>0.717</cell><cell>0.718</cell><cell>0.719</cell></row><row><cell cols="11">Table 14 The numbers of parameters, memory footprint, and inference time on the ShapeNet dataset. Note that the memory</cell></row><row><cell cols="11">is measured in backward computation of single-view reconstruction with a batch size of 1. The voxel reconstruction methods,</cell></row><row><cell cols="9">including 3D-R2N2, AttSets, Pix2Vox++/F, and Pix2Vox++/A, output 3D volumes at 32 3 resolution.</cell><cell></cell></row><row><cell>Methods</cell><cell cols="10">AtlasNet Pixel2Mesh OccNet IM-Net 3D-R2N2 AttSets Pix2Vox++/F Pix2Vox++/A</cell></row><row><cell>#Parameters (M)</cell><cell cols="2">45.06</cell><cell>21.36</cell><cell>13.43</cell><cell>55.45</cell><cell>35.97</cell><cell>17.71</cell><cell>4.83</cell><cell></cell><cell>96.31</cell></row><row><cell>Memory (MB)</cell><cell cols="2">1293</cell><cell>1289</cell><cell>955</cell><cell>3935</cell><cell>1407</cell><cell>3911</cell><cell>647</cell><cell></cell><cell>2411</cell></row><row><cell cols="2">Inference Time (ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 view</cell><cell cols="2">38.47</cell><cell>60.78</cell><cell>1261</cell><cell>10886</cell><cell>78.86</cell><cell>26.32</cell><cell>9.93</cell><cell></cell><cell>10.64</cell></row><row><cell>2 views</cell><cell cols="2">N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>112.27</cell><cell>47.62</cell><cell>13.55</cell><cell></cell><cell>17.51</cell></row><row><cell>4 views</cell><cell cols="2">N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>116.68</cell><cell>52.63</cell><cell>23.72</cell><cell></cell><cell>29.88</cell></row><row><cell>8 views</cell><cell cols="2">N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>122.04</cell><cell>58.83</cell><cell>39.02</cell><cell></cell><cell>56.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by the National Natural Science Foundation of China under Project (Nos. 61772158, 61702136 and 61872112), in part by National Key Research and Development Program of China (Nos. 2018YFC0806802 and 2018YFC0832105), and in part by Self-Planned Task (No. SKLRS202002D) from the State Key Laboratory of Robotics and System (HIT). We would like to thank anonymous reviewers for their valuable feedback during this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno>arXiv 1512.03012 15</idno>
		<title level="m">ShapeNet: An informationrich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multiview 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 1, 3, 7</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human shape from silhouettes using generative HKS descriptors and cross-modal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual simultaneous localization and mapping: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fuentes-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ascencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rendón-Mancha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>NIPS 3</idno>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A papier-mâché approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno type="DOI">DOI10.1109/TPAMI.2019.29548853</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harltey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CVPR 12</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 1</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Single stream parallelization of generalized LSTM-like RNNs on a GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning view priors for single-view 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR 8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ICLR 3</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning efficient point cloud generation for dense 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<idno>AAAI 3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Photometric mesh optimization for video-aligned 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Marching cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
		<idno>SIGGRAPH 7</idno>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Occupancy networks: Learning 3D reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">StructureNet: hierarchical graph networks for 3D shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>242:1-242:19 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ozyeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305364" to="305365" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Raynet: Learning volumetric 3D reconstruction with ray potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 1</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning unsupervised hierarchical part decomposition of 3D objects from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintala</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 8</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discriminative shape from shading in uncalibrated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno>CVPR 3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matryoshka networks: Predicting 3D geometry via nested shape layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 3, 8, 9</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>MICCAI 6</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>CVPR 14</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Render for CNN: viewpoint estimation in images using cnns trained with rendered 3D model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pix3D: Dataset and methods for single-image 3D shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 3, 6</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 3</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">What do single-view 3D reconstruction networks learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pixel2Mesh++: Multi-view 3D mesh generation via deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno>ICCV 3</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recovering surface shape and orientation from texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="45" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>NIPS 3</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">MarrNet: 3D shape reconstruction via 2.5D sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>NIPS 3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning shape priors for single-view 3D completion and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>ECCV 3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Recognizing scene viewpoint using panoramic place representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pix2Vox: Context-aware 3D reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DISN: deep implicit surface network for high-quality single-view 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dense 3D object reconstruction from a single depth view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attentional aggregation of deep feature sets for multi-view 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Real-Point3D: An efficient generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SCORES: shape composition with recursive substructure priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno>211:1-211:14 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
