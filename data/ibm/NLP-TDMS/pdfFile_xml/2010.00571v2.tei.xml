<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding tables with intermediate pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
							<email>syrinekrichene@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
							<email>thomasmueller@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding tables with intermediate pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to finetuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TAB-FACT (Chen et al., 2020) and SQA datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual entailment <ref type="bibr" target="#b9">(Dagan et al., 2005)</ref>, also known as natural language inference <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>, is a core natural language processing (NLP) task. It can predict effectiveness of reading comprehension <ref type="bibr" target="#b8">(Dagan et al., 2010)</ref>, which argues that it can form the foundation of many other NLP tasks, and is a useful neural pre-training task <ref type="bibr" target="#b42">(Subramanian et al., 2018;</ref><ref type="bibr" target="#b7">Conneau et al., 2017)</ref>.</p><p>Textual entailment is well studied, but many relevant data sources are structured or semi-structured: health data both worldwide and personal, fitness trackers, stock markets, and sport statistics. While some information needs can be anticipated by handcrafted templates, user queries are often surprising, and having models that can reason and parse that structure can have a great impact in real world applications <ref type="bibr" target="#b25">(Khashabi et al., 2016;</ref><ref type="bibr" target="#b6">Clark, 2019)</ref>.</p><p>A recent example is TABFACT , a dataset of statements that are either entailed or refuted by tables from Wikipedia <ref type="figure" target="#fig_0">(Figure 1)</ref>. Because solving these entailment problems requires sophisticated reasoning and higher-order operations like arg max, averaging, or comparing, human accuracy remains substantially (18 points) ahead of the best models <ref type="bibr" target="#b52">(Zhong et al., 2020)</ref>.</p><p>The current models are dominated by semantic parsing approaches that attempt to create logical forms from weak supervision. We, on the other hand, follow <ref type="bibr" target="#b19">Herzig et al. (2020)</ref> and  and encode the tables with BERT-based models to directly predict the entailment decision. But while BERT models for text have been scrutinized and optimized for how to best pre-train and represent textual data, the same attention has not been applied to tabular data, limiting the effectiveness in this setting. This paper addresses these shortcomings using intermediate task pretraining <ref type="bibr">(Pruksachatkun et al., 2020)</ref>, creating efficient data representations, and applying these improvements to the tabular entailment task.</p><p>Our methods are tested on the English language, mainly due to the availability of the end task resources. However, we believe that the proposed solutions could be applied in other languages where a pre-training corpus of text and tables is available, such as the Wikipedia datasets.</p><p>Our main contributions are the following: i) We introduce two intermediate pre-training tasks, which are learned from a trained MASK-LM model, one based on synthetic and the other from counterfactual statements. The first one generates a sentence by sampling from a set of logical expressions that filter, combine and compare the information on the table, which is required in table entailment (e.g., knowing that Gerald Ford is taller than the average president requires summing all presidents and dividing by the number of presidents). The second one corrupts sentences about tables appearing on Wikipedia by swapping entities for plausible alternatives. Examples of the two tasks can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. The procedure is described in detail in section 3.</p><p>ii) We demonstrate column pruning to be an effective means of lowering computational cost at minor drops in accuracy, doubling the inference speed at the cost of less than one accuracy point.</p><p>iii) Using the pre-training tasks, we set the new state-of-the-art on TABFACT out-performing previous models by 6 points when using a BERT-base model and 9 points for a BERT-large model. The procedure is data efficient and can get comparable accuracies to previous approaches when using only 10% of the data. We perform a detailed analysis of the improvements in Section 6. Finally, we show that our method improves the state-of-the-art on a question answering task (SQA) by 4 points.</p><p>We release the pre-training checkpoints, data generation and training code at github.com/googleresearch/tapas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We use a model architecture derived from BERT and add additional embeddings to encode the table structure, following the approach of <ref type="bibr" target="#b19">Herzig et al. (2020)</ref> to encode the input.</p><p>The statement and Six types of learnable input embeddings are added together as shown in Appendix B. Token embeddings, position embeddings and segment embeddings are analogous to the ones used in standard BERT. Additionally we follow <ref type="bibr" target="#b19">Herzig et al. (2020)</ref> and use column and row embeddings which encode the two dimensional position of the cell that the token corresponds to and rank embeddings for numeric columns that encode the numeric rank of the cell with respect to the column, and provide a simple way for the model to know how a row is ranked according to a specific column.</p><p>Recall that the bi-directional self-attention mechanism in transformers is unaware of order, which motivates the usage of positional and segment embeddings for text in BERT, and generalizes naturally to column and row embeddings when processing tables, in the 2-dimensional case.</p><p>Let s and T represent the sentence and table respectively and E s and E T be their corresponding input embeddings. The sequence <ref type="bibr" target="#b45">(Vaswani et al., 2017)</ref> denoted f and a contextual representation is obtained for every token. We model the probability of entailment P (s|T ) with a single hidden layer neural network computed from the output of the [CLS] token:</p><formula xml:id="formula_0">E = [E [CLS] ; E s ; E [SEP] ; E T ] is passed through a transformer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>The use of challenging pre-training tasks has been successful in improving downstream accuracy <ref type="bibr" target="#b5">(Clark et al., 2020)</ref>. One clear caveat of the method adopted in <ref type="bibr" target="#b19">Herzig et al. (2020)</ref> which attempts to fill in the blanks of sentences and cells in the table is that not much understanding of the table in relation with the sentence is needed.</p><p>With that in mind, we propose two tasks that require sentence-table reasoning and feature complex operations performed on the table and entities grounded in sentences in non-trivial forms.</p><p>We discuss two methods to create pre-training data that lead to stronger table entailment models. Both methods create statements for existing Wikipedia tables 2 . We extract all tables that have at least two columns, a header row and two data rows. We recursively split tables row-wise into the upper and lower half until they have at most 50 cells. This way we obtain 3.7 million tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Counterfactual Statements</head><p>Motivated by work on counterfactually-augmented data <ref type="bibr" target="#b24">(Kaushik et al., 2020;</ref><ref type="bibr" target="#b12">Gardner et al., 2020)</ref>, we propose an automated and scalable method to get table entailments from Wikipedia and, for each such positive examples, create a minimally differing refuted example. For this pair to be useful we want that their truth value can be predicted from the associated table but not without it.</p><p>The tables and sentences are extracted from Wikipedia as follows: We use the page title, description, section title, text and caption. We also use all sentences on Wikipedia that link to the table's page and mentions at least one page (entity) that is also mentioned in the table. Then these snippets are split into sentences using the NLTK <ref type="bibr" target="#b31">(Loper and Bird, 2002)</ref> implementation of Punkt <ref type="bibr" target="#b26">(Kiss and Strunk, 2006)</ref>. For each relevant sentence we create one positive and one negative statement.</p><p>Consider the table in <ref type="figure" target="#fig_0">Figure 1</ref> and the sentence '[Greg Norman] is <ref type="bibr">[Australian]</ref>.' (Square brackets indicate mention boundaries.). A mention 3 is a potential focus mention if the same entity or value is also mentioned in the table. In our example, Greg Norman and Australian are potential focus mentions. Given a focus mention (Greg Norman) we define all the mentions that occur in the same column (but do not refer to the same entity) as the replacement mentions (e.g., Billy Mayfair, Lee Janzen, . . . ). We expect to create a false statement if we replace the focus mention with a replacement mention (e.g., 'Billy Mayfair is Australian.'), but there is no guarantee it will be actually false.</p><p>We call a mention of an entity that occurs in the same row as the focus entity a supporting mention, because it increases the chance that we falsify the statement by replacing the focus entity. In our example, Australian would be a supporting mention for Greg Norman (and vice versa). If we find a supporting mention we restrict the replacement candidates to the ones that have a different value. In the example, we would not use Steve Elkington since his row also refers to Australia.</p><p>Some replacements can lead to ungrammatical statements that a model could use to identify the negative statements, so we found it is useful to also replace the entity in the original positive sentence from Wikipedia with the mention from the table. <ref type="bibr">4</ref> We also introduce a simple type system for entities (named entity, date, cardinal number and ordinal number) and only replace entities of the same type. Short sentences having less than 4 tokens not counting the mention, are filtered out.</p><p>Using this approach we extract 4.1 million counterfactual pairs of which 546 thousand do have a supporting mention and the remaining do not.</p><p>We evaluated 100 random examples manually and found that the percentage of negative statements that are false and can be refuted by the table is 82% when they have a supporting mention and 22% otherwise. Despite this low value we still found the examples without supporting mention to improve accuracy on the end tasks (Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic Statements</head><p>Motivated by previous work <ref type="bibr" target="#b14">(Geva et al., 2020)</ref>, we propose a synthetic data generation method to improve the handling of numerical operations and comparisons. We build a table-dependent statement that compares two simplified SQL-like expressions. We define the (probabilistic) context-free grammar shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Synthetic statements are sampled from the CFG. We constrain the select values of the left and right expression to be either both the count or to have the same value for column .   <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Result first the value in C with the lowest row index. last the value in C with the highest row index. greatest the value in C with the highest numeric value. lowest the value in C with the lowest numeric value. sum</p><p>The sum of all the numeric values. average The average of all the numeric values. range</p><p>The difference between greatest and lowest. This guarantees that the domains of both expressions are comparable. value is chosen as at random from the respective column. A statement is redrawn if it yields an error (see <ref type="table" target="#tab_2">Table 1</ref>). With probability 0.5 we replace one of both expressions by the values it evaluates to. In the example given in <ref type="figure" target="#fig_0">figure 1</ref></p><formula xml:id="formula_1">, "[The [sum] of [Earnings]] when [[Country] [is] [Australia]</formula><p>]" is an expr that can be replaced by the constant value 2, 909, 311.</p><p>We set P ( select → the count) to 0.2 in all our experiments. Everything else is sampled uniformly. For each Wikipedia table we generate a positive and a negative statement which yields 3.7M pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Table pruning</head><p>Some input examples from TABFACT can be too long for BERT-based models. We evaluate table pruning techniques as a pre-processing step to select relevant columns that respect the input example length limits. As described in section 2, an example is built by concatenating the statement with the flattened table. For large tables the example length can exceed the capacity limit of the transformer.</p><p>The TAPAS model handles this by shrinking the text in cells. A token selection algorithm loops over the cells. For each cell it starts by selecting the first token, then the second and so on until the maximal length is reached. Unless stated otherwise we use the same approach. Crucially, selecting only relevant columns would allow longer examples to fit without discarding potentially relevant tokens.</p><p>Heuristic entity linking (HEL) is used as a baseline. It is the table pruning used in TABLE-BERT . The algorithm aligns spans in statement to the columns by extracting the longest character n-gram that matches a cell. The span matches represent linked entities. Each entity in the statement can be linked to only one column. We use the provided entity linking statements data 5 . We run the TAPAS algorithm on top of the input data to limit the input size.</p><p>We propose a different method that tries to retain as many columns as possible. In our method, the columns are ranked by a relevance score and added in order of decreasing relevance. Columns that exceed the maximum input length are skipped. The algorithm is detailed in Appendix F. Heuristic exact match (HEM) computes the Jaccard coefficient between the statement and each column. Let T S be the set of tokens in the statement S and T C the tokens in column C, with C ∈ C the set of columns. Then the score between the statement and column is given by |T S ∩T C | |T S ∪T C | . We also experimented with approaches based on word2vec <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref>, character overlap and TF-IDF. Generally, they produced worse results than HEM. Details are shown in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In all experiments, we start with the public TAPAS checkpoint, 6 train an entailment model on the data from Section 3 and then fine-tune on the end task (TABFACT or SQA). We report the median accuracy values over 3 pre-training and 3 fine-tuning runs (9 runs in total). We estimate the error margin as half the interquartile range, that is half the difference between the 25th and 75th percentiles. The hyper-parameters, how we chose them, hardware and other information to reproduce our experiments are explained in detail in Appendix A.</p><p>The training time depends on the sequence length used. For a BERT-Base model it takes around 78 minutes using 128 tokens and it scales almost linearly up to 512. For our pre-training tasks, we explore multiple lengths and how they trade-off speed for downstream results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our model on the recently released TABFACT dataset . The tables are extracted from Wikipedia and the sentences written by crowd workers in two batches. The first batch consisted of simple sentences, that instructed the writers to refer to a single row in the table. The second one, created complex sentences by asking writers to use information from multiple rows.</p><p>In both cases, crowd workers initially created only positive (entailed) pairs, and in a subsequent annotation job, the sentences were copied and edited into negative ones, with instructions of avoiding simple negations. Finally, there was a third verification step to filter out bad rewrites. The final count is 118, 000. The split sizes are given in Appendix C. An example of a table and the sentences is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the standard TABFACT split and the official accuracy metric.</p><p>We also use the SQA <ref type="bibr" target="#b22">(Iyyer et al., 2017)</ref> dataset for pre-training (following <ref type="bibr" target="#b19">Herzig et al. (2020)</ref>) and for testing if our pre-training is useful for related tasks. SQA is a question answering dataset that was created by asking crowd workers to split a compositional subset of WikiTableQuestions (Pasupat and Liang, 2015) into multiple referential questions. The dataset consists of 6,066 sequences (2.9 question per sequence on average). We use the standard split and official evaluation script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Chen et al. (2020) present two models, TABLE-BERT and the Latent Program Algorithm (LPA), that yield similar accuracy on the TABFACT data.</p><p>LPA tries to predict a latent program that is then executed to verify if the statement is correct or false. The search over programs is restricted using lexical heuristics. Each program and sentence is encoded with an independent transformer model and then a linear layer gives a relevance score to the pair. The model is trained with weak supervision where programs that give the correct binary answer are considered positive and the rest negative.</p><p>TABLE-BERT is a BERT-base model that similar to our approach directly predicts the truth value of the statement. However, the model does not use special embeddings to encode the table structure but relies on a template approach to format the table as natural language. The table is mapped into a single sequence of the form: "Row 1 Rank is 1; the Player is Greg Norman; ... . Row 2 ...". The model is also not pre-trained on table data.</p><p>LOGICALFACTCHECKER <ref type="bibr" target="#b52">(Zhong et al., 2020)</ref> is another transformer-based model that given a candidate logical expression, combines contextual embeddings of program, sentence and table, with a tree-RNN <ref type="bibr" target="#b41">(Socher et al., 2013)</ref> to encode the parse tree of the expression. The programs are obtained through either LPA or an LSTM generator (Seq2Action).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>TABFACT In <ref type="table" target="#tab_4">Table 2</ref> we find that our approach outperforms the previous state-of-the-art on TAB-FACT by more than 6 points (Base) or more than 9 points (Large). A model initialized only with the public TAPAS MASK-LM checkpoint is behind state-of-the-art by 2 points (71.7% vs 69.9%). If we train on the counterfactual data, it out-performs LOGICALFACTCHECKER and reaches 75.2% test accuracy (+5.3), slightly above using SQA. Only using the synthetic data is better (77.9%), and when using both datasets it achieves 78.5%. Switching from BERT-Base to Large improves the accuracy by another 2.5 points. The improvements are consistent across all test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-Shot Accuracy and low resource regimes</head><p>The pre-trained models are in principle already complete table entailment predictors. Therefore it is interesting to look at their accuracy on the TABFACT evaluation set before fine-tuning them. We find that the best model trained on all the pretraining data is only two points behind the fully trained TABLE-BERT (63.8% vs 66.1%). This relatively good accuracy mostly stems from the counterfactual data.</p><p>When looking at low data regimes in <ref type="figure" target="#fig_3">Figure  3</ref> we find that pre-training on SQA or our artificial data consistently leads to better results than just training with the MASK-LM objective. The models with synthetic pre-training data start outperforming    and <ref type="bibr" target="#b52">Zhong et al. (2020)</ref>. The best BERT-base model while comparable in parameters out-performs TABLE-BERT by more than 12 points. Pre-training with counterfactual and synthetic data gives an accuracy 8 points higher than only using MASK-LM and more than 3 points higher than using SQA. Both counterfactual and synthetic data out-perform pre-training with a MASK-LM objective and SQA. Joining the two datasets gives an additional improvement. Error margins are estimated as half the interquartile range.    training set. The setup with all the data is consistently better than the others and synthetic and counterfactual are both better than SQA.</p><p>SQA Our pre-training data also improves the accuracy on a QA task. On SQA <ref type="bibr" target="#b22">(Iyyer et al., 2017)</ref> a model pre-trained on the synthetic entailment data outperforms one pre-trained on the MASK-LM task alone <ref type="table" target="#tab_8">(Table 3)</ref>   Efficiency As discussed in Section 3.3 and Appendix A.4, we can increase the model efficiency by reducing the input length. By pruning the input of the TABFACT data we can improve training as well as inference time. We compare pruning with the heuristic entity linking (HEL)  and heuristic exact match (HEM) to different target lengths. We also studied other pruning methods, the results are reported in Appendix F. In <ref type="table" target="#tab_10">Table 4</ref> we find that HEM consistently outperforms HEL. The best model at length 256, while twice as fast to train (and apply), is only 0.8 points behind the best full length model. Even the model    with length 128, while using a much shorter length, out-performs TABLE-BERT by more than 7 points. Given a pre-trained MASK-LM model our training consists of training on the artificial pre-training data and then fine-tuning on TABFACT. We can therefore improve the training time by pre-training with shorter input sizes. <ref type="table" target="#tab_10">Table 4</ref> shows that 512 and 256 give similar accuracy while the results for 128 are about 1 point lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Salient Groups To obtain detailed information of the improvements of our approach, we manually annotated 200 random examples with the complex operations needed to answer them. We found 4 salient groups: Aggregations, superlatives, com-paratives and negations, and sort pairs into these groups via keywords in the text. To make the groups exclusive, we add a fifth case when more than one operation is needed. The accuracy of the heuristics was validated through further manual inspection of 50 samples per group. The trigger words of each group are described in Appendix G.</p><p>For each group within the validation set, we look at the difference in accuracy between different models. We also look at how the total error rate is divided among the groups as a way to guide the focus on pre-training tasks and modeling. The error rate defined in this way measures potential accuracy gains if all the errors in a group S were fixed: ER(S) = |{ Errors in S}| |{ Validation examples}| . Among the groups, the intermediate task data improve superlatives (39% error reduction) and negations (31%) most <ref type="table" target="#tab_11">(Table 5)</ref>. For example, we see that the accuracy is higher for superlatives than the for the overall validation set.</p><p>In <ref type="figure" target="#fig_6">Figure 4</ref> we show examples in every group where our model is correct on the majority of the cases (across 9 trials), and the MASK-LM baseline is not. We also show examples that continue to produce errors after our pre-training. Many examples in this last group require multi-hop reasoning or complex numerical operations.</p><p>Model Agreement Similar to other complex binary classification datasets such as BOOLQ , for TABFACT one may question whether models are guessing the right answer. To detect the magnitude of this issue we look at 9 independent runs of each variant and analyze how many of them agree on the correct answer. <ref type="figure">Figure 5</ref> shows that while for MASK-LM only for 24.2% of the examples all models agree on the right answer, it goes up to 55.5% when using using the counterfactual and synthetic pre-training. This suggests that the amount of guessing decreases substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Logic-free Semantic Parsing Recently, methods that skip creating logical forms and generate answers directly have been used successfully for semantic parsing <ref type="bibr" target="#b33">(Mueller et al., 2019)</ref>. In this group, TAPAS <ref type="bibr" target="#b19">(Herzig et al., 2020)</ref> uses special learned embeddings to encode row/column index and numerical order and pretrains a MASK-LM model on a large corpus of text and tables co-occurring on Wikipedia articles. Importantly, next sentence prediction from , which in this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistently Better Persisting Errors Aggregations</head><p>Choi Moon -Sik played in Seoul three times in total.</p><p>The total number of bronze medals were half of the total number of medals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superlatives</head><p>Mapiu school has the highest roll in the state authority.</p><p>Carlos Moya won the most tournaments with two wins. Comparatives Bernard Holsey has 3 more yards than Angel Rubio.</p><p>In 1982, the Kansas City Chiefs played more away games than home games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negations</head><p>The Warriors were not the home team at the game on 11-24-2006.</p><p>Dean Semmens is not one of the four players born after 1981.  <ref type="figure">Figure 5</ref>: Frequency of the number of models that give the correct answer, out of 9 runs. Better pre-training leads to more consistency across models. The ratio of samples answered correctly by all models is 24.2% for MASK-LM but 55.5% for Synthetic + Counterfactual. context amounts to detecting whether the table and the sentence appear in the same article, was not found to be effective. Our hypothesis is that the task was not hard enough to provide a training signal. We build on top of the TAPAS model and propose harder and more effective pre-training tasks to achieve strong performance on the TAB-FACT dataset.</p><p>Entailment tasks Recognizing entailment has a long history in NLP <ref type="bibr" target="#b8">(Dagan et al., 2010)</ref>. Recently, the text to text framework has been expanded to incorporate structured data, like knowledge graphs <ref type="bibr" target="#b46">(Vlachos and Riedel, 2015)</ref>, tables <ref type="bibr" target="#b23">(Jo et al., 2019;</ref> or images <ref type="bibr" target="#b43">(Suhr et al., 2017</ref><ref type="bibr" target="#b44">(Suhr et al., , 2019</ref>. The large-scale TABFACT dataset  is one such example. Among the top performing models in the task is a BERT based model, acting on a flattened versioned of the table using textual templates to make the tables resemble natural text. Our approach has two key improvements: the usage of special embeddings, as introduced in <ref type="bibr" target="#b19">Herzig et al. (2020)</ref>, and our novel counterfactual and synthetic pre-training (Section 3).</p><p>Pre-training objectives Next Sentence Prediction (NSP) was introduced in Devlin et al. <ref type="formula">(2019)</ref>, but follow-up work such as  identified that it did not contribute to model performance in some tasks. Other studies have found that application specific self-supervised pre-training objectives can improve performance of MASK-LM models. One examples of such an objective is the Inverse Cloze Task (ICT) , that uses in-batch negatives and a two-tower dot-product similarity metric. <ref type="bibr" target="#b2">Chang et al. (2020)</ref> further expands on this idea and uses hyperlinks in Wikipedia as a weak label for topic overlap.</p><p>Intermediate Pre-training Language model fine-tuning <ref type="bibr" target="#b20">(Howard and Ruder, 2018</ref>) also know as domain adaptive pre-training (Gururangan et al., 2020) has been studied as a way to handle covariate shift. Our work is closer to intermediate task fine-tuning <ref type="bibr">(Pruksachatkun et al., 2020)</ref> where one tries to teach the model higher-level abilities. Similarly we try to improve the discrete and numeric reasoning capabilities of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterfactual data generation</head><p>The most similar approach to ours appears in <ref type="bibr" target="#b51">Xiong et al. (2020)</ref>, replacing entities in Wikipedia by others with the same type for a MASK-LM model objective. We, on the one hand, take advantage of other rows in the table to produce plausible negatives, and also replace dates and numbers. Recently, <ref type="bibr" target="#b24">Kaushik et al. (2020)</ref>; <ref type="bibr" target="#b12">Gardner et al. (2020)</ref> have shown that exposing models to pairs of examples which are similar but have different labels can help to improve generalization, in some sense our Counterfactual task is a heuristic version of this, that does not rely on manual annotation. <ref type="bibr" target="#b40">Sellam et al. (2020)</ref> use perturbations of Wikipedia sentences for intermediate pre-training of a learned metric for text generation.</p><p>Numeric reasoning Numeric reasoning in Natural Language processing has been recognized as an important part in entailment models <ref type="bibr" target="#b39">(Sammons et al., 2010)</ref> and reading comprehension <ref type="bibr" target="#b37">(Ran et al., 2019)</ref>. <ref type="bibr" target="#b47">Wallace et al. (2019)</ref> studied the capacity of different models on understanding numerical operations and show that BERT-based model still have headroom. This motivates the use of the synthetic generation approach to improve numerical reasoning in our model.</p><p>Synthetic data generation Synthetic data has been used to improve learning in NLP tasks <ref type="bibr" target="#b0">(Alberti et al., 2019;</ref><ref type="bibr" target="#b50">Wu et al., 2016;</ref><ref type="bibr" target="#b28">Leonandya et al., 2019)</ref>. In semantic parsing for example <ref type="bibr" target="#b48">(Wang et al., 2015;</ref><ref type="bibr" target="#b21">Iyer et al., 2017;</ref><ref type="bibr" target="#b49">Weir et al., 2020)</ref>, templates are used to bootstrap models that map text to logical forms or SQL. <ref type="bibr" target="#b38">Salvatore et al. (2019)</ref> use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT). <ref type="bibr" target="#b13">Geiger et al. (2019)</ref> use synthetic data to create fair evaluation sets for natural language inference. <ref type="bibr" target="#b14">Geva et al. (2020)</ref> show the importance of injecting numerical reasoning via generated data into the model to solve reading comprehension tasks. They propose different templates for generating synthetic numerical examples. In our work we use a method that is better suited for tables and to the entailment task, and is arguably simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced two pre-training tasks, counterfactual and synthetic, to obtain state-of-the-art results on the TABFACT (Chen et al., 2020) entailment task on tabular data. We adapted the BERT-based architecture of TAPAS <ref type="bibr" target="#b19">(Herzig et al., 2020)</ref> to binary classification and showed that pre-training on both tasks yields substantial improvements on TAB-FACT but also on a QA dataset, SQA <ref type="bibr" target="#b22">(Iyyer et al., 2017)</ref>, even with only a subset of the training data.</p><p>We ran a study on column selection methods to speed-up training and inference. We found that we can speed up the model by a factor of 2 at a moderate drop in accuracy (≈ 1 point) and by a factor of 4 at a larger drop but still with higher accuracy than previous approaches.</p><p>We characterized the complex operations required for table entailment to guide future research in this topic. Our code and models will be opensourced.</p><p>We provide details on our experimental setup and hyper-parameter tuning in Section A. Section B and C give additional information on model and the TABFACT dataset. We give details and results regarding our column pruning approach in Section D. Full results for SQA are displayed in Section E. Section F shows the accuracy on the pre-training tasks held-out sets. Section G contains the trigger words used for identifying the salient groups in the analysis section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyper-Parameter Search</head><p>The hyper-parameters are optimized using a black box Bayesian optimizer similar to Google Vizier <ref type="bibr" target="#b15">(Golovin et al., 2017)</ref> which looked at validation accuracy after 8, 000 steps only, in order to prevent over-fitting and use resources effectively. The ranges used were a learning rate from 10 −6 to 3 × 10 −4 , dropout probabilities from 0 to 0.2 and warm-up ratio from 0 to 0.05. We used 200 runs and kept the median values for the top 20 trials.</p><p>In order to show the impact of the number of trials in the expected validation results, we follow <ref type="bibr" target="#b18">Henderson et al. (2018)</ref> and <ref type="bibr" target="#b11">Dodge et al. (2019)</ref>. Given that we used Bayesian optimization instead of random search, we applied the bootstrap method to estimate mean and variance of the max validation accuracy at 8, 000 steps for different number of trials. From trial 10 to 200 we noted an increase of 0.4% in accuracy and a standard deviation that decreases from 2% to 1.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-Parameters</head><p>We use the same hyper-parameters for pre-training and fine-tuning. For pre-training, the input length is 256 and 512 for fine-tuning if not stated otherwise. We use 80, 000 training steps, a learning rate of 2e −5 and a warm-up ratio of 0.05. We disable the attention dropout in BERT but use a hidden dropout probability of 0.07 . Finally, we use an Adam optimizer with weight decay with the same configuration as BERT.</p><p>For SQA we do not use any search algorithm and use the same model and the same hyper-parameters as the ones used in <ref type="bibr" target="#b19">Herzig et al. (2020)</ref>. The only difference is that we start the fine-tuning from a checkpoint trained on our intermediate pre-training entailment task.</p><p>[CLS] a claim <ref type="bibr">[SEP]</ref> col <ref type="table" target="#tab_2">##1  col  ##2  0  1  2  3   SEG0  SEG0  SEG0  SEG0  SEG1  SEG1  SEG1  SEG1  SEG1  SEG1  SEG1  SEG1   COL0  COL0  COL0  COL0  COL1  COL1  COL2  COL2  COL1  COL2  COL1  COL2   ROW0  ROW0  ROW0  ROW0  ROW0  ROW0  ROW0  ROW0  ROW1  ROW1</ref> ROW2 ROW2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segment Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column Embeddings</head><p>Row Embeddings  <ref type="figure">Figure 6</ref>: Input representation for model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Number of Parameters</head><p>The number of parameters is the same as for BERT: 110M for base models and 340M for Large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Training Time</head><p>We train all our models on Cloud TPUs V3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model</head><p>For illustrative purposes, we include the input representation using the 6 types of embeddings, as depicted by <ref type="bibr" target="#b19">Herzig et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset</head><p>Statistics of the TABFACT dataset can be found in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Columns selection algorithm</head><p>Let cost(.) ∈ N be the function that computes the number of tokens given a text using the BERT tokenizer, t s the tokenized statement text, t c i the text of the column i. We denote the columns as (c 1 , .., c n ) ordered by their scores</p><formula xml:id="formula_2">∀i ∈ [1, ..n − 1]f (c i ) &gt; f (c i+1 )</formula><p>where n is the number of columns. Let m be the maximum number of tokens. Then the cost of the column must verify the following condition.</p><formula xml:id="formula_3">∀i ∈ [1..n], c i ∈ C + i if 2 + cost(t s ) + tc j ∈C + i−1 cost(t c j ) + cost(t c i ) ≤ m</formula><p>where C + i is the set of retained columns at the iteration i. 2 is added to the condition as two special tokens are added to the input:</p><p>[CLS], t s , [SEP ], tc 1 , ..., tc n . If a current column c i doesn't respect the condition then the column is not selected. Whether or not the column is retained, the algorithm continues and verifies if the next column can fit. It follows C +n contains the maximum number of columns that can fit under m by respecting the columns scoring order.</p><p>There is a number of heuristic pruning approaches we have experimented with. Results are given in 7.</p><p>Word2Vec (W2V) uses a publicly available word2vec <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref>  where v i represents the embedding of the token i. For a given column token c we define the relevance with respect to the statement as the average similarity to every token:</p><formula xml:id="formula_4">f (S, c) = avg s∈T S :f (s,c)&gt;τ f (s, c)</formula><p>Where τ is a threshold that helps to remove noise from unrelated word embeddings. We set τ to 0.89. We experimented with max and sum as other aggregation function but found the average to perform best. The final score between the statement S and the column C is given by</p><formula xml:id="formula_5">f (S, C) = max c∈T C f (S, c)</formula><p>Term frequency-inverse document frequency (IWF) Scores the columns' tokens proportional to the word frequency in the statement and offset by the word frequency computed over all the tables and statements from the training set.</p><formula xml:id="formula_6">f (t s , c) = T F (t s , c) log(W F (c) + 1)</formula><p>Where T F (t s , c) is how often the token c occurs in the statement t s , and W F (c) is the frequency of c in a word count list. The final score of a column C is given by</p><formula xml:id="formula_7">f (t s , C) = max c∈T C T F (t s , c) log(W F (c) + 1)</formula><p>Character N-gram (CHAR) Scores columns by character overlap with the statement. This method looks for sub-list of word's characters in the statement. The length of the characters' list has a minimum and maximum length allowed. In the experiments we use 5 and 20 as minimum and maximum length. Let L s,c be the set of all the overlapping characters' lengths. The scoring for each column is given by f (t s , t c ) = min(max(L s,c , 5)), 20) cost(t c ) E SQA <ref type="table">Table 8</ref> shows the accuracy on the first development fold and the test set. As for the main results, the error margins displayed are half the interquartile range over 9 runs, which is half the difference between the first and third quartile. This range contains half of the runs and provides a measure of robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Pre-Training Data</head><p>When training on the pre-training data we hold out approximately 1% of the data for testing how well the model is solving the pre-training task. In <ref type="table">Table  9</ref>, we compare the test pre-training accuracy on synthetic and counterfactual data to models that are only trained on the statements to understand whether there is considerable bias in the data. Both datasets have some bias (i.e. the accuracy without table is higher than 50%.). Still there is a sufficient enough gap between training with and without tables so that the data is still useful. The synthetic data can be solved almost perfectly whereas for the counterfactual data we only reach an accuracy of 84.3%. This is expected as there is no guarantee that the model has enough information to decide whether a statement is true or false for the counterfactual examples.  <ref type="table">Table 9</ref>: Accuracy on synthetic (Val S ) and counterfactual held-out sets (Val C ) of the pre-traininig data.</p><p>In table 10 we show the ablation results when removing the counterfactual statements that lack a supporting entity, that is a second entity that appears in both the table and sentence. This increases the probability that our generated negative pairs are incorrect, but it also discards 7 out of 8 examples, which ends up hurting the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Salient Groups Definition</head><p>In table 11 we show the words that are used as markers to define each of the groups. We first identified manually the operations that were most often needed to solve the task and found relevant words linked with each group. The heuristic was validated by manually inspecting 50 samples from each group and observing higher than 90% accuracy.  <ref type="table">Table 8</ref>: SQA dev (first fold) and test results. ALL is the average question accuracy, SEQ the sequence accuracy, and QX, the accuracy of the X'th question in a sequence. We show the median over 9 trials, and errors are estimated with half the interquartile range .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slice Words</head><p>Aggregations total, count, average, sum, amount, there, only Superlatives first, highest, best, newest, most, greatest, latest, biggest and their opposites Comparatives than, less, more, better, worse, higher, lower, shorter, same Negations not, any, none, no, never </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A TABFACT table with real statements 1 and counterfactual and synthetic examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Grammar of synthetic phrases. column is the set of column names in the table. We also generate constant expressions by replacing expressions with their values. Aggregations are defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Results for training on a subset of the data. Counterfactual + Synthetic (C+S) consistently out-performs only Counterfactual (C) or Synthetic (S), which in turn out-perform pre-training on SQA. C+S and S surpass</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. Our best BERT Base model outpeforms the BERT-Large model of Herzig et al. (2020) and a BERT-Large model trained on our data improves the previous state-of-the-art by 4 points on average question and sequence accuracy. See dev results and error bars in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>On the left column we show examples that our model gets correct for most runs and that MASK-LM gets wrong for most runs. The right column shows examples that the model continues to make mistakes on. Many of those include deeper chains of reasoning or more complex numeric operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Input length histogram for TABFACT validation dataset when tokenized with BERT tokenizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>model 8 to extract one embedding for each token. Let T S be the set of tokens in the statement and T C the set of tokens in a column. The cosine similarity for each pair is given by ∀(s, c) ∈ T S × T C f (s, c) = c are unknown cos(v s , v c ) else 8 https://tfhub.dev/google/tf2-preview/ gnews-swivel-20dim/1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table in a pair are tokenized into word pieces and concatenated using the standard [CLS] and [SEP] tokens in between. The table is flattened row by row and no additional separator is added between the cells or rows.</figDesc><table /><note>1 Based on table 2-14611590-3.html with light edits.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Aggregations used in synthetic statements,</cell></row><row><cell>where C are the column values. When C is empty or a</cell></row><row><cell>singleton, it results in an error. Numeric functions also</cell></row><row><cell>fail if any of their values is non-numeric.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE -</head><label>-</label><figDesc>BERT when using 5% of the</figDesc><table><row><cell>Model</cell><cell></cell><cell>Val</cell><cell>Test</cell><cell cols="3">Test simple Test complex Test small</cell></row><row><cell cols="2">BERT classifier w/o Table</cell><cell>50.9</cell><cell>50.5</cell><cell>51.0</cell><cell>50.1</cell><cell>50.4</cell></row><row><cell cols="2">TABLE-BERT-Horizontal-T+F-Template</cell><cell>66.1</cell><cell>65.1</cell><cell>79.1</cell><cell>58.2</cell><cell>68.1</cell></row><row><cell cols="2">LPA-Ranking w/ Discriminator (Caption)</cell><cell>65.1</cell><cell>65.3</cell><cell>78.7</cell><cell>58.5</cell><cell>68.9</cell></row><row><cell cols="2">LOGICALFACTCHECKER (program from LPA)</cell><cell>71.7</cell><cell>71.6</cell><cell>85.5</cell><cell>64.8</cell><cell>74.2</cell></row><row><cell cols="3">LOGICALFACTCHECKER (program from Seq2Action) 71.8</cell><cell>71.7</cell><cell>85.4</cell><cell>65.1</cell><cell>74.3</cell></row><row><cell>OURS Base</cell><cell>MASK-LM</cell><cell cols="3">69.6 ±4.4 69.9 ±3.8 82.0 ±5.9</cell><cell>63.9 ±2.8</cell><cell>72.2 ±4.7</cell></row><row><cell>OURS Base</cell><cell>SQA</cell><cell cols="3">74.9 ±0.2 74.6 ±0.2 87.2 ±0.2</cell><cell>68.4 ±0.4</cell><cell>77.3 ±0.3</cell></row><row><cell>OURS Base</cell><cell>Counterfactual</cell><cell cols="3">75.5 ±0.5 75.2 ±0.4 87.8 ±0.4</cell><cell>68.9 ±0.5</cell><cell>77.4 ±0.3</cell></row><row><cell>OURS Base</cell><cell>Synthetic</cell><cell cols="3">77.6 ±0.2 77.9 ±0.3 89.7 ±0.4</cell><cell>72.0 ±0.2</cell><cell>80.4 ±0.2</cell></row><row><cell>OURS Base</cell><cell>Counterfactual + Synthetic</cell><cell cols="3">78.6 ±0.3 78.5 ±0.3 90.5 ±0.4</cell><cell>72.5 ±0.3</cell><cell>81.0 ±0.3</cell></row><row><cell cols="2">OURS Large Counterfactual + Synthetic</cell><cell cols="3">81.0 ±0.1 81.0 ±0.1 92.3 ±0.3</cell><cell>75.6 ±0.1</cell><cell>83.9 ±0.3</cell></row><row><cell cols="2">Human Performance</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The TABFACT results. Baseline and human results are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE - BERT</head><label>-</label><figDesc></figDesc><table><row><cell>LogicalFactChecker</cell></row><row><cell>Counterfactual + Synthetic Synthetic Counterfactual SQA MASK-LM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE -</head><label>-</label><figDesc>BERT at 5% (around 4,500) of examples, C and SQA at 10%. C+S is comparable with LOGICALFACTCHECKER when using 10% of the data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>±0.2 34.6 ±0.0 Counterfactual Base 65.0 ±0.5 36.5 ±0.6 Synthetic Base 67.4 ±0.2 39.8 ±0.4 Counterf. + Synthetic Base 67.9 ±0.3 40.5 ±0.7 Counterf. + Synthetic Large 71.0 ±0.4 44.8 ±0.8</figDesc><table><row><cell>Data</cell><cell>Size</cell><cell>ALL</cell><cell>SEQ</cell></row><row><cell>Iyyer et al. (2017)</cell><cell></cell><cell>44.7</cell><cell>12.8</cell></row><row><cell>Mueller et al. (2019)</cell><cell></cell><cell>55.1</cell><cell>28.1</cell></row><row><cell>Herzig et al. (2020)</cell><cell cols="2">Large 67.2</cell><cell>40.4</cell></row><row><cell>MASK-LM</cell><cell cols="2">Base 64.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>SQA test results. ALL is the average question accuracy and SEQ the sequence accuracy. Both counterfactual and synthetic data out-perform the MASK-LM objective. Our Large model outperforms the MASK-LM model by almost 4 points on both metrics. Our best Base model is comparable to the previous state-of-the-art. Error margins are estimated as half the interquartile range.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE -</head><label>-</label><figDesc></figDesc><table><row><cell>BERT</cell><cell></cell><cell>512 7</cell><cell>66.1</cell></row><row><cell>OURS</cell><cell>512</cell><cell>512</cell><cell>78.3 ±0.2</cell></row><row><cell></cell><cell>256</cell><cell>512</cell><cell>78.6 ±0.3</cell></row><row><cell></cell><cell>128</cell><cell>512</cell><cell>77.5 ±0.3</cell></row><row><cell>OURS -HEL</cell><cell>128</cell><cell>512</cell><cell>76.7 ±0.4</cell></row><row><cell></cell><cell>128</cell><cell>256</cell><cell>76.3 ±0.1</cell></row><row><cell></cell><cell>128</cell><cell>128</cell><cell>71.0 ±0.3</cell></row><row><cell>OURS -HEM</cell><cell>256</cell><cell>512</cell><cell>78.8 ±0.3</cell></row><row><cell></cell><cell>256</cell><cell>256</cell><cell>78.1 ±0.1</cell></row><row><cell></cell><cell>128</cell><cell>512</cell><cell>78.2 ±0.4</cell></row><row><cell></cell><cell>128</cell><cell>256</cell><cell>77.0 ±0.2</cell></row><row><cell></cell><cell>128</cell><cell>128</cell><cell>72.7 ±0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">: Accuracy of column pruning methods, that re-</cell></row><row><cell cols="6">duce input length for faster training and prediction: The</cell></row><row><cell cols="6">heuristic entity linking (HEL) (Chen et al., 2020) and</cell></row><row><cell cols="6">Heuristic exact match (HEM) at various pre-training</cell></row><row><cell cols="6">(PT) and fine-tuning (FT) sizes. HEM out-performs</cell></row><row><cell cols="6">HEL on all input sizes, and in the faster case (128) out-</cell></row><row><cell cols="6">performs TABLE-BERT by 6.6 points. Accuracy with</cell></row><row><cell cols="6">size 256 is 0.7 points behind the full input size. Error</cell></row><row><cell cols="6">margins are estimated as half the interquartile range.</cell></row><row><cell></cell><cell></cell><cell>C+S</cell><cell></cell><cell></cell><cell>MASK-LM</cell></row><row><cell></cell><cell>Size</cell><cell>Acc</cell><cell>ER</cell><cell cols="2">Acc ∆ Acc ∆ ER</cell></row><row><cell>Validation</cell><cell cols="4">100.0 78.6 21.4 69.6</cell><cell>9.0</cell><cell>9.0</cell></row><row><cell>Superlatives</cell><cell cols="2">13.4 79.6</cell><cell cols="2">2.7 66.9</cell><cell>12.6</cell><cell>1.7</cell></row><row><cell>Aggregations</cell><cell cols="2">11.6 71.1</cell><cell cols="2">3.4 62.3</cell><cell>8.9</cell><cell>1.0</cell></row><row><cell>Comparatives</cell><cell cols="2">10.4 72.3</cell><cell cols="2">2.9 62.6</cell><cell>9.7</cell><cell>1.0</cell></row><row><cell>Negations</cell><cell cols="2">3.3 72.6</cell><cell cols="2">0.9 60.5</cell><cell>12.1</cell><cell>0.4</cell></row><row><cell>Multiple of the above</cell><cell cols="2">9.2 72.0</cell><cell cols="2">2.6 63.9</cell><cell>8.2</cell><cell>0.8</cell></row><row><cell>Other</cell><cell cols="2">51.9 82.6</cell><cell cols="2">9.1 75.2</cell><cell>7.4</cell><cell>3.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparing accuracy and total error rate (ER) for counterfactual and synthetic (C+S) and MASK-LM. Groups are derived from word heuristics. The error rate in each group is taken with respect to the full set. Nega- tions and superlatives show the highest relative gains.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Statements Tables</cell></row><row><cell>Train</cell><cell>92,283</cell><cell>13,182</cell></row><row><cell>Val</cell><cell>12,792</cell><cell>1,696</cell></row><row><cell>Test</cell><cell>12,779</cell><cell>1,695</cell></row><row><cell>Total</cell><cell>118,275</cell><cell>16,573</cell></row><row><cell>Simple</cell><cell>50,244</cell><cell>9,189</cell></row><row><cell cols="2">Complex 68,031</cell><cell>7,392</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 6 :</head><label>6</label><figDesc>TABFACT dataset statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Accuracy of different pruning methods: The</cell></row><row><cell>heuristic entity linking (HEL) (Chen et al., 2020),</cell></row><row><cell>Heuristic exact match (HEM), word-to-vec (W2V), in-</cell></row><row><cell>verse word frequency (IWF), character ngram (CHAR)</cell></row><row><cell>at different pre-training (PT) and fine-tuning (FT) sizes.</cell></row><row><cell>Error margins are estimated as half the interquartile</cell></row><row><cell>range.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Comparisons of training on counterfactual data with and without statements that don't have support mentions.</figDesc><table><row><cell>Data</cell><cell>Val</cell></row><row><cell>Synthetic</cell><cell>77.6</cell></row><row><cell>Counterfactual</cell><cell>75.5</cell></row><row><cell>Counterfactual + Synthetic</cell><cell>78.6</cell></row><row><cell>Counterfactual (only supported)</cell><cell>73.6</cell></row><row><cell cols="2">Counterfactual (only supported) + Synthetic 77.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>±0.3 64.0 ±0.2 35.3 ±0.7 34.6 ±0.0 72.4 ±0.4 79.2 ±0.6 59.7 ±0.4 61.2 ±0.4 50.5 ±1.1 55.6 ±0.7 Counterfactual Base 63.2 ±0.7 65.0 ±0.5 39.3 ±0.6 36.5 ±0.6 74.7 ±0.3 78.4 ±0.4 63.8 ±1.2 63.7 ±0.3 52.4 ±0.7 57.5 ±0.7 Synthetic Base 64.1 ±0.4 67.4 ±0.2 41.6 ±0.8 39.8 ±0.4 75.3 ±0.7 79.3 ±0.1 64.4 ±0.6 66.2 ±0.2 55.8 ±0.7 60.2 ±0.6 Counterfactual + Synthetic Base 64.5 ±0.2 67.9 ±0.3 40.2 ±0.4 40.5 ±0.7 75.6 ±0.3 79.3 ±0.3 65.3 ±0.6 67.0 ±0.3 55.4 ±0.5 61.1 ±0.9 Counterfactual + Synthetic Large 68.0 ±0.2 71.0 ±0.4 45.8 ±0.3 44.8 ±0.8 77.7 ±0.6 80.9 ±0.5 68.8 ±0.4 70.6 ±0.3 59.6 ±0.5 64.0 ±0.3</figDesc><table><row><cell></cell><cell></cell><cell>ALL</cell><cell></cell><cell>SEQ</cell><cell></cell><cell>Q1</cell><cell></cell><cell>Q2</cell><cell></cell><cell>Q3</cell><cell></cell></row><row><cell>Data</cell><cell>Size</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>MASK-LM</cell><cell cols="2">Base 60.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Trigger words for different groups.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Extracted from a Wikipedia dump from 12-2019.3  We annotate numbers and dates in the table and sentence with a simple parser and rely on the Wikipedia mention annotations (anchors) for identifying entities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Consider that if Australian is our focus and we replace it with United States we get 'Greg Norman is United States.'.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">github.com/wenhuchen/Table-Fact-Checking/ blob/master/tokenized_data 6 github.com/google-research/tapas</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Not explicitly mentioned in the paper but implied by the batch size given (6) and the defaults in the code.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Jordan Boyd-Graber, Yasemin Altun, Emily Pitler, Benjamin Boerschinger, William Cohen, Jonathan Herzig, Slav Petrov, and the anonymous reviewers for their time, constructive feedback, useful comments and suggestions about this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1300</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Project aristo: Towards machines that capture and reason with science knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360901.3364451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Knowledge Capture, K-CAP &apos;19</title>
		<meeting>the 10th International Conference on Knowledge Capture, K-CAP &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rationale, evaluation and approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Show your work: Improved reporting of experimental results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2185" to="2194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evaluating NLP models via contrast sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Basmova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Gottumukkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<idno>abs/2004.02709</idno>
		<editor>Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou</editor>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Posing fair generalization tasks for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauri</forename><surname>Karttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1456</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4485" to="4495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Injecting numerical reasoning skills into language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.89</idno>
	</analytic>
	<monogr>
		<title level="m">acl</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="946" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Google Vizier: A Service for Black-Box Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Elliot</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Infotabs: Inference on tables as semi-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitrey</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Nokhiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kornél</forename><surname>Csernai</surname></persName>
		</author>
		<title level="m">Quora question pairs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Aggchecker: A fact-checking system for text summaries of relational data sets. International Conference on Very Large Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352104</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1938" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning the difference that makes A difference with counterfactuallyaugmented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Question answering via integer programming over semistructured knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibor</forename><surname>Kiss</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2006.32.4.485</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The fast and the flexible: Training neural networks to learn to follow instructions from small data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezka</forename><surname>Leonandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-0419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Semantics</title>
		<meeting>the International Conference on Computational Semantics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="223" to="234" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised question answering by cloze translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4896" to="4910" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and methodologies for teaching</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>Advances in Neural Information Processing Systems, NIPS&apos;13<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Answering conversational questions on structured data without logical forms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1603</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5902" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<editor>Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NumNet: Machine reading comprehension with numerical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Qiu Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1251</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2474" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A logical-based corpus for crosslingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Salvatore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Finger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Hirata</surname><genName>Jr</genName></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="22" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ask not what textual entailment can do for you</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Vinod Vydiswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Identification and verification of simple claims about statistical properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1312</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2596" to="2601" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1534</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dbpal: A fully pluggable nl2sql training pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasetya</forename><surname>Utama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Galakatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Crotty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ilkhechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekar</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohin</forename><surname>Bhushan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadja</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hättasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Cetintemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Binnig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3380589</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIG-MOD &apos;20</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data, SIG-MOD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2347" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bilinguallyconstrained synthetic data for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1253</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Logical-FactChecker: Leveraging logical operations for fact checking with graph module network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
