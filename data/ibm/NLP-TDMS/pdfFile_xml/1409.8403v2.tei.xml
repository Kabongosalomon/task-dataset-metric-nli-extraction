<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Multimodal Computing † Computer Science and Engineering Division Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Saarbrucken, Ann Arbor</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Multimodal Computing † Computer Science and Engineering Division Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Saarbrucken, Ann Arbor</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Multimodal Computing † Computer Science and Engineering Division Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Saarbrucken, Ann Arbor</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Multimodal Computing † Computer Science and Engineering Division Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Saarbrucken, Ann Arbor</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision and Multimodal Computing † Computer Science and Engineering Division Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Saarbrucken, Ann Arbor</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of finegrained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with finegrained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The image classification problem has been redefined by the emergence of large scale datasets such as ImageNet <ref type="bibr" target="#b6">[7]</ref>. Since deep learning methods <ref type="bibr" target="#b26">[27]</ref> dominated recent Large-Scale Visual Recognition Challenges (ILSVRC12- <ref type="bibr" target="#b13">14)</ref>, the attention of the computer vision community has been drawn to Convolutional Neural Networks (CNN) <ref type="bibr" target="#b30">[31]</ref>. Training CNNs requires massive amounts of labeled data; but, in fine-grained image collections, where the categories are visually very similar, the data population decreases significantly. We are interested in the most extreme case of learning with a limited amount of labeled data, zero-shot learning, in which no labeled data is available for some classes.</p><p>Without labels, we need alternative sources of information that relate object classes. Attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>, which <ref type="bibr">Figure 1</ref>. Structured Joint Embedding leverages images (xi) and labels (yi) by learning parameters W of a function F (xi, yi, W ) that measures the compatibility between input (θ(xi)) and output embeddings (ϕ(yi)). It is a general framework that can be applied to any learning problem with more than one modality. describe well-known common characteristics of objects, are an appealing source of information, and they can be easily obtained through crowd-sourcing techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41]</ref>. However, fine-grained concepts present a special challenge: due to the high degree of similarity among categories, a large number of attributes are required to effectively model these subtle differences. This increases the cost of attribute annotation. One aim of this work is to move towards eliminating the human labeling component from zero-shot learning, e.g. by using alternative sources of information.</p><p>On the other hand, large-margin support vector machines (SVM) operate with labeled training images, so a lack of labels limits their use for this task. Inspired by previous work on label embedding <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref> and structured SVMs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b37">38]</ref>, we propose to use a Structured Joint Embedding (SJE) framework ( <ref type="figure">Fig. 1</ref>) that relates input embeddings (i.e. image features) and output embeddings (i.e. side information) through a compatibility function, therefore taking advantage of a structure in the output space. The SJE framework separates the subspace learning problem from the specific input and output features used in a given application. As a general framework, it can be applied to any learning problem where more than one modality is provided for an object.</p><p>Our contributions are: <ref type="bibr" target="#b0">(1)</ref> We demonstrate that unsuper-vised class embeddings trained from large unlabeled text corpora are competitive to previously published results that use human supervision. <ref type="bibr" target="#b1">(2)</ref> Using the most recent deep architectures as input embeddings, we significantly improve the state-of-the-art (SoA). <ref type="bibr" target="#b2">(3)</ref> We extensively evaluate several unsupervised output embeddings for fine-grained classification in a zero-shot setting on three challenging datasets. (4) By combining different output embeddings we obtain best results, surpassing the SoA by a large margin. <ref type="bibr" target="#b4">(5)</ref> We propose a novel weakly-supervised Word2Vec variant that improves the accuracy when combined with other output embeddings. The rest of the paper is organized as follows. Section 2 provides a review of the relevant literature; Sec. 3 details the SJE method; Sec. 4 explains the output embeddings that we analyze; Sec. 5 presents our experimental evaluation; Sec. 6 presents the discussion and our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning to classify in the absence of labeled data (zeroshot learning) <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref> is a challenging problem, and achieving better-than-chance performance requires structure in the output space. Attributes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref> provide one such space; they relate different classes through well-known and shared characteristics of objects.</p><p>Attributes, which are often collected manually <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12]</ref>, have shown promising results in various applications, i.e. caption generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, face recognition <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b5">6]</ref>, image retrieval <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11]</ref>, action recognition <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57]</ref> and image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>. The main challenge of attribute-based zero-shot learning arises on more challenging fine-grained data collections <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b25">26]</ref>, in which categories may visually differ only subtly. Therefore, generic attributes fail at modeling small intra-class variance between objects. Improved performance requires a large number of specific attributes which increases the cost of data gathering.</p><p>As an alternative to manual annotation, side information can be collected automatically from text corpora. Bag-ofwords <ref type="bibr" target="#b18">[19]</ref> is an example where class embeddings correspond to histograms of vocabulary words extracted automatically from unlabeled text. Another example is using taxonomical order of classes <ref type="bibr" target="#b51">[52]</ref> as structured output embeddings. Such a taxonomy can be built automatically from a pre-defined ontology such as WordNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1]</ref> . In this case, the distance between nodes is measured using semantic similarity metrics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. Finally, distributed text representations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref> learned from large unsupervised text corpora can be employed as structured embeddings. We compare several representatives of these methods (and their combinations) in our evaluation.</p><p>Embedding labels in an Euclidean space is an effective tool to model latent relationships between classes <ref type="bibr" target="#b2">[3]</ref>. These relationships can be collected separately from the data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>, learned from the data <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b19">20]</ref> or derived from side information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref>. In order to collect relationships independently of data, compressed sensing <ref type="bibr" target="#b20">[21]</ref> uses random projections whereas Error Correcting Output Codes <ref type="bibr" target="#b8">[9]</ref> builds embeddings inspired from information theory. WSABIE <ref type="bibr" target="#b55">[56]</ref> uses images with their corresponding labels to learn an embedding of the labels, and CCA <ref type="bibr" target="#b19">[20]</ref> maximizes the correlation between two different data modalities. DeViSE <ref type="bibr" target="#b15">[16]</ref> employs a ranking formulation for zero-shot learning using images and distributed text representations. The ALE <ref type="bibr" target="#b0">[1]</ref> method employs an approximate ranking formulation for the same using images and attributes. ConSe <ref type="bibr" target="#b36">[37]</ref> uses the probabilities of a softmaxoutput layer to weigh the semantic vectors of all the classes. In this work, we use the multiclass objective to learn structured output embeddings obtained from various sources.</p><p>Among the closest related work, ALE <ref type="bibr" target="#b0">[1]</ref> uses Fisher Vectors (FV <ref type="bibr" target="#b42">[43]</ref>) as input and binary attributes / hierarchies as output embeddings. Similarly, DeviSe <ref type="bibr" target="#b15">[16]</ref> uses CNN <ref type="bibr" target="#b26">[27]</ref> features as input and Word2Vec <ref type="bibr" target="#b34">[35]</ref> representations as output embeddings. In this work, we benefit from both ideas: (1) We use SoA image features, i.e. FV and CNN, (2) among others, we also use attributes and Word2Vec as output embeddings. Our work differs from <ref type="bibr" target="#b15">[16]</ref> w.r.t. two aspects: (1) We propose and evaluate several output embedding methods specifically built for finegrained classification. <ref type="bibr" target="#b1">(2)</ref> We show how some of these output embeddings complement each other for zero-shot learning on general and fine-grained datasets. The reader should be aware of <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structured Joint Embeddings</head><p>In this work, we aim to leverage input and output embeddings in a joint framework by learning a compatibility between these embeddings. We are interested in the problem of zero-shot learning for image classification where training and test images belong to two disjoint sets of classes.</p><p>Following <ref type="bibr" target="#b0">[1]</ref>, given input/output x n ∈ X and y n ∈ Y from S = {(x n , y n ), n = 1 . . . N }, Structured Joint Embedding (SJE) learns f : X → Y by minimizing the empirical risk 1</p><formula xml:id="formula_0">N N n=1 ∆(y n , f (x n )) where ∆ : Y × Y → R defines the cost of predicting f (x) when the true label is y.</formula><p>Here, we use the 0/1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>We define a compatibility function F : X × Y → R between an input space X and a structured output space Y. Given a specific input embedding, we derive a prediction by maximizing the compatibility F over SJE as follows:</p><formula xml:id="formula_1">f (x; w) = arg max y∈Y F (x, y; w).</formula><p>The parameter vector w can be written as a D×E matrix W with D being the input embedding dimension and E being the output embedding dimension. This leads to the bi-linear form of the compatibility function:</p><p>F (x, y; W ) = θ(x) W ϕ(y).</p><p>(1)</p><p>Here, the input embedding is denoted by θ(x) and the output embedding by ϕ(y). The matrix W is learned by enforcing the correct label to be ranked higher than any of the other labels (Sec. 3.2), i.e. multiclass objective. This formulation is closely related to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>. Within the label embedding framework, ALE <ref type="bibr" target="#b0">[1]</ref> and DeViSe <ref type="bibr" target="#b15">[16]</ref> use pairwise ranking objective, WSABIE <ref type="bibr" target="#b55">[56]</ref> learns both ϕ(y) and W through ranking, whereas we use multiclass objective. Similarly, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b49">50]</ref> use the regression objective and CCA <ref type="bibr" target="#b19">[20]</ref> maximizes the correlation of input and output embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameter Learning</head><p>According to the unregularized structured SVM formulation <ref type="bibr" target="#b51">[52]</ref>, the objective is:</p><formula xml:id="formula_2">1 N N n=1 max y∈Y {0, (x n , y n , y)}.<label>(2)</label></formula><p>where the misclassification loss (x n , y n , y) takes the form:</p><formula xml:id="formula_3">∆(y n , y) + θ(x n ) W ϕ(y) − θ(x n ) W ϕ(y n ) (3)</formula><p>For the zero-shot learning scenario, the training and test classes are disjoint. Therefore, we fix ϕ to the output embeddings of training classes and learn W . For prediction, we project a test image onto the W and search for the nearest output embedding vector (using the dot product similarity) that corresponds to one of the test classes. We use Stochastic Gradient Descent (SGD) for optimization which consists in sampling (x n , y n ) at each step and searching for the highest ranked class y. If arg max y∈Y (x n , y n , y) = y n , we update W as follows:</p><formula xml:id="formula_4">W (t) = W (t−1) + η t θ(x n )[ϕ(y n ) − ϕ(y)]<label>(4)</label></formula><p>where η t is the learning step-size used at iteration t. We use a constant step size chosen by cross-validation and we perform regularization through early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Combined Embeddings</head><p>For some classification tasks, there may be multiple output embeddings available, each capturing a different aspect of the structure of the output space. Each may also have a different signal-to-noise ratio. Since each output embedding possibly offers non-redundant information about the output space, as also shown in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b1">2]</ref>, we can learn a better joint embedding by combining them together. We model the resulting compatibility score as</p><formula xml:id="formula_5">F (x, y; {W } 1..K ) = k α k θ(x) W k ϕ k (y) (5) s.t. k α k = 1</formula><p>where W 1 , ..., W K are the joint embedding weight matrices corresponding to the K output embeddings (ϕ k ). In our experiments, we first train each W k independently, then perform a grid search over α k on a validation set. Interestingly, we found that the optimal α k for previously-seen classes is often different from the one for unseen classes. Therefore, it is critical to cross-validate α k on the zero-shot setting.</p><p>Note that if we take α k = 1/K, ∀k, Equation 5 is equivalent to simply concatenating the ϕ k . This corresponds to stacking the W k into a single matrix W and computing the standard compatibility as in Equation 1. However, such a stacking learns a large W where a high dimensional ϕ biases the final prediction. In contrast, α eliminates the bias, leading to better predictions. Thus, α k can be thought of as the confidence associated with ϕ k whose contribution we can control. We show in Sec. 5.2 that finding an appropriate α k can yield improved accuracy compared to any single ϕ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Output Embeddings</head><p>In this section, we describe three types of output embeddings: human-annotated attributes, unsupervised word embeddings learned from large text corpora, and hierarchical embeddings derived from WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Embedding by Human Annotation: Attributes</head><p>Annotating images with class labels is a laborious process when the objects represent fine-grained concepts that are not common in our daily lives. Attributes provide a means to describe such fine-grained concepts. They model shared characteristics of objects such as color and texture which are easily annotated by humans and converted to machine-readable vector format. The set of descriptive attributes may be determined by language experts <ref type="bibr" target="#b28">[29]</ref> or by fine-grained object experts <ref type="bibr" target="#b54">[55]</ref>. The association between an attribute and a category can be a binary value depicting the presence/absence of an attribute (ϕ 0,1 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b44">45]</ref>) or a continuous value that defines the confidence level of an attribute (ϕ A <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47]</ref>) for each class. We write per-class attributes as:</p><formula xml:id="formula_6">ϕ(y) = [ρ y,1 , . . . , ρ y,E ]</formula><p>where ρ y,i can be {0, 1} or a real number that associates a class with an attribute, y denotes the associated class and E is the number of attributes. Potentially, ϕ A encodes more information than ϕ 0,1 . For instance, for classes rat, monkey, whale and the attribute big, ϕ 0,1 = [0, 0, 1] im-plies that in terms of size rat = monkey &lt; whale, whereas ϕ A = [2, 10, 90] can be interpreted as rat &lt; monkey &lt;&lt; whale which is more accurate. We empirically show the benefit of ϕ A over ϕ 0,1 in Sec. 5.2. In practice, our output embeddings use a per-class vector form, but they can vary in dimensionality (E). For the rest of the section we denote the output embeddings as ϕ for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning Label Embeddings from Text</head><p>In this section, we describe unsupervised and weaklysupervised label embeddings mined from text. With these label embeddings, we can (1) avoid dependence on costly manual annotation of attributes and (2) combine the embeddings with attributes, where available, to achieve better performance. Word2Vec (ϕ W ). In Word2Vec <ref type="bibr" target="#b34">[35]</ref>, a two-layer neural network is trained to predict a set of target words from a set of context words. Words in the vocabulary are assigned with one-shot encoding so that the first layer acts as a lookup table to retrieve the embedding for any word in the vocabulary. The second layer predicts the target word(s) via hierarchical soft-max. Word2Vec has two main formulations for the target prediction: skip-gram (SG) and continuous bag-of-words (CBOW). In SG, words within a local context window are predicted from the centering word. In CBOW, the center word of a context window is predicted from the surrounding words. Embeddings are obtained by back-propagating the prediction error gradient over a training set of context windows sampled from the text corpus. GloVe (ϕ G ). GloVe <ref type="bibr" target="#b41">[42]</ref> incorporates co-occurrence statistics of words that frequently appear together within the document. Intuitively, the co-occurrence statistics encode meaning since semantically similar words such as "ice" and "water" occur together more frequently than semantically dissimilar words such as "ice" and "fashion." The training objective is to learn word vectors such that their dot product equals the co-occurrence probability of these two words. This approach has recently been shown to outperform Word2Vec on the word analogy prediction task <ref type="bibr" target="#b41">[42]</ref>. Weakly-supervised Word2Vec (ϕ Wws ). The standard Word2Vec <ref type="bibr" target="#b34">[35]</ref> scans the entire document using each word within a sample window as the target for prediction. However, if we know the global context, i.e. the topic of the document, we can use that topic as our target. For instance, in Wikipedia, the entire article is related to the same topic. Therefore, we can sample our context windows from any location within the article rather than searching for context windows where the topic explicitly appears in the text. We consider this method as a weak form of supervision.</p><p>We achieve the best results in our experiments using our novel variant of the CBOW formulation. Here, we pretrain the first layer weights using standard Word2Vec on Wikipedia, and fine-tune the second layer weights using a</p><formula xml:id="formula_7">ρjcn = 2 * IC(mscs(u, v)) − (IC(u) + IC(v)) ρ lin = 2 * IC(mscs(u, v)) IC(u) + IC(v) ρ path = min p∈pth(u,v)</formula><p>len(p) <ref type="table">Table 1</ref>. Notations <ref type="bibr" target="#b4">[5]</ref>: mscs (most specific common subsumer), pth (set of paths between two nodes), len (path length), IC (Information Content, defined as the log of the probability of finding a word in a text corpus independent of the hierarchy).</p><p>negative-sampling objective <ref type="bibr" target="#b17">[18]</ref> only on the fine-grained text corpus. These weights correspond to the final output embedding. The negative sampling objective is formulated as follows:</p><formula xml:id="formula_8">L = w,c∈D+ log σ(v T c v w ) + w ,c∈D− log σ(−v T c v w ) (6) v c = i∈context(w) v i /|context(w)|</formula><p>where v w and v w are the label embeddings we seek to learn, and v c is the average of word embeddings v i within a context window around word w. D + consists of context v c and matching targets v w , and D − consists of the same v c and mismatching v w . To find the v i (which are the columns of the first-layer network weights), we take them from a standard unsupervised Word2Vec model trained on Wikipedia. During SGD, the v i are fixed and we update each sampled v w and v w at each iteration. Intuitively, we seek to maximize the similarity between context and target vectors for matching pairs, and minimize it for mismatching pairs. Bag-of-Words (ϕ B ). BoW <ref type="bibr" target="#b18">[19]</ref> builds a "bag" of word frequencies by counting the occurrence of each vocabulary word that appears within a document. It does not preserve the order in which words appear in a document, so it disregards the grammar. We collect Wikipedia articles that correspond to each object class and build a vocabulary of most frequently occurring words. We then build histograms of these words to vectorize our classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hierarchical Embeddings</head><p>Semantic similarity measures how closely related two word senses are according to their meaning. Such a similarity can be estimated by measuring the distance between terms in an ontology. WordNet 1 , a large-scale hierarchical database of over 100,000 words for English, provides us a means of building our class hierarchy. To measure similarity, we use Jiang-Conrath <ref type="bibr" target="#b23">[24]</ref> (ϕ jcn ), Lin <ref type="bibr" target="#b31">[32]</ref> (ϕ lin ) and path (ϕ path ) similarities formulated in <ref type="table">Table 1</ref>. We denote our whole family of hierarchical embeddings as ϕ H . For a more detailed survey, the reader may refer to <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>While our main contribution is a detailed analysis of output embeddings, good image representations are crucial to obtain good classification performance. In Sec. 5.1 we detail datasets, input and output embeddings used in our experiments and in Sec. 5.2 we present our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setting</head><p>We evaluate SJE on three datasets: Caltech UCSD Birds (CUB) <ref type="bibr" target="#b53">[54]</ref> and Stanford Dogs (Dogs) 2 <ref type="bibr" target="#b25">[26]</ref> are finegrained, and Animals With Attributes (AWA) <ref type="bibr" target="#b28">[29]</ref> is a standard attribute dataset for zero-shot classification. CUB contains 11,788 images of 200 bird species, Dogs contains 19,501 images of 113 dog breeds and AWA contains 30,475 images of 50 different animals. We use a truly zero-shot setting where the train, val, and test sets belong to mutually exclusive classes. We employ train and val, i.e. disjoint subsets of training set, for cross-validation. We report average per-class top-1 accuracy on the test set. For CUB, we use the same zero-shot split as <ref type="bibr" target="#b0">[1]</ref> with 150 classes for the train+val set and 50 disjoint classes for the test set. AWA has a predefined split for 40 train+val and 10 test classes. For Dogs, we use approximately the same ratio of classes for train+val/test as CUB, i.e. 85 classes for train+val and 28 classes for test. This is the first attempt to perform zeroshot learning on the Dogs dataset. Input Embeddings. We use Fisher Vectors (FV) and Deep CNN Features (CNN). FV <ref type="bibr" target="#b42">[43]</ref> aggregates per image statistics computed from local image patches into a fixed-length local image descriptor. We extract 128-dim SIFT from regular grids at multiple scales, reduce them to 64-dim using PCA, build a visual vocabulary with 256 Gaussians <ref type="bibr" target="#b52">[53]</ref> and finally reduce the FVs to 4,096. As an alternative, we extract features from a deep convolutional network. Features that are typically obtained from the activations of the fully connected layers have been shown to induce semantic similarities. We resize each image to 224×224 and feed into the network which was pre-trained following the model architecture of either AlexNet <ref type="bibr" target="#b26">[27]</ref> or GoogLeNet <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b21">22]</ref>. For AlexNet (denoted as CNN) we use the 4,096-dim top-layer hidden unit activations (fc7) as features, and for GoogLeNet (denoted as GOOG) we use the 1,024-dim top-layer pooling units. For both networks, we used the publicly-available BVLC implementations <ref type="bibr" target="#b22">[23]</ref>. We do not perform any taskspecific pre-processing, such as cropping foreground objects or detecting parts. Output Embeddings. AWA classes have 85 binary and continuous attributes. CUB classes have 312 continuous attributes and the continuous values are thresholded around the mean to obtain binary attributes. The Dogs dataset does <ref type="bibr" target="#b1">2</ref> We use 113 classes that appear in the Federation Cynologique Internationale (FCI) database of dog breeds. We train Word2Vec (ϕ W ) and GloVe (ϕ G ) on the English-language Wikipedia from 13.02.2014. We first pre-process it by replacing the class-names, i.e. blackfooted albatross, with alternative unique names, i.e. scientific name, phoebastrianigripes. We cross-validate the skip-window size and embedding dimensions. For our proposed weakly-supervised Word2Vec (ϕ Wws ), we use the same embedding dimensions as the plain Word2Vec (ϕ W ). For BoW, we download the Wikipedia articles that correspond to each class and build the vocabulary by omitting least-and most-frequently occurring words. We crossvalidate the vocabulary size. ϕ B is a histogram of the vocabulary words as they appear in the respective document.</p><p>For hierarchical embeddings (ϕ H ), we use the WordNet hierarchy spanning our classes and their ancestors up to the root of the tree. We employ the widely used NLTK library 3 for building the hierarchy and measuring the similarity between nodes. Therefore, each ϕ H vector is populated with similarity measures of the class to all other classes. Combination of output embeddings. We explore combinations of five types of output embeddings: supervised attributes ϕ A , unsupervised Word2Vec ϕ W , GloVe ϕ G , BoW ϕ B and WordNet-derived similarity embeddings ϕ H . We either concatenate (cnc) or combine (cmb) different embeddings. In cnc, for instance in AWA, 85-dim ϕ A and 400-dim ϕ W would be merged to 485-dim output embeddings. In this case, if we use 1,024-dim GOOG as input embeddings, we learn a single 1,024×485-dim W . In cmb, we first learn 1,024×85-dim W A and 1,024×400-dim W W and then cross-validate the α coefficients to determine the amount each embedding contributes to the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>In this section, we evaluate several output embeddings on the CUB, AWA and Dogs datasets. Discrete vs Continuous Attributes. Attribute representations are defined as a vector per class, or a column of the (class × attribute) matrix. These vectors (85-dim for AWA, 312-dim for CUB) can either model the presence/absence (ϕ 0,1 ) or the confidence level (ϕ A ) of each attribute. We show that continuous attributes indeed encode more semantics than binary attributes by observing a substantial improvement with ϕ A over ϕ 0,1 with deep features (Tab. 2). Overall, CNN outperforms FV, while GOOG gives the best performing results; therefore in the following, we comment only on our results obtained using GOOG. On CUB, i.e. a fine-grained dataset, ϕ 0,1 obtains 37.8% accuracy, which is significantly above the SoA (26.9% <ref type="bibr" target="#b1">[2]</ref>). Moreover, ϕ A achieves an impressive 50.1% accuracy; outperforming the SoA by a large margin. We observe the same trend for AWA, which is a benchmark dataset for zeroshot learning. On AWA, ϕ 0,1 obtains 52.0% accuracy and ϕ A improves the accuracy substantially to 66.7%, significantly outperforming the SoA (48.5% <ref type="bibr" target="#b1">[2]</ref>). To summarize, we have shown that ϕ A improves the performance of ϕ 0,1 using deep features, which indicates that with ϕ A , the SJE method learns a matrix W that better approximates the compatibility of images and side information than ϕ 0,1 .</p><p>Learned Embeddings from Text. As the visual similarity between objects in different classes increases, e.g. in fine-grained datasets, the cost of collecting attributes also increases. Therefore, we aim to extract class similarities automatically from unlabeled online textual resources. We evaluate three methods, Word2Vec (ϕ W ), GloVe (ϕ G ) and the historically most commonly-used method BoW (ϕ B ). We build ϕ W and ϕ G on the entire English Wikipedia dump. Note that the plain Word2Vec <ref type="bibr" target="#b34">[35]</ref> was used in <ref type="bibr" target="#b1">[2]</ref>; however, rather than using Word2Vec in an averaging mechanism, we pre-process the Wikipedia as described in Sec 4.2 so that our class names are directly present in the Word2Vec vocabulary. This leads to a significant accuracy improvement. For ϕ B we use a subset of Wikipedia populated only with articles that correspond to our classes. On CUB (Tab. 3), the best accuracy is observed with ϕ W (28.4%) improving the supervised SoA (26.9% <ref type="bibr" target="#b1">[2]</ref>, Tab. 2). This is promising and impressive since ϕ W does not use any human supervision. On AWA (Tab. 3), the best accuracy is observed with ϕ G (58.8%) followed by ϕ W <ref type="figure">(51.2%)</ref> belong to the same species and thus they share a common scientific name. As a result, our method of cleanly preprocessing Wikipedia by replacing the occurrences of bird names with a unique scientific name was not possible for Dogs. This may lead to vectors obtained from Wikipedia for dogs that are vulnerable to variation in nomenclature. In summary, our results indicate no winner among ϕ W , ϕ G and ϕ B . These embeddings may be task specific and complement each other. We investigate the complementarity of embeddings in the following sections. Effect of Text Corpus. For ϕ W and ϕ G , we analyze the effects of three text corpora (B, W, B+W) with varying size and specificity. We build our specialized bird corpus (B) by collecting bird-related information from various online resources, i.e. audubon.org, birdweb.org, allaboutbirds.org and BNA <ref type="bibr" target="#b3">4</ref> . In combination, this corresponds to 50MB of bird-related text. We use the English-language Wikipedia from 13.02.2014 as our large and general corpus (W) which is 40GB of text. Finally, we combine B and W to build a large-scale text corpus enriched with bird specific text (B+W). On W and B+W, a small window size (10 for ϕ W and 20 for ϕ G ); on B, a large window size (35 for ϕ W and 50 for ϕ G ) is required. We choose parameters after a grid search. Increased specificity of the text corpus implies semantic consistency throughout the text. Therefore, large context windows capture semantics well in our bird specific (B) corpus. On the other hand, W is organized alphabetically w.r.t. the document title; hence, a large sampling window can include content from another article that is adjacent to the target word alphabetically. Here, small windows capture semantics better by looking at the text locally. We report our results in Tab. 4. Using ϕ G , B+W (26.1%) gives the highest accuracy, followed by W (24.2%). One possible reason is that when the semantic similarity is modeled with cooccurrence statistics, output embeddings become more informative with the increasing corpus size, since the probability of cooccurrence of similar concepts increases.</p><p>Using ϕ W , the accuracy obtained with B (22.5%) is already higher than the ϕ 0,1 -based SoA (22.3%), illustrating the benefit of using fine-grained text for fine-grained tasks. Another advantage of using B is that, since it is short,  <ref type="bibr" target="#b3">[4]</ref>, a deep architecture, it may learn more from negative data than positives. This was also observed for CNN features learned with a large number of unlabeled surrogate classes <ref type="bibr" target="#b9">[10]</ref>. Additionally, we propose a weakly-supervised alternative to Word2Vec framework (ϕ Wws , Sec. 4.2). The weaksupervision comes from using the specialized B corpus to fine-tune the weights of the network and model the birdrelated information. With ϕ Wws alone, we obtain 21.0% accuracy. However, when it is combined with ϕ W (28.4%), the accuracy improves to 29.7%. Compared to the results in Tab. 4, 29.7% is the highest accuracy obtained using unsupervised embeddings. We regard these results as a very encouraging evidence that Word2Vec representations can indeed be made more discriminative for fine-grained zeroshot learning by integrating a fine-grained text corpus directly to the output embedding learning problem.</p><p>Hierarchical Embeddings. The hierarchical organization of concepts typically embodies a fair amount of hidden information about language, such as synonymy, semantic relations, etc. Therefore, semantic relatedness defined by hierarchical distance between classes can form numerical vectors to be used as output embeddings for zero-shot learning. We build ontological relationships between our classes using the WordNet <ref type="bibr" target="#b35">[36]</ref> taxonomy. Due to its large size, WordNet encapsulates all of our AWA and Dog classes. For CUB, the high level bird species, i.e. albatross, appear as synsets in WordNet, but the specific bird names, i.e. blackfooted albatross, are not always present. Therefore we take the hierarchy up to high level bird species as-is and we assume the specific bird classes are all at the bottom of the hierarchy located with the same distance to their immediate ancestors. The WordNet hierarchy contains 319 nodes for CUB (200 classes), 104 nodes for AWA (50 classes) and 163 nodes for Dogs (113 classes). We measure the distance between classes using the similarity measures from Sec 4.1.</p><p>While as shown in <ref type="figure">Fig. 2</ref>   <ref type="figure">Figure 3</ref>. Highest ranked 5 images for chimpanzee, leopard and seal (AWA) using ϕ A , ϕ G and ϕ G+A . For chimpanzee, ϕ A ranks chimpanzees on trees at the top, whereas ϕ G models the social nature of the animal ranking a group of chimpanzees highest, ϕ G+A synthesizes both aspects. For leopard ϕ A puts an emphasis on the head, ϕ G seems to place the animal in the wild. In case of seal, ϕ A retrieves images related to water, whereas ϕ G adds more context by placing seals in the icy natural environment and ϕ G+A combines both. . We have shown with these experiments that output embeddings obtained through human annotation can also be complemented with unsupervised output embeddings using the SJE framework.</p><p>Qualitative Results. <ref type="figure">Fig. 3</ref> shows top-5 highest ranked images for classes chimpanzee, leopard and seal that are selected from 10 test classes of AWA. We use GOOG as input embeddings and as output embeddings we use supervised ϕ A , the best performing unsupervised embedding on AWA (ϕ G ), and the combination of the two (ϕ G+A ). For the class chimpanzee, ϕ A emphasizes that chimpanzees live on trees, which is among the list of attributes. On the other hand, ϕ G models the social nature of the animal, ranking a group of chimpanzees interacting with each other at the highest. Indeed this information can easily be retrieved from Wikipedia. ϕ G+A synthesizes both aspects. Similarly, for leopard ϕ A puts an emphasis on the head where we can observe several of the attributes, i.e. color, spotted, whereas ϕ G seems to place the animal in the wild. ϕ G+A combines both aspects. In case of class seal, ϕ A retrieves images related to water and ranks whales and seals highest, whereas ϕ G adds more context by placing seals in the icy natural environment and within groups. Finally, ϕ G+A ranks seal-shaped animals on ice, close to water and within groups the highest. We find these qualitative results interesting as they depict how (1) unsupervised embeddings capture nameable semantics about objects and (2) different output embeddings are semantically complementary for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We evaluated the Structured Joint Embedding (SJE) framework on supervised attributes and unsupervised output embeddings obtained from hierarchies and unlabeled text corpora. We proposed a novel weakly-supervised label embedding technique. By combining multiple output embeddings (cmb), we established a new SoA on AWA (73.9%, Tab. 6) and CUB (51.7%, Tab. 6). Moreover, we showed that unsupervised zero-shot learning with SJE improves the SoA, to 60.1% on AWA and 29.9% on CUB, and obtains 35.1% on Dogs (Tab. 6).</p><p>We emphasize the following take-home points: (1) Unsupervised label embeddings learned from text corpora yield compelling zero-shot results, outperforming previous supervised SoA on AWA and CUB (Tab. 2 and 3). <ref type="bibr" target="#b1">(2)</ref> Integrating specialized text corpora helps due to incorporating more fine-grained information to output embeddings (Tab. 4). (3) Combining unsupervised output embeddings improve the zero-shot performance, suggesting that they provide complementary information (Tab. 5). (4) There is still a large gap between the performance of unsupervised output embeddings and human-annotated attributes on AWA and CUB, suggesting that better methods are needed for learning discriminative output embeddings from text. (5) Finally, supporting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47]</ref>, encoding continuous nature of attributes significantly improve upon binary attributes for zero-shot classification (Tab. 2).</p><p>As future work, we plan to investigate other methods to combine multiple output embeddings and to improve the discriminative power of unsupervised and weaklysupervised label embeddings for fine-grained classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>4K) 36.6 42.3 15.2 19.0 CNN (4K) 45.9 61.9 30.0 40.3 GOOG (1K) 52.0 66.7 37.8 50.1 SoA ALE [2] (64K) 44.6 48.5 22.3 26.9 Discrete (ϕ 0,1 ) and continuous (ϕ A ) attributes with SJE vs SoA. For AWA (CUB) [2] achieves 49.4% (27.3%) by combining ϕ A and binary hierarchies. not have human-annotated attributes available.</figDesc><table><row><cell cols="2">AWA</cell><cell cols="2">CUB</cell></row><row><cell>ϕ 0,1</cell><cell>ϕ A</cell><cell>ϕ 0,1</cell><cell>ϕ A</cell></row><row><cell>FV (</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, improving the supervised SoA (48.5% [2]) significantly. On Dogs (Tab. 3), the best accuracy is obtained with ϕ B (33.0%). On the other hand, using ϕ W (19.6%) and ϕ G (17.8%) leads to significantly lower accuracies. Unlike birds, different dog breeds Wws (B) FV 10.5 13.3 13.2 16.0 16.0 16.5 17.1 CNN 13.4 20.6 20.6 20.0 24.1 21.4 25.1 GOOG 13.7 24.2 26.1 22.5 28.4 27.5 29.7</figDesc><table><row><cell>ϕ G</cell><cell>ϕ W</cell><cell>ϕ W (W) +</cell></row><row><cell cols="3">B B+W ϕ Table 4. Comparison of Word2Vec (ϕ W ) and GloVe (ϕ G ) learned W B+W B W</cell></row><row><cell cols="3">from a bird specific corpus (B), Wikipedia (W) and their combi-</cell></row><row><cell cols="3">nation (B + W), evaluated on CUB (Input embeddings: 4K-FV,</cell></row><row><cell>4K-CNN and 1K-GOOG).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparison of WordNet similarity measures: ϕ jcn , ϕ lin and ϕ path . We use ϕ H as a general name for hierarchical output embedding. (Input embedding: 1K-GOOG).</figDesc><table><row><cell>Top−1 Acc. (in %)</cell><cell>0 50</cell><cell>AWA</cell><cell>CUB ϕ jcn</cell><cell>ϕ lin</cell><cell>DOGS ϕ path</cell></row><row><cell cols="6">Figure 2. building ϕ W is efficient. Moreover, building ϕ W with B</cell></row><row><cell cols="6">does not require any annotation effort. Building ϕ W us-</cell></row><row><cell cols="6">ing W (28.4%) gives the highest accuracy, followed by W</cell></row><row><cell cols="6">+ B (27.5%) which improves the supervised SoA (26.9%).</cell></row><row><cell cols="6">We speculate that since Word2Vec is a variant of the Feed-</cell></row><row><cell cols="6">forward Neural Network Language Model (FNNLM)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>different hierarchical similarity measures have very different behaviors on each dataset. The best performing ϕ H obtains 51.2% (Tab. 3) accuracy AWA CUB Dogs ϕ A ϕ W ϕ G ϕ B ϕ H cnc cmb cnc cmb cnc cmb 53.9 55.5 28.2 29.4 23.5 26.6 60.1 59.5 28.5 29.9 23.5 26.7 49.4 49.2 26.4 27.7 35.1 28.Attribute ensemble results for all datasets. ϕ H : lin for CUB, path for AWA and Dogs. Top part shows combination results of unsupervised embeddings and bottom part integrates supervised embeddings to the rest (Input embeddings: 1K-GOOG).on AWA which reaches our ϕ 0,1 (52.0%) and improves ϕ B (44.9%) significantly. On CUB, ϕ H obtains 20.6% (Tab. 3) which remain below our ϕ 0,1 (37.8%) and approaches ϕ B (22.1%). On the other hand, on Dogs ϕ H obtains 24.3% (Tab. 3) which is significantly higher than the unsupervised text embeddings ϕ W (19.6%) and ϕ G (17.8%). For cnc, we perform full SJE training and cross-validation on the concatenated output embeddings. For cmb, we learn joint embeddings W k for each output separately (which is trivially parallelized), and find ensemble weights α k via cross-validation. In contrast to the cnc method, no additional joint training is used, although it can improve performance in practice. We observe (Tab. 5) in almost all cases cmb outperforms cnc.We analyze the combination of unsupervised embeddings (ϕ W,G,B,H). On AWA, ϕ G (58.8%, Tab. 3) combined with ϕ H (51.2%, Tab. 3), we achieve 60.1% (Tab. 5) which improves the SoA (48.5%, Tab. 2) by a large margin. On CUB, combining ϕ G (24.2%, Tab. 3) with ϕ H (20.6%, Tab. 3), we get 29.9% (Tab. 5) and improve the supervised-SoA (26.9%, Tab. 2). Supporting our initial claim, unsupervised output embeddings obtained from different sources, i.e. text vs hierarchy, seem to be complementary to each other. In some cases, cmb performs worse than cnc; e.g. 28.2% versus 35.1% when using ϕ B with ϕ H on Dogs. In most other cases cmb performs equivalent or better. Combining supervised (ϕ A ) and unsupervised</figDesc><table><row><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>A (66.7%, Tab. 3) with ϕ G and ϕ H leads to 73.9% (Tab. 5) which significantly exceeds the SoA (48.5%, Tab. 2). On CUB, combining ϕ A with ϕ G and ϕ H leads to 51.7% (Tab. 5), improving both the results we obtained with ϕ A (50.1%, Tab. 3) and the supervised-SoA (26.9%, Tab. 2)</figDesc><table><row><cell>supervision</cell><cell>method</cell><cell cols="3">AWA CUB Dogs</cell></row><row><cell cols="2">unsupervised SJE (best from Tab. 5)</cell><cell>60.1</cell><cell>29.9</cell><cell>35.1</cell></row><row><cell>supervised</cell><cell>SJE (best from Tab. 5) AHLE [2]</cell><cell>73.9 49.4</cell><cell>51.7 27.3</cell><cell>--</cell></row><row><cell cols="5">Table 6. Summary of best zero-shot learning results with SJE with</cell></row><row><cell cols="2">or without supervision along with SoA.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">embeddings (ϕ W,G,B,H ) shows a similar trend. On AWA,</cell></row><row><cell>combining ϕ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://wordnetweb.princeton.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.nltk.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://bna.birds.cornell.edu/bna/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We empirically found that the hierarchical embeddings ϕ H consistently improved performance when combined or concatenated with other embeddings. Therefore, we report results using ϕ H by default.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by ONR N00014-13-1-0762, NSF CMMI-1266184, Google Faculty Research Award, and NSF Graduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label embedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Labelembedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08677</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A typology of ontology-based semantic measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Pauc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMOI-INTEROP&apos;05 workshop, at CAiSE&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? first names as facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained crowdsourcing for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving multiclass learning problems via error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bakiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6909</idno>
		<title level="m">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining attributes and Fisher vectors for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Springer Series in Statistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-label prediction via compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online incremental attribute-based zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kankuekul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kawewong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tangruamsub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://vision.stanford.edu/aditya86/ImageNetDogs/.2,5" />
		<title level="m">Stanford dogs dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2012. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baby talk: understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Filling in a sparse training space for word sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Costa: Cooccurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E J</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Structured Learning and Prediction in Computer Vision. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Im2Text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What helps here -and why? Semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Combining language sources and robust semantic relatedness for attribute-based knowledge transfer. Trends and Topics in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-attribute spaces: Calibration for attribute fusion and similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiclass recognition and part localization with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large scale image annotation: Learning to rank with joint word-image embeddings. ECML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero or one training example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
